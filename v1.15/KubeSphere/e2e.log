I1205 06:04:07.378135      16 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-771445370
I1205 06:04:07.378247      16 e2e.go:243] Starting e2e run "a461aeec-f126-42b0-aba5-4d4c7e0810c1" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1575525846 - Will randomize all specs
Will run 215 of 4413 specs

Dec  5 06:04:07.545: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:04:07.546: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  5 06:04:09.173: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  5 06:04:09.227: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  5 06:04:09.227: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Dec  5 06:04:09.227: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  5 06:04:09.240: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec  5 06:04:09.240: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec  5 06:04:09.240: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
Dec  5 06:04:09.240: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'openebs-ndm' (0 seconds elapsed)
Dec  5 06:04:09.240: INFO: e2e test version: v1.15.5
Dec  5 06:04:09.241: INFO: kube-apiserver version: v1.15.5
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:04:09.241: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
Dec  5 06:04:09.266: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 06:04:09.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-2707'
Dec  5 06:04:09.591: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 06:04:09.591: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec  5 06:04:09.605: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-6m9wp]
Dec  5 06:04:09.605: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-6m9wp" in namespace "kubectl-2707" to be "running and ready"
Dec  5 06:04:09.608: INFO: Pod "e2e-test-nginx-rc-6m9wp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.281654ms
Dec  5 06:04:11.612: INFO: Pod "e2e-test-nginx-rc-6m9wp": Phase="Running", Reason="", readiness=true. Elapsed: 2.007176097s
Dec  5 06:04:11.612: INFO: Pod "e2e-test-nginx-rc-6m9wp" satisfied condition "running and ready"
Dec  5 06:04:11.612: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-6m9wp]
Dec  5 06:04:11.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 logs rc/e2e-test-nginx-rc --namespace=kubectl-2707'
Dec  5 06:04:11.760: INFO: stderr: ""
Dec  5 06:04:11.760: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Dec  5 06:04:11.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete rc e2e-test-nginx-rc --namespace=kubectl-2707'
Dec  5 06:04:11.841: INFO: stderr: ""
Dec  5 06:04:11.841: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:04:11.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2707" for this suite.
Dec  5 06:04:17.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:04:17.951: INFO: namespace kubectl-2707 deletion completed in 6.099055084s

• [SLOW TEST:8.709 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:04:17.951: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  5 06:04:17.979: INFO: Waiting up to 5m0s for pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c" in namespace "emptydir-1133" to be "success or failure"
Dec  5 06:04:17.981: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.862933ms
Dec  5 06:04:19.985: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005914965s
Dec  5 06:04:21.989: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009292519s
Dec  5 06:04:23.992: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012695735s
Dec  5 06:04:25.996: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016523764s
Dec  5 06:04:28.002: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022944991s
Dec  5 06:04:30.006: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026949484s
Dec  5 06:04:32.022: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.042973469s
Dec  5 06:04:34.026: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.046537906s
Dec  5 06:04:36.030: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.050525268s
Dec  5 06:04:38.033: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.053980729s
Dec  5 06:04:40.038: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.058070707s
Dec  5 06:04:42.040: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 24.060975829s
Dec  5 06:04:44.044: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 26.064100847s
Dec  5 06:04:46.047: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Pending", Reason="", readiness=false. Elapsed: 28.067247436s
Dec  5 06:04:48.050: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.07047571s
STEP: Saw pod success
Dec  5 06:04:48.050: INFO: Pod "pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c" satisfied condition "success or failure"
Dec  5 06:04:48.052: INFO: Trying to get logs from node node1 pod pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c container test-container: <nil>
STEP: delete the pod
Dec  5 06:04:48.089: INFO: Waiting for pod pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c to disappear
Dec  5 06:04:48.091: INFO: Pod pod-3ab29652-1cd1-43cf-9b78-f19d3c66ff2c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:04:48.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1133" for this suite.
Dec  5 06:04:54.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:04:54.203: INFO: namespace emptydir-1133 deletion completed in 6.10292253s

• [SLOW TEST:36.252 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:04:54.203: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  5 06:04:54.235: INFO: Waiting up to 5m0s for pod "pod-7a92376a-a4f3-4065-b23e-c28e0cf28d61" in namespace "emptydir-4972" to be "success or failure"
Dec  5 06:04:54.237: INFO: Pod "pod-7a92376a-a4f3-4065-b23e-c28e0cf28d61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.778372ms
Dec  5 06:04:56.241: INFO: Pod "pod-7a92376a-a4f3-4065-b23e-c28e0cf28d61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006126036s
Dec  5 06:04:58.244: INFO: Pod "pod-7a92376a-a4f3-4065-b23e-c28e0cf28d61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009541957s
STEP: Saw pod success
Dec  5 06:04:58.244: INFO: Pod "pod-7a92376a-a4f3-4065-b23e-c28e0cf28d61" satisfied condition "success or failure"
Dec  5 06:04:58.246: INFO: Trying to get logs from node node2 pod pod-7a92376a-a4f3-4065-b23e-c28e0cf28d61 container test-container: <nil>
STEP: delete the pod
Dec  5 06:04:58.262: INFO: Waiting for pod pod-7a92376a-a4f3-4065-b23e-c28e0cf28d61 to disappear
Dec  5 06:04:58.264: INFO: Pod pod-7a92376a-a4f3-4065-b23e-c28e0cf28d61 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:04:58.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4972" for this suite.
Dec  5 06:05:04.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:05:04.390: INFO: namespace emptydir-4972 deletion completed in 6.117304261s

• [SLOW TEST:10.186 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:05:04.391: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:05:04.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5721" for this suite.
Dec  5 06:05:10.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:05:10.571: INFO: namespace kubelet-test-5721 deletion completed in 6.13465221s

• [SLOW TEST:6.180 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:05:10.571: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Dec  5 06:05:10.597: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-771445370 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:05:10.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1601" for this suite.
Dec  5 06:05:16.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:05:16.796: INFO: namespace kubectl-1601 deletion completed in 6.109863228s

• [SLOW TEST:6.224 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:05:16.796: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-4188c999-ac2a-4e81-bc5d-38862dfe57d9
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:05:16.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4343" for this suite.
Dec  5 06:05:22.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:05:22.954: INFO: namespace secrets-4343 deletion completed in 6.127962915s

• [SLOW TEST:6.159 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:05:22.955: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3713
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3713
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3713
Dec  5 06:05:23.182: INFO: Found 0 stateful pods, waiting for 1
Dec  5 06:05:33.186: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec  5 06:05:33.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-3713 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 06:05:33.469: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 06:05:33.469: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 06:05:33.469: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 06:05:33.472: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  5 06:05:43.476: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 06:05:43.476: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 06:05:43.487: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999684s
Dec  5 06:05:44.491: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996885572s
Dec  5 06:05:45.496: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993278764s
Dec  5 06:05:46.500: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988404912s
Dec  5 06:05:47.504: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983876531s
Dec  5 06:05:48.508: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.980137046s
Dec  5 06:05:49.512: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976079475s
Dec  5 06:05:50.516: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972096059s
Dec  5 06:05:51.520: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.9678096s
Dec  5 06:05:52.525: INFO: Verifying statefulset ss doesn't scale past 1 for another 963.59711ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3713
Dec  5 06:05:53.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-3713 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 06:05:53.820: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 06:05:53.820: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 06:05:53.820: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 06:05:53.823: INFO: Found 1 stateful pods, waiting for 3
Dec  5 06:06:03.829: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 06:06:03.829: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 06:06:03.829: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec  5 06:06:03.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-3713 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 06:06:04.125: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 06:06:04.125: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 06:06:04.125: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 06:06:04.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-3713 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 06:06:04.393: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 06:06:04.393: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 06:06:04.395: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 06:06:04.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-3713 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 06:06:04.696: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 06:06:04.696: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 06:06:04.696: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 06:06:04.696: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 06:06:04.699: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec  5 06:06:14.706: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 06:06:14.706: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 06:06:14.706: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 06:06:14.714: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999407s
Dec  5 06:06:15.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996888533s
Dec  5 06:06:16.723: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992516647s
Dec  5 06:06:17.727: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988062081s
Dec  5 06:06:18.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98375537s
Dec  5 06:06:19.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97369621s
Dec  5 06:06:20.745: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970316984s
Dec  5 06:06:21.750: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966504547s
Dec  5 06:06:22.753: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.961546707s
Dec  5 06:06:23.757: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.49337ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3713
Dec  5 06:06:24.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-3713 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 06:06:25.091: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 06:06:25.091: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 06:06:25.091: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 06:06:25.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-3713 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 06:06:25.353: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 06:06:25.353: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 06:06:25.353: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 06:06:25.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-3713 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 06:06:25.608: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 06:06:25.608: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 06:06:25.608: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 06:06:25.608: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  5 06:06:45.621: INFO: Deleting all statefulset in ns statefulset-3713
Dec  5 06:06:45.623: INFO: Scaling statefulset ss to 0
Dec  5 06:06:45.628: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 06:06:45.630: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:06:45.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3713" for this suite.
Dec  5 06:06:51.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:06:51.751: INFO: namespace statefulset-3713 deletion completed in 6.111604625s

• [SLOW TEST:88.796 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:06:51.751: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 06:06:51.777: INFO: Creating deployment "nginx-deployment"
Dec  5 06:06:51.781: INFO: Waiting for observed generation 1
Dec  5 06:06:53.792: INFO: Waiting for all required pods to come up
Dec  5 06:06:53.801: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec  5 06:06:55.811: INFO: Waiting for deployment "nginx-deployment" to complete
Dec  5 06:06:55.817: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec  5 06:06:55.823: INFO: Updating deployment nginx-deployment
Dec  5 06:06:55.823: INFO: Waiting for observed generation 2
Dec  5 06:06:57.829: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  5 06:06:57.832: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  5 06:06:57.833: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  5 06:06:57.838: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  5 06:06:57.838: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  5 06:06:57.840: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  5 06:06:57.844: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec  5 06:06:57.844: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec  5 06:06:57.849: INFO: Updating deployment nginx-deployment
Dec  5 06:06:57.849: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec  5 06:06:57.856: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  5 06:07:00.144: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  5 06:07:00.166: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-830,SelfLink:/apis/apps/v1/namespaces/deployment-830/deployments/nginx-deployment,UID:f72a2a65-ece1-44ee-811a-149404053714,ResourceVersion:24860,Generation:3,CreationTimestamp:2019-12-05 06:06:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-12-05 06:06:57 +0000 UTC 2019-12-05 06:06:57 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-05 06:06:57 +0000 UTC 2019-12-05 06:06:51 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec  5 06:07:00.170: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-830,SelfLink:/apis/apps/v1/namespaces/deployment-830/replicasets/nginx-deployment-55fb7cb77f,UID:7e0378a7-15a9-49f4-94a9-24f5989fcf4a,ResourceVersion:24858,Generation:3,CreationTimestamp:2019-12-05 06:06:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment f72a2a65-ece1-44ee-811a-149404053714 0xc00052e0d7 0xc00052e0d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 06:07:00.170: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec  5 06:07:00.170: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-830,SelfLink:/apis/apps/v1/namespaces/deployment-830/replicasets/nginx-deployment-7b8c6f4498,UID:f79cf816-05ee-4854-a5b8-a3c9bf813b6e,ResourceVersion:24841,Generation:3,CreationTimestamp:2019-12-05 06:06:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment f72a2a65-ece1-44ee-811a-149404053714 0xc00052e707 0xc00052e708}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec  5 06:07:00.183: INFO: Pod "nginx-deployment-55fb7cb77f-6ffn8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6ffn8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-6ffn8,UID:cfc6156d-b635-4ead-87dc-3dc718049a3c,ResourceVersion:24934,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000761627 0xc000761628}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007616a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007616c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.183: INFO: Pod "nginx-deployment-55fb7cb77f-6gzr4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6gzr4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-6gzr4,UID:a3651d6f-3030-4c1e-8181-5baf586a2987,ResourceVersion:24731,Generation:0,CreationTimestamp:2019-12-05 06:06:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000761790 0xc000761791}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000761810} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000761830}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.184: INFO: Pod "nginx-deployment-55fb7cb77f-cj4zj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cj4zj,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-cj4zj,UID:b0dc12fd-46f3-4402-9923-8160b31ae68a,ResourceVersion:24756,Generation:0,CreationTimestamp:2019-12-05 06:06:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000761900 0xc000761901}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000761980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007619a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2019-12-05 06:06:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.184: INFO: Pod "nginx-deployment-55fb7cb77f-gdzv6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-gdzv6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-gdzv6,UID:fa954e50-19d2-4292-b2d2-b7c08e5db199,ResourceVersion:24903,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000761a70 0xc000761a71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000761af0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000761b10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.184: INFO: Pod "nginx-deployment-55fb7cb77f-gm99v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-gm99v,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-gm99v,UID:0d285c77-b5af-4634-9c73-c7fdc6501f4c,ResourceVersion:24863,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000761be0 0xc000761be1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000761c70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000761c90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.184: INFO: Pod "nginx-deployment-55fb7cb77f-gthwr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-gthwr,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-gthwr,UID:fb5c1a86-32cd-492f-823c-5b2a5837c603,ResourceVersion:24894,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000761d60 0xc000761d61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000761de0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000761e00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.185: INFO: Pod "nginx-deployment-55fb7cb77f-hhjzs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hhjzs,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-hhjzs,UID:9ea2da10-c76a-441d-a2c0-2302e25d16a5,ResourceVersion:24986,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000761ed0 0xc000761ed1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000761f50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000761f80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.185: INFO: Pod "nginx-deployment-55fb7cb77f-j945v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-j945v,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-j945v,UID:109d51c2-b91f-4b0f-8884-bb45d0443f28,ResourceVersion:24844,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000f54050 0xc000f54051}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f54120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f54140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.185: INFO: Pod "nginx-deployment-55fb7cb77f-msp5d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-msp5d,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-msp5d,UID:08f3e9e3-6b85-4877-aa3e-fc70b7f82dba,ResourceVersion:24735,Generation:0,CreationTimestamp:2019-12-05 06:06:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000f541c0 0xc000f541c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f54240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f54260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2019-12-05 06:06:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.185: INFO: Pod "nginx-deployment-55fb7cb77f-p9gvh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-p9gvh,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-p9gvh,UID:8151d741-9f10-4235-aed9-a8945beea87c,ResourceVersion:24758,Generation:0,CreationTimestamp:2019-12-05 06:06:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000f54330 0xc000f54331}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f543b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f543d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.185: INFO: Pod "nginx-deployment-55fb7cb77f-s7mh2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-s7mh2,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-s7mh2,UID:b717568a-31c1-45c3-8282-6a9ff0c0fabc,ResourceVersion:24930,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000f544a0 0xc000f544a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f54520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f54540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.185: INFO: Pod "nginx-deployment-55fb7cb77f-shtvv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-shtvv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-shtvv,UID:f6c880f0-04de-424c-8c66-024a80a726a3,ResourceVersion:24736,Generation:0,CreationTimestamp:2019-12-05 06:06:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000f54620 0xc000f54621}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f546a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f546c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.186: INFO: Pod "nginx-deployment-55fb7cb77f-zj54n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zj54n,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-55fb7cb77f-zj54n,UID:3e9f0b49-89c8-4c07-be9a-da8d6ea18513,ResourceVersion:24883,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e0378a7-15a9-49f4-94a9-24f5989fcf4a 0xc000f547b0 0xc000f547b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f54850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f54870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.186: INFO: Pod "nginx-deployment-7b8c6f4498-898b6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-898b6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-898b6,UID:fd577884-382b-467c-92fd-f4fa2d78240b,ResourceVersion:24884,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc000f54970 0xc000f54971}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f549f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f54a10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.186: INFO: Pod "nginx-deployment-7b8c6f4498-8ghx4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8ghx4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-8ghx4,UID:461a9e90-c5e4-498e-98f9-1357ede18422,ResourceVersion:24854,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc000f54ad0 0xc000f54ad1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f54b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f54b60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.186: INFO: Pod "nginx-deployment-7b8c6f4498-8gqdr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8gqdr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-8gqdr,UID:ccf6fd96-234b-4f10-9aa3-568ae6d7fbc5,ResourceVersion:24834,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc000f54c30 0xc000f54c31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f54cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f54d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.186: INFO: Pod "nginx-deployment-7b8c6f4498-9ggnl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9ggnl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-9ggnl,UID:f5b63bcf-b97d-438c-ace6-58e71739b83c,ResourceVersion:24680,Generation:0,CreationTimestamp:2019-12-05 06:06:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc000f54f80 0xc000f54f81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f55050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f55090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.90.13,StartTime:2019-12-05 06:06:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 06:06:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6392747f44461d2af048fbb9fd8e5ee50573fc8d047c470d4996b133dd73a4c6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.186: INFO: Pod "nginx-deployment-7b8c6f4498-bp48s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bp48s,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-bp48s,UID:f68ce360-54a9-41b2-8db0-25e75dbf8d42,ResourceVersion:24867,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc000f55350 0xc000f55351}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f55460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f554c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.186: INFO: Pod "nginx-deployment-7b8c6f4498-cr4lp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cr4lp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-cr4lp,UID:3d3378e6-f654-42ec-a73e-df77a6a2d549,ResourceVersion:24871,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc000f555d0 0xc000f555d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f55640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f55660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.187: INFO: Pod "nginx-deployment-7b8c6f4498-f7f9k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-f7f9k,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-f7f9k,UID:1d404577-47a2-4d94-9b53-44cfbf684d43,ResourceVersion:24810,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc000f55720 0xc000f55721}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f557b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f557d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.187: INFO: Pod "nginx-deployment-7b8c6f4498-fwv49" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fwv49,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-fwv49,UID:579a8994-9bb3-4967-af5a-d3c047db6fd9,ResourceVersion:24673,Generation:0,CreationTimestamp:2019-12-05 06:06:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af20b0 0xc002af20b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af2120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af2140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:10.233.96.17,StartTime:2019-12-05 06:06:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 06:06:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://86b4d6bac7ee4c4c07f0a315aabf62215aea451b88eec7551aa7a7c82d62008d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.187: INFO: Pod "nginx-deployment-7b8c6f4498-g9z9f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-g9z9f,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-g9z9f,UID:b05f8628-2ac2-4dbd-92a6-d90f4848cb15,ResourceVersion:24839,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af2210 0xc002af2211}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af2280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af22a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.187: INFO: Pod "nginx-deployment-7b8c6f4498-jfzww" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jfzww,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-jfzww,UID:f55f6c77-725a-4482-b636-24aeb631a84a,ResourceVersion:24900,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af2360 0xc002af2361}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af23d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af23f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.187: INFO: Pod "nginx-deployment-7b8c6f4498-jh9rl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jh9rl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-jh9rl,UID:86816623-03b8-4677-819f-69e36572862b,ResourceVersion:24683,Generation:0,CreationTimestamp:2019-12-05 06:06:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af24b0 0xc002af24b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af2520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af2540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.90.15,StartTime:2019-12-05 06:06:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 06:06:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://accebae854d49904e19ed2d1f4a4a79d6a83e130b5debd305835ba1e8bc05e25}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.187: INFO: Pod "nginx-deployment-7b8c6f4498-k7sq4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-k7sq4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-k7sq4,UID:a8740ebd-a914-4ee4-a596-5cbdee796cac,ResourceVersion:24866,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af2610 0xc002af2611}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af2680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af26a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.188: INFO: Pod "nginx-deployment-7b8c6f4498-kg2qp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kg2qp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-kg2qp,UID:e64de943-cfea-4f98-b4a5-4e2993d8df16,ResourceVersion:24664,Generation:0,CreationTimestamp:2019-12-05 06:06:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af2760 0xc002af2761}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af27d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af27f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:10.233.96.18,StartTime:2019-12-05 06:06:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 06:06:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d2efc46bf486d744144efd5dfb849c5a656ef5886a2f63c25791a9cbc32732f8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.188: INFO: Pod "nginx-deployment-7b8c6f4498-nbjpp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-nbjpp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-nbjpp,UID:4de0a298-6b41-412b-b41a-d32f0d22f986,ResourceVersion:24671,Generation:0,CreationTimestamp:2019-12-05 06:06:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af28d0 0xc002af28d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af2940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af2960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:10.233.96.21,StartTime:2019-12-05 06:06:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 06:06:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d913575c92eb04375b38b3daf908096519100c661ce529e24882fe0bdeff184c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.188: INFO: Pod "nginx-deployment-7b8c6f4498-pjwwq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-pjwwq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-pjwwq,UID:25b38ba0-3df1-434f-8b68-80128d9af57e,ResourceVersion:24687,Generation:0,CreationTimestamp:2019-12-05 06:06:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af2a30 0xc002af2a31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af2aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af2ac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.90.12,StartTime:2019-12-05 06:06:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 06:06:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ac6f18a64dc15eb17dc37056da179ba57e87f4b86fbac72a3156feb53aca1ffd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.188: INFO: Pod "nginx-deployment-7b8c6f4498-t5fzb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-t5fzb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-t5fzb,UID:3fd0679c-37d8-47e3-868a-63129d4cc7db,ResourceVersion:24667,Generation:0,CreationTimestamp:2019-12-05 06:06:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af2b90 0xc002af2b91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af2c00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af2c20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:10.233.96.20,StartTime:2019-12-05 06:06:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 06:06:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8347cad0c9c83ba0a35c7d1557038d9f64e780f6b7ca8319894ee04302a6acc8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.188: INFO: Pod "nginx-deployment-7b8c6f4498-w2k2r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-w2k2r,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-w2k2r,UID:7f0c77c6-f46d-44c9-bc64-29ab690f96cb,ResourceVersion:24893,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af2cf0 0xc002af2cf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af2d60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af2d80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.188: INFO: Pod "nginx-deployment-7b8c6f4498-wrqbz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wrqbz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-wrqbz,UID:1f8d4ba0-137f-4a40-a4ac-58289243b02a,ResourceVersion:24862,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af2e40 0xc002af2e41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af2eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af2ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.188: INFO: Pod "nginx-deployment-7b8c6f4498-x9qsc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-x9qsc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-x9qsc,UID:e41300a8-02e5-420d-ba9d-18e01b64befa,ResourceVersion:24694,Generation:0,CreationTimestamp:2019-12-05 06:06:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af2f90 0xc002af2f91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af3000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af3020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:51 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:10.233.96.19,StartTime:2019-12-05 06:06:51 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 06:06:53 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a65a701278fa910324d9563189bc762d4701ab62b45a53169ff184a36dd8b9f1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:07:00.189: INFO: Pod "nginx-deployment-7b8c6f4498-xx5vm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xx5vm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-830,SelfLink:/api/v1/namespaces/deployment-830/pods/nginx-deployment-7b8c6f4498-xx5vm,UID:66e65139-12fc-4ee8-9beb-a00497859d4f,ResourceVersion:24855,Generation:0,CreationTimestamp:2019-12-05 06:06:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f79cf816-05ee-4854-a5b8-a3c9bf813b6e 0xc002af30f0 0xc002af30f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ttzzf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ttzzf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ttzzf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af3160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af3180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:06:57 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:,StartTime:2019-12-05 06:06:57 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:07:00.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-830" for this suite.
Dec  5 06:07:06.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:07:06.463: INFO: namespace deployment-830 deletion completed in 6.261213561s

• [SLOW TEST:14.711 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:07:06.463: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec  5 06:07:06.503: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  5 06:07:06.558: INFO: Waiting for terminating namespaces to be deleted...
Dec  5 06:07:06.585: INFO: 
Logging pods the kubelet thinks is on node node1 before test
Dec  5 06:07:06.608: INFO: openebs-ndm-operator-69cbc86975-r5pph from kube-system started at 2019-12-05 04:00:08 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container node-disk-operator ready: true, restart count 1
Dec  5 06:07:06.608: INFO: node-exporter-n87mk from kubesphere-monitoring-system started at 2019-12-05 04:01:42 +0000 UTC (2 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  5 06:07:06.608: INFO: 	Container node-exporter ready: true, restart count 0
Dec  5 06:07:06.608: INFO: prometheus-k8s-system-1 from kubesphere-monitoring-system started at 2019-12-05 04:05:13 +0000 UTC (3 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container prometheus ready: true, restart count 1
Dec  5 06:07:06.608: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec  5 06:07:06.608: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec  5 06:07:06.608: INFO: ks-installer-7987c659d6-hljj9 from kubesphere-system started at 2019-12-05 04:00:17 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container installer ready: true, restart count 0
Dec  5 06:07:06.608: INFO: kube-state-metrics-dfdf4d964-pdd5z from kubesphere-monitoring-system started at 2019-12-05 04:10:38 +0000 UTC (4 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container addon-resizer ready: true, restart count 0
Dec  5 06:07:06.608: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec  5 06:07:06.608: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec  5 06:07:06.608: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec  5 06:07:06.608: INFO: kube-proxy-bbf7w from kube-system started at 2019-12-05 03:16:38 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  5 06:07:06.608: INFO: tiller-deploy-55fc49c595-wb92t from kube-system started at 2019-12-05 03:25:52 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container tiller ready: true, restart count 0
Dec  5 06:07:06.608: INFO: calico-node-p48b5 from kube-system started at 2019-12-05 03:16:55 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container calico-node ready: true, restart count 0
Dec  5 06:07:06.608: INFO: openebs-ndm-dd2vs from kube-system started at 2019-12-05 04:00:08 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container node-disk-manager ready: true, restart count 0
Dec  5 06:07:06.608: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2019-12-05 04:05:04 +0000 UTC (3 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container prometheus ready: true, restart count 1
Dec  5 06:07:06.608: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec  5 06:07:06.608: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec  5 06:07:06.608: INFO: prometheus-operator-685bc484cb-k5cmh from kubesphere-monitoring-system started at 2019-12-05 04:01:41 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container prometheus-operator ready: true, restart count 0
Dec  5 06:07:06.608: INFO: sonobuoy-systemd-logs-daemon-set-4e7bfb56bab946ed-6dtlb from sonobuoy started at 2019-12-05 05:58:13 +0000 UTC (2 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 06:07:06.608: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  5 06:07:06.608: INFO: nodelocaldns-4cspq from kube-system started at 2019-12-05 03:17:33 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.608: INFO: 	Container node-cache ready: true, restart count 0
Dec  5 06:07:06.608: INFO: 
Logging pods the kubelet thinks is on node node2 before test
Dec  5 06:07:06.623: INFO: kubectl-admin-74fdfc47c7-r5wx2 from kubesphere-controls-system started at 2019-12-05 04:08:47 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container kubectl ready: true, restart count 0
Dec  5 06:07:06.623: INFO: node-exporter-xkgf6 from kubesphere-monitoring-system started at 2019-12-05 04:01:42 +0000 UTC (2 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  5 06:07:06.623: INFO: 	Container node-exporter ready: true, restart count 0
Dec  5 06:07:06.623: INFO: openebs-localpv-provisioner-7b55587dbd-p5ktt from kube-system started at 2019-12-05 04:00:08 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
Dec  5 06:07:06.623: INFO: sonobuoy-e2e-job-4d426e35b88a483d from sonobuoy started at 2019-12-05 05:58:13 +0000 UTC (2 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container e2e ready: true, restart count 0
Dec  5 06:07:06.623: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 06:07:06.623: INFO: nodelocaldns-g7lh5 from kube-system started at 2019-12-05 03:17:33 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container node-cache ready: true, restart count 0
Dec  5 06:07:06.623: INFO: calico-node-x7nbv from kube-system started at 2019-12-05 03:16:55 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container calico-node ready: true, restart count 0
Dec  5 06:07:06.623: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2019-12-05 04:06:32 +0000 UTC (3 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container prometheus ready: true, restart count 1
Dec  5 06:07:06.623: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec  5 06:07:06.623: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec  5 06:07:06.623: INFO: kube-proxy-68pbh from kube-system started at 2019-12-05 03:16:42 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  5 06:07:06.623: INFO: openebs-ndm-92tm6 from kube-system started at 2019-12-05 04:00:08 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container node-disk-manager ready: true, restart count 0
Dec  5 06:07:06.623: INFO: sonobuoy-systemd-logs-daemon-set-4e7bfb56bab946ed-65798 from sonobuoy started at 2019-12-05 05:58:13 +0000 UTC (2 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 06:07:06.623: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  5 06:07:06.623: INFO: prometheus-k8s-system-0 from kubesphere-monitoring-system started at 2019-12-05 04:07:26 +0000 UTC (3 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container prometheus ready: true, restart count 1
Dec  5 06:07:06.623: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec  5 06:07:06.623: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec  5 06:07:06.623: INFO: sonobuoy from sonobuoy started at 2019-12-05 05:56:52 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  5 06:07:06.623: INFO: default-http-backend-6555ff6898-qz96c from kubesphere-controls-system started at 2019-12-05 04:01:04 +0000 UTC (1 container statuses recorded)
Dec  5 06:07:06.623: INFO: 	Container default-http-backend ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node node1
STEP: verifying the node has the label node node2
Dec  5 06:07:06.699: INFO: Pod calico-node-p48b5 requesting resource cpu=150m on Node node1
Dec  5 06:07:06.699: INFO: Pod calico-node-x7nbv requesting resource cpu=150m on Node node2
Dec  5 06:07:06.699: INFO: Pod kube-proxy-68pbh requesting resource cpu=0m on Node node2
Dec  5 06:07:06.699: INFO: Pod kube-proxy-bbf7w requesting resource cpu=0m on Node node1
Dec  5 06:07:06.699: INFO: Pod nodelocaldns-4cspq requesting resource cpu=100m on Node node1
Dec  5 06:07:06.699: INFO: Pod nodelocaldns-g7lh5 requesting resource cpu=100m on Node node2
Dec  5 06:07:06.699: INFO: Pod openebs-localpv-provisioner-7b55587dbd-p5ktt requesting resource cpu=0m on Node node2
Dec  5 06:07:06.699: INFO: Pod openebs-ndm-92tm6 requesting resource cpu=0m on Node node2
Dec  5 06:07:06.699: INFO: Pod openebs-ndm-dd2vs requesting resource cpu=0m on Node node1
Dec  5 06:07:06.699: INFO: Pod openebs-ndm-operator-69cbc86975-r5pph requesting resource cpu=0m on Node node1
Dec  5 06:07:06.699: INFO: Pod tiller-deploy-55fc49c595-wb92t requesting resource cpu=0m on Node node1
Dec  5 06:07:06.699: INFO: Pod default-http-backend-6555ff6898-qz96c requesting resource cpu=10m on Node node2
Dec  5 06:07:06.699: INFO: Pod kubectl-admin-74fdfc47c7-r5wx2 requesting resource cpu=0m on Node node2
Dec  5 06:07:06.699: INFO: Pod kube-state-metrics-dfdf4d964-pdd5z requesting resource cpu=66m on Node node1
Dec  5 06:07:06.699: INFO: Pod node-exporter-n87mk requesting resource cpu=20m on Node node1
Dec  5 06:07:06.699: INFO: Pod node-exporter-xkgf6 requesting resource cpu=20m on Node node2
Dec  5 06:07:06.699: INFO: Pod prometheus-k8s-0 requesting resource cpu=210m on Node node2
Dec  5 06:07:06.699: INFO: Pod prometheus-k8s-1 requesting resource cpu=210m on Node node1
Dec  5 06:07:06.699: INFO: Pod prometheus-k8s-system-0 requesting resource cpu=210m on Node node2
Dec  5 06:07:06.699: INFO: Pod prometheus-k8s-system-1 requesting resource cpu=210m on Node node1
Dec  5 06:07:06.699: INFO: Pod prometheus-operator-685bc484cb-k5cmh requesting resource cpu=10m on Node node1
Dec  5 06:07:06.699: INFO: Pod ks-installer-7987c659d6-hljj9 requesting resource cpu=0m on Node node1
Dec  5 06:07:06.699: INFO: Pod sonobuoy requesting resource cpu=0m on Node node2
Dec  5 06:07:06.699: INFO: Pod sonobuoy-e2e-job-4d426e35b88a483d requesting resource cpu=0m on Node node2
Dec  5 06:07:06.699: INFO: Pod sonobuoy-systemd-logs-daemon-set-4e7bfb56bab946ed-65798 requesting resource cpu=0m on Node node2
Dec  5 06:07:06.699: INFO: Pod sonobuoy-systemd-logs-daemon-set-4e7bfb56bab946ed-6dtlb requesting resource cpu=0m on Node node1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c208586-b909-43cc-9924-2e424da8652b.15dd6492e4c3b947], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4076/filler-pod-8c208586-b909-43cc-9924-2e424da8652b to node1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c208586-b909-43cc-9924-2e424da8652b.15dd649322388036], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c208586-b909-43cc-9924-2e424da8652b.15dd64d46b52cc55], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c208586-b909-43cc-9924-2e424da8652b.15dd64d46dae1a50], Reason = [Created], Message = [Created container filler-pod-8c208586-b909-43cc-9924-2e424da8652b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c208586-b909-43cc-9924-2e424da8652b.15dd64d478b25144], Reason = [Started], Message = [Started container filler-pod-8c208586-b909-43cc-9924-2e424da8652b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a2256c77-070c-4f20-8333-a19dbe3acb9d.15dd6492e54b0047], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4076/filler-pod-a2256c77-070c-4f20-8333-a19dbe3acb9d to node2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a2256c77-070c-4f20-8333-a19dbe3acb9d.15dd64935b489fd5], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a2256c77-070c-4f20-8333-a19dbe3acb9d.15dd64c7e7c78485], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a2256c77-070c-4f20-8333-a19dbe3acb9d.15dd64c7ea6dc81f], Reason = [Created], Message = [Created container filler-pod-a2256c77-070c-4f20-8333-a19dbe3acb9d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a2256c77-070c-4f20-8333-a19dbe3acb9d.15dd64c7f65a2c9b], Reason = [Started], Message = [Started container filler-pod-a2256c77-070c-4f20-8333-a19dbe3acb9d]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15dd64d5065936de], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node node1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node node2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:11:51.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4076" for this suite.
Dec  5 06:11:57.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:11:57.904: INFO: namespace sched-pred-4076 deletion completed in 6.107082576s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:291.441 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:11:57.904: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 06:11:57.939: INFO: (0) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 5.929426ms)
Dec  5 06:11:57.942: INFO: (1) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.620886ms)
Dec  5 06:11:57.951: INFO: (2) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 9.071046ms)
Dec  5 06:11:57.960: INFO: (3) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 7.838653ms)
Dec  5 06:11:57.968: INFO: (4) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 8.035768ms)
Dec  5 06:11:57.975: INFO: (5) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 7.489045ms)
Dec  5 06:11:57.978: INFO: (6) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.924397ms)
Dec  5 06:11:57.981: INFO: (7) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.772657ms)
Dec  5 06:11:57.985: INFO: (8) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.746304ms)
Dec  5 06:11:57.988: INFO: (9) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.11736ms)
Dec  5 06:11:57.991: INFO: (10) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.333216ms)
Dec  5 06:11:57.995: INFO: (11) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.277802ms)
Dec  5 06:11:57.998: INFO: (12) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.288548ms)
Dec  5 06:11:58.001: INFO: (13) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.373483ms)
Dec  5 06:11:58.004: INFO: (14) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.948974ms)
Dec  5 06:11:58.007: INFO: (15) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.918559ms)
Dec  5 06:11:58.010: INFO: (16) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.017776ms)
Dec  5 06:11:58.013: INFO: (17) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.660816ms)
Dec  5 06:11:58.016: INFO: (18) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.900515ms)
Dec  5 06:11:58.019: INFO: (19) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.045582ms)
[AfterEach] version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:11:58.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6902" for this suite.
Dec  5 06:12:04.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:12:04.136: INFO: namespace proxy-6902 deletion completed in 6.113870217s

• [SLOW TEST:6.232 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:12:04.138: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 06:12:04.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-558c58bf-bf8b-4992-a10a-2748782bb3f8" in namespace "projected-1213" to be "success or failure"
Dec  5 06:12:04.175: INFO: Pod "downwardapi-volume-558c58bf-bf8b-4992-a10a-2748782bb3f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.32075ms
Dec  5 06:12:06.179: INFO: Pod "downwardapi-volume-558c58bf-bf8b-4992-a10a-2748782bb3f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006894677s
STEP: Saw pod success
Dec  5 06:12:06.179: INFO: Pod "downwardapi-volume-558c58bf-bf8b-4992-a10a-2748782bb3f8" satisfied condition "success or failure"
Dec  5 06:12:06.181: INFO: Trying to get logs from node node2 pod downwardapi-volume-558c58bf-bf8b-4992-a10a-2748782bb3f8 container client-container: <nil>
STEP: delete the pod
Dec  5 06:12:06.197: INFO: Waiting for pod downwardapi-volume-558c58bf-bf8b-4992-a10a-2748782bb3f8 to disappear
Dec  5 06:12:06.199: INFO: Pod downwardapi-volume-558c58bf-bf8b-4992-a10a-2748782bb3f8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:12:06.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1213" for this suite.
Dec  5 06:12:12.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:12:12.343: INFO: namespace projected-1213 deletion completed in 6.141514046s

• [SLOW TEST:8.205 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:12:12.344: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec  5 06:12:12.375: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  5 06:12:12.381: INFO: Waiting for terminating namespaces to be deleted...
Dec  5 06:12:12.384: INFO: 
Logging pods the kubelet thinks is on node node1 before test
Dec  5 06:12:12.399: INFO: calico-node-p48b5 from kube-system started at 2019-12-05 03:16:55 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container calico-node ready: true, restart count 0
Dec  5 06:12:12.399: INFO: openebs-ndm-dd2vs from kube-system started at 2019-12-05 04:00:08 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container node-disk-manager ready: true, restart count 0
Dec  5 06:12:12.399: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2019-12-05 04:05:04 +0000 UTC (3 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container prometheus ready: true, restart count 1
Dec  5 06:12:12.399: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec  5 06:12:12.399: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec  5 06:12:12.399: INFO: prometheus-operator-685bc484cb-k5cmh from kubesphere-monitoring-system started at 2019-12-05 04:01:41 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container prometheus-operator ready: true, restart count 0
Dec  5 06:12:12.399: INFO: sonobuoy-systemd-logs-daemon-set-4e7bfb56bab946ed-6dtlb from sonobuoy started at 2019-12-05 05:58:13 +0000 UTC (2 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 06:12:12.399: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  5 06:12:12.399: INFO: nodelocaldns-4cspq from kube-system started at 2019-12-05 03:17:33 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container node-cache ready: true, restart count 0
Dec  5 06:12:12.399: INFO: openebs-ndm-operator-69cbc86975-r5pph from kube-system started at 2019-12-05 04:00:08 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container node-disk-operator ready: true, restart count 1
Dec  5 06:12:12.399: INFO: node-exporter-n87mk from kubesphere-monitoring-system started at 2019-12-05 04:01:42 +0000 UTC (2 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  5 06:12:12.399: INFO: 	Container node-exporter ready: true, restart count 0
Dec  5 06:12:12.399: INFO: prometheus-k8s-system-1 from kubesphere-monitoring-system started at 2019-12-05 04:05:13 +0000 UTC (3 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container prometheus ready: true, restart count 1
Dec  5 06:12:12.399: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec  5 06:12:12.399: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec  5 06:12:12.399: INFO: ks-installer-7987c659d6-hljj9 from kubesphere-system started at 2019-12-05 04:00:17 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container installer ready: true, restart count 0
Dec  5 06:12:12.399: INFO: kube-state-metrics-dfdf4d964-pdd5z from kubesphere-monitoring-system started at 2019-12-05 04:10:38 +0000 UTC (4 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container addon-resizer ready: true, restart count 0
Dec  5 06:12:12.399: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec  5 06:12:12.399: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec  5 06:12:12.399: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec  5 06:12:12.399: INFO: kube-proxy-bbf7w from kube-system started at 2019-12-05 03:16:38 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  5 06:12:12.399: INFO: tiller-deploy-55fc49c595-wb92t from kube-system started at 2019-12-05 03:25:52 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.399: INFO: 	Container tiller ready: true, restart count 0
Dec  5 06:12:12.399: INFO: 
Logging pods the kubelet thinks is on node node2 before test
Dec  5 06:12:12.410: INFO: sonobuoy-systemd-logs-daemon-set-4e7bfb56bab946ed-65798 from sonobuoy started at 2019-12-05 05:58:13 +0000 UTC (2 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 06:12:12.410: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  5 06:12:12.410: INFO: default-http-backend-6555ff6898-qz96c from kubesphere-controls-system started at 2019-12-05 04:01:04 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container default-http-backend ready: true, restart count 0
Dec  5 06:12:12.410: INFO: prometheus-k8s-system-0 from kubesphere-monitoring-system started at 2019-12-05 04:07:26 +0000 UTC (3 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container prometheus ready: true, restart count 1
Dec  5 06:12:12.410: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec  5 06:12:12.410: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec  5 06:12:12.410: INFO: sonobuoy from sonobuoy started at 2019-12-05 05:56:52 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  5 06:12:12.410: INFO: node-exporter-xkgf6 from kubesphere-monitoring-system started at 2019-12-05 04:01:42 +0000 UTC (2 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  5 06:12:12.410: INFO: 	Container node-exporter ready: true, restart count 0
Dec  5 06:12:12.410: INFO: kubectl-admin-74fdfc47c7-r5wx2 from kubesphere-controls-system started at 2019-12-05 04:08:47 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container kubectl ready: true, restart count 0
Dec  5 06:12:12.410: INFO: openebs-localpv-provisioner-7b55587dbd-p5ktt from kube-system started at 2019-12-05 04:00:08 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
Dec  5 06:12:12.410: INFO: nodelocaldns-g7lh5 from kube-system started at 2019-12-05 03:17:33 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container node-cache ready: true, restart count 0
Dec  5 06:12:12.410: INFO: sonobuoy-e2e-job-4d426e35b88a483d from sonobuoy started at 2019-12-05 05:58:13 +0000 UTC (2 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container e2e ready: true, restart count 0
Dec  5 06:12:12.410: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 06:12:12.410: INFO: kube-proxy-68pbh from kube-system started at 2019-12-05 03:16:42 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  5 06:12:12.410: INFO: calico-node-x7nbv from kube-system started at 2019-12-05 03:16:55 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container calico-node ready: true, restart count 0
Dec  5 06:12:12.410: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2019-12-05 04:06:32 +0000 UTC (3 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container prometheus ready: true, restart count 1
Dec  5 06:12:12.410: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec  5 06:12:12.410: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec  5 06:12:12.410: INFO: openebs-ndm-92tm6 from kube-system started at 2019-12-05 04:00:08 +0000 UTC (1 container statuses recorded)
Dec  5 06:12:12.410: INFO: 	Container node-disk-manager ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-96581990-fe77-44eb-b99c-5879f7967114 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-96581990-fe77-44eb-b99c-5879f7967114 off the node node1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-96581990-fe77-44eb-b99c-5879f7967114
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:12:16.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9461" for this suite.
Dec  5 06:12:34.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:12:34.584: INFO: namespace sched-pred-9461 deletion completed in 18.101774973s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:22.240 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:12:34.584: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 06:12:34.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-6181'
Dec  5 06:12:34.721: INFO: stderr: ""
Dec  5 06:12:34.721: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec  5 06:12:39.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pod e2e-test-nginx-pod --namespace=kubectl-6181 -o json'
Dec  5 06:12:39.873: INFO: stderr: ""
Dec  5 06:12:39.873: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-12-05T06:12:34Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-6181\",\n        \"resourceVersion\": \"26307\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6181/pods/e2e-test-nginx-pod\",\n        \"uid\": \"7736b7a5-577b-483f-a25c-a0515b16d36a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-6w6np\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-6w6np\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-6w6np\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-05T06:12:34Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-05T06:12:36Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-05T06:12:36Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-05T06:12:34Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://c4bebbaf3eac7f13516ababd104af802bc1f09c272b7ca7f03980df03fb911cf\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-05T06:12:35Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.0.8\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.96.35\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-05T06:12:34Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec  5 06:12:39.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 replace -f - --namespace=kubectl-6181'
Dec  5 06:12:40.158: INFO: stderr: ""
Dec  5 06:12:40.158: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Dec  5 06:12:40.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete pods e2e-test-nginx-pod --namespace=kubectl-6181'
Dec  5 06:13:31.577: INFO: stderr: ""
Dec  5 06:13:31.577: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:13:31.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6181" for this suite.
Dec  5 06:13:37.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:13:37.683: INFO: namespace kubectl-6181 deletion completed in 6.101638483s

• [SLOW TEST:63.098 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:13:37.683: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Dec  5 06:13:39.266: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 7
	[quantile=0.9] = 245
	[quantile=0.99] = 245
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 604480
	[quantile=0.9] = 609302
	[quantile=0.99] = 609302
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 4
	[quantile=0.9] = 5
	[quantile=0.99] = 20
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 15
	[quantile=0.9] = 28
	[quantile=0.99] = 51
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 14
	[quantile=0.9] = 22
	[quantile=0.99] = 40
For namespace_queue_latency_sum:
	[] = 370
For namespace_queue_latency_count:
	[] = 24
For namespace_retries:
	[] = 37
For namespace_work_duration:
	[quantile=0.5] = 145498
	[quantile=0.9] = 191558
	[quantile=0.99] = 771749
For namespace_work_duration_sum:
	[] = 4333298
For namespace_work_duration_count:
	[] = 24
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:13:39.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1461" for this suite.
Dec  5 06:13:45.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:13:45.391: INFO: namespace gc-1461 deletion completed in 6.122181818s

• [SLOW TEST:7.708 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:13:45.391: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 06:13:45.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-220'
Dec  5 06:13:45.497: INFO: stderr: ""
Dec  5 06:13:45.497: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Dec  5 06:13:45.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete pods e2e-test-nginx-pod --namespace=kubectl-220'
Dec  5 06:13:48.852: INFO: stderr: ""
Dec  5 06:13:48.852: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:13:48.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-220" for this suite.
Dec  5 06:13:54.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:13:54.965: INFO: namespace kubectl-220 deletion completed in 6.110177287s

• [SLOW TEST:9.574 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:13:54.966: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 06:14:01.029: INFO: Waiting up to 5m0s for pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3" in namespace "pods-4083" to be "success or failure"
Dec  5 06:14:01.037: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.470697ms
Dec  5 06:14:03.041: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011211978s
Dec  5 06:14:05.044: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014397422s
Dec  5 06:14:07.047: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017744745s
Dec  5 06:14:09.050: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021038258s
Dec  5 06:14:11.053: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023877747s
Dec  5 06:14:13.057: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.027616357s
Dec  5 06:14:15.061: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.031402539s
Dec  5 06:14:17.065: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.035230167s
Dec  5 06:14:19.068: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.039005015s
Dec  5 06:14:21.072: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.042994654s
Dec  5 06:14:23.090: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.060141687s
Dec  5 06:14:25.093: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.063591526s
Dec  5 06:14:27.097: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 26.067265124s
Dec  5 06:14:29.105: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 28.075564702s
Dec  5 06:14:31.109: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 30.079138612s
Dec  5 06:14:33.112: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 32.082506806s
Dec  5 06:14:35.117: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 34.08744513s
Dec  5 06:14:37.121: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 36.091254422s
Dec  5 06:14:39.124: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 38.094299457s
Dec  5 06:14:41.127: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 40.098015998s
Dec  5 06:14:43.132: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 42.102711833s
Dec  5 06:14:45.135: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Pending", Reason="", readiness=false. Elapsed: 44.105692191s
Dec  5 06:14:47.139: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 46.109186954s
STEP: Saw pod success
Dec  5 06:14:47.139: INFO: Pod "client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3" satisfied condition "success or failure"
Dec  5 06:14:47.141: INFO: Trying to get logs from node node1 pod client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3 container env3cont: <nil>
STEP: delete the pod
Dec  5 06:14:47.155: INFO: Waiting for pod client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3 to disappear
Dec  5 06:14:47.157: INFO: Pod client-envvars-1836664e-c5af-4d2d-aa37-0fa9914785b3 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:14:47.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4083" for this suite.
Dec  5 06:15:37.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:15:37.258: INFO: namespace pods-4083 deletion completed in 50.097500045s

• [SLOW TEST:102.292 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:15:37.258: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec  5 06:15:47.326: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8133 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 06:15:47.326: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:15:47.524: INFO: Exec stderr: ""
Dec  5 06:15:47.524: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8133 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 06:15:47.524: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:15:47.707: INFO: Exec stderr: ""
Dec  5 06:15:47.707: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8133 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 06:15:47.707: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:15:47.876: INFO: Exec stderr: ""
Dec  5 06:15:47.876: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8133 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 06:15:47.876: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:15:48.058: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec  5 06:15:48.058: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8133 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 06:15:48.058: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:15:48.252: INFO: Exec stderr: ""
Dec  5 06:15:48.252: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8133 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 06:15:48.252: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:15:48.438: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec  5 06:15:48.438: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8133 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 06:15:48.438: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:15:48.649: INFO: Exec stderr: ""
Dec  5 06:15:48.649: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8133 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 06:15:48.649: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:15:48.829: INFO: Exec stderr: ""
Dec  5 06:15:48.829: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8133 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 06:15:48.829: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:15:48.999: INFO: Exec stderr: ""
Dec  5 06:15:48.999: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8133 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 06:15:48.999: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:15:49.185: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:15:49.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8133" for this suite.
Dec  5 06:16:35.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:16:35.292: INFO: namespace e2e-kubelet-etc-hosts-8133 deletion completed in 46.103614931s

• [SLOW TEST:58.034 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:16:35.293: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-8b7b5f2c-dd76-46ca-8a4b-08870a73c924
STEP: Creating a pod to test consume secrets
Dec  5 06:16:35.330: INFO: Waiting up to 5m0s for pod "pod-secrets-8407c4bc-b34d-4f97-b79a-4e03f3fc0bdd" in namespace "secrets-1814" to be "success or failure"
Dec  5 06:16:35.334: INFO: Pod "pod-secrets-8407c4bc-b34d-4f97-b79a-4e03f3fc0bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.763932ms
Dec  5 06:16:37.337: INFO: Pod "pod-secrets-8407c4bc-b34d-4f97-b79a-4e03f3fc0bdd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00749923s
Dec  5 06:16:39.342: INFO: Pod "pod-secrets-8407c4bc-b34d-4f97-b79a-4e03f3fc0bdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011992889s
STEP: Saw pod success
Dec  5 06:16:39.342: INFO: Pod "pod-secrets-8407c4bc-b34d-4f97-b79a-4e03f3fc0bdd" satisfied condition "success or failure"
Dec  5 06:16:39.344: INFO: Trying to get logs from node node2 pod pod-secrets-8407c4bc-b34d-4f97-b79a-4e03f3fc0bdd container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 06:16:39.363: INFO: Waiting for pod pod-secrets-8407c4bc-b34d-4f97-b79a-4e03f3fc0bdd to disappear
Dec  5 06:16:39.365: INFO: Pod pod-secrets-8407c4bc-b34d-4f97-b79a-4e03f3fc0bdd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:16:39.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1814" for this suite.
Dec  5 06:16:45.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:16:45.469: INFO: namespace secrets-1814 deletion completed in 6.100041005s

• [SLOW TEST:10.177 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:16:45.470: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-6e24873b-1b6b-4661-845f-6038b84f3994
STEP: Creating a pod to test consume configMaps
Dec  5 06:16:45.506: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-137b0976-d498-4e49-a8a5-803cc09e0a44" in namespace "projected-4619" to be "success or failure"
Dec  5 06:16:45.510: INFO: Pod "pod-projected-configmaps-137b0976-d498-4e49-a8a5-803cc09e0a44": Phase="Pending", Reason="", readiness=false. Elapsed: 3.48326ms
Dec  5 06:16:47.513: INFO: Pod "pod-projected-configmaps-137b0976-d498-4e49-a8a5-803cc09e0a44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006842612s
Dec  5 06:16:49.516: INFO: Pod "pod-projected-configmaps-137b0976-d498-4e49-a8a5-803cc09e0a44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009803343s
STEP: Saw pod success
Dec  5 06:16:49.516: INFO: Pod "pod-projected-configmaps-137b0976-d498-4e49-a8a5-803cc09e0a44" satisfied condition "success or failure"
Dec  5 06:16:49.518: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-137b0976-d498-4e49-a8a5-803cc09e0a44 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 06:16:49.533: INFO: Waiting for pod pod-projected-configmaps-137b0976-d498-4e49-a8a5-803cc09e0a44 to disappear
Dec  5 06:16:49.547: INFO: Pod pod-projected-configmaps-137b0976-d498-4e49-a8a5-803cc09e0a44 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:16:49.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4619" for this suite.
Dec  5 06:16:55.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:16:55.667: INFO: namespace projected-4619 deletion completed in 6.114367508s

• [SLOW TEST:10.197 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:16:55.667: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:17:01.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5473" for this suite.
Dec  5 06:17:07.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:17:07.470: INFO: namespace watch-5473 deletion completed in 6.194030943s

• [SLOW TEST:11.803 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:17:07.471: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Dec  5 06:17:09.517: INFO: Pod pod-hostip-18f35e25-5cdb-4f5b-b3ea-d421cff7edce has hostIP: 192.168.0.8
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:17:09.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3062" for this suite.
Dec  5 06:17:31.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:17:31.624: INFO: namespace pods-3062 deletion completed in 22.102902665s

• [SLOW TEST:24.154 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:17:31.625: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Dec  5 06:17:31.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-2606'
Dec  5 06:17:32.012: INFO: stderr: ""
Dec  5 06:17:32.012: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 06:17:32.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2606'
Dec  5 06:17:32.097: INFO: stderr: ""
Dec  5 06:17:32.097: INFO: stdout: "update-demo-nautilus-7p7qb update-demo-nautilus-ml42z "
Dec  5 06:17:32.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-7p7qb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2606'
Dec  5 06:17:32.180: INFO: stderr: ""
Dec  5 06:17:32.180: INFO: stdout: ""
Dec  5 06:17:32.180: INFO: update-demo-nautilus-7p7qb is created but not running
Dec  5 06:17:37.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2606'
Dec  5 06:17:37.267: INFO: stderr: ""
Dec  5 06:17:37.267: INFO: stdout: "update-demo-nautilus-7p7qb update-demo-nautilus-ml42z "
Dec  5 06:17:37.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-7p7qb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2606'
Dec  5 06:17:37.343: INFO: stderr: ""
Dec  5 06:17:37.343: INFO: stdout: "true"
Dec  5 06:17:37.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-7p7qb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2606'
Dec  5 06:17:37.419: INFO: stderr: ""
Dec  5 06:17:37.419: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 06:17:37.419: INFO: validating pod update-demo-nautilus-7p7qb
Dec  5 06:17:37.423: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 06:17:37.423: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 06:17:37.423: INFO: update-demo-nautilus-7p7qb is verified up and running
Dec  5 06:17:37.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-ml42z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2606'
Dec  5 06:17:37.499: INFO: stderr: ""
Dec  5 06:17:37.499: INFO: stdout: "true"
Dec  5 06:17:37.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-ml42z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2606'
Dec  5 06:17:37.594: INFO: stderr: ""
Dec  5 06:17:37.594: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 06:17:37.594: INFO: validating pod update-demo-nautilus-ml42z
Dec  5 06:17:37.598: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 06:17:37.598: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 06:17:37.598: INFO: update-demo-nautilus-ml42z is verified up and running
STEP: rolling-update to new replication controller
Dec  5 06:17:37.601: INFO: scanned /root for discovery docs: <nil>
Dec  5 06:17:37.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2606'
Dec  5 06:17:59.999: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  5 06:17:59.999: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 06:17:59.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2606'
Dec  5 06:18:00.082: INFO: stderr: ""
Dec  5 06:18:00.082: INFO: stdout: "update-demo-kitten-4l8mt update-demo-kitten-5jg76 "
Dec  5 06:18:00.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-kitten-4l8mt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2606'
Dec  5 06:18:00.161: INFO: stderr: ""
Dec  5 06:18:00.161: INFO: stdout: "true"
Dec  5 06:18:00.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-kitten-4l8mt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2606'
Dec  5 06:18:00.251: INFO: stderr: ""
Dec  5 06:18:00.251: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  5 06:18:00.251: INFO: validating pod update-demo-kitten-4l8mt
Dec  5 06:18:00.254: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  5 06:18:00.254: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  5 06:18:00.254: INFO: update-demo-kitten-4l8mt is verified up and running
Dec  5 06:18:00.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-kitten-5jg76 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2606'
Dec  5 06:18:00.331: INFO: stderr: ""
Dec  5 06:18:00.331: INFO: stdout: "true"
Dec  5 06:18:00.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-kitten-5jg76 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2606'
Dec  5 06:18:00.400: INFO: stderr: ""
Dec  5 06:18:00.400: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  5 06:18:00.400: INFO: validating pod update-demo-kitten-5jg76
Dec  5 06:18:00.405: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  5 06:18:00.405: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  5 06:18:00.405: INFO: update-demo-kitten-5jg76 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:18:00.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2606" for this suite.
Dec  5 06:18:22.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:18:22.553: INFO: namespace kubectl-2606 deletion completed in 22.145826264s

• [SLOW TEST:50.929 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:18:22.554: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3479
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3479
STEP: Creating statefulset with conflicting port in namespace statefulset-3479
STEP: Waiting until pod test-pod will start running in namespace statefulset-3479
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3479
Dec  5 06:18:24.635: INFO: Observed stateful pod in namespace: statefulset-3479, name: ss-0, uid: 12a10bce-7556-465a-b4af-8f065dea2890, status phase: Pending. Waiting for statefulset controller to delete.
Dec  5 06:18:25.017: INFO: Observed stateful pod in namespace: statefulset-3479, name: ss-0, uid: 12a10bce-7556-465a-b4af-8f065dea2890, status phase: Failed. Waiting for statefulset controller to delete.
Dec  5 06:18:25.021: INFO: Observed stateful pod in namespace: statefulset-3479, name: ss-0, uid: 12a10bce-7556-465a-b4af-8f065dea2890, status phase: Failed. Waiting for statefulset controller to delete.
Dec  5 06:18:25.025: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3479
STEP: Removing pod with conflicting port in namespace statefulset-3479
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3479 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  5 06:18:29.044: INFO: Deleting all statefulset in ns statefulset-3479
Dec  5 06:18:29.047: INFO: Scaling statefulset ss to 0
Dec  5 06:18:39.059: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 06:18:39.062: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:18:39.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3479" for this suite.
Dec  5 06:18:45.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:18:45.179: INFO: namespace statefulset-3479 deletion completed in 6.097865559s

• [SLOW TEST:22.625 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:18:45.180: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-f5ac3244-670c-43fb-99a5-4cf48435d01d
STEP: Creating a pod to test consume secrets
Dec  5 06:18:45.211: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9c82696c-8438-4cfa-9a93-d4d29bed5159" in namespace "projected-9882" to be "success or failure"
Dec  5 06:18:45.216: INFO: Pod "pod-projected-secrets-9c82696c-8438-4cfa-9a93-d4d29bed5159": Phase="Pending", Reason="", readiness=false. Elapsed: 5.151097ms
Dec  5 06:18:47.220: INFO: Pod "pod-projected-secrets-9c82696c-8438-4cfa-9a93-d4d29bed5159": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009058149s
STEP: Saw pod success
Dec  5 06:18:47.220: INFO: Pod "pod-projected-secrets-9c82696c-8438-4cfa-9a93-d4d29bed5159" satisfied condition "success or failure"
Dec  5 06:18:47.222: INFO: Trying to get logs from node node1 pod pod-projected-secrets-9c82696c-8438-4cfa-9a93-d4d29bed5159 container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 06:18:47.241: INFO: Waiting for pod pod-projected-secrets-9c82696c-8438-4cfa-9a93-d4d29bed5159 to disappear
Dec  5 06:18:47.243: INFO: Pod pod-projected-secrets-9c82696c-8438-4cfa-9a93-d4d29bed5159 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:18:47.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9882" for this suite.
Dec  5 06:18:53.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:18:53.357: INFO: namespace projected-9882 deletion completed in 6.110976448s

• [SLOW TEST:8.177 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:18:53.357: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  5 06:18:56.416: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:18:56.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6168" for this suite.
Dec  5 06:19:02.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:19:02.543: INFO: namespace container-runtime-6168 deletion completed in 6.114885092s

• [SLOW TEST:9.186 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:19:02.543: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 06:19:02.647: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"bc6e550c-2c58-4f1b-8076-30b309d3bd0a", Controller:(*bool)(0xc0026b99e6), BlockOwnerDeletion:(*bool)(0xc0026b99e7)}}
Dec  5 06:19:02.655: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"2ed0b13b-7f34-4a1e-b5a0-3d655689e94e", Controller:(*bool)(0xc0026b9b96), BlockOwnerDeletion:(*bool)(0xc0026b9b97)}}
Dec  5 06:19:02.659: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a41df959-d461-4165-9e4c-b7b72e45fab2", Controller:(*bool)(0xc00368588e), BlockOwnerDeletion:(*bool)(0xc00368588f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:19:07.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8045" for this suite.
Dec  5 06:19:13.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:19:13.773: INFO: namespace gc-8045 deletion completed in 6.103723872s

• [SLOW TEST:11.230 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:19:13.773: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  5 06:19:13.804: INFO: Waiting up to 5m0s for pod "downward-api-8c33b25e-0c8b-40cd-99ad-7ac4f7ea0cdc" in namespace "downward-api-1220" to be "success or failure"
Dec  5 06:19:13.807: INFO: Pod "downward-api-8c33b25e-0c8b-40cd-99ad-7ac4f7ea0cdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.987238ms
Dec  5 06:19:15.811: INFO: Pod "downward-api-8c33b25e-0c8b-40cd-99ad-7ac4f7ea0cdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006875052s
STEP: Saw pod success
Dec  5 06:19:15.811: INFO: Pod "downward-api-8c33b25e-0c8b-40cd-99ad-7ac4f7ea0cdc" satisfied condition "success or failure"
Dec  5 06:19:15.814: INFO: Trying to get logs from node node2 pod downward-api-8c33b25e-0c8b-40cd-99ad-7ac4f7ea0cdc container dapi-container: <nil>
STEP: delete the pod
Dec  5 06:19:15.837: INFO: Waiting for pod downward-api-8c33b25e-0c8b-40cd-99ad-7ac4f7ea0cdc to disappear
Dec  5 06:19:15.840: INFO: Pod downward-api-8c33b25e-0c8b-40cd-99ad-7ac4f7ea0cdc no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:19:15.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1220" for this suite.
Dec  5 06:19:21.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:19:21.941: INFO: namespace downward-api-1220 deletion completed in 6.098116823s

• [SLOW TEST:8.167 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:19:21.941: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-e18119fb-dd3b-4933-9665-8ba6ae9d4bbc
STEP: Creating a pod to test consume configMaps
Dec  5 06:19:21.974: INFO: Waiting up to 5m0s for pod "pod-configmaps-c998bc55-46af-4e13-ac95-4a57635c589d" in namespace "configmap-7394" to be "success or failure"
Dec  5 06:19:21.978: INFO: Pod "pod-configmaps-c998bc55-46af-4e13-ac95-4a57635c589d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.704114ms
Dec  5 06:19:23.981: INFO: Pod "pod-configmaps-c998bc55-46af-4e13-ac95-4a57635c589d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007082911s
STEP: Saw pod success
Dec  5 06:19:23.981: INFO: Pod "pod-configmaps-c998bc55-46af-4e13-ac95-4a57635c589d" satisfied condition "success or failure"
Dec  5 06:19:23.983: INFO: Trying to get logs from node node1 pod pod-configmaps-c998bc55-46af-4e13-ac95-4a57635c589d container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 06:19:24.004: INFO: Waiting for pod pod-configmaps-c998bc55-46af-4e13-ac95-4a57635c589d to disappear
Dec  5 06:19:24.005: INFO: Pod pod-configmaps-c998bc55-46af-4e13-ac95-4a57635c589d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:19:24.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7394" for this suite.
Dec  5 06:19:30.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:19:30.134: INFO: namespace configmap-7394 deletion completed in 6.126517115s

• [SLOW TEST:8.194 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:19:30.137: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  5 06:19:32.700: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1528c211-1b55-4b23-896f-e6b0ea6f335a"
Dec  5 06:19:32.700: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1528c211-1b55-4b23-896f-e6b0ea6f335a" in namespace "pods-6356" to be "terminated due to deadline exceeded"
Dec  5 06:19:32.708: INFO: Pod "pod-update-activedeadlineseconds-1528c211-1b55-4b23-896f-e6b0ea6f335a": Phase="Running", Reason="", readiness=true. Elapsed: 7.588475ms
Dec  5 06:19:34.712: INFO: Pod "pod-update-activedeadlineseconds-1528c211-1b55-4b23-896f-e6b0ea6f335a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011407872s
Dec  5 06:19:36.715: INFO: Pod "pod-update-activedeadlineseconds-1528c211-1b55-4b23-896f-e6b0ea6f335a": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.014993843s
Dec  5 06:19:36.716: INFO: Pod "pod-update-activedeadlineseconds-1528c211-1b55-4b23-896f-e6b0ea6f335a" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:19:36.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6356" for this suite.
Dec  5 06:19:42.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:19:42.846: INFO: namespace pods-6356 deletion completed in 6.12700497s

• [SLOW TEST:12.710 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:19:42.848: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-w4rt
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 06:19:42.892: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-w4rt" in namespace "subpath-4009" to be "success or failure"
Dec  5 06:19:42.898: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Pending", Reason="", readiness=false. Elapsed: 5.568865ms
Dec  5 06:19:44.901: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Running", Reason="", readiness=true. Elapsed: 2.008779875s
Dec  5 06:19:46.904: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Running", Reason="", readiness=true. Elapsed: 4.012136964s
Dec  5 06:19:48.908: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Running", Reason="", readiness=true. Elapsed: 6.015474015s
Dec  5 06:19:50.911: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Running", Reason="", readiness=true. Elapsed: 8.01900917s
Dec  5 06:19:52.915: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Running", Reason="", readiness=true. Elapsed: 10.022994246s
Dec  5 06:19:54.918: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Running", Reason="", readiness=true. Elapsed: 12.025656594s
Dec  5 06:19:56.921: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Running", Reason="", readiness=true. Elapsed: 14.028958037s
Dec  5 06:19:58.924: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Running", Reason="", readiness=true. Elapsed: 16.032066567s
Dec  5 06:20:00.927: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Running", Reason="", readiness=true. Elapsed: 18.035230326s
Dec  5 06:20:02.931: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Running", Reason="", readiness=true. Elapsed: 20.038917767s
Dec  5 06:20:04.935: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Running", Reason="", readiness=true. Elapsed: 22.042780073s
Dec  5 06:20:06.938: INFO: Pod "pod-subpath-test-downwardapi-w4rt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.045903878s
STEP: Saw pod success
Dec  5 06:20:06.938: INFO: Pod "pod-subpath-test-downwardapi-w4rt" satisfied condition "success or failure"
Dec  5 06:20:06.940: INFO: Trying to get logs from node node1 pod pod-subpath-test-downwardapi-w4rt container test-container-subpath-downwardapi-w4rt: <nil>
STEP: delete the pod
Dec  5 06:20:06.957: INFO: Waiting for pod pod-subpath-test-downwardapi-w4rt to disappear
Dec  5 06:20:06.959: INFO: Pod pod-subpath-test-downwardapi-w4rt no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-w4rt
Dec  5 06:20:06.959: INFO: Deleting pod "pod-subpath-test-downwardapi-w4rt" in namespace "subpath-4009"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:20:06.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4009" for this suite.
Dec  5 06:20:12.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:20:13.079: INFO: namespace subpath-4009 deletion completed in 6.113341709s

• [SLOW TEST:30.231 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:20:13.082: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-665e88fa-d6a7-428b-83ab-2f49d214f60d
STEP: Creating secret with name s-test-opt-upd-014d4005-9d27-44c7-8743-01cd4147be3b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-665e88fa-d6a7-428b-83ab-2f49d214f60d
STEP: Updating secret s-test-opt-upd-014d4005-9d27-44c7-8743-01cd4147be3b
STEP: Creating secret with name s-test-opt-create-3ba0efe9-8372-4a2f-ac08-18d2779aaceb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:21:41.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1822" for this suite.
Dec  5 06:22:03.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:22:03.850: INFO: namespace projected-1822 deletion completed in 22.11434806s

• [SLOW TEST:110.704 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:22:03.851: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-8dedbdda-046e-4914-9d20-dae608099cbe
STEP: Creating a pod to test consume configMaps
Dec  5 06:22:03.891: INFO: Waiting up to 5m0s for pod "pod-configmaps-e2a40621-54cb-44ec-b2bc-46298d97e68e" in namespace "configmap-1285" to be "success or failure"
Dec  5 06:22:03.894: INFO: Pod "pod-configmaps-e2a40621-54cb-44ec-b2bc-46298d97e68e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37471ms
Dec  5 06:22:05.897: INFO: Pod "pod-configmaps-e2a40621-54cb-44ec-b2bc-46298d97e68e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005702249s
STEP: Saw pod success
Dec  5 06:22:05.897: INFO: Pod "pod-configmaps-e2a40621-54cb-44ec-b2bc-46298d97e68e" satisfied condition "success or failure"
Dec  5 06:22:05.899: INFO: Trying to get logs from node node1 pod pod-configmaps-e2a40621-54cb-44ec-b2bc-46298d97e68e container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 06:22:05.914: INFO: Waiting for pod pod-configmaps-e2a40621-54cb-44ec-b2bc-46298d97e68e to disappear
Dec  5 06:22:05.916: INFO: Pod pod-configmaps-e2a40621-54cb-44ec-b2bc-46298d97e68e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:22:05.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1285" for this suite.
Dec  5 06:22:11.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:22:12.042: INFO: namespace configmap-1285 deletion completed in 6.12260752s

• [SLOW TEST:8.191 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:22:12.044: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-0c7a5503-8db2-4ec4-9574-1209a36ec284
Dec  5 06:22:12.083: INFO: Pod name my-hostname-basic-0c7a5503-8db2-4ec4-9574-1209a36ec284: Found 0 pods out of 1
Dec  5 06:22:17.087: INFO: Pod name my-hostname-basic-0c7a5503-8db2-4ec4-9574-1209a36ec284: Found 1 pods out of 1
Dec  5 06:22:17.087: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0c7a5503-8db2-4ec4-9574-1209a36ec284" are running
Dec  5 06:22:17.089: INFO: Pod "my-hostname-basic-0c7a5503-8db2-4ec4-9574-1209a36ec284-7wd6b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 06:22:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 06:22:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 06:22:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 06:22:12 +0000 UTC Reason: Message:}])
Dec  5 06:22:17.090: INFO: Trying to dial the pod
Dec  5 06:22:22.101: INFO: Controller my-hostname-basic-0c7a5503-8db2-4ec4-9574-1209a36ec284: Got expected result from replica 1 [my-hostname-basic-0c7a5503-8db2-4ec4-9574-1209a36ec284-7wd6b]: "my-hostname-basic-0c7a5503-8db2-4ec4-9574-1209a36ec284-7wd6b", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:22:22.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7706" for this suite.
Dec  5 06:22:28.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:22:28.201: INFO: namespace replication-controller-7706 deletion completed in 6.09636606s

• [SLOW TEST:16.158 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:22:28.201: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-d3d7f8f8-da87-4788-b7e9-f5d142d44f48
STEP: Creating a pod to test consume configMaps
Dec  5 06:22:28.232: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d74642a2-61ce-4497-b852-e98d641a1f7b" in namespace "projected-8524" to be "success or failure"
Dec  5 06:22:28.235: INFO: Pod "pod-projected-configmaps-d74642a2-61ce-4497-b852-e98d641a1f7b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.61316ms
Dec  5 06:22:30.238: INFO: Pod "pod-projected-configmaps-d74642a2-61ce-4497-b852-e98d641a1f7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005892809s
STEP: Saw pod success
Dec  5 06:22:30.238: INFO: Pod "pod-projected-configmaps-d74642a2-61ce-4497-b852-e98d641a1f7b" satisfied condition "success or failure"
Dec  5 06:22:30.241: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-d74642a2-61ce-4497-b852-e98d641a1f7b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 06:22:30.257: INFO: Waiting for pod pod-projected-configmaps-d74642a2-61ce-4497-b852-e98d641a1f7b to disappear
Dec  5 06:22:30.259: INFO: Pod pod-projected-configmaps-d74642a2-61ce-4497-b852-e98d641a1f7b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:22:30.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8524" for this suite.
Dec  5 06:22:36.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:22:36.373: INFO: namespace projected-8524 deletion completed in 6.111130347s

• [SLOW TEST:8.172 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:22:36.373: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  5 06:22:44.431: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 06:22:44.432: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 06:22:46.433: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 06:22:46.436: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 06:22:48.433: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 06:22:48.436: INFO: Pod pod-with-prestop-http-hook still exists
Dec  5 06:22:50.433: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  5 06:22:50.437: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:22:50.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9008" for this suite.
Dec  5 06:23:12.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:23:12.566: INFO: namespace container-lifecycle-hook-9008 deletion completed in 22.116522254s

• [SLOW TEST:36.193 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:23:12.567: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 06:23:12.599: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  5 06:23:12.609: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  5 06:23:17.613: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  5 06:23:17.613: INFO: Creating deployment "test-rolling-update-deployment"
Dec  5 06:23:17.620: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  5 06:23:17.627: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec  5 06:23:19.634: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  5 06:23:19.636: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711123797, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711123797, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711123797, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711123797, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 06:23:21.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711123797, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711123797, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711123797, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711123797, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 06:23:23.640: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  5 06:23:23.648: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-1915,SelfLink:/apis/apps/v1/namespaces/deployment-1915/deployments/test-rolling-update-deployment,UID:594776f8-ef8c-4220-a405-9cd61279894f,ResourceVersion:29135,Generation:1,CreationTimestamp:2019-12-05 06:23:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-05 06:23:17 +0000 UTC 2019-12-05 06:23:17 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-05 06:23:22 +0000 UTC 2019-12-05 06:23:17 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  5 06:23:23.650: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-1915,SelfLink:/apis/apps/v1/namespaces/deployment-1915/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:8a14ec57-6978-4ec5-9684-2a40b5b08de3,ResourceVersion:29124,Generation:1,CreationTimestamp:2019-12-05 06:23:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 594776f8-ef8c-4220-a405-9cd61279894f 0xc0031e22c7 0xc0031e22c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  5 06:23:23.650: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  5 06:23:23.650: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-1915,SelfLink:/apis/apps/v1/namespaces/deployment-1915/replicasets/test-rolling-update-controller,UID:d2fce485-a921-47b1-9ef5-220ddf136961,ResourceVersion:29133,Generation:2,CreationTimestamp:2019-12-05 06:23:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 594776f8-ef8c-4220-a405-9cd61279894f 0xc0031e21f7 0xc0031e21f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 06:23:23.653: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-tfnnw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-tfnnw,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-1915,SelfLink:/api/v1/namespaces/deployment-1915/pods/test-rolling-update-deployment-79f6b9d75c-tfnnw,UID:55e6e0fa-a3cd-4340-b627-7a89cd0a5ad8,ResourceVersion:29123,Generation:0,CreationTimestamp:2019-12-05 06:23:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 8a14ec57-6978-4ec5-9684-2a40b5b08de3 0xc001d3e5d7 0xc001d3e5d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jlz9w {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jlz9w,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-jlz9w true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d3e650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d3e670}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:23:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:23:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:23:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:23:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.90.47,StartTime:2019-12-05 06:23:17 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-05 06:23:21 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://8e641f2627f37755313a4f5dbb59c3d78a6da5a0c40ed24e14cc5ec15a553b1a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:23:23.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1915" for this suite.
Dec  5 06:23:29.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:23:29.784: INFO: namespace deployment-1915 deletion completed in 6.127691099s

• [SLOW TEST:17.218 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:23:29.784: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-7642/configmap-test-00274ecb-cf17-4394-ab85-544810922d89
STEP: Creating a pod to test consume configMaps
Dec  5 06:23:29.817: INFO: Waiting up to 5m0s for pod "pod-configmaps-d3835523-96e0-40f6-9b4d-7b11060ddd5f" in namespace "configmap-7642" to be "success or failure"
Dec  5 06:23:29.820: INFO: Pod "pod-configmaps-d3835523-96e0-40f6-9b4d-7b11060ddd5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.851057ms
Dec  5 06:23:31.825: INFO: Pod "pod-configmaps-d3835523-96e0-40f6-9b4d-7b11060ddd5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007957835s
Dec  5 06:23:33.828: INFO: Pod "pod-configmaps-d3835523-96e0-40f6-9b4d-7b11060ddd5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010747353s
STEP: Saw pod success
Dec  5 06:23:33.828: INFO: Pod "pod-configmaps-d3835523-96e0-40f6-9b4d-7b11060ddd5f" satisfied condition "success or failure"
Dec  5 06:23:33.831: INFO: Trying to get logs from node node2 pod pod-configmaps-d3835523-96e0-40f6-9b4d-7b11060ddd5f container env-test: <nil>
STEP: delete the pod
Dec  5 06:23:33.848: INFO: Waiting for pod pod-configmaps-d3835523-96e0-40f6-9b4d-7b11060ddd5f to disappear
Dec  5 06:23:33.849: INFO: Pod pod-configmaps-d3835523-96e0-40f6-9b4d-7b11060ddd5f no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:23:33.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7642" for this suite.
Dec  5 06:23:39.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:23:39.966: INFO: namespace configmap-7642 deletion completed in 6.112316808s

• [SLOW TEST:10.182 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:23:39.966: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 06:23:40.002: INFO: Waiting up to 5m0s for pod "downwardapi-volume-393d97a9-7108-4b08-b654-7dc872a59b82" in namespace "projected-5925" to be "success or failure"
Dec  5 06:23:40.005: INFO: Pod "downwardapi-volume-393d97a9-7108-4b08-b654-7dc872a59b82": Phase="Pending", Reason="", readiness=false. Elapsed: 3.687351ms
Dec  5 06:23:42.009: INFO: Pod "downwardapi-volume-393d97a9-7108-4b08-b654-7dc872a59b82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007366677s
STEP: Saw pod success
Dec  5 06:23:42.009: INFO: Pod "downwardapi-volume-393d97a9-7108-4b08-b654-7dc872a59b82" satisfied condition "success or failure"
Dec  5 06:23:42.011: INFO: Trying to get logs from node node1 pod downwardapi-volume-393d97a9-7108-4b08-b654-7dc872a59b82 container client-container: <nil>
STEP: delete the pod
Dec  5 06:23:42.028: INFO: Waiting for pod downwardapi-volume-393d97a9-7108-4b08-b654-7dc872a59b82 to disappear
Dec  5 06:23:42.030: INFO: Pod downwardapi-volume-393d97a9-7108-4b08-b654-7dc872a59b82 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:23:42.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5925" for this suite.
Dec  5 06:23:48.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:23:48.139: INFO: namespace projected-5925 deletion completed in 6.1061582s

• [SLOW TEST:8.174 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:23:48.140: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  5 06:23:56.199: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 06:23:56.204: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 06:23:58.205: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 06:23:58.209: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 06:24:00.205: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 06:24:00.208: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 06:24:02.205: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 06:24:02.208: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 06:24:04.205: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 06:24:04.207: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 06:24:06.205: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 06:24:06.208: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 06:24:08.205: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 06:24:08.208: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 06:24:10.205: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 06:24:10.208: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 06:24:12.205: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 06:24:12.207: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 06:24:14.205: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 06:24:14.207: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  5 06:24:16.205: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  5 06:24:16.208: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:24:16.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-419" for this suite.
Dec  5 06:24:38.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:24:38.324: INFO: namespace container-lifecycle-hook-419 deletion completed in 22.100637707s

• [SLOW TEST:50.185 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:24:38.325: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec  5 06:24:38.353: INFO: PodSpec: initContainers in spec.initContainers
Dec  5 06:25:25.501: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-6d252135-fd20-4ddb-9c73-8e6977d1748f", GenerateName:"", Namespace:"init-container-9376", SelfLink:"/api/v1/namespaces/init-container-9376/pods/pod-init-6d252135-fd20-4ddb-9c73-8e6977d1748f", UID:"f9001309-d3d7-4faa-ae31-0f4c81b2b08b", ResourceVersion:"29612", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63711123878, loc:(*time.Location)(0x7ed0a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"353289131"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-z9vnv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0038dc540), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-z9vnv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-z9vnv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-z9vnv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc003684638), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003453920), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0036846c0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0036846e0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0036846e8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0036846ec), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711123878, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711123878, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711123878, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711123878, loc:(*time.Location)(0x7ed0a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.0.8", PodIP:"10.233.96.52", StartTime:(*v1.Time)(0xc00231ff00), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002d90d90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002d90e00)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://049c594edfe6dff2c9b66928131e3779e9c3e75b1c507838b50881081ad735c4"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00231ff40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00231ff20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:25:25.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9376" for this suite.
Dec  5 06:25:47.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:25:47.652: INFO: namespace init-container-9376 deletion completed in 22.145449798s

• [SLOW TEST:69.328 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:25:47.653: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-e3012f07-43c1-4ef7-9fd3-b100e28149c0
STEP: Creating configMap with name cm-test-opt-upd-69524946-cd6b-4d89-9703-9afc548cf98b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e3012f07-43c1-4ef7-9fd3-b100e28149c0
STEP: Updating configmap cm-test-opt-upd-69524946-cd6b-4d89-9703-9afc548cf98b
STEP: Creating configMap with name cm-test-opt-create-10412aa1-afb6-4f0a-8e5a-3f6174b1a512
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:25:51.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6228" for this suite.
Dec  5 06:26:13.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:26:13.911: INFO: namespace projected-6228 deletion completed in 22.118588327s

• [SLOW TEST:26.258 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:26:13.913: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec  5 06:26:16.477: INFO: Successfully updated pod "labelsupdate3f5d2769-2595-48bc-a64f-da67538d2e95"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:26:18.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8634" for this suite.
Dec  5 06:26:40.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:26:40.594: INFO: namespace downward-api-8634 deletion completed in 22.097956031s

• [SLOW TEST:26.681 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:26:40.594: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-28bed2ba-d490-4d3a-a7ec-94c1ed571566
STEP: Creating a pod to test consume configMaps
Dec  5 06:26:40.626: INFO: Waiting up to 5m0s for pod "pod-configmaps-9a9bcb54-57f7-4a06-b679-8d6963671659" in namespace "configmap-3746" to be "success or failure"
Dec  5 06:26:40.628: INFO: Pod "pod-configmaps-9a9bcb54-57f7-4a06-b679-8d6963671659": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138128ms
Dec  5 06:26:42.631: INFO: Pod "pod-configmaps-9a9bcb54-57f7-4a06-b679-8d6963671659": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005323319s
STEP: Saw pod success
Dec  5 06:26:42.631: INFO: Pod "pod-configmaps-9a9bcb54-57f7-4a06-b679-8d6963671659" satisfied condition "success or failure"
Dec  5 06:26:42.633: INFO: Trying to get logs from node node1 pod pod-configmaps-9a9bcb54-57f7-4a06-b679-8d6963671659 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 06:26:42.652: INFO: Waiting for pod pod-configmaps-9a9bcb54-57f7-4a06-b679-8d6963671659 to disappear
Dec  5 06:26:42.656: INFO: Pod pod-configmaps-9a9bcb54-57f7-4a06-b679-8d6963671659 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:26:42.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3746" for this suite.
Dec  5 06:26:48.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:26:48.777: INFO: namespace configmap-3746 deletion completed in 6.117920592s

• [SLOW TEST:8.183 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:26:48.777: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1213.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1213.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1213.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1213.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1213.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1213.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  5 06:28:14.843: INFO: DNS probes using dns-1213/dns-test-4f8d4331-32c1-40e8-a654-389282b7d439 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:28:14.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1213" for this suite.
Dec  5 06:28:20.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:28:20.959: INFO: namespace dns-1213 deletion completed in 6.105793923s

• [SLOW TEST:92.182 seconds]
[sig-network] DNS
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:28:20.960: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-d3321555-a54d-423f-aa77-23d64e9c24c3
STEP: Creating a pod to test consume configMaps
Dec  5 06:28:21.002: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4f8363a9-d512-403a-8076-f86b7f77464b" in namespace "projected-2036" to be "success or failure"
Dec  5 06:28:21.004: INFO: Pod "pod-projected-configmaps-4f8363a9-d512-403a-8076-f86b7f77464b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.547093ms
Dec  5 06:28:23.007: INFO: Pod "pod-projected-configmaps-4f8363a9-d512-403a-8076-f86b7f77464b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005748889s
STEP: Saw pod success
Dec  5 06:28:23.007: INFO: Pod "pod-projected-configmaps-4f8363a9-d512-403a-8076-f86b7f77464b" satisfied condition "success or failure"
Dec  5 06:28:23.009: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-4f8363a9-d512-403a-8076-f86b7f77464b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 06:28:23.029: INFO: Waiting for pod pod-projected-configmaps-4f8363a9-d512-403a-8076-f86b7f77464b to disappear
Dec  5 06:28:23.032: INFO: Pod pod-projected-configmaps-4f8363a9-d512-403a-8076-f86b7f77464b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:28:23.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2036" for this suite.
Dec  5 06:28:29.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:28:29.139: INFO: namespace projected-2036 deletion completed in 6.102892077s

• [SLOW TEST:8.179 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:28:29.139: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec  5 06:28:29.168: INFO: namespace kubectl-4948
Dec  5 06:28:29.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-4948'
Dec  5 06:28:29.564: INFO: stderr: ""
Dec  5 06:28:29.564: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  5 06:28:30.568: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 06:28:30.568: INFO: Found 0 / 1
Dec  5 06:28:31.567: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 06:28:31.567: INFO: Found 0 / 1
Dec  5 06:28:32.567: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 06:28:32.567: INFO: Found 0 / 1
Dec  5 06:28:33.568: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 06:28:33.568: INFO: Found 1 / 1
Dec  5 06:28:33.568: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  5 06:28:33.581: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 06:28:33.581: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  5 06:28:33.581: INFO: wait on redis-master startup in kubectl-4948 
Dec  5 06:28:33.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 logs redis-master-vdkwh redis-master --namespace=kubectl-4948'
Dec  5 06:28:33.681: INFO: stderr: ""
Dec  5 06:28:33.681: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Dec 06:28:32.298 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Dec 06:28:32.298 # Server started, Redis version 3.2.12\n1:M 05 Dec 06:28:32.298 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Dec 06:28:32.298 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec  5 06:28:33.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4948'
Dec  5 06:28:33.771: INFO: stderr: ""
Dec  5 06:28:33.771: INFO: stdout: "service/rm2 exposed\n"
Dec  5 06:28:33.777: INFO: Service rm2 in namespace kubectl-4948 found.
STEP: exposing service
Dec  5 06:28:35.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4948'
Dec  5 06:28:35.893: INFO: stderr: ""
Dec  5 06:28:35.893: INFO: stdout: "service/rm3 exposed\n"
Dec  5 06:28:35.896: INFO: Service rm3 in namespace kubectl-4948 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:28:37.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4948" for this suite.
Dec  5 06:28:59.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:29:00.016: INFO: namespace kubectl-4948 deletion completed in 22.110523289s

• [SLOW TEST:30.877 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:29:00.017: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-220fff8f-a4f6-416a-8801-420079051566 in namespace container-probe-599
Dec  5 06:29:04.054: INFO: Started pod test-webserver-220fff8f-a4f6-416a-8801-420079051566 in namespace container-probe-599
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 06:29:04.056: INFO: Initial restart count of pod test-webserver-220fff8f-a4f6-416a-8801-420079051566 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:33:04.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-599" for this suite.
Dec  5 06:33:10.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:33:10.655: INFO: namespace container-probe-599 deletion completed in 6.110775616s

• [SLOW TEST:250.638 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:33:10.656: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 06:33:10.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-258'
Dec  5 06:33:10.775: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 06:33:10.775: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Dec  5 06:33:10.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete jobs e2e-test-nginx-job --namespace=kubectl-258'
Dec  5 06:33:10.865: INFO: stderr: ""
Dec  5 06:33:10.865: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:33:10.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-258" for this suite.
Dec  5 06:33:16.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:33:16.972: INFO: namespace kubectl-258 deletion completed in 6.103476241s

• [SLOW TEST:6.316 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:33:16.972: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec  5 06:33:23.015: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-966584a2-900f-4ca6-a2e1-778d40a57bc5,GenerateName:,Namespace:events-8963,SelfLink:/api/v1/namespaces/events-8963/pods/send-events-966584a2-900f-4ca6-a2e1-778d40a57bc5,UID:158ef319-d182-4ad3-b41f-e150f9c42dd1,ResourceVersion:31088,Generation:0,CreationTimestamp:2019-12-05 06:33:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 998697475,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-58k6v {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-58k6v,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-58k6v true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002af2110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002af2130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:33:17 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:33:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:33:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:33:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:10.233.90.54,StartTime:2019-12-05 06:33:17 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-12-05 06:33:20 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://a222b48beff248e5f6cf7d50959a6c0afc5a30f25bee18247f1bf5da40523dc8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec  5 06:33:25.019: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec  5 06:33:27.023: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:33:27.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8963" for this suite.
Dec  5 06:34:05.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:34:05.137: INFO: namespace events-8963 deletion completed in 38.105011911s

• [SLOW TEST:48.165 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:34:05.138: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec  5 06:34:07.701: INFO: Successfully updated pod "labelsupdate0e4d8f9f-81e4-485e-8e3c-87ae7932b496"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:34:09.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9087" for this suite.
Dec  5 06:34:31.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:34:31.875: INFO: namespace projected-9087 deletion completed in 22.151165297s

• [SLOW TEST:26.738 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:34:31.875: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-2f9f1559-71aa-4f6f-baef-01b0ed38efb9
STEP: Creating a pod to test consume secrets
Dec  5 06:34:31.914: INFO: Waiting up to 5m0s for pod "pod-secrets-7ccf48a6-c054-44d2-a292-eb5dc9289467" in namespace "secrets-943" to be "success or failure"
Dec  5 06:34:31.918: INFO: Pod "pod-secrets-7ccf48a6-c054-44d2-a292-eb5dc9289467": Phase="Pending", Reason="", readiness=false. Elapsed: 3.764934ms
Dec  5 06:34:33.921: INFO: Pod "pod-secrets-7ccf48a6-c054-44d2-a292-eb5dc9289467": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006685143s
STEP: Saw pod success
Dec  5 06:34:33.921: INFO: Pod "pod-secrets-7ccf48a6-c054-44d2-a292-eb5dc9289467" satisfied condition "success or failure"
Dec  5 06:34:33.923: INFO: Trying to get logs from node node1 pod pod-secrets-7ccf48a6-c054-44d2-a292-eb5dc9289467 container secret-env-test: <nil>
STEP: delete the pod
Dec  5 06:34:33.941: INFO: Waiting for pod pod-secrets-7ccf48a6-c054-44d2-a292-eb5dc9289467 to disappear
Dec  5 06:34:33.943: INFO: Pod pod-secrets-7ccf48a6-c054-44d2-a292-eb5dc9289467 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:34:33.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-943" for this suite.
Dec  5 06:34:39.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:34:40.056: INFO: namespace secrets-943 deletion completed in 6.11012254s

• [SLOW TEST:8.181 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:34:40.056: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec  5 06:34:40.085: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  5 06:34:40.099: INFO: Waiting for terminating namespaces to be deleted...
Dec  5 06:34:40.101: INFO: 
Logging pods the kubelet thinks is on node node1 before test
Dec  5 06:34:40.113: INFO: prometheus-k8s-1 from kubesphere-monitoring-system started at 2019-12-05 04:05:04 +0000 UTC (3 container statuses recorded)
Dec  5 06:34:40.113: INFO: 	Container prometheus ready: true, restart count 1
Dec  5 06:34:40.113: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec  5 06:34:40.113: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec  5 06:34:40.114: INFO: openebs-ndm-dd2vs from kube-system started at 2019-12-05 04:00:08 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.114: INFO: 	Container node-disk-manager ready: true, restart count 0
Dec  5 06:34:40.114: INFO: sonobuoy-systemd-logs-daemon-set-4e7bfb56bab946ed-6dtlb from sonobuoy started at 2019-12-05 05:58:13 +0000 UTC (2 container statuses recorded)
Dec  5 06:34:40.114: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 06:34:40.114: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  5 06:34:40.114: INFO: prometheus-operator-685bc484cb-k5cmh from kubesphere-monitoring-system started at 2019-12-05 04:01:41 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.114: INFO: 	Container prometheus-operator ready: true, restart count 0
Dec  5 06:34:40.114: INFO: nodelocaldns-4cspq from kube-system started at 2019-12-05 03:17:33 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.114: INFO: 	Container node-cache ready: true, restart count 0
Dec  5 06:34:40.114: INFO: node-exporter-n87mk from kubesphere-monitoring-system started at 2019-12-05 04:01:42 +0000 UTC (2 container statuses recorded)
Dec  5 06:34:40.114: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  5 06:34:40.114: INFO: 	Container node-exporter ready: true, restart count 0
Dec  5 06:34:40.114: INFO: prometheus-k8s-system-1 from kubesphere-monitoring-system started at 2019-12-05 04:05:13 +0000 UTC (3 container statuses recorded)
Dec  5 06:34:40.114: INFO: 	Container prometheus ready: true, restart count 1
Dec  5 06:34:40.114: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec  5 06:34:40.114: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec  5 06:34:40.114: INFO: openebs-ndm-operator-69cbc86975-r5pph from kube-system started at 2019-12-05 04:00:08 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.114: INFO: 	Container node-disk-operator ready: true, restart count 1
Dec  5 06:34:40.114: INFO: kube-state-metrics-dfdf4d964-pdd5z from kubesphere-monitoring-system started at 2019-12-05 04:10:38 +0000 UTC (4 container statuses recorded)
Dec  5 06:34:40.114: INFO: 	Container addon-resizer ready: true, restart count 0
Dec  5 06:34:40.114: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Dec  5 06:34:40.114: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Dec  5 06:34:40.114: INFO: 	Container kube-state-metrics ready: true, restart count 0
Dec  5 06:34:40.114: INFO: ks-installer-7987c659d6-hljj9 from kubesphere-system started at 2019-12-05 04:00:17 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.114: INFO: 	Container installer ready: true, restart count 0
Dec  5 06:34:40.114: INFO: kube-proxy-bbf7w from kube-system started at 2019-12-05 03:16:38 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.114: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  5 06:34:40.114: INFO: tiller-deploy-55fc49c595-wb92t from kube-system started at 2019-12-05 03:25:52 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.114: INFO: 	Container tiller ready: true, restart count 0
Dec  5 06:34:40.114: INFO: calico-node-p48b5 from kube-system started at 2019-12-05 03:16:55 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.114: INFO: 	Container calico-node ready: true, restart count 0
Dec  5 06:34:40.114: INFO: 
Logging pods the kubelet thinks is on node node2 before test
Dec  5 06:34:40.125: INFO: nodelocaldns-g7lh5 from kube-system started at 2019-12-05 03:17:33 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.125: INFO: 	Container node-cache ready: true, restart count 0
Dec  5 06:34:40.125: INFO: sonobuoy-e2e-job-4d426e35b88a483d from sonobuoy started at 2019-12-05 05:58:13 +0000 UTC (2 container statuses recorded)
Dec  5 06:34:40.125: INFO: 	Container e2e ready: true, restart count 0
Dec  5 06:34:40.125: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 06:34:40.125: INFO: kube-proxy-68pbh from kube-system started at 2019-12-05 03:16:42 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.125: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  5 06:34:40.125: INFO: calico-node-x7nbv from kube-system started at 2019-12-05 03:16:55 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.125: INFO: 	Container calico-node ready: true, restart count 0
Dec  5 06:34:40.125: INFO: prometheus-k8s-0 from kubesphere-monitoring-system started at 2019-12-05 04:06:32 +0000 UTC (3 container statuses recorded)
Dec  5 06:34:40.126: INFO: 	Container prometheus ready: true, restart count 1
Dec  5 06:34:40.126: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec  5 06:34:40.126: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec  5 06:34:40.126: INFO: openebs-ndm-92tm6 from kube-system started at 2019-12-05 04:00:08 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.126: INFO: 	Container node-disk-manager ready: true, restart count 0
Dec  5 06:34:40.126: INFO: sonobuoy-systemd-logs-daemon-set-4e7bfb56bab946ed-65798 from sonobuoy started at 2019-12-05 05:58:13 +0000 UTC (2 container statuses recorded)
Dec  5 06:34:40.126: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  5 06:34:40.126: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  5 06:34:40.126: INFO: default-http-backend-6555ff6898-qz96c from kubesphere-controls-system started at 2019-12-05 04:01:04 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.126: INFO: 	Container default-http-backend ready: true, restart count 0
Dec  5 06:34:40.126: INFO: prometheus-k8s-system-0 from kubesphere-monitoring-system started at 2019-12-05 04:07:26 +0000 UTC (3 container statuses recorded)
Dec  5 06:34:40.126: INFO: 	Container prometheus ready: true, restart count 1
Dec  5 06:34:40.126: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Dec  5 06:34:40.126: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Dec  5 06:34:40.126: INFO: sonobuoy from sonobuoy started at 2019-12-05 05:56:52 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.126: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  5 06:34:40.126: INFO: node-exporter-xkgf6 from kubesphere-monitoring-system started at 2019-12-05 04:01:42 +0000 UTC (2 container statuses recorded)
Dec  5 06:34:40.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec  5 06:34:40.126: INFO: 	Container node-exporter ready: true, restart count 0
Dec  5 06:34:40.126: INFO: kubectl-admin-74fdfc47c7-r5wx2 from kubesphere-controls-system started at 2019-12-05 04:08:47 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.126: INFO: 	Container kubectl ready: true, restart count 0
Dec  5 06:34:40.126: INFO: openebs-localpv-provisioner-7b55587dbd-p5ktt from kube-system started at 2019-12-05 04:00:08 +0000 UTC (1 container statuses recorded)
Dec  5 06:34:40.126: INFO: 	Container openebs-localpv-provisioner ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15dd6613dd919063], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:34:41.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-240" for this suite.
Dec  5 06:34:47.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:34:47.280: INFO: namespace sched-pred-240 deletion completed in 6.114975288s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.223 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:34:47.280: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-42e2c23d-23d3-4ab4-8250-61894d0c9c10
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:34:47.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5922" for this suite.
Dec  5 06:34:53.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:34:53.417: INFO: namespace configmap-5922 deletion completed in 6.103061464s

• [SLOW TEST:6.137 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:34:53.418: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 06:34:53.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6d85ce1-c33d-40b6-bdbf-8982909cf284" in namespace "downward-api-8641" to be "success or failure"
Dec  5 06:34:53.451: INFO: Pod "downwardapi-volume-f6d85ce1-c33d-40b6-bdbf-8982909cf284": Phase="Pending", Reason="", readiness=false. Elapsed: 2.842554ms
Dec  5 06:34:55.455: INFO: Pod "downwardapi-volume-f6d85ce1-c33d-40b6-bdbf-8982909cf284": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006573758s
STEP: Saw pod success
Dec  5 06:34:55.455: INFO: Pod "downwardapi-volume-f6d85ce1-c33d-40b6-bdbf-8982909cf284" satisfied condition "success or failure"
Dec  5 06:34:55.458: INFO: Trying to get logs from node node2 pod downwardapi-volume-f6d85ce1-c33d-40b6-bdbf-8982909cf284 container client-container: <nil>
STEP: delete the pod
Dec  5 06:34:55.478: INFO: Waiting for pod downwardapi-volume-f6d85ce1-c33d-40b6-bdbf-8982909cf284 to disappear
Dec  5 06:34:55.480: INFO: Pod downwardapi-volume-f6d85ce1-c33d-40b6-bdbf-8982909cf284 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:34:55.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8641" for this suite.
Dec  5 06:35:01.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:35:01.596: INFO: namespace downward-api-8641 deletion completed in 6.113313418s

• [SLOW TEST:8.179 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:35:01.596: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:35:03.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9723" for this suite.
Dec  5 06:35:53.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:35:53.770: INFO: namespace kubelet-test-9723 deletion completed in 50.118795433s

• [SLOW TEST:52.173 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:35:53.770: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec  5 06:35:58.322: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5703 pod-service-account-8571b4fc-50b1-433d-a82f-0875015af926 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec  5 06:35:58.595: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5703 pod-service-account-8571b4fc-50b1-433d-a82f-0875015af926 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec  5 06:35:58.851: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5703 pod-service-account-8571b4fc-50b1-433d-a82f-0875015af926 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:35:59.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5703" for this suite.
Dec  5 06:36:05.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:36:05.229: INFO: namespace svcaccounts-5703 deletion completed in 6.100627702s

• [SLOW TEST:11.459 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:36:05.229: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-676ab75d-ba5c-4df4-b630-a729acec29e2
STEP: Creating a pod to test consume secrets
Dec  5 06:36:05.262: INFO: Waiting up to 5m0s for pod "pod-secrets-323d34b4-71f1-44a4-b100-25b63d1347b6" in namespace "secrets-902" to be "success or failure"
Dec  5 06:36:05.265: INFO: Pod "pod-secrets-323d34b4-71f1-44a4-b100-25b63d1347b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.494155ms
Dec  5 06:36:07.268: INFO: Pod "pod-secrets-323d34b4-71f1-44a4-b100-25b63d1347b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005768214s
STEP: Saw pod success
Dec  5 06:36:07.268: INFO: Pod "pod-secrets-323d34b4-71f1-44a4-b100-25b63d1347b6" satisfied condition "success or failure"
Dec  5 06:36:07.270: INFO: Trying to get logs from node node1 pod pod-secrets-323d34b4-71f1-44a4-b100-25b63d1347b6 container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 06:36:07.288: INFO: Waiting for pod pod-secrets-323d34b4-71f1-44a4-b100-25b63d1347b6 to disappear
Dec  5 06:36:07.290: INFO: Pod pod-secrets-323d34b4-71f1-44a4-b100-25b63d1347b6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:36:07.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-902" for this suite.
Dec  5 06:36:13.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:36:13.407: INFO: namespace secrets-902 deletion completed in 6.112329019s

• [SLOW TEST:8.178 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:36:13.408: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 06:36:13.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b5c86082-fe39-44f8-b479-4752f562290e" in namespace "projected-6355" to be "success or failure"
Dec  5 06:36:13.444: INFO: Pod "downwardapi-volume-b5c86082-fe39-44f8-b479-4752f562290e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.186673ms
Dec  5 06:36:15.449: INFO: Pod "downwardapi-volume-b5c86082-fe39-44f8-b479-4752f562290e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007919318s
STEP: Saw pod success
Dec  5 06:36:15.449: INFO: Pod "downwardapi-volume-b5c86082-fe39-44f8-b479-4752f562290e" satisfied condition "success or failure"
Dec  5 06:36:15.451: INFO: Trying to get logs from node node2 pod downwardapi-volume-b5c86082-fe39-44f8-b479-4752f562290e container client-container: <nil>
STEP: delete the pod
Dec  5 06:36:15.469: INFO: Waiting for pod downwardapi-volume-b5c86082-fe39-44f8-b479-4752f562290e to disappear
Dec  5 06:36:15.472: INFO: Pod downwardapi-volume-b5c86082-fe39-44f8-b479-4752f562290e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:36:15.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6355" for this suite.
Dec  5 06:36:21.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:36:21.592: INFO: namespace projected-6355 deletion completed in 6.117444696s

• [SLOW TEST:8.185 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:36:21.593: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec  5 06:36:21.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-2473'
Dec  5 06:36:21.836: INFO: stderr: ""
Dec  5 06:36:21.836: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 06:36:21.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2473'
Dec  5 06:36:21.917: INFO: stderr: ""
Dec  5 06:36:21.917: INFO: stdout: "update-demo-nautilus-bpq68 update-demo-nautilus-r4fm6 "
Dec  5 06:36:21.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-bpq68 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2473'
Dec  5 06:36:21.987: INFO: stderr: ""
Dec  5 06:36:21.987: INFO: stdout: ""
Dec  5 06:36:21.987: INFO: update-demo-nautilus-bpq68 is created but not running
Dec  5 06:36:26.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2473'
Dec  5 06:36:27.103: INFO: stderr: ""
Dec  5 06:36:27.103: INFO: stdout: "update-demo-nautilus-bpq68 update-demo-nautilus-r4fm6 "
Dec  5 06:36:27.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-bpq68 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2473'
Dec  5 06:36:27.198: INFO: stderr: ""
Dec  5 06:36:27.198: INFO: stdout: "true"
Dec  5 06:36:27.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-bpq68 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2473'
Dec  5 06:36:27.308: INFO: stderr: ""
Dec  5 06:36:27.308: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 06:36:27.308: INFO: validating pod update-demo-nautilus-bpq68
Dec  5 06:36:27.313: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 06:36:27.313: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 06:36:27.313: INFO: update-demo-nautilus-bpq68 is verified up and running
Dec  5 06:36:27.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-r4fm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2473'
Dec  5 06:36:27.380: INFO: stderr: ""
Dec  5 06:36:27.380: INFO: stdout: "true"
Dec  5 06:36:27.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-r4fm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2473'
Dec  5 06:36:27.461: INFO: stderr: ""
Dec  5 06:36:27.461: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 06:36:27.461: INFO: validating pod update-demo-nautilus-r4fm6
Dec  5 06:36:27.482: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 06:36:27.482: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 06:36:27.482: INFO: update-demo-nautilus-r4fm6 is verified up and running
STEP: scaling down the replication controller
Dec  5 06:36:27.484: INFO: scanned /root for discovery docs: <nil>
Dec  5 06:36:27.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-2473'
Dec  5 06:36:28.608: INFO: stderr: ""
Dec  5 06:36:28.608: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 06:36:28.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2473'
Dec  5 06:36:28.709: INFO: stderr: ""
Dec  5 06:36:28.709: INFO: stdout: "update-demo-nautilus-bpq68 update-demo-nautilus-r4fm6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  5 06:36:33.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2473'
Dec  5 06:36:33.798: INFO: stderr: ""
Dec  5 06:36:33.798: INFO: stdout: "update-demo-nautilus-r4fm6 "
Dec  5 06:36:33.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-r4fm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2473'
Dec  5 06:36:33.872: INFO: stderr: ""
Dec  5 06:36:33.872: INFO: stdout: "true"
Dec  5 06:36:33.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-r4fm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2473'
Dec  5 06:36:33.948: INFO: stderr: ""
Dec  5 06:36:33.948: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 06:36:33.948: INFO: validating pod update-demo-nautilus-r4fm6
Dec  5 06:36:33.951: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 06:36:33.951: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 06:36:33.951: INFO: update-demo-nautilus-r4fm6 is verified up and running
STEP: scaling up the replication controller
Dec  5 06:36:33.952: INFO: scanned /root for discovery docs: <nil>
Dec  5 06:36:33.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-2473'
Dec  5 06:36:35.072: INFO: stderr: ""
Dec  5 06:36:35.072: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 06:36:35.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2473'
Dec  5 06:36:35.162: INFO: stderr: ""
Dec  5 06:36:35.162: INFO: stdout: "update-demo-nautilus-lw64p update-demo-nautilus-r4fm6 "
Dec  5 06:36:35.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-lw64p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2473'
Dec  5 06:36:35.230: INFO: stderr: ""
Dec  5 06:36:35.230: INFO: stdout: ""
Dec  5 06:36:35.230: INFO: update-demo-nautilus-lw64p is created but not running
Dec  5 06:36:40.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2473'
Dec  5 06:36:40.339: INFO: stderr: ""
Dec  5 06:36:40.339: INFO: stdout: "update-demo-nautilus-lw64p update-demo-nautilus-r4fm6 "
Dec  5 06:36:40.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-lw64p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2473'
Dec  5 06:36:40.416: INFO: stderr: ""
Dec  5 06:36:40.416: INFO: stdout: "true"
Dec  5 06:36:40.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-lw64p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2473'
Dec  5 06:36:40.498: INFO: stderr: ""
Dec  5 06:36:40.498: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 06:36:40.498: INFO: validating pod update-demo-nautilus-lw64p
Dec  5 06:36:40.503: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 06:36:40.503: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 06:36:40.503: INFO: update-demo-nautilus-lw64p is verified up and running
Dec  5 06:36:40.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-r4fm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2473'
Dec  5 06:36:40.598: INFO: stderr: ""
Dec  5 06:36:40.598: INFO: stdout: "true"
Dec  5 06:36:40.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-r4fm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2473'
Dec  5 06:36:40.682: INFO: stderr: ""
Dec  5 06:36:40.682: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 06:36:40.682: INFO: validating pod update-demo-nautilus-r4fm6
Dec  5 06:36:40.684: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 06:36:40.685: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 06:36:40.685: INFO: update-demo-nautilus-r4fm6 is verified up and running
STEP: using delete to clean up resources
Dec  5 06:36:40.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete --grace-period=0 --force -f - --namespace=kubectl-2473'
Dec  5 06:36:40.767: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 06:36:40.767: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  5 06:36:40.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2473'
Dec  5 06:36:40.885: INFO: stderr: "No resources found.\n"
Dec  5 06:36:40.885: INFO: stdout: ""
Dec  5 06:36:40.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -l name=update-demo --namespace=kubectl-2473 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 06:36:40.975: INFO: stderr: ""
Dec  5 06:36:40.975: INFO: stdout: "update-demo-nautilus-lw64p\nupdate-demo-nautilus-r4fm6\n"
Dec  5 06:36:41.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2473'
Dec  5 06:36:41.570: INFO: stderr: "No resources found.\n"
Dec  5 06:36:41.570: INFO: stdout: ""
Dec  5 06:36:41.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -l name=update-demo --namespace=kubectl-2473 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 06:36:41.682: INFO: stderr: ""
Dec  5 06:36:41.682: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:36:41.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2473" for this suite.
Dec  5 06:37:03.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:37:03.795: INFO: namespace kubectl-2473 deletion completed in 22.107912566s

• [SLOW TEST:42.202 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:37:03.796: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  5 06:37:03.848: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 06:37:03.850: INFO: Number of nodes with available pods: 0
Dec  5 06:37:03.850: INFO: Node node1 is running more than one daemon pod
Dec  5 06:37:04.854: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 06:37:04.856: INFO: Number of nodes with available pods: 0
Dec  5 06:37:04.856: INFO: Node node1 is running more than one daemon pod
Dec  5 06:37:05.855: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 06:37:05.858: INFO: Number of nodes with available pods: 2
Dec  5 06:37:05.858: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec  5 06:37:05.870: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 06:37:05.873: INFO: Number of nodes with available pods: 1
Dec  5 06:37:05.873: INFO: Node node2 is running more than one daemon pod
Dec  5 06:37:06.877: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 06:37:06.879: INFO: Number of nodes with available pods: 1
Dec  5 06:37:06.879: INFO: Node node2 is running more than one daemon pod
Dec  5 06:37:07.878: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 06:37:07.880: INFO: Number of nodes with available pods: 2
Dec  5 06:37:07.880: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3177, will wait for the garbage collector to delete the pods
Dec  5 06:37:07.943: INFO: Deleting DaemonSet.extensions daemon-set took: 5.430178ms
Dec  5 06:37:08.643: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.282184ms
Dec  5 06:37:19.046: INFO: Number of nodes with available pods: 0
Dec  5 06:37:19.046: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 06:37:19.052: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3177/daemonsets","resourceVersion":"32123"},"items":null}

Dec  5 06:37:19.058: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3177/pods","resourceVersion":"32123"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:37:19.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3177" for this suite.
Dec  5 06:37:25.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:37:25.202: INFO: namespace daemonsets-3177 deletion completed in 6.105004584s

• [SLOW TEST:21.406 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:37:25.203: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  5 06:37:25.235: INFO: Waiting up to 5m0s for pod "pod-d6adeabf-7cc9-4137-85b1-9cd6729776cd" in namespace "emptydir-84" to be "success or failure"
Dec  5 06:37:25.240: INFO: Pod "pod-d6adeabf-7cc9-4137-85b1-9cd6729776cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.882594ms
Dec  5 06:37:27.244: INFO: Pod "pod-d6adeabf-7cc9-4137-85b1-9cd6729776cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008675722s
Dec  5 06:37:29.247: INFO: Pod "pod-d6adeabf-7cc9-4137-85b1-9cd6729776cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01170051s
STEP: Saw pod success
Dec  5 06:37:29.247: INFO: Pod "pod-d6adeabf-7cc9-4137-85b1-9cd6729776cd" satisfied condition "success or failure"
Dec  5 06:37:29.248: INFO: Trying to get logs from node node2 pod pod-d6adeabf-7cc9-4137-85b1-9cd6729776cd container test-container: <nil>
STEP: delete the pod
Dec  5 06:37:29.264: INFO: Waiting for pod pod-d6adeabf-7cc9-4137-85b1-9cd6729776cd to disappear
Dec  5 06:37:29.267: INFO: Pod pod-d6adeabf-7cc9-4137-85b1-9cd6729776cd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:37:29.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-84" for this suite.
Dec  5 06:37:35.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:37:35.381: INFO: namespace emptydir-84 deletion completed in 6.110338107s

• [SLOW TEST:10.178 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:37:35.382: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:37:59.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-819" for this suite.
Dec  5 06:38:05.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:38:05.581: INFO: namespace namespaces-819 deletion completed in 6.103984225s
STEP: Destroying namespace "nsdeletetest-6094" for this suite.
Dec  5 06:38:05.582: INFO: Namespace nsdeletetest-6094 was already deleted
STEP: Destroying namespace "nsdeletetest-8553" for this suite.
Dec  5 06:38:11.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:38:11.721: INFO: namespace nsdeletetest-8553 deletion completed in 6.138154121s

• [SLOW TEST:36.339 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:38:11.722: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-d83409f7-beb5-49b0-a28d-8eb84e50ebae
STEP: Creating a pod to test consume configMaps
Dec  5 06:38:11.763: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5fb9fdb8-fc65-4dfd-a8b3-eb70aec72f3c" in namespace "projected-2916" to be "success or failure"
Dec  5 06:38:11.765: INFO: Pod "pod-projected-configmaps-5fb9fdb8-fc65-4dfd-a8b3-eb70aec72f3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.337141ms
Dec  5 06:38:13.769: INFO: Pod "pod-projected-configmaps-5fb9fdb8-fc65-4dfd-a8b3-eb70aec72f3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006087034s
STEP: Saw pod success
Dec  5 06:38:13.769: INFO: Pod "pod-projected-configmaps-5fb9fdb8-fc65-4dfd-a8b3-eb70aec72f3c" satisfied condition "success or failure"
Dec  5 06:38:13.771: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-5fb9fdb8-fc65-4dfd-a8b3-eb70aec72f3c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 06:38:13.790: INFO: Waiting for pod pod-projected-configmaps-5fb9fdb8-fc65-4dfd-a8b3-eb70aec72f3c to disappear
Dec  5 06:38:13.791: INFO: Pod pod-projected-configmaps-5fb9fdb8-fc65-4dfd-a8b3-eb70aec72f3c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:38:13.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2916" for this suite.
Dec  5 06:38:19.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:38:19.921: INFO: namespace projected-2916 deletion completed in 6.125412947s

• [SLOW TEST:8.199 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:38:19.922: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 06:38:19.964: INFO: Waiting up to 5m0s for pod "downwardapi-volume-faad1702-53aa-47a0-b2de-b5b10b5fd3f3" in namespace "downward-api-8197" to be "success or failure"
Dec  5 06:38:19.969: INFO: Pod "downwardapi-volume-faad1702-53aa-47a0-b2de-b5b10b5fd3f3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.871853ms
Dec  5 06:38:21.972: INFO: Pod "downwardapi-volume-faad1702-53aa-47a0-b2de-b5b10b5fd3f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008241069s
STEP: Saw pod success
Dec  5 06:38:21.972: INFO: Pod "downwardapi-volume-faad1702-53aa-47a0-b2de-b5b10b5fd3f3" satisfied condition "success or failure"
Dec  5 06:38:21.974: INFO: Trying to get logs from node node1 pod downwardapi-volume-faad1702-53aa-47a0-b2de-b5b10b5fd3f3 container client-container: <nil>
STEP: delete the pod
Dec  5 06:38:21.990: INFO: Waiting for pod downwardapi-volume-faad1702-53aa-47a0-b2de-b5b10b5fd3f3 to disappear
Dec  5 06:38:21.991: INFO: Pod downwardapi-volume-faad1702-53aa-47a0-b2de-b5b10b5fd3f3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:38:21.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8197" for this suite.
Dec  5 06:38:28.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:38:28.108: INFO: namespace downward-api-8197 deletion completed in 6.112990994s

• [SLOW TEST:8.186 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:38:28.108: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:38:28.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3927" for this suite.
Dec  5 06:38:50.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:38:50.250: INFO: namespace pods-3927 deletion completed in 22.107416212s

• [SLOW TEST:22.142 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:38:50.251: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-b6f7326d-6b17-47ba-a5ab-67c5652e7e44 in namespace container-probe-5657
Dec  5 06:38:52.291: INFO: Started pod busybox-b6f7326d-6b17-47ba-a5ab-67c5652e7e44 in namespace container-probe-5657
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 06:38:52.293: INFO: Initial restart count of pod busybox-b6f7326d-6b17-47ba-a5ab-67c5652e7e44 is 0
Dec  5 06:39:42.388: INFO: Restart count of pod container-probe-5657/busybox-b6f7326d-6b17-47ba-a5ab-67c5652e7e44 is now 1 (50.094556677s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:39:42.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5657" for this suite.
Dec  5 06:39:48.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:39:48.517: INFO: namespace container-probe-5657 deletion completed in 6.11474041s

• [SLOW TEST:58.266 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:39:48.517: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Dec  5 06:39:48.546: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec  5 06:39:48.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-9795'
Dec  5 06:39:48.902: INFO: stderr: ""
Dec  5 06:39:48.902: INFO: stdout: "service/redis-slave created\n"
Dec  5 06:39:48.902: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec  5 06:39:48.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-9795'
Dec  5 06:39:49.088: INFO: stderr: ""
Dec  5 06:39:49.088: INFO: stdout: "service/redis-master created\n"
Dec  5 06:39:49.089: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  5 06:39:49.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-9795'
Dec  5 06:39:49.271: INFO: stderr: ""
Dec  5 06:39:49.271: INFO: stdout: "service/frontend created\n"
Dec  5 06:39:49.271: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec  5 06:39:49.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-9795'
Dec  5 06:39:49.454: INFO: stderr: ""
Dec  5 06:39:49.454: INFO: stdout: "deployment.apps/frontend created\n"
Dec  5 06:39:49.454: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  5 06:39:49.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-9795'
Dec  5 06:39:49.673: INFO: stderr: ""
Dec  5 06:39:49.673: INFO: stdout: "deployment.apps/redis-master created\n"
Dec  5 06:39:49.673: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec  5 06:39:49.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-9795'
Dec  5 06:39:49.912: INFO: stderr: ""
Dec  5 06:39:49.912: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec  5 06:39:49.912: INFO: Waiting for all frontend pods to be Running.
Dec  5 06:43:24.974: INFO: Waiting for frontend to serve content.
Dec  5 06:43:25.003: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:43:30.030: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:43:35.066: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:43:40.092: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:43:45.115: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:43:50.146: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:43:55.168: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:44:00.197: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:44:05.224: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:44:10.243: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:44:15.260: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:44:20.282: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:44:25.309: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  5 06:44:30.321: INFO: Trying to add a new entry to the guestbook.
Dec  5 06:44:30.330: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec  5 06:44:30.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete --grace-period=0 --force -f - --namespace=kubectl-9795'
Dec  5 06:44:30.448: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 06:44:30.448: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 06:44:30.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete --grace-period=0 --force -f - --namespace=kubectl-9795'
Dec  5 06:44:30.560: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 06:44:30.560: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 06:44:30.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete --grace-period=0 --force -f - --namespace=kubectl-9795'
Dec  5 06:44:30.656: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 06:44:30.656: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 06:44:30.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete --grace-period=0 --force -f - --namespace=kubectl-9795'
Dec  5 06:44:30.739: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 06:44:30.739: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 06:44:30.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete --grace-period=0 --force -f - --namespace=kubectl-9795'
Dec  5 06:44:30.817: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 06:44:30.817: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  5 06:44:30.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete --grace-period=0 --force -f - --namespace=kubectl-9795'
Dec  5 06:44:30.919: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 06:44:30.919: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:44:30.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9795" for this suite.
Dec  5 06:45:16.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:45:17.022: INFO: namespace kubectl-9795 deletion completed in 46.099044081s

• [SLOW TEST:328.504 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:45:17.022: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 06:45:17.053: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec  5 06:45:22.057: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  5 06:45:22.057: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  5 06:45:24.061: INFO: Creating deployment "test-rollover-deployment"
Dec  5 06:45:24.066: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  5 06:45:26.072: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  5 06:45:26.078: INFO: Ensure that both replica sets have 1 created replica
Dec  5 06:45:26.083: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  5 06:45:26.089: INFO: Updating deployment test-rollover-deployment
Dec  5 06:45:26.089: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  5 06:45:28.094: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  5 06:45:28.098: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  5 06:45:28.102: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 06:45:28.102: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125127, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 06:45:30.110: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 06:45:30.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125127, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 06:45:32.110: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 06:45:32.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125127, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 06:45:34.108: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 06:45:34.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125127, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 06:45:36.108: INFO: all replica sets need to contain the pod-template-hash label
Dec  5 06:45:36.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125127, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125124, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 06:45:38.108: INFO: 
Dec  5 06:45:38.108: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  5 06:45:38.123: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-827,SelfLink:/apis/apps/v1/namespaces/deployment-827/deployments/test-rollover-deployment,UID:e1905054-622e-4080-8bb3-27fba0e6465d,ResourceVersion:33850,Generation:2,CreationTimestamp:2019-12-05 06:45:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-05 06:45:24 +0000 UTC 2019-12-05 06:45:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-05 06:45:37 +0000 UTC 2019-12-05 06:45:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  5 06:45:38.126: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-827,SelfLink:/apis/apps/v1/namespaces/deployment-827/replicasets/test-rollover-deployment-854595fc44,UID:b19cfc19-61c2-4962-b096-44aa3db97906,ResourceVersion:33839,Generation:2,CreationTimestamp:2019-12-05 06:45:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e1905054-622e-4080-8bb3-27fba0e6465d 0xc0031dbab7 0xc0031dbab8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  5 06:45:38.126: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  5 06:45:38.129: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-827,SelfLink:/apis/apps/v1/namespaces/deployment-827/replicasets/test-rollover-controller,UID:22b041e7-e721-4741-8cab-c55785a03839,ResourceVersion:33849,Generation:2,CreationTimestamp:2019-12-05 06:45:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e1905054-622e-4080-8bb3-27fba0e6465d 0xc0031db9e7 0xc0031db9e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 06:45:38.129: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-827,SelfLink:/apis/apps/v1/namespaces/deployment-827/replicasets/test-rollover-deployment-9b8b997cf,UID:6dd8b006-4001-4647-bebb-e90bffc88abc,ResourceVersion:33792,Generation:2,CreationTimestamp:2019-12-05 06:45:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e1905054-622e-4080-8bb3-27fba0e6465d 0xc0031dbb80 0xc0031dbb81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 06:45:38.132: INFO: Pod "test-rollover-deployment-854595fc44-66dvm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-66dvm,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-827,SelfLink:/api/v1/namespaces/deployment-827/pods/test-rollover-deployment-854595fc44-66dvm,UID:b841a12a-f2e2-43a9-97f3-0c39968f46b2,ResourceVersion:33814,Generation:0,CreationTimestamp:2019-12-05 06:45:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 b19cfc19-61c2-4962-b096-44aa3db97906 0xc0037de767 0xc0037de768}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rllsb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rllsb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-rllsb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037de7e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037de800}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:45:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:45:27 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:45:27 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:45:26 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:10.233.96.71,StartTime:2019-12-05 06:45:26 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-05 06:45:27 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://1a4550206432cce5cc06446f00dcf8b593712096ebef1d1b9b7bcf3e327fc459}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:45:38.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-827" for this suite.
Dec  5 06:45:44.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:45:44.250: INFO: namespace deployment-827 deletion completed in 6.114422602s

• [SLOW TEST:27.228 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:45:44.250: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  5 06:45:44.282: INFO: Waiting up to 5m0s for pod "pod-044f19b3-70e2-4273-b74c-9dfbbb5eae64" in namespace "emptydir-2703" to be "success or failure"
Dec  5 06:45:44.284: INFO: Pod "pod-044f19b3-70e2-4273-b74c-9dfbbb5eae64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.833951ms
Dec  5 06:45:46.288: INFO: Pod "pod-044f19b3-70e2-4273-b74c-9dfbbb5eae64": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00591188s
Dec  5 06:45:48.290: INFO: Pod "pod-044f19b3-70e2-4273-b74c-9dfbbb5eae64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008198019s
STEP: Saw pod success
Dec  5 06:45:48.290: INFO: Pod "pod-044f19b3-70e2-4273-b74c-9dfbbb5eae64" satisfied condition "success or failure"
Dec  5 06:45:48.291: INFO: Trying to get logs from node node1 pod pod-044f19b3-70e2-4273-b74c-9dfbbb5eae64 container test-container: <nil>
STEP: delete the pod
Dec  5 06:45:48.307: INFO: Waiting for pod pod-044f19b3-70e2-4273-b74c-9dfbbb5eae64 to disappear
Dec  5 06:45:48.308: INFO: Pod pod-044f19b3-70e2-4273-b74c-9dfbbb5eae64 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:45:48.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2703" for this suite.
Dec  5 06:45:54.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:45:54.421: INFO: namespace emptydir-2703 deletion completed in 6.109787155s

• [SLOW TEST:10.171 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:45:54.422: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 06:45:54.448: INFO: Creating deployment "test-recreate-deployment"
Dec  5 06:45:54.451: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  5 06:45:54.458: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec  5 06:45:56.463: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  5 06:45:56.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125154, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125154, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125154, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711125154, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 06:45:58.469: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  5 06:45:58.474: INFO: Updating deployment test-recreate-deployment
Dec  5 06:45:58.474: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  5 06:45:58.518: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-4618,SelfLink:/apis/apps/v1/namespaces/deployment-4618/deployments/test-recreate-deployment,UID:62251cf9-74d3-4cfd-9b15-9b74a9e41dfd,ResourceVersion:34027,Generation:2,CreationTimestamp:2019-12-05 06:45:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-12-05 06:45:58 +0000 UTC 2019-12-05 06:45:58 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-05 06:45:58 +0000 UTC 2019-12-05 06:45:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec  5 06:45:58.522: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-4618,SelfLink:/apis/apps/v1/namespaces/deployment-4618/replicasets/test-recreate-deployment-5c8c9cc69d,UID:25792004-6f0a-45f7-998c-26b976ae04b2,ResourceVersion:34025,Generation:1,CreationTimestamp:2019-12-05 06:45:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 62251cf9-74d3-4cfd-9b15-9b74a9e41dfd 0xc00205d407 0xc00205d408}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 06:45:58.522: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  5 06:45:58.522: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-4618,SelfLink:/apis/apps/v1/namespaces/deployment-4618/replicasets/test-recreate-deployment-6df85df6b9,UID:fa25b493-f8ec-4fe4-9459-11f4d8c92f55,ResourceVersion:34016,Generation:2,CreationTimestamp:2019-12-05 06:45:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 62251cf9-74d3-4cfd-9b15-9b74a9e41dfd 0xc00205d4d7 0xc00205d4d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 06:45:58.526: INFO: Pod "test-recreate-deployment-5c8c9cc69d-9wbqg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-9wbqg,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-4618,SelfLink:/api/v1/namespaces/deployment-4618/pods/test-recreate-deployment-5c8c9cc69d-9wbqg,UID:0e543edd-1638-4e4a-bb3d-c292faad4c62,ResourceVersion:34028,Generation:0,CreationTimestamp:2019-12-05 06:45:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 25792004-6f0a-45f7-998c-26b976ae04b2 0xc002b836e7 0xc002b836e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dfp9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dfp9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dfp9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b83760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b83780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:45:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:45:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:45:58 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:45:58 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.7,PodIP:,StartTime:2019-12-05 06:45:58 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:45:58.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4618" for this suite.
Dec  5 06:46:04.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:46:04.631: INFO: namespace deployment-4618 deletion completed in 6.102016155s

• [SLOW TEST:10.210 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:46:04.631: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-ttmq
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 06:46:04.666: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ttmq" in namespace "subpath-3171" to be "success or failure"
Dec  5 06:46:04.673: INFO: Pod "pod-subpath-test-configmap-ttmq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.605408ms
Dec  5 06:46:06.677: INFO: Pod "pod-subpath-test-configmap-ttmq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010608048s
Dec  5 06:46:08.680: INFO: Pod "pod-subpath-test-configmap-ttmq": Phase="Running", Reason="", readiness=true. Elapsed: 4.014286844s
Dec  5 06:46:10.684: INFO: Pod "pod-subpath-test-configmap-ttmq": Phase="Running", Reason="", readiness=true. Elapsed: 6.018257269s
Dec  5 06:46:12.691: INFO: Pod "pod-subpath-test-configmap-ttmq": Phase="Running", Reason="", readiness=true. Elapsed: 8.025314595s
Dec  5 06:46:14.696: INFO: Pod "pod-subpath-test-configmap-ttmq": Phase="Running", Reason="", readiness=true. Elapsed: 10.029732858s
Dec  5 06:46:16.703: INFO: Pod "pod-subpath-test-configmap-ttmq": Phase="Running", Reason="", readiness=true. Elapsed: 12.036771816s
Dec  5 06:46:18.707: INFO: Pod "pod-subpath-test-configmap-ttmq": Phase="Running", Reason="", readiness=true. Elapsed: 14.040549987s
Dec  5 06:46:20.710: INFO: Pod "pod-subpath-test-configmap-ttmq": Phase="Running", Reason="", readiness=true. Elapsed: 16.044002244s
Dec  5 06:46:22.718: INFO: Pod "pod-subpath-test-configmap-ttmq": Phase="Running", Reason="", readiness=true. Elapsed: 18.051970079s
Dec  5 06:46:24.721: INFO: Pod "pod-subpath-test-configmap-ttmq": Phase="Running", Reason="", readiness=true. Elapsed: 20.054884826s
Dec  5 06:46:26.724: INFO: Pod "pod-subpath-test-configmap-ttmq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.058187731s
STEP: Saw pod success
Dec  5 06:46:26.724: INFO: Pod "pod-subpath-test-configmap-ttmq" satisfied condition "success or failure"
Dec  5 06:46:26.727: INFO: Trying to get logs from node node2 pod pod-subpath-test-configmap-ttmq container test-container-subpath-configmap-ttmq: <nil>
STEP: delete the pod
Dec  5 06:46:26.747: INFO: Waiting for pod pod-subpath-test-configmap-ttmq to disappear
Dec  5 06:46:26.749: INFO: Pod pod-subpath-test-configmap-ttmq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ttmq
Dec  5 06:46:26.749: INFO: Deleting pod "pod-subpath-test-configmap-ttmq" in namespace "subpath-3171"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:46:26.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3171" for this suite.
Dec  5 06:46:32.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:46:32.877: INFO: namespace subpath-3171 deletion completed in 6.123491463s

• [SLOW TEST:28.246 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:46:32.877: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4914
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-4914
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4914
Dec  5 06:46:32.931: INFO: Found 0 stateful pods, waiting for 1
Dec  5 06:46:42.935: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec  5 06:46:42.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-4914 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 06:46:43.222: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 06:46:43.222: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 06:46:43.222: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 06:46:43.225: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  5 06:46:53.230: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 06:46:53.230: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 06:46:53.241: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Dec  5 06:46:53.241: INFO: ss-0  node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:32 +0000 UTC  }]
Dec  5 06:46:53.241: INFO: 
Dec  5 06:46:53.241: INFO: StatefulSet ss has not reached scale 3, at 1
Dec  5 06:46:54.245: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997486998s
Dec  5 06:46:55.249: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993415762s
Dec  5 06:46:56.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98942788s
Dec  5 06:46:57.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98573462s
Dec  5 06:46:58.262: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981694249s
Dec  5 06:46:59.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977100179s
Dec  5 06:47:00.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.972691914s
Dec  5 06:47:01.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96882403s
Dec  5 06:47:02.279: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.796074ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4914
Dec  5 06:47:03.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-4914 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 06:47:03.579: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 06:47:03.579: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 06:47:03.579: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 06:47:03.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-4914 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 06:47:03.847: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  5 06:47:03.847: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 06:47:03.847: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 06:47:03.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-4914 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 06:47:04.110: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  5 06:47:04.110: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 06:47:04.110: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 06:47:04.113: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 06:47:04.113: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 06:47:04.113: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec  5 06:47:04.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-4914 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 06:47:04.398: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 06:47:04.398: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 06:47:04.398: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 06:47:04.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-4914 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 06:47:04.673: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 06:47:04.673: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 06:47:04.673: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 06:47:04.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-4914 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 06:47:04.964: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 06:47:04.964: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 06:47:04.964: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 06:47:04.964: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 06:47:04.967: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec  5 06:47:14.973: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 06:47:14.973: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 06:47:14.973: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  5 06:47:14.981: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Dec  5 06:47:14.981: INFO: ss-0  node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:32 +0000 UTC  }]
Dec  5 06:47:14.981: INFO: ss-1  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  }]
Dec  5 06:47:14.981: INFO: ss-2  node1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  }]
Dec  5 06:47:14.981: INFO: 
Dec  5 06:47:14.981: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  5 06:47:15.985: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Dec  5 06:47:15.985: INFO: ss-0  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:32 +0000 UTC  }]
Dec  5 06:47:15.985: INFO: ss-1  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  }]
Dec  5 06:47:15.985: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  }]
Dec  5 06:47:15.985: INFO: 
Dec  5 06:47:15.985: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  5 06:47:16.989: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Dec  5 06:47:16.989: INFO: ss-0  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:32 +0000 UTC  }]
Dec  5 06:47:16.989: INFO: ss-1  node2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  }]
Dec  5 06:47:16.989: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  }]
Dec  5 06:47:16.989: INFO: 
Dec  5 06:47:16.989: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  5 06:47:17.994: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
Dec  5 06:47:17.994: INFO: ss-0  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:32 +0000 UTC  }]
Dec  5 06:47:17.994: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:47:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:46:53 +0000 UTC  }]
Dec  5 06:47:17.994: INFO: 
Dec  5 06:47:17.994: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  5 06:47:18.997: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.982951126s
Dec  5 06:47:20.001: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.980342805s
Dec  5 06:47:21.004: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.97640788s
Dec  5 06:47:22.008: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.972879428s
Dec  5 06:47:23.013: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.968743712s
Dec  5 06:47:24.017: INFO: Verifying statefulset ss doesn't scale past 0 for another 963.899125ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4914
Dec  5 06:47:25.019: INFO: Scaling statefulset ss to 0
Dec  5 06:47:25.026: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  5 06:47:25.027: INFO: Deleting all statefulset in ns statefulset-4914
Dec  5 06:47:25.029: INFO: Scaling statefulset ss to 0
Dec  5 06:47:25.035: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 06:47:25.036: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:47:25.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4914" for this suite.
Dec  5 06:47:31.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:47:31.169: INFO: namespace statefulset-4914 deletion completed in 6.121767233s

• [SLOW TEST:58.292 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:47:31.170: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  5 06:47:31.199: INFO: Waiting up to 5m0s for pod "pod-cd72fb52-9a74-49fd-90ac-fa32d81851d0" in namespace "emptydir-9779" to be "success or failure"
Dec  5 06:47:31.203: INFO: Pod "pod-cd72fb52-9a74-49fd-90ac-fa32d81851d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029154ms
Dec  5 06:47:33.206: INFO: Pod "pod-cd72fb52-9a74-49fd-90ac-fa32d81851d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007354541s
STEP: Saw pod success
Dec  5 06:47:33.206: INFO: Pod "pod-cd72fb52-9a74-49fd-90ac-fa32d81851d0" satisfied condition "success or failure"
Dec  5 06:47:33.209: INFO: Trying to get logs from node node2 pod pod-cd72fb52-9a74-49fd-90ac-fa32d81851d0 container test-container: <nil>
STEP: delete the pod
Dec  5 06:47:33.230: INFO: Waiting for pod pod-cd72fb52-9a74-49fd-90ac-fa32d81851d0 to disappear
Dec  5 06:47:33.231: INFO: Pod pod-cd72fb52-9a74-49fd-90ac-fa32d81851d0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:47:33.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9779" for this suite.
Dec  5 06:47:39.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:47:39.332: INFO: namespace emptydir-9779 deletion completed in 6.096419514s

• [SLOW TEST:8.162 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:47:39.332: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec  5 06:47:41.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec pod-sharedvolume-5e6b2ca0-fc05-4e97-96fc-4f594793662e -c busybox-main-container --namespace=emptydir-3653 -- cat /usr/share/volumeshare/shareddata.txt'
Dec  5 06:47:41.701: INFO: stderr: ""
Dec  5 06:47:41.701: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:47:41.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3653" for this suite.
Dec  5 06:47:47.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:47:47.802: INFO: namespace emptydir-3653 deletion completed in 6.097526475s

• [SLOW TEST:8.471 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:47:47.803: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:47:49.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3889" for this suite.
Dec  5 06:48:27.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:48:27.960: INFO: namespace kubelet-test-3889 deletion completed in 38.106996928s

• [SLOW TEST:40.157 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:48:27.960: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 06:48:27.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f2b195d-0993-4042-b745-888497172c12" in namespace "downward-api-2206" to be "success or failure"
Dec  5 06:48:27.994: INFO: Pod "downwardapi-volume-3f2b195d-0993-4042-b745-888497172c12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.502114ms
Dec  5 06:48:29.998: INFO: Pod "downwardapi-volume-3f2b195d-0993-4042-b745-888497172c12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006483823s
STEP: Saw pod success
Dec  5 06:48:29.998: INFO: Pod "downwardapi-volume-3f2b195d-0993-4042-b745-888497172c12" satisfied condition "success or failure"
Dec  5 06:48:30.000: INFO: Trying to get logs from node node1 pod downwardapi-volume-3f2b195d-0993-4042-b745-888497172c12 container client-container: <nil>
STEP: delete the pod
Dec  5 06:48:30.019: INFO: Waiting for pod downwardapi-volume-3f2b195d-0993-4042-b745-888497172c12 to disappear
Dec  5 06:48:30.021: INFO: Pod downwardapi-volume-3f2b195d-0993-4042-b745-888497172c12 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:48:30.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2206" for this suite.
Dec  5 06:48:36.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:48:36.133: INFO: namespace downward-api-2206 deletion completed in 6.10799543s

• [SLOW TEST:8.173 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:48:36.133: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  5 06:48:38.173: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:48:38.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4207" for this suite.
Dec  5 06:48:44.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:48:44.283: INFO: namespace container-runtime-4207 deletion completed in 6.096757974s

• [SLOW TEST:8.150 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:48:44.284: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec  5 06:48:44.307: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:48:48.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7898" for this suite.
Dec  5 06:49:10.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:49:11.022: INFO: namespace init-container-7898 deletion completed in 22.110097885s

• [SLOW TEST:26.738 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:49:11.022: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 06:49:11.060: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec  5 06:49:16.063: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  5 06:49:16.063: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  5 06:49:16.077: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8058,SelfLink:/apis/apps/v1/namespaces/deployment-8058/deployments/test-cleanup-deployment,UID:e0fbebf6-49bb-4b5c-a077-1ab07a7c62c6,ResourceVersion:34994,Generation:1,CreationTimestamp:2019-12-05 06:49:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Dec  5 06:49:16.083: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-8058,SelfLink:/apis/apps/v1/namespaces/deployment-8058/replicasets/test-cleanup-deployment-55bbcbc84c,UID:27f0e436-b9bb-40b1-915f-81984ea4f702,ResourceVersion:34996,Generation:1,CreationTimestamp:2019-12-05 06:49:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment e0fbebf6-49bb-4b5c-a077-1ab07a7c62c6 0xc000a2b757 0xc000a2b758}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  5 06:49:16.083: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Dec  5 06:49:16.083: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-8058,SelfLink:/apis/apps/v1/namespaces/deployment-8058/replicasets/test-cleanup-controller,UID:1767a22b-49dd-4826-895c-2c9c93d6d898,ResourceVersion:34995,Generation:1,CreationTimestamp:2019-12-05 06:49:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment e0fbebf6-49bb-4b5c-a077-1ab07a7c62c6 0xc000a2b687 0xc000a2b688}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  5 06:49:16.088: INFO: Pod "test-cleanup-controller-twhfw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-twhfw,GenerateName:test-cleanup-controller-,Namespace:deployment-8058,SelfLink:/api/v1/namespaces/deployment-8058/pods/test-cleanup-controller-twhfw,UID:9aeaeccd-084b-4d78-9d55-36ad21fea287,ResourceVersion:34986,Generation:0,CreationTimestamp:2019-12-05 06:49:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 1767a22b-49dd-4826-895c-2c9c93d6d898 0xc003df4057 0xc003df4058}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hn2vj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hn2vj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hn2vj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003df40d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003df40f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:49:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:49:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:49:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:49:11 +0000 UTC  }],Message:,Reason:,HostIP:192.168.0.8,PodIP:10.233.96.78,StartTime:2019-12-05 06:49:11 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-05 06:49:12 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://7ba4110b6025e49796774b936e3532579d75a54035f7eaed599501fa2b1ffa27}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  5 06:49:16.088: INFO: Pod "test-cleanup-deployment-55bbcbc84c-rcng6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-rcng6,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-8058,SelfLink:/api/v1/namespaces/deployment-8058/pods/test-cleanup-deployment-55bbcbc84c-rcng6,UID:311215a2-15e4-4213-9a23-39e9b3a59c7d,ResourceVersion:35002,Generation:0,CreationTimestamp:2019-12-05 06:49:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 27f0e436-b9bb-40b1-915f-81984ea4f702 0xc003df41c7 0xc003df41c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hn2vj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hn2vj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-hn2vj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003df4240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003df4260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-05 06:49:16 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:49:16.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8058" for this suite.
Dec  5 06:49:22.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:49:22.205: INFO: namespace deployment-8058 deletion completed in 6.113074636s

• [SLOW TEST:11.183 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:49:22.206: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 06:49:22.248: INFO: Waiting up to 5m0s for pod "downwardapi-volume-78bdb2e8-ef98-4bbf-bcb8-ff92ed7a55ef" in namespace "downward-api-1891" to be "success or failure"
Dec  5 06:49:22.252: INFO: Pod "downwardapi-volume-78bdb2e8-ef98-4bbf-bcb8-ff92ed7a55ef": Phase="Pending", Reason="", readiness=false. Elapsed: 3.784269ms
Dec  5 06:49:24.258: INFO: Pod "downwardapi-volume-78bdb2e8-ef98-4bbf-bcb8-ff92ed7a55ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008972425s
STEP: Saw pod success
Dec  5 06:49:24.258: INFO: Pod "downwardapi-volume-78bdb2e8-ef98-4bbf-bcb8-ff92ed7a55ef" satisfied condition "success or failure"
Dec  5 06:49:24.260: INFO: Trying to get logs from node node2 pod downwardapi-volume-78bdb2e8-ef98-4bbf-bcb8-ff92ed7a55ef container client-container: <nil>
STEP: delete the pod
Dec  5 06:49:24.281: INFO: Waiting for pod downwardapi-volume-78bdb2e8-ef98-4bbf-bcb8-ff92ed7a55ef to disappear
Dec  5 06:49:24.283: INFO: Pod downwardapi-volume-78bdb2e8-ef98-4bbf-bcb8-ff92ed7a55ef no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:49:24.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1891" for this suite.
Dec  5 06:49:30.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:49:30.395: INFO: namespace downward-api-1891 deletion completed in 6.106384985s

• [SLOW TEST:8.189 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:49:30.395: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-cmsq
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 06:49:30.441: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cmsq" in namespace "subpath-9046" to be "success or failure"
Dec  5 06:49:30.443: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.705813ms
Dec  5 06:49:32.447: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Running", Reason="", readiness=true. Elapsed: 2.006278424s
Dec  5 06:49:34.450: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Running", Reason="", readiness=true. Elapsed: 4.009892103s
Dec  5 06:49:36.454: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Running", Reason="", readiness=true. Elapsed: 6.013701501s
Dec  5 06:49:38.458: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Running", Reason="", readiness=true. Elapsed: 8.017126774s
Dec  5 06:49:40.461: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Running", Reason="", readiness=true. Elapsed: 10.020411537s
Dec  5 06:49:42.466: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Running", Reason="", readiness=true. Elapsed: 12.025493618s
Dec  5 06:49:44.470: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Running", Reason="", readiness=true. Elapsed: 14.029453257s
Dec  5 06:49:46.474: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Running", Reason="", readiness=true. Elapsed: 16.033530391s
Dec  5 06:49:48.478: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Running", Reason="", readiness=true. Elapsed: 18.037525448s
Dec  5 06:49:50.482: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Running", Reason="", readiness=true. Elapsed: 20.04100291s
Dec  5 06:49:52.486: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Running", Reason="", readiness=true. Elapsed: 22.045123427s
Dec  5 06:49:54.489: INFO: Pod "pod-subpath-test-projected-cmsq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.048842113s
STEP: Saw pod success
Dec  5 06:49:54.489: INFO: Pod "pod-subpath-test-projected-cmsq" satisfied condition "success or failure"
Dec  5 06:49:54.491: INFO: Trying to get logs from node node1 pod pod-subpath-test-projected-cmsq container test-container-subpath-projected-cmsq: <nil>
STEP: delete the pod
Dec  5 06:49:54.522: INFO: Waiting for pod pod-subpath-test-projected-cmsq to disappear
Dec  5 06:49:54.524: INFO: Pod pod-subpath-test-projected-cmsq no longer exists
STEP: Deleting pod pod-subpath-test-projected-cmsq
Dec  5 06:49:54.524: INFO: Deleting pod "pod-subpath-test-projected-cmsq" in namespace "subpath-9046"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:49:54.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9046" for this suite.
Dec  5 06:50:00.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:50:00.634: INFO: namespace subpath-9046 deletion completed in 6.105271744s

• [SLOW TEST:30.239 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:50:00.634: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-2b371644-ae92-443c-a2dc-48bf7ac9d48b
STEP: Creating a pod to test consume configMaps
Dec  5 06:50:00.668: INFO: Waiting up to 5m0s for pod "pod-configmaps-c1846da3-22ed-45f9-a1e0-63703b30a08e" in namespace "configmap-1245" to be "success or failure"
Dec  5 06:50:00.670: INFO: Pod "pod-configmaps-c1846da3-22ed-45f9-a1e0-63703b30a08e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.469075ms
Dec  5 06:50:02.675: INFO: Pod "pod-configmaps-c1846da3-22ed-45f9-a1e0-63703b30a08e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007146561s
STEP: Saw pod success
Dec  5 06:50:02.675: INFO: Pod "pod-configmaps-c1846da3-22ed-45f9-a1e0-63703b30a08e" satisfied condition "success or failure"
Dec  5 06:50:02.678: INFO: Trying to get logs from node node2 pod pod-configmaps-c1846da3-22ed-45f9-a1e0-63703b30a08e container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 06:50:02.694: INFO: Waiting for pod pod-configmaps-c1846da3-22ed-45f9-a1e0-63703b30a08e to disappear
Dec  5 06:50:02.697: INFO: Pod pod-configmaps-c1846da3-22ed-45f9-a1e0-63703b30a08e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:50:02.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1245" for this suite.
Dec  5 06:50:08.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:50:08.820: INFO: namespace configmap-1245 deletion completed in 6.118871209s

• [SLOW TEST:8.186 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:50:08.821: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec  5 06:50:08.846: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:50:12.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-731" for this suite.
Dec  5 06:50:18.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:50:18.140: INFO: namespace init-container-731 deletion completed in 6.107108742s

• [SLOW TEST:9.320 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:50:18.140: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3639
I1205 06:50:18.170962      16 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3639, replica count: 1
I1205 06:50:19.222123      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 06:50:20.222506      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  5 06:50:20.328: INFO: Created: latency-svc-kdk7d
Dec  5 06:50:20.333: INFO: Got endpoints: latency-svc-kdk7d [10.701067ms]
Dec  5 06:50:20.341: INFO: Created: latency-svc-qqgdq
Dec  5 06:50:20.346: INFO: Got endpoints: latency-svc-qqgdq [12.359251ms]
Dec  5 06:50:20.347: INFO: Created: latency-svc-nfst6
Dec  5 06:50:20.350: INFO: Got endpoints: latency-svc-nfst6 [16.519966ms]
Dec  5 06:50:20.354: INFO: Created: latency-svc-7cwrx
Dec  5 06:50:20.357: INFO: Created: latency-svc-mj4x8
Dec  5 06:50:20.359: INFO: Got endpoints: latency-svc-7cwrx [25.237741ms]
Dec  5 06:50:20.361: INFO: Got endpoints: latency-svc-mj4x8 [27.40363ms]
Dec  5 06:50:20.363: INFO: Created: latency-svc-ktdsd
Dec  5 06:50:20.367: INFO: Got endpoints: latency-svc-ktdsd [33.267211ms]
Dec  5 06:50:20.368: INFO: Created: latency-svc-brsdw
Dec  5 06:50:20.373: INFO: Created: latency-svc-z7h74
Dec  5 06:50:20.374: INFO: Got endpoints: latency-svc-brsdw [39.826595ms]
Dec  5 06:50:20.377: INFO: Got endpoints: latency-svc-z7h74 [42.933866ms]
Dec  5 06:50:20.378: INFO: Created: latency-svc-dvnch
Dec  5 06:50:20.381: INFO: Got endpoints: latency-svc-dvnch [47.604552ms]
Dec  5 06:50:20.386: INFO: Created: latency-svc-fdm2n
Dec  5 06:50:20.388: INFO: Created: latency-svc-zgqbc
Dec  5 06:50:20.391: INFO: Created: latency-svc-jsgrz
Dec  5 06:50:20.392: INFO: Got endpoints: latency-svc-zgqbc [58.008749ms]
Dec  5 06:50:20.392: INFO: Got endpoints: latency-svc-fdm2n [58.363629ms]
Dec  5 06:50:20.396: INFO: Got endpoints: latency-svc-jsgrz [61.863308ms]
Dec  5 06:50:20.399: INFO: Created: latency-svc-hjscs
Dec  5 06:50:20.401: INFO: Created: latency-svc-x7dq6
Dec  5 06:50:20.402: INFO: Got endpoints: latency-svc-hjscs [68.52275ms]
Dec  5 06:50:20.406: INFO: Got endpoints: latency-svc-x7dq6 [13.437694ms]
Dec  5 06:50:20.407: INFO: Created: latency-svc-dp9h4
Dec  5 06:50:20.413: INFO: Got endpoints: latency-svc-dp9h4 [78.705029ms]
Dec  5 06:50:20.415: INFO: Created: latency-svc-7p88n
Dec  5 06:50:20.420: INFO: Got endpoints: latency-svc-7p88n [85.941883ms]
Dec  5 06:50:20.423: INFO: Created: latency-svc-6th6f
Dec  5 06:50:20.423: INFO: Created: latency-svc-8dtw5
Dec  5 06:50:20.427: INFO: Got endpoints: latency-svc-8dtw5 [81.213776ms]
Dec  5 06:50:20.427: INFO: Got endpoints: latency-svc-6th6f [93.299687ms]
Dec  5 06:50:20.427: INFO: Created: latency-svc-lgnqh
Dec  5 06:50:20.432: INFO: Got endpoints: latency-svc-lgnqh [82.142322ms]
Dec  5 06:50:20.432: INFO: Created: latency-svc-ht2vf
Dec  5 06:50:20.442: INFO: Got endpoints: latency-svc-ht2vf [83.10743ms]
Dec  5 06:50:20.442: INFO: Created: latency-svc-4sv5h
Dec  5 06:50:20.442: INFO: Got endpoints: latency-svc-4sv5h [80.772574ms]
Dec  5 06:50:20.445: INFO: Created: latency-svc-hnfk6
Dec  5 06:50:20.445: INFO: Got endpoints: latency-svc-hnfk6 [78.490355ms]
Dec  5 06:50:20.447: INFO: Created: latency-svc-t7spq
Dec  5 06:50:20.450: INFO: Created: latency-svc-spzlx
Dec  5 06:50:20.450: INFO: Got endpoints: latency-svc-t7spq [76.239264ms]
Dec  5 06:50:20.455: INFO: Created: latency-svc-vhsj2
Dec  5 06:50:20.456: INFO: Got endpoints: latency-svc-spzlx [78.686895ms]
Dec  5 06:50:20.458: INFO: Created: latency-svc-4m6nj
Dec  5 06:50:20.460: INFO: Got endpoints: latency-svc-vhsj2 [78.694519ms]
Dec  5 06:50:20.462: INFO: Got endpoints: latency-svc-4m6nj [70.470808ms]
Dec  5 06:50:20.465: INFO: Created: latency-svc-5scmz
Dec  5 06:50:20.468: INFO: Got endpoints: latency-svc-5scmz [71.866812ms]
Dec  5 06:50:20.471: INFO: Created: latency-svc-sktsv
Dec  5 06:50:20.474: INFO: Got endpoints: latency-svc-sktsv [71.612586ms]
Dec  5 06:50:20.475: INFO: Created: latency-svc-96gbn
Dec  5 06:50:20.477: INFO: Got endpoints: latency-svc-96gbn [70.922357ms]
Dec  5 06:50:20.480: INFO: Created: latency-svc-86wmv
Dec  5 06:50:20.483: INFO: Got endpoints: latency-svc-86wmv [70.330874ms]
Dec  5 06:50:20.484: INFO: Created: latency-svc-zjx6c
Dec  5 06:50:20.486: INFO: Got endpoints: latency-svc-zjx6c [66.202739ms]
Dec  5 06:50:20.496: INFO: Created: latency-svc-jc79c
Dec  5 06:50:20.496: INFO: Created: latency-svc-48977
Dec  5 06:50:20.496: INFO: Got endpoints: latency-svc-48977 [68.985613ms]
Dec  5 06:50:20.498: INFO: Got endpoints: latency-svc-jc79c [70.305643ms]
Dec  5 06:50:20.498: INFO: Created: latency-svc-dhzdx
Dec  5 06:50:20.501: INFO: Created: latency-svc-5hv6v
Dec  5 06:50:20.506: INFO: Created: latency-svc-4m5t5
Dec  5 06:50:20.507: INFO: Created: latency-svc-d2qtj
Dec  5 06:50:20.510: INFO: Created: latency-svc-4qn7w
Dec  5 06:50:20.512: INFO: Created: latency-svc-56zlz
Dec  5 06:50:20.514: INFO: Created: latency-svc-crkld
Dec  5 06:50:20.520: INFO: Created: latency-svc-f28l8
Dec  5 06:50:20.525: INFO: Created: latency-svc-s9p5k
Dec  5 06:50:20.527: INFO: Created: latency-svc-2xvt7
Dec  5 06:50:20.530: INFO: Created: latency-svc-mkcld
Dec  5 06:50:20.538: INFO: Got endpoints: latency-svc-dhzdx [105.550388ms]
Dec  5 06:50:20.539: INFO: Created: latency-svc-kb4tm
Dec  5 06:50:20.545: INFO: Created: latency-svc-vrddq
Dec  5 06:50:20.548: INFO: Created: latency-svc-dxpbd
Dec  5 06:50:20.552: INFO: Created: latency-svc-8c8kt
Dec  5 06:50:20.555: INFO: Created: latency-svc-zx8fj
Dec  5 06:50:20.582: INFO: Got endpoints: latency-svc-5hv6v [140.402928ms]
Dec  5 06:50:20.589: INFO: Created: latency-svc-g762t
Dec  5 06:50:20.635: INFO: Got endpoints: latency-svc-4m5t5 [192.968033ms]
Dec  5 06:50:20.640: INFO: Created: latency-svc-mjv9r
Dec  5 06:50:20.683: INFO: Got endpoints: latency-svc-d2qtj [237.425138ms]
Dec  5 06:50:20.689: INFO: Created: latency-svc-sj644
Dec  5 06:50:20.733: INFO: Got endpoints: latency-svc-4qn7w [282.825081ms]
Dec  5 06:50:20.740: INFO: Created: latency-svc-78xmg
Dec  5 06:50:20.782: INFO: Got endpoints: latency-svc-56zlz [326.673427ms]
Dec  5 06:50:20.787: INFO: Created: latency-svc-pqfrv
Dec  5 06:50:20.833: INFO: Got endpoints: latency-svc-crkld [372.640106ms]
Dec  5 06:50:20.837: INFO: Created: latency-svc-swm28
Dec  5 06:50:20.882: INFO: Got endpoints: latency-svc-f28l8 [420.027491ms]
Dec  5 06:50:20.887: INFO: Created: latency-svc-lpfxg
Dec  5 06:50:20.933: INFO: Got endpoints: latency-svc-s9p5k [465.787798ms]
Dec  5 06:50:20.942: INFO: Created: latency-svc-nlxmq
Dec  5 06:50:20.985: INFO: Got endpoints: latency-svc-2xvt7 [511.114123ms]
Dec  5 06:50:20.990: INFO: Created: latency-svc-sclwr
Dec  5 06:50:21.038: INFO: Got endpoints: latency-svc-mkcld [561.67461ms]
Dec  5 06:50:21.044: INFO: Created: latency-svc-4sw2f
Dec  5 06:50:21.082: INFO: Got endpoints: latency-svc-kb4tm [598.83552ms]
Dec  5 06:50:21.092: INFO: Created: latency-svc-vqcxc
Dec  5 06:50:21.131: INFO: Got endpoints: latency-svc-vrddq [645.189754ms]
Dec  5 06:50:21.137: INFO: Created: latency-svc-5rtg8
Dec  5 06:50:21.183: INFO: Got endpoints: latency-svc-dxpbd [686.999147ms]
Dec  5 06:50:21.188: INFO: Created: latency-svc-v8j8b
Dec  5 06:50:21.232: INFO: Got endpoints: latency-svc-8c8kt [734.627405ms]
Dec  5 06:50:21.237: INFO: Created: latency-svc-lr852
Dec  5 06:50:21.282: INFO: Got endpoints: latency-svc-zx8fj [744.272652ms]
Dec  5 06:50:21.296: INFO: Created: latency-svc-kp799
Dec  5 06:50:21.332: INFO: Got endpoints: latency-svc-g762t [749.550159ms]
Dec  5 06:50:21.337: INFO: Created: latency-svc-46lh8
Dec  5 06:50:21.382: INFO: Got endpoints: latency-svc-mjv9r [747.020784ms]
Dec  5 06:50:21.388: INFO: Created: latency-svc-nw449
Dec  5 06:50:21.433: INFO: Got endpoints: latency-svc-sj644 [749.967754ms]
Dec  5 06:50:21.440: INFO: Created: latency-svc-d9nc5
Dec  5 06:50:21.482: INFO: Got endpoints: latency-svc-78xmg [749.523719ms]
Dec  5 06:50:21.486: INFO: Created: latency-svc-6g8gr
Dec  5 06:50:21.532: INFO: Got endpoints: latency-svc-pqfrv [749.884533ms]
Dec  5 06:50:21.538: INFO: Created: latency-svc-8mpkr
Dec  5 06:50:21.584: INFO: Got endpoints: latency-svc-swm28 [751.204354ms]
Dec  5 06:50:21.590: INFO: Created: latency-svc-rz5wh
Dec  5 06:50:21.633: INFO: Got endpoints: latency-svc-lpfxg [750.500358ms]
Dec  5 06:50:21.638: INFO: Created: latency-svc-9xqvx
Dec  5 06:50:21.684: INFO: Got endpoints: latency-svc-nlxmq [750.446549ms]
Dec  5 06:50:21.690: INFO: Created: latency-svc-mvfz6
Dec  5 06:50:21.732: INFO: Got endpoints: latency-svc-sclwr [747.128837ms]
Dec  5 06:50:21.739: INFO: Created: latency-svc-6hw2x
Dec  5 06:50:21.782: INFO: Got endpoints: latency-svc-4sw2f [743.902914ms]
Dec  5 06:50:21.788: INFO: Created: latency-svc-7txk5
Dec  5 06:50:21.833: INFO: Got endpoints: latency-svc-vqcxc [750.955123ms]
Dec  5 06:50:21.839: INFO: Created: latency-svc-2xxwt
Dec  5 06:50:21.883: INFO: Got endpoints: latency-svc-5rtg8 [751.987351ms]
Dec  5 06:50:21.889: INFO: Created: latency-svc-7hhrz
Dec  5 06:50:21.932: INFO: Got endpoints: latency-svc-v8j8b [749.283718ms]
Dec  5 06:50:21.937: INFO: Created: latency-svc-86jf6
Dec  5 06:50:21.983: INFO: Got endpoints: latency-svc-lr852 [750.414067ms]
Dec  5 06:50:21.990: INFO: Created: latency-svc-4bdh2
Dec  5 06:50:22.033: INFO: Got endpoints: latency-svc-kp799 [750.507932ms]
Dec  5 06:50:22.040: INFO: Created: latency-svc-mw25g
Dec  5 06:50:22.083: INFO: Got endpoints: latency-svc-46lh8 [751.399007ms]
Dec  5 06:50:22.093: INFO: Created: latency-svc-wq8h4
Dec  5 06:50:22.135: INFO: Got endpoints: latency-svc-nw449 [752.865009ms]
Dec  5 06:50:22.140: INFO: Created: latency-svc-sznn6
Dec  5 06:50:22.181: INFO: Got endpoints: latency-svc-d9nc5 [748.414892ms]
Dec  5 06:50:22.186: INFO: Created: latency-svc-vqrw9
Dec  5 06:50:22.232: INFO: Got endpoints: latency-svc-6g8gr [749.490532ms]
Dec  5 06:50:22.237: INFO: Created: latency-svc-cnsjz
Dec  5 06:50:22.284: INFO: Got endpoints: latency-svc-8mpkr [751.487307ms]
Dec  5 06:50:22.292: INFO: Created: latency-svc-rgfz6
Dec  5 06:50:22.335: INFO: Got endpoints: latency-svc-rz5wh [750.778ms]
Dec  5 06:50:22.342: INFO: Created: latency-svc-fwd2h
Dec  5 06:50:22.386: INFO: Got endpoints: latency-svc-9xqvx [753.499157ms]
Dec  5 06:50:22.399: INFO: Created: latency-svc-kgt9q
Dec  5 06:50:22.432: INFO: Got endpoints: latency-svc-mvfz6 [748.530602ms]
Dec  5 06:50:22.446: INFO: Created: latency-svc-4r2vl
Dec  5 06:50:22.482: INFO: Got endpoints: latency-svc-6hw2x [749.637987ms]
Dec  5 06:50:22.487: INFO: Created: latency-svc-xcslq
Dec  5 06:50:22.534: INFO: Got endpoints: latency-svc-7txk5 [751.531408ms]
Dec  5 06:50:22.542: INFO: Created: latency-svc-6qcvx
Dec  5 06:50:22.584: INFO: Got endpoints: latency-svc-2xxwt [750.678184ms]
Dec  5 06:50:22.589: INFO: Created: latency-svc-bttk4
Dec  5 06:50:22.632: INFO: Got endpoints: latency-svc-7hhrz [749.016754ms]
Dec  5 06:50:22.639: INFO: Created: latency-svc-ddjtf
Dec  5 06:50:22.685: INFO: Got endpoints: latency-svc-86jf6 [752.62494ms]
Dec  5 06:50:22.692: INFO: Created: latency-svc-fv8qm
Dec  5 06:50:22.742: INFO: Got endpoints: latency-svc-4bdh2 [759.18164ms]
Dec  5 06:50:22.751: INFO: Created: latency-svc-kwb9k
Dec  5 06:50:22.786: INFO: Got endpoints: latency-svc-mw25g [753.136039ms]
Dec  5 06:50:22.792: INFO: Created: latency-svc-bcjqp
Dec  5 06:50:22.833: INFO: Got endpoints: latency-svc-wq8h4 [749.826636ms]
Dec  5 06:50:22.839: INFO: Created: latency-svc-5rx2p
Dec  5 06:50:22.885: INFO: Got endpoints: latency-svc-sznn6 [749.852571ms]
Dec  5 06:50:22.892: INFO: Created: latency-svc-pg9w2
Dec  5 06:50:22.933: INFO: Got endpoints: latency-svc-vqrw9 [751.320103ms]
Dec  5 06:50:22.939: INFO: Created: latency-svc-n8djf
Dec  5 06:50:22.985: INFO: Got endpoints: latency-svc-cnsjz [752.71642ms]
Dec  5 06:50:22.990: INFO: Created: latency-svc-88v2t
Dec  5 06:50:23.033: INFO: Got endpoints: latency-svc-rgfz6 [749.291269ms]
Dec  5 06:50:23.038: INFO: Created: latency-svc-5w6gv
Dec  5 06:50:23.083: INFO: Got endpoints: latency-svc-fwd2h [748.251437ms]
Dec  5 06:50:23.095: INFO: Created: latency-svc-rw68f
Dec  5 06:50:23.134: INFO: Got endpoints: latency-svc-kgt9q [747.547947ms]
Dec  5 06:50:23.138: INFO: Created: latency-svc-gcrhr
Dec  5 06:50:23.182: INFO: Got endpoints: latency-svc-4r2vl [749.852552ms]
Dec  5 06:50:23.190: INFO: Created: latency-svc-qwf2r
Dec  5 06:50:23.232: INFO: Got endpoints: latency-svc-xcslq [750.144131ms]
Dec  5 06:50:23.240: INFO: Created: latency-svc-l2ksg
Dec  5 06:50:23.282: INFO: Got endpoints: latency-svc-6qcvx [748.498038ms]
Dec  5 06:50:23.288: INFO: Created: latency-svc-8f99z
Dec  5 06:50:23.333: INFO: Got endpoints: latency-svc-bttk4 [749.179247ms]
Dec  5 06:50:23.339: INFO: Created: latency-svc-hgv9x
Dec  5 06:50:23.382: INFO: Got endpoints: latency-svc-ddjtf [749.988382ms]
Dec  5 06:50:23.389: INFO: Created: latency-svc-9952t
Dec  5 06:50:23.432: INFO: Got endpoints: latency-svc-fv8qm [747.127099ms]
Dec  5 06:50:23.439: INFO: Created: latency-svc-l26wq
Dec  5 06:50:23.483: INFO: Got endpoints: latency-svc-kwb9k [741.151117ms]
Dec  5 06:50:23.489: INFO: Created: latency-svc-z9xqq
Dec  5 06:50:23.532: INFO: Got endpoints: latency-svc-bcjqp [746.376306ms]
Dec  5 06:50:23.539: INFO: Created: latency-svc-2m8bb
Dec  5 06:50:23.591: INFO: Got endpoints: latency-svc-5rx2p [757.48754ms]
Dec  5 06:50:23.597: INFO: Created: latency-svc-dgk87
Dec  5 06:50:23.633: INFO: Got endpoints: latency-svc-pg9w2 [748.164494ms]
Dec  5 06:50:23.643: INFO: Created: latency-svc-hmrq7
Dec  5 06:50:23.683: INFO: Got endpoints: latency-svc-n8djf [749.947743ms]
Dec  5 06:50:23.689: INFO: Created: latency-svc-657ms
Dec  5 06:50:23.733: INFO: Got endpoints: latency-svc-88v2t [748.144951ms]
Dec  5 06:50:23.739: INFO: Created: latency-svc-z9ps2
Dec  5 06:50:23.782: INFO: Got endpoints: latency-svc-5w6gv [749.154218ms]
Dec  5 06:50:23.788: INFO: Created: latency-svc-75zsn
Dec  5 06:50:23.834: INFO: Got endpoints: latency-svc-rw68f [750.592953ms]
Dec  5 06:50:23.841: INFO: Created: latency-svc-pdj5m
Dec  5 06:50:23.883: INFO: Got endpoints: latency-svc-gcrhr [748.604968ms]
Dec  5 06:50:23.887: INFO: Created: latency-svc-dtdcg
Dec  5 06:50:23.933: INFO: Got endpoints: latency-svc-qwf2r [750.524108ms]
Dec  5 06:50:23.939: INFO: Created: latency-svc-x2nfw
Dec  5 06:50:23.986: INFO: Got endpoints: latency-svc-l2ksg [753.19952ms]
Dec  5 06:50:23.990: INFO: Created: latency-svc-bf2n9
Dec  5 06:50:24.032: INFO: Got endpoints: latency-svc-8f99z [749.570695ms]
Dec  5 06:50:24.037: INFO: Created: latency-svc-lzddc
Dec  5 06:50:24.082: INFO: Got endpoints: latency-svc-hgv9x [749.133054ms]
Dec  5 06:50:24.087: INFO: Created: latency-svc-5h9fh
Dec  5 06:50:24.135: INFO: Got endpoints: latency-svc-9952t [752.46145ms]
Dec  5 06:50:24.145: INFO: Created: latency-svc-l7nlx
Dec  5 06:50:24.183: INFO: Got endpoints: latency-svc-l26wq [750.822996ms]
Dec  5 06:50:24.192: INFO: Created: latency-svc-bqtd9
Dec  5 06:50:24.233: INFO: Got endpoints: latency-svc-z9xqq [749.672613ms]
Dec  5 06:50:24.239: INFO: Created: latency-svc-5xkt7
Dec  5 06:50:24.284: INFO: Got endpoints: latency-svc-2m8bb [751.317091ms]
Dec  5 06:50:24.291: INFO: Created: latency-svc-8nb5g
Dec  5 06:50:24.334: INFO: Got endpoints: latency-svc-dgk87 [743.052708ms]
Dec  5 06:50:24.341: INFO: Created: latency-svc-k2494
Dec  5 06:50:24.383: INFO: Got endpoints: latency-svc-hmrq7 [749.471732ms]
Dec  5 06:50:24.389: INFO: Created: latency-svc-nflws
Dec  5 06:50:24.433: INFO: Got endpoints: latency-svc-657ms [750.45706ms]
Dec  5 06:50:24.439: INFO: Created: latency-svc-wcgzq
Dec  5 06:50:24.482: INFO: Got endpoints: latency-svc-z9ps2 [749.381185ms]
Dec  5 06:50:24.489: INFO: Created: latency-svc-dlf2x
Dec  5 06:50:24.536: INFO: Got endpoints: latency-svc-75zsn [753.356911ms]
Dec  5 06:50:24.543: INFO: Created: latency-svc-4dhcj
Dec  5 06:50:24.582: INFO: Got endpoints: latency-svc-pdj5m [748.517665ms]
Dec  5 06:50:24.588: INFO: Created: latency-svc-75b5l
Dec  5 06:50:24.633: INFO: Got endpoints: latency-svc-dtdcg [749.851794ms]
Dec  5 06:50:24.637: INFO: Created: latency-svc-46dg7
Dec  5 06:50:24.684: INFO: Got endpoints: latency-svc-x2nfw [751.128565ms]
Dec  5 06:50:24.691: INFO: Created: latency-svc-2dkrp
Dec  5 06:50:24.732: INFO: Got endpoints: latency-svc-bf2n9 [746.867697ms]
Dec  5 06:50:24.739: INFO: Created: latency-svc-qn5z9
Dec  5 06:50:24.783: INFO: Got endpoints: latency-svc-lzddc [750.564871ms]
Dec  5 06:50:24.787: INFO: Created: latency-svc-42qdc
Dec  5 06:50:24.834: INFO: Got endpoints: latency-svc-5h9fh [751.664899ms]
Dec  5 06:50:24.840: INFO: Created: latency-svc-n5ncw
Dec  5 06:50:24.883: INFO: Got endpoints: latency-svc-l7nlx [747.838483ms]
Dec  5 06:50:24.888: INFO: Created: latency-svc-qjfd7
Dec  5 06:50:24.932: INFO: Got endpoints: latency-svc-bqtd9 [748.897195ms]
Dec  5 06:50:24.937: INFO: Created: latency-svc-pq4nd
Dec  5 06:50:24.983: INFO: Got endpoints: latency-svc-5xkt7 [750.073575ms]
Dec  5 06:50:24.988: INFO: Created: latency-svc-rh7c9
Dec  5 06:50:25.036: INFO: Got endpoints: latency-svc-8nb5g [752.052281ms]
Dec  5 06:50:25.040: INFO: Created: latency-svc-dqz2z
Dec  5 06:50:25.083: INFO: Got endpoints: latency-svc-k2494 [749.19824ms]
Dec  5 06:50:25.090: INFO: Created: latency-svc-zkprx
Dec  5 06:50:25.133: INFO: Got endpoints: latency-svc-nflws [750.811856ms]
Dec  5 06:50:25.139: INFO: Created: latency-svc-dmpkw
Dec  5 06:50:25.182: INFO: Got endpoints: latency-svc-wcgzq [749.085797ms]
Dec  5 06:50:25.191: INFO: Created: latency-svc-2rh45
Dec  5 06:50:25.233: INFO: Got endpoints: latency-svc-dlf2x [750.450611ms]
Dec  5 06:50:25.238: INFO: Created: latency-svc-rlvvn
Dec  5 06:50:25.284: INFO: Got endpoints: latency-svc-4dhcj [748.795474ms]
Dec  5 06:50:25.291: INFO: Created: latency-svc-l9bps
Dec  5 06:50:25.332: INFO: Got endpoints: latency-svc-75b5l [749.881974ms]
Dec  5 06:50:25.341: INFO: Created: latency-svc-22vbh
Dec  5 06:50:25.383: INFO: Got endpoints: latency-svc-46dg7 [750.008238ms]
Dec  5 06:50:25.389: INFO: Created: latency-svc-lnqtg
Dec  5 06:50:25.432: INFO: Got endpoints: latency-svc-2dkrp [747.948446ms]
Dec  5 06:50:25.441: INFO: Created: latency-svc-kcnrl
Dec  5 06:50:25.483: INFO: Got endpoints: latency-svc-qn5z9 [750.710101ms]
Dec  5 06:50:25.489: INFO: Created: latency-svc-vkjxr
Dec  5 06:50:25.533: INFO: Got endpoints: latency-svc-42qdc [750.541766ms]
Dec  5 06:50:25.539: INFO: Created: latency-svc-9bgsv
Dec  5 06:50:25.582: INFO: Got endpoints: latency-svc-n5ncw [748.37953ms]
Dec  5 06:50:25.592: INFO: Created: latency-svc-4kddl
Dec  5 06:50:25.632: INFO: Got endpoints: latency-svc-qjfd7 [749.528664ms]
Dec  5 06:50:25.637: INFO: Created: latency-svc-mp7dn
Dec  5 06:50:25.685: INFO: Got endpoints: latency-svc-pq4nd [753.22677ms]
Dec  5 06:50:25.691: INFO: Created: latency-svc-dw6gk
Dec  5 06:50:25.733: INFO: Got endpoints: latency-svc-rh7c9 [749.613329ms]
Dec  5 06:50:25.739: INFO: Created: latency-svc-7ppvl
Dec  5 06:50:25.784: INFO: Got endpoints: latency-svc-dqz2z [748.328943ms]
Dec  5 06:50:25.792: INFO: Created: latency-svc-mn2c8
Dec  5 06:50:25.832: INFO: Got endpoints: latency-svc-zkprx [748.828754ms]
Dec  5 06:50:25.837: INFO: Created: latency-svc-d6tnb
Dec  5 06:50:25.883: INFO: Got endpoints: latency-svc-dmpkw [749.710208ms]
Dec  5 06:50:25.889: INFO: Created: latency-svc-k9tqt
Dec  5 06:50:25.933: INFO: Got endpoints: latency-svc-2rh45 [750.38759ms]
Dec  5 06:50:25.939: INFO: Created: latency-svc-j6r7m
Dec  5 06:50:25.983: INFO: Got endpoints: latency-svc-rlvvn [750.474238ms]
Dec  5 06:50:25.989: INFO: Created: latency-svc-ztz64
Dec  5 06:50:26.033: INFO: Got endpoints: latency-svc-l9bps [749.015277ms]
Dec  5 06:50:26.047: INFO: Created: latency-svc-kvjj7
Dec  5 06:50:26.087: INFO: Got endpoints: latency-svc-22vbh [754.589345ms]
Dec  5 06:50:26.092: INFO: Created: latency-svc-bj626
Dec  5 06:50:26.137: INFO: Got endpoints: latency-svc-lnqtg [754.415361ms]
Dec  5 06:50:26.148: INFO: Created: latency-svc-xdrgp
Dec  5 06:50:26.183: INFO: Got endpoints: latency-svc-kcnrl [750.91712ms]
Dec  5 06:50:26.188: INFO: Created: latency-svc-q96k4
Dec  5 06:50:26.233: INFO: Got endpoints: latency-svc-vkjxr [749.47912ms]
Dec  5 06:50:26.240: INFO: Created: latency-svc-6s5sm
Dec  5 06:50:26.283: INFO: Got endpoints: latency-svc-9bgsv [749.557476ms]
Dec  5 06:50:26.288: INFO: Created: latency-svc-g2j5w
Dec  5 06:50:26.334: INFO: Got endpoints: latency-svc-4kddl [751.594477ms]
Dec  5 06:50:26.339: INFO: Created: latency-svc-hdcm8
Dec  5 06:50:26.383: INFO: Got endpoints: latency-svc-mp7dn [750.221251ms]
Dec  5 06:50:26.391: INFO: Created: latency-svc-pwrpf
Dec  5 06:50:26.432: INFO: Got endpoints: latency-svc-dw6gk [747.119202ms]
Dec  5 06:50:26.438: INFO: Created: latency-svc-pn4bc
Dec  5 06:50:26.483: INFO: Got endpoints: latency-svc-7ppvl [750.515601ms]
Dec  5 06:50:26.490: INFO: Created: latency-svc-w5785
Dec  5 06:50:26.534: INFO: Got endpoints: latency-svc-mn2c8 [749.521362ms]
Dec  5 06:50:26.539: INFO: Created: latency-svc-2pmzd
Dec  5 06:50:26.583: INFO: Got endpoints: latency-svc-d6tnb [750.718932ms]
Dec  5 06:50:26.588: INFO: Created: latency-svc-5tmnk
Dec  5 06:50:26.632: INFO: Got endpoints: latency-svc-k9tqt [749.271331ms]
Dec  5 06:50:26.639: INFO: Created: latency-svc-h22g6
Dec  5 06:50:26.685: INFO: Got endpoints: latency-svc-j6r7m [751.799581ms]
Dec  5 06:50:26.697: INFO: Created: latency-svc-99shb
Dec  5 06:50:26.733: INFO: Got endpoints: latency-svc-ztz64 [749.1968ms]
Dec  5 06:50:26.738: INFO: Created: latency-svc-dtj9j
Dec  5 06:50:26.782: INFO: Got endpoints: latency-svc-kvjj7 [748.579568ms]
Dec  5 06:50:26.788: INFO: Created: latency-svc-8lz4l
Dec  5 06:50:26.833: INFO: Got endpoints: latency-svc-bj626 [745.980357ms]
Dec  5 06:50:26.838: INFO: Created: latency-svc-528jc
Dec  5 06:50:26.895: INFO: Got endpoints: latency-svc-xdrgp [757.602265ms]
Dec  5 06:50:26.909: INFO: Created: latency-svc-bjf5t
Dec  5 06:50:26.932: INFO: Got endpoints: latency-svc-q96k4 [749.015884ms]
Dec  5 06:50:26.938: INFO: Created: latency-svc-wzps9
Dec  5 06:50:26.984: INFO: Got endpoints: latency-svc-6s5sm [751.160378ms]
Dec  5 06:50:26.992: INFO: Created: latency-svc-fgdx4
Dec  5 06:50:27.033: INFO: Got endpoints: latency-svc-g2j5w [750.366246ms]
Dec  5 06:50:27.038: INFO: Created: latency-svc-fd68c
Dec  5 06:50:27.083: INFO: Got endpoints: latency-svc-hdcm8 [749.220888ms]
Dec  5 06:50:27.089: INFO: Created: latency-svc-c75rq
Dec  5 06:50:27.133: INFO: Got endpoints: latency-svc-pwrpf [750.136794ms]
Dec  5 06:50:27.140: INFO: Created: latency-svc-69pq2
Dec  5 06:50:27.184: INFO: Got endpoints: latency-svc-pn4bc [751.570116ms]
Dec  5 06:50:27.196: INFO: Created: latency-svc-2hdhk
Dec  5 06:50:27.234: INFO: Got endpoints: latency-svc-w5785 [750.495193ms]
Dec  5 06:50:27.240: INFO: Created: latency-svc-ct8d6
Dec  5 06:50:27.283: INFO: Got endpoints: latency-svc-2pmzd [748.887793ms]
Dec  5 06:50:27.290: INFO: Created: latency-svc-tn7vt
Dec  5 06:50:27.332: INFO: Got endpoints: latency-svc-5tmnk [749.086035ms]
Dec  5 06:50:27.339: INFO: Created: latency-svc-ltqgj
Dec  5 06:50:27.382: INFO: Got endpoints: latency-svc-h22g6 [749.590238ms]
Dec  5 06:50:27.387: INFO: Created: latency-svc-wqsk5
Dec  5 06:50:27.432: INFO: Got endpoints: latency-svc-99shb [747.7175ms]
Dec  5 06:50:27.442: INFO: Created: latency-svc-gfwxq
Dec  5 06:50:27.489: INFO: Got endpoints: latency-svc-dtj9j [756.423372ms]
Dec  5 06:50:27.497: INFO: Created: latency-svc-5nbp5
Dec  5 06:50:27.533: INFO: Got endpoints: latency-svc-8lz4l [751.358999ms]
Dec  5 06:50:27.539: INFO: Created: latency-svc-l2mf8
Dec  5 06:50:27.583: INFO: Got endpoints: latency-svc-528jc [749.526155ms]
Dec  5 06:50:27.590: INFO: Created: latency-svc-8tmn5
Dec  5 06:50:27.633: INFO: Got endpoints: latency-svc-bjf5t [738.501054ms]
Dec  5 06:50:27.643: INFO: Created: latency-svc-lnvgl
Dec  5 06:50:27.682: INFO: Got endpoints: latency-svc-wzps9 [750.284118ms]
Dec  5 06:50:27.688: INFO: Created: latency-svc-gshwm
Dec  5 06:50:27.747: INFO: Got endpoints: latency-svc-fgdx4 [763.243612ms]
Dec  5 06:50:27.760: INFO: Created: latency-svc-scxgz
Dec  5 06:50:27.785: INFO: Got endpoints: latency-svc-fd68c [752.091694ms]
Dec  5 06:50:27.792: INFO: Created: latency-svc-f29jn
Dec  5 06:50:27.833: INFO: Got endpoints: latency-svc-c75rq [749.875305ms]
Dec  5 06:50:27.839: INFO: Created: latency-svc-bfwnk
Dec  5 06:50:27.885: INFO: Got endpoints: latency-svc-69pq2 [752.178744ms]
Dec  5 06:50:27.891: INFO: Created: latency-svc-7srst
Dec  5 06:50:27.932: INFO: Got endpoints: latency-svc-2hdhk [748.246718ms]
Dec  5 06:50:27.938: INFO: Created: latency-svc-9k5v9
Dec  5 06:50:27.982: INFO: Got endpoints: latency-svc-ct8d6 [748.589756ms]
Dec  5 06:50:27.988: INFO: Created: latency-svc-ktgpz
Dec  5 06:50:28.032: INFO: Got endpoints: latency-svc-tn7vt [749.606022ms]
Dec  5 06:50:28.037: INFO: Created: latency-svc-rpvhj
Dec  5 06:50:28.085: INFO: Got endpoints: latency-svc-ltqgj [753.329424ms]
Dec  5 06:50:28.097: INFO: Created: latency-svc-5lqbc
Dec  5 06:50:28.134: INFO: Got endpoints: latency-svc-wqsk5 [751.6609ms]
Dec  5 06:50:28.142: INFO: Created: latency-svc-tk4t5
Dec  5 06:50:28.184: INFO: Got endpoints: latency-svc-gfwxq [751.665437ms]
Dec  5 06:50:28.233: INFO: Got endpoints: latency-svc-5nbp5 [743.404797ms]
Dec  5 06:50:28.282: INFO: Got endpoints: latency-svc-l2mf8 [748.721971ms]
Dec  5 06:50:28.333: INFO: Got endpoints: latency-svc-8tmn5 [750.205444ms]
Dec  5 06:50:28.382: INFO: Got endpoints: latency-svc-lnvgl [749.003462ms]
Dec  5 06:50:28.433: INFO: Got endpoints: latency-svc-gshwm [750.643497ms]
Dec  5 06:50:28.483: INFO: Got endpoints: latency-svc-scxgz [735.662065ms]
Dec  5 06:50:28.533: INFO: Got endpoints: latency-svc-f29jn [747.664867ms]
Dec  5 06:50:28.583: INFO: Got endpoints: latency-svc-bfwnk [749.663419ms]
Dec  5 06:50:28.633: INFO: Got endpoints: latency-svc-7srst [747.888895ms]
Dec  5 06:50:28.682: INFO: Got endpoints: latency-svc-9k5v9 [749.799923ms]
Dec  5 06:50:28.733: INFO: Got endpoints: latency-svc-ktgpz [751.101114ms]
Dec  5 06:50:28.783: INFO: Got endpoints: latency-svc-rpvhj [750.576593ms]
Dec  5 06:50:28.834: INFO: Got endpoints: latency-svc-5lqbc [748.547098ms]
Dec  5 06:50:28.883: INFO: Got endpoints: latency-svc-tk4t5 [748.75733ms]
Dec  5 06:50:28.883: INFO: Latencies: [12.359251ms 13.437694ms 16.519966ms 25.237741ms 27.40363ms 33.267211ms 39.826595ms 42.933866ms 47.604552ms 58.008749ms 58.363629ms 61.863308ms 66.202739ms 68.52275ms 68.985613ms 70.305643ms 70.330874ms 70.470808ms 70.922357ms 71.612586ms 71.866812ms 76.239264ms 78.490355ms 78.686895ms 78.694519ms 78.705029ms 80.772574ms 81.213776ms 82.142322ms 83.10743ms 85.941883ms 93.299687ms 105.550388ms 140.402928ms 192.968033ms 237.425138ms 282.825081ms 326.673427ms 372.640106ms 420.027491ms 465.787798ms 511.114123ms 561.67461ms 598.83552ms 645.189754ms 686.999147ms 734.627405ms 735.662065ms 738.501054ms 741.151117ms 743.052708ms 743.404797ms 743.902914ms 744.272652ms 745.980357ms 746.376306ms 746.867697ms 747.020784ms 747.119202ms 747.127099ms 747.128837ms 747.547947ms 747.664867ms 747.7175ms 747.838483ms 747.888895ms 747.948446ms 748.144951ms 748.164494ms 748.246718ms 748.251437ms 748.328943ms 748.37953ms 748.414892ms 748.498038ms 748.517665ms 748.530602ms 748.547098ms 748.579568ms 748.589756ms 748.604968ms 748.721971ms 748.75733ms 748.795474ms 748.828754ms 748.887793ms 748.897195ms 749.003462ms 749.015277ms 749.015884ms 749.016754ms 749.085797ms 749.086035ms 749.133054ms 749.154218ms 749.179247ms 749.1968ms 749.19824ms 749.220888ms 749.271331ms 749.283718ms 749.291269ms 749.381185ms 749.471732ms 749.47912ms 749.490532ms 749.521362ms 749.523719ms 749.526155ms 749.528664ms 749.550159ms 749.557476ms 749.570695ms 749.590238ms 749.606022ms 749.613329ms 749.637987ms 749.663419ms 749.672613ms 749.710208ms 749.799923ms 749.826636ms 749.851794ms 749.852552ms 749.852571ms 749.875305ms 749.881974ms 749.884533ms 749.947743ms 749.967754ms 749.988382ms 750.008238ms 750.073575ms 750.136794ms 750.144131ms 750.205444ms 750.221251ms 750.284118ms 750.366246ms 750.38759ms 750.414067ms 750.446549ms 750.450611ms 750.45706ms 750.474238ms 750.495193ms 750.500358ms 750.507932ms 750.515601ms 750.524108ms 750.541766ms 750.564871ms 750.576593ms 750.592953ms 750.643497ms 750.678184ms 750.710101ms 750.718932ms 750.778ms 750.811856ms 750.822996ms 750.91712ms 750.955123ms 751.101114ms 751.128565ms 751.160378ms 751.204354ms 751.317091ms 751.320103ms 751.358999ms 751.399007ms 751.487307ms 751.531408ms 751.570116ms 751.594477ms 751.6609ms 751.664899ms 751.665437ms 751.799581ms 751.987351ms 752.052281ms 752.091694ms 752.178744ms 752.46145ms 752.62494ms 752.71642ms 752.865009ms 753.136039ms 753.19952ms 753.22677ms 753.329424ms 753.356911ms 753.499157ms 754.415361ms 754.589345ms 756.423372ms 757.48754ms 757.602265ms 759.18164ms 763.243612ms]
Dec  5 06:50:28.883: INFO: 50 %ile: 749.283718ms
Dec  5 06:50:28.883: INFO: 90 %ile: 752.052281ms
Dec  5 06:50:28.883: INFO: 99 %ile: 759.18164ms
Dec  5 06:50:28.883: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:50:28.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3639" for this suite.
Dec  5 06:50:36.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:50:36.989: INFO: namespace svc-latency-3639 deletion completed in 8.102235947s

• [SLOW TEST:18.849 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:50:36.990: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:50:40.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9203" for this suite.
Dec  5 06:51:02.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:51:02.209: INFO: namespace replication-controller-9203 deletion completed in 22.11173406s

• [SLOW TEST:25.220 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:51:02.210: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec  5 06:51:02.250: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6298,SelfLink:/api/v1/namespaces/watch-6298/configmaps/e2e-watch-test-label-changed,UID:81052b7b-418b-4d8a-9167-073d47948e7b,ResourceVersion:36801,Generation:0,CreationTimestamp:2019-12-05 06:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 06:51:02.250: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6298,SelfLink:/api/v1/namespaces/watch-6298/configmaps/e2e-watch-test-label-changed,UID:81052b7b-418b-4d8a-9167-073d47948e7b,ResourceVersion:36802,Generation:0,CreationTimestamp:2019-12-05 06:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  5 06:51:02.250: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6298,SelfLink:/api/v1/namespaces/watch-6298/configmaps/e2e-watch-test-label-changed,UID:81052b7b-418b-4d8a-9167-073d47948e7b,ResourceVersion:36803,Generation:0,CreationTimestamp:2019-12-05 06:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec  5 06:51:12.271: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6298,SelfLink:/api/v1/namespaces/watch-6298/configmaps/e2e-watch-test-label-changed,UID:81052b7b-418b-4d8a-9167-073d47948e7b,ResourceVersion:36828,Generation:0,CreationTimestamp:2019-12-05 06:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 06:51:12.271: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6298,SelfLink:/api/v1/namespaces/watch-6298/configmaps/e2e-watch-test-label-changed,UID:81052b7b-418b-4d8a-9167-073d47948e7b,ResourceVersion:36829,Generation:0,CreationTimestamp:2019-12-05 06:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec  5 06:51:12.271: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6298,SelfLink:/api/v1/namespaces/watch-6298/configmaps/e2e-watch-test-label-changed,UID:81052b7b-418b-4d8a-9167-073d47948e7b,ResourceVersion:36830,Generation:0,CreationTimestamp:2019-12-05 06:51:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:51:12.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6298" for this suite.
Dec  5 06:51:18.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:51:18.392: INFO: namespace watch-6298 deletion completed in 6.115905043s

• [SLOW TEST:16.182 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:51:18.392: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec  5 06:51:28.510: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 7
	[quantile=0.9] = 255
	[quantile=0.99] = 363
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 593031
	[quantile=0.9] = 613433
	[quantile=0.99] = 615315
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 4
	[quantile=0.9] = 5
	[quantile=0.99] = 21
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 14
	[quantile=0.9] = 23
	[quantile=0.99] = 50
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 14
	[quantile=0.9] = 19
	[quantile=0.99] = 22
For namespace_queue_latency_sum:
	[] = 2322
For namespace_queue_latency_count:
	[] = 161
For namespace_retries:
	[] = 251
For namespace_work_duration:
	[quantile=0.5] = 158085
	[quantile=0.9] = 232169
	[quantile=0.99] = 2671351
For namespace_work_duration_sum:
	[] = 29363765
For namespace_work_duration_count:
	[] = 161
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:51:28.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2903" for this suite.
Dec  5 06:51:34.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:51:34.616: INFO: namespace gc-2903 deletion completed in 6.10179488s

• [SLOW TEST:16.224 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:51:34.616: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec  5 06:51:34.648: INFO: Pod name pod-release: Found 0 pods out of 1
Dec  5 06:51:39.652: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:51:40.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4339" for this suite.
Dec  5 06:51:46.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:51:46.783: INFO: namespace replication-controller-4339 deletion completed in 6.114831029s

• [SLOW TEST:12.168 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:51:46.784: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 06:51:46.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6280'
Dec  5 06:51:47.078: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 06:51:47.078: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Dec  5 06:51:47.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete deployment e2e-test-nginx-deployment --namespace=kubectl-6280'
Dec  5 06:51:47.182: INFO: stderr: ""
Dec  5 06:51:47.182: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:51:47.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6280" for this suite.
Dec  5 06:51:53.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:51:53.320: INFO: namespace kubectl-6280 deletion completed in 6.133452105s

• [SLOW TEST:6.536 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:51:53.320: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  5 06:51:56.367: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:51:56.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2389" for this suite.
Dec  5 06:52:02.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:52:02.517: INFO: namespace container-runtime-2389 deletion completed in 6.135954858s

• [SLOW TEST:9.196 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:52:02.517: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  5 06:52:02.547: INFO: Waiting up to 5m0s for pod "pod-21322ee7-478b-471c-82ef-071a2ef11f38" in namespace "emptydir-5012" to be "success or failure"
Dec  5 06:52:02.550: INFO: Pod "pod-21322ee7-478b-471c-82ef-071a2ef11f38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.706864ms
Dec  5 06:52:04.554: INFO: Pod "pod-21322ee7-478b-471c-82ef-071a2ef11f38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007204868s
STEP: Saw pod success
Dec  5 06:52:04.554: INFO: Pod "pod-21322ee7-478b-471c-82ef-071a2ef11f38" satisfied condition "success or failure"
Dec  5 06:52:04.556: INFO: Trying to get logs from node node2 pod pod-21322ee7-478b-471c-82ef-071a2ef11f38 container test-container: <nil>
STEP: delete the pod
Dec  5 06:52:04.573: INFO: Waiting for pod pod-21322ee7-478b-471c-82ef-071a2ef11f38 to disappear
Dec  5 06:52:04.576: INFO: Pod pod-21322ee7-478b-471c-82ef-071a2ef11f38 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:52:04.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5012" for this suite.
Dec  5 06:52:10.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:52:10.689: INFO: namespace emptydir-5012 deletion completed in 6.109761362s

• [SLOW TEST:8.172 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:52:10.689: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Dec  5 06:52:10.717: INFO: Waiting up to 5m0s for pod "var-expansion-eb68a90a-d96a-4198-86fc-3fae720993db" in namespace "var-expansion-5324" to be "success or failure"
Dec  5 06:52:10.720: INFO: Pod "var-expansion-eb68a90a-d96a-4198-86fc-3fae720993db": Phase="Pending", Reason="", readiness=false. Elapsed: 3.391126ms
Dec  5 06:52:12.724: INFO: Pod "var-expansion-eb68a90a-d96a-4198-86fc-3fae720993db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007605145s
Dec  5 06:52:14.728: INFO: Pod "var-expansion-eb68a90a-d96a-4198-86fc-3fae720993db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011448933s
STEP: Saw pod success
Dec  5 06:52:14.728: INFO: Pod "var-expansion-eb68a90a-d96a-4198-86fc-3fae720993db" satisfied condition "success or failure"
Dec  5 06:52:14.730: INFO: Trying to get logs from node node1 pod var-expansion-eb68a90a-d96a-4198-86fc-3fae720993db container dapi-container: <nil>
STEP: delete the pod
Dec  5 06:52:14.747: INFO: Waiting for pod var-expansion-eb68a90a-d96a-4198-86fc-3fae720993db to disappear
Dec  5 06:52:14.749: INFO: Pod var-expansion-eb68a90a-d96a-4198-86fc-3fae720993db no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:52:14.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5324" for this suite.
Dec  5 06:52:20.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:52:20.860: INFO: namespace var-expansion-5324 deletion completed in 6.108488999s

• [SLOW TEST:10.171 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:52:20.861: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Dec  5 06:52:20.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 cluster-info'
Dec  5 06:52:20.990: INFO: stderr: ""
Dec  5 06:52:20.990: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:52:20.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8657" for this suite.
Dec  5 06:52:27.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:52:27.115: INFO: namespace kubectl-8657 deletion completed in 6.121733888s

• [SLOW TEST:6.255 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:52:27.116: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-6a3bc1a3-4656-45e3-a75a-f6e6c95b7047
STEP: Creating a pod to test consume secrets
Dec  5 06:52:27.151: INFO: Waiting up to 5m0s for pod "pod-secrets-2fd2585a-b391-48dc-9c30-4f79cd032b43" in namespace "secrets-6487" to be "success or failure"
Dec  5 06:52:27.155: INFO: Pod "pod-secrets-2fd2585a-b391-48dc-9c30-4f79cd032b43": Phase="Pending", Reason="", readiness=false. Elapsed: 3.784773ms
Dec  5 06:52:29.157: INFO: Pod "pod-secrets-2fd2585a-b391-48dc-9c30-4f79cd032b43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006265468s
Dec  5 06:52:31.161: INFO: Pod "pod-secrets-2fd2585a-b391-48dc-9c30-4f79cd032b43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009534822s
STEP: Saw pod success
Dec  5 06:52:31.161: INFO: Pod "pod-secrets-2fd2585a-b391-48dc-9c30-4f79cd032b43" satisfied condition "success or failure"
Dec  5 06:52:31.162: INFO: Trying to get logs from node node2 pod pod-secrets-2fd2585a-b391-48dc-9c30-4f79cd032b43 container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 06:52:31.178: INFO: Waiting for pod pod-secrets-2fd2585a-b391-48dc-9c30-4f79cd032b43 to disappear
Dec  5 06:52:31.181: INFO: Pod pod-secrets-2fd2585a-b391-48dc-9c30-4f79cd032b43 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:52:31.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6487" for this suite.
Dec  5 06:52:37.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:52:37.278: INFO: namespace secrets-6487 deletion completed in 6.092203671s

• [SLOW TEST:10.162 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:52:37.278: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 06:52:37.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4699'
Dec  5 06:52:37.399: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 06:52:37.399: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Dec  5 06:52:37.404: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Dec  5 06:52:37.410: INFO: scanned /root for discovery docs: <nil>
Dec  5 06:52:37.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4699'
Dec  5 06:52:53.192: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  5 06:52:53.192: INFO: stdout: "Created e2e-test-nginx-rc-2e74694bc2e709dd894461fc270d03e8\nScaling up e2e-test-nginx-rc-2e74694bc2e709dd894461fc270d03e8 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-2e74694bc2e709dd894461fc270d03e8 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-2e74694bc2e709dd894461fc270d03e8 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec  5 06:52:53.192: INFO: stdout: "Created e2e-test-nginx-rc-2e74694bc2e709dd894461fc270d03e8\nScaling up e2e-test-nginx-rc-2e74694bc2e709dd894461fc270d03e8 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-2e74694bc2e709dd894461fc270d03e8 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-2e74694bc2e709dd894461fc270d03e8 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec  5 06:52:53.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-4699'
Dec  5 06:52:53.291: INFO: stderr: ""
Dec  5 06:52:53.291: INFO: stdout: "e2e-test-nginx-rc-2e74694bc2e709dd894461fc270d03e8-ql248 "
Dec  5 06:52:53.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods e2e-test-nginx-rc-2e74694bc2e709dd894461fc270d03e8-ql248 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4699'
Dec  5 06:52:53.382: INFO: stderr: ""
Dec  5 06:52:53.382: INFO: stdout: "true"
Dec  5 06:52:53.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods e2e-test-nginx-rc-2e74694bc2e709dd894461fc270d03e8-ql248 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4699'
Dec  5 06:52:53.464: INFO: stderr: ""
Dec  5 06:52:53.464: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec  5 06:52:53.464: INFO: e2e-test-nginx-rc-2e74694bc2e709dd894461fc270d03e8-ql248 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Dec  5 06:52:53.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete rc e2e-test-nginx-rc --namespace=kubectl-4699'
Dec  5 06:52:53.540: INFO: stderr: ""
Dec  5 06:52:53.540: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:52:53.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4699" for this suite.
Dec  5 06:52:59.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:52:59.670: INFO: namespace kubectl-4699 deletion completed in 6.125343796s

• [SLOW TEST:22.392 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:52:59.670: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-c60eb4e1-58c9-4825-adce-31da8142c491
STEP: Creating a pod to test consume secrets
Dec  5 06:52:59.701: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06f82d5f-e440-4273-b75a-fafa3a3a6d7e" in namespace "projected-68" to be "success or failure"
Dec  5 06:52:59.705: INFO: Pod "pod-projected-secrets-06f82d5f-e440-4273-b75a-fafa3a3a6d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.214747ms
Dec  5 06:53:01.709: INFO: Pod "pod-projected-secrets-06f82d5f-e440-4273-b75a-fafa3a3a6d7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008096648s
STEP: Saw pod success
Dec  5 06:53:01.709: INFO: Pod "pod-projected-secrets-06f82d5f-e440-4273-b75a-fafa3a3a6d7e" satisfied condition "success or failure"
Dec  5 06:53:01.712: INFO: Trying to get logs from node node1 pod pod-projected-secrets-06f82d5f-e440-4273-b75a-fafa3a3a6d7e container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 06:53:01.726: INFO: Waiting for pod pod-projected-secrets-06f82d5f-e440-4273-b75a-fafa3a3a6d7e to disappear
Dec  5 06:53:01.728: INFO: Pod pod-projected-secrets-06f82d5f-e440-4273-b75a-fafa3a3a6d7e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:53:01.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-68" for this suite.
Dec  5 06:53:07.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:53:07.840: INFO: namespace projected-68 deletion completed in 6.109557143s

• [SLOW TEST:8.170 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:53:07.840: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-0d2f8a91-caae-497d-81b8-cf70d36f5ba6
STEP: Creating a pod to test consume secrets
Dec  5 06:53:07.871: INFO: Waiting up to 5m0s for pod "pod-secrets-2fce8e23-20d6-405b-a1e6-e66d20961cb5" in namespace "secrets-1296" to be "success or failure"
Dec  5 06:53:07.874: INFO: Pod "pod-secrets-2fce8e23-20d6-405b-a1e6-e66d20961cb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.819634ms
Dec  5 06:53:09.878: INFO: Pod "pod-secrets-2fce8e23-20d6-405b-a1e6-e66d20961cb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006725705s
STEP: Saw pod success
Dec  5 06:53:09.878: INFO: Pod "pod-secrets-2fce8e23-20d6-405b-a1e6-e66d20961cb5" satisfied condition "success or failure"
Dec  5 06:53:09.880: INFO: Trying to get logs from node node2 pod pod-secrets-2fce8e23-20d6-405b-a1e6-e66d20961cb5 container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 06:53:09.901: INFO: Waiting for pod pod-secrets-2fce8e23-20d6-405b-a1e6-e66d20961cb5 to disappear
Dec  5 06:53:09.907: INFO: Pod pod-secrets-2fce8e23-20d6-405b-a1e6-e66d20961cb5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:53:09.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1296" for this suite.
Dec  5 06:53:15.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:53:16.010: INFO: namespace secrets-1296 deletion completed in 6.100113052s

• [SLOW TEST:8.169 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:53:16.010: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  5 06:53:16.040: INFO: Waiting up to 5m0s for pod "pod-8cbf4b73-ac6c-422c-b7d1-18fb496f85fb" in namespace "emptydir-5021" to be "success or failure"
Dec  5 06:53:16.041: INFO: Pod "pod-8cbf4b73-ac6c-422c-b7d1-18fb496f85fb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.581204ms
Dec  5 06:53:18.045: INFO: Pod "pod-8cbf4b73-ac6c-422c-b7d1-18fb496f85fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005017533s
STEP: Saw pod success
Dec  5 06:53:18.045: INFO: Pod "pod-8cbf4b73-ac6c-422c-b7d1-18fb496f85fb" satisfied condition "success or failure"
Dec  5 06:53:18.048: INFO: Trying to get logs from node node1 pod pod-8cbf4b73-ac6c-422c-b7d1-18fb496f85fb container test-container: <nil>
STEP: delete the pod
Dec  5 06:53:18.066: INFO: Waiting for pod pod-8cbf4b73-ac6c-422c-b7d1-18fb496f85fb to disappear
Dec  5 06:53:18.069: INFO: Pod pod-8cbf4b73-ac6c-422c-b7d1-18fb496f85fb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:53:18.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5021" for this suite.
Dec  5 06:53:24.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:53:24.175: INFO: namespace emptydir-5021 deletion completed in 6.100978086s

• [SLOW TEST:8.164 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:53:24.176: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  5 06:53:24.204: INFO: Waiting up to 5m0s for pod "pod-ca828ce6-aa37-4a7a-9da4-33e32dca414d" in namespace "emptydir-9127" to be "success or failure"
Dec  5 06:53:24.208: INFO: Pod "pod-ca828ce6-aa37-4a7a-9da4-33e32dca414d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.687385ms
Dec  5 06:53:26.212: INFO: Pod "pod-ca828ce6-aa37-4a7a-9da4-33e32dca414d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007209742s
STEP: Saw pod success
Dec  5 06:53:26.212: INFO: Pod "pod-ca828ce6-aa37-4a7a-9da4-33e32dca414d" satisfied condition "success or failure"
Dec  5 06:53:26.215: INFO: Trying to get logs from node node2 pod pod-ca828ce6-aa37-4a7a-9da4-33e32dca414d container test-container: <nil>
STEP: delete the pod
Dec  5 06:53:26.232: INFO: Waiting for pod pod-ca828ce6-aa37-4a7a-9da4-33e32dca414d to disappear
Dec  5 06:53:26.234: INFO: Pod pod-ca828ce6-aa37-4a7a-9da4-33e32dca414d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:53:26.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9127" for this suite.
Dec  5 06:53:32.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:53:32.365: INFO: namespace emptydir-9127 deletion completed in 6.127532093s

• [SLOW TEST:8.190 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:53:32.366: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2966.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2966.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2966.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2966.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2966.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2966.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2966.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2966.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2966.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2966.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2966.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2966.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2966.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 108.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.108_udp@PTR;check="$$(dig +tcp +noall +answer +search 108.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.108_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2966.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2966.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2966.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2966.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2966.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2966.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2966.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2966.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2966.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2966.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2966.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2966.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2966.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 108.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.108_udp@PTR;check="$$(dig +tcp +noall +answer +search 108.61.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.61.108_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  5 06:54:54.430: INFO: Unable to read wheezy_udp@dns-test-service.dns-2966.svc.cluster.local from pod dns-2966/dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0: the server could not find the requested resource (get pods dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0)
Dec  5 06:54:54.433: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2966.svc.cluster.local from pod dns-2966/dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0: the server could not find the requested resource (get pods dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0)
Dec  5 06:54:54.435: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2966.svc.cluster.local from pod dns-2966/dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0: the server could not find the requested resource (get pods dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0)
Dec  5 06:54:54.438: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2966.svc.cluster.local from pod dns-2966/dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0: the server could not find the requested resource (get pods dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0)
Dec  5 06:54:54.459: INFO: Unable to read jessie_udp@dns-test-service.dns-2966.svc.cluster.local from pod dns-2966/dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0: the server could not find the requested resource (get pods dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0)
Dec  5 06:54:54.462: INFO: Unable to read jessie_tcp@dns-test-service.dns-2966.svc.cluster.local from pod dns-2966/dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0: the server could not find the requested resource (get pods dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0)
Dec  5 06:54:54.465: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2966.svc.cluster.local from pod dns-2966/dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0: the server could not find the requested resource (get pods dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0)
Dec  5 06:54:54.467: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2966.svc.cluster.local from pod dns-2966/dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0: the server could not find the requested resource (get pods dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0)
Dec  5 06:54:54.482: INFO: Lookups using dns-2966/dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0 failed for: [wheezy_udp@dns-test-service.dns-2966.svc.cluster.local wheezy_tcp@dns-test-service.dns-2966.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2966.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2966.svc.cluster.local jessie_udp@dns-test-service.dns-2966.svc.cluster.local jessie_tcp@dns-test-service.dns-2966.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2966.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2966.svc.cluster.local]

Dec  5 06:54:59.542: INFO: DNS probes using dns-2966/dns-test-1f7ec1ca-5d7f-4d50-b510-ec7dd59497c0 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:54:59.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2966" for this suite.
Dec  5 06:55:05.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:55:05.717: INFO: namespace dns-2966 deletion completed in 6.135398466s

• [SLOW TEST:93.350 seconds]
[sig-network] DNS
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:55:05.717: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 06:55:05.748: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c065188-47e8-415a-90c5-0588bf0f589a" in namespace "projected-9049" to be "success or failure"
Dec  5 06:55:05.752: INFO: Pod "downwardapi-volume-0c065188-47e8-415a-90c5-0588bf0f589a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.181453ms
Dec  5 06:55:07.756: INFO: Pod "downwardapi-volume-0c065188-47e8-415a-90c5-0588bf0f589a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007963651s
STEP: Saw pod success
Dec  5 06:55:07.756: INFO: Pod "downwardapi-volume-0c065188-47e8-415a-90c5-0588bf0f589a" satisfied condition "success or failure"
Dec  5 06:55:07.758: INFO: Trying to get logs from node node2 pod downwardapi-volume-0c065188-47e8-415a-90c5-0588bf0f589a container client-container: <nil>
STEP: delete the pod
Dec  5 06:55:07.776: INFO: Waiting for pod downwardapi-volume-0c065188-47e8-415a-90c5-0588bf0f589a to disappear
Dec  5 06:55:07.778: INFO: Pod downwardapi-volume-0c065188-47e8-415a-90c5-0588bf0f589a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:55:07.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9049" for this suite.
Dec  5 06:55:13.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:55:13.880: INFO: namespace projected-9049 deletion completed in 6.097717514s

• [SLOW TEST:8.163 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:55:13.880: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Dec  5 06:55:13.911: INFO: Waiting up to 5m0s for pod "client-containers-80c2921a-e673-48c1-b891-e870689f0ded" in namespace "containers-8831" to be "success or failure"
Dec  5 06:55:13.913: INFO: Pod "client-containers-80c2921a-e673-48c1-b891-e870689f0ded": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068175ms
Dec  5 06:55:15.916: INFO: Pod "client-containers-80c2921a-e673-48c1-b891-e870689f0ded": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004930012s
Dec  5 06:55:17.920: INFO: Pod "client-containers-80c2921a-e673-48c1-b891-e870689f0ded": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00856861s
STEP: Saw pod success
Dec  5 06:55:17.920: INFO: Pod "client-containers-80c2921a-e673-48c1-b891-e870689f0ded" satisfied condition "success or failure"
Dec  5 06:55:17.921: INFO: Trying to get logs from node node1 pod client-containers-80c2921a-e673-48c1-b891-e870689f0ded container test-container: <nil>
STEP: delete the pod
Dec  5 06:55:17.938: INFO: Waiting for pod client-containers-80c2921a-e673-48c1-b891-e870689f0ded to disappear
Dec  5 06:55:17.940: INFO: Pod client-containers-80c2921a-e673-48c1-b891-e870689f0ded no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:55:17.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8831" for this suite.
Dec  5 06:55:23.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:55:24.060: INFO: namespace containers-8831 deletion completed in 6.116489992s

• [SLOW TEST:10.180 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:55:24.061: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec  5 06:55:26.103: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-771445370 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec  5 06:55:36.211: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:55:36.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1424" for this suite.
Dec  5 06:55:42.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:55:42.343: INFO: namespace pods-1424 deletion completed in 6.123314203s

• [SLOW TEST:18.282 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:55:42.343: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Dec  5 06:55:42.382: INFO: Waiting up to 5m0s for pod "pod-da7d2630-f79c-4eac-b528-8ee905cc091c" in namespace "emptydir-4856" to be "success or failure"
Dec  5 06:55:42.385: INFO: Pod "pod-da7d2630-f79c-4eac-b528-8ee905cc091c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.824698ms
Dec  5 06:55:44.388: INFO: Pod "pod-da7d2630-f79c-4eac-b528-8ee905cc091c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006441633s
Dec  5 06:55:46.393: INFO: Pod "pod-da7d2630-f79c-4eac-b528-8ee905cc091c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01056092s
STEP: Saw pod success
Dec  5 06:55:46.393: INFO: Pod "pod-da7d2630-f79c-4eac-b528-8ee905cc091c" satisfied condition "success or failure"
Dec  5 06:55:46.395: INFO: Trying to get logs from node node1 pod pod-da7d2630-f79c-4eac-b528-8ee905cc091c container test-container: <nil>
STEP: delete the pod
Dec  5 06:55:46.408: INFO: Waiting for pod pod-da7d2630-f79c-4eac-b528-8ee905cc091c to disappear
Dec  5 06:55:46.410: INFO: Pod pod-da7d2630-f79c-4eac-b528-8ee905cc091c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:55:46.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4856" for this suite.
Dec  5 06:55:52.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:55:52.532: INFO: namespace emptydir-4856 deletion completed in 6.119529674s

• [SLOW TEST:10.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:55:52.533: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-a0e45f22-45e1-4977-ae12-e64995f26894
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-a0e45f22-45e1-4977-ae12-e64995f26894
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:57:08.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4016" for this suite.
Dec  5 06:57:31.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:57:31.105: INFO: namespace configmap-4016 deletion completed in 22.105550786s

• [SLOW TEST:98.572 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:57:31.106: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-846
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-846
STEP: Deleting pre-stop pod
Dec  5 06:57:48.164: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:57:48.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-846" for this suite.
Dec  5 06:58:26.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:58:26.263: INFO: namespace prestop-846 deletion completed in 38.092446204s

• [SLOW TEST:55.158 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:58:26.264: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9137.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9137.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  5 06:58:28.328: INFO: DNS probes using dns-9137/dns-test-15e788ee-e347-43cb-9dce-20ed9ce76dc0 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:58:28.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9137" for this suite.
Dec  5 06:58:34.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:58:34.454: INFO: namespace dns-9137 deletion completed in 6.114980143s

• [SLOW TEST:8.190 seconds]
[sig-network] DNS
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:58:34.455: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 06:58:34.487: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b152ed7f-8b76-4e7b-8b78-b1bd206617c9" in namespace "projected-383" to be "success or failure"
Dec  5 06:58:34.489: INFO: Pod "downwardapi-volume-b152ed7f-8b76-4e7b-8b78-b1bd206617c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009704ms
Dec  5 06:58:36.493: INFO: Pod "downwardapi-volume-b152ed7f-8b76-4e7b-8b78-b1bd206617c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006031084s
STEP: Saw pod success
Dec  5 06:58:36.493: INFO: Pod "downwardapi-volume-b152ed7f-8b76-4e7b-8b78-b1bd206617c9" satisfied condition "success or failure"
Dec  5 06:58:36.495: INFO: Trying to get logs from node node2 pod downwardapi-volume-b152ed7f-8b76-4e7b-8b78-b1bd206617c9 container client-container: <nil>
STEP: delete the pod
Dec  5 06:58:36.509: INFO: Waiting for pod downwardapi-volume-b152ed7f-8b76-4e7b-8b78-b1bd206617c9 to disappear
Dec  5 06:58:36.511: INFO: Pod downwardapi-volume-b152ed7f-8b76-4e7b-8b78-b1bd206617c9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:58:36.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-383" for this suite.
Dec  5 06:58:42.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:58:42.650: INFO: namespace projected-383 deletion completed in 6.134825312s

• [SLOW TEST:8.196 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:58:42.651: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8213
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  5 06:58:42.686: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  5 06:59:08.803: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.96.95 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8213 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 06:59:08.803: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:59:09.987: INFO: Found all expected endpoints: [netserver-0]
Dec  5 06:59:09.990: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.90.91 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8213 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 06:59:09.990: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 06:59:11.197: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:59:11.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8213" for this suite.
Dec  5 06:59:33.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:59:33.324: INFO: namespace pod-network-test-8213 deletion completed in 22.121270717s

• [SLOW TEST:50.673 seconds]
[sig-network] Networking
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:59:33.325: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8675.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8675.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8675.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8675.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  5 06:59:37.383: INFO: DNS probes using dns-test-b13b5bd5-8ae0-4689-b881-77628bd69858 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8675.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8675.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8675.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8675.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  5 06:59:41.418: INFO: DNS probes using dns-test-d1260ee9-e708-4d56-b93d-58cf39bfce00 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8675.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8675.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8675.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8675.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  5 06:59:45.466: INFO: DNS probes using dns-test-12b6b947-d5b6-4cd0-b18e-59b63e009045 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 06:59:45.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8675" for this suite.
Dec  5 06:59:51.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 06:59:51.609: INFO: namespace dns-8675 deletion completed in 6.121485157s

• [SLOW TEST:18.284 seconds]
[sig-network] DNS
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 06:59:51.610: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7086, will wait for the garbage collector to delete the pods
Dec  5 06:59:53.704: INFO: Deleting Job.batch foo took: 5.509573ms
Dec  5 06:59:54.404: INFO: Terminating Job.batch foo pods took: 700.23686ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:00:29.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7086" for this suite.
Dec  5 07:00:35.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:00:35.132: INFO: namespace job-7086 deletion completed in 6.122033534s

• [SLOW TEST:43.522 seconds]
[sig-apps] Job
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:00:35.134: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  5 07:00:35.172: INFO: Waiting up to 5m0s for pod "downward-api-d0044464-3932-4cdc-bd5a-d71caee70594" in namespace "downward-api-5331" to be "success or failure"
Dec  5 07:00:35.173: INFO: Pod "downward-api-d0044464-3932-4cdc-bd5a-d71caee70594": Phase="Pending", Reason="", readiness=false. Elapsed: 1.577924ms
Dec  5 07:00:37.177: INFO: Pod "downward-api-d0044464-3932-4cdc-bd5a-d71caee70594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004835766s
STEP: Saw pod success
Dec  5 07:00:37.177: INFO: Pod "downward-api-d0044464-3932-4cdc-bd5a-d71caee70594" satisfied condition "success or failure"
Dec  5 07:00:37.179: INFO: Trying to get logs from node node2 pod downward-api-d0044464-3932-4cdc-bd5a-d71caee70594 container dapi-container: <nil>
STEP: delete the pod
Dec  5 07:00:37.193: INFO: Waiting for pod downward-api-d0044464-3932-4cdc-bd5a-d71caee70594 to disappear
Dec  5 07:00:37.195: INFO: Pod downward-api-d0044464-3932-4cdc-bd5a-d71caee70594 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:00:37.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5331" for this suite.
Dec  5 07:00:43.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:00:43.334: INFO: namespace downward-api-5331 deletion completed in 6.135336804s

• [SLOW TEST:8.200 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:00:43.334: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-5246e390-e025-498a-b410-41674d6bef92
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:00:45.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6223" for this suite.
Dec  5 07:01:07.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:01:07.497: INFO: namespace configmap-6223 deletion completed in 22.097887283s

• [SLOW TEST:24.163 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:01:07.498: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  5 07:01:07.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-5339'
Dec  5 07:01:07.649: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  5 07:01:07.649: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Dec  5 07:01:09.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete deployment e2e-test-nginx-deployment --namespace=kubectl-5339'
Dec  5 07:01:09.770: INFO: stderr: ""
Dec  5 07:01:09.770: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:01:09.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5339" for this suite.
Dec  5 07:01:15.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:01:15.877: INFO: namespace kubectl-5339 deletion completed in 6.102057424s

• [SLOW TEST:8.379 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:01:15.877: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec  5 07:01:18.926: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:01:18.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3801" for this suite.
Dec  5 07:01:40.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:01:41.068: INFO: namespace replicaset-3801 deletion completed in 22.115948346s

• [SLOW TEST:25.191 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:01:41.069: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5743
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  5 07:01:41.093: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  5 07:01:59.143: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.90.97:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5743 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 07:01:59.144: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 07:01:59.345: INFO: Found all expected endpoints: [netserver-0]
Dec  5 07:01:59.349: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.96.102:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5743 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 07:01:59.349: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 07:01:59.545: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:01:59.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5743" for this suite.
Dec  5 07:02:21.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:02:21.664: INFO: namespace pod-network-test-5743 deletion completed in 22.114136524s

• [SLOW TEST:40.595 seconds]
[sig-network] Networking
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:02:21.664: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Dec  5 07:02:21.697: INFO: Waiting up to 5m0s for pod "var-expansion-1953402a-4635-4be7-9ddb-1a40c2f91edd" in namespace "var-expansion-4439" to be "success or failure"
Dec  5 07:02:21.700: INFO: Pod "var-expansion-1953402a-4635-4be7-9ddb-1a40c2f91edd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.827022ms
Dec  5 07:02:23.703: INFO: Pod "var-expansion-1953402a-4635-4be7-9ddb-1a40c2f91edd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006005533s
Dec  5 07:02:25.706: INFO: Pod "var-expansion-1953402a-4635-4be7-9ddb-1a40c2f91edd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008976342s
STEP: Saw pod success
Dec  5 07:02:25.706: INFO: Pod "var-expansion-1953402a-4635-4be7-9ddb-1a40c2f91edd" satisfied condition "success or failure"
Dec  5 07:02:25.708: INFO: Trying to get logs from node node1 pod var-expansion-1953402a-4635-4be7-9ddb-1a40c2f91edd container dapi-container: <nil>
STEP: delete the pod
Dec  5 07:02:25.721: INFO: Waiting for pod var-expansion-1953402a-4635-4be7-9ddb-1a40c2f91edd to disappear
Dec  5 07:02:25.723: INFO: Pod var-expansion-1953402a-4635-4be7-9ddb-1a40c2f91edd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:02:25.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4439" for this suite.
Dec  5 07:02:31.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:02:31.912: INFO: namespace var-expansion-4439 deletion completed in 6.185707011s

• [SLOW TEST:10.248 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:02:31.912: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 07:02:31.940: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:02:36.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3146" for this suite.
Dec  5 07:03:26.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:03:26.234: INFO: namespace pods-3146 deletion completed in 50.08762228s

• [SLOW TEST:54.321 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:03:26.234: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  5 07:03:26.278: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:26.280: INFO: Number of nodes with available pods: 0
Dec  5 07:03:26.280: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:27.290: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:27.294: INFO: Number of nodes with available pods: 0
Dec  5 07:03:27.294: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:28.286: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:28.289: INFO: Number of nodes with available pods: 1
Dec  5 07:03:28.289: INFO: Node node2 is running more than one daemon pod
Dec  5 07:03:29.285: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:29.288: INFO: Number of nodes with available pods: 2
Dec  5 07:03:29.288: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec  5 07:03:29.301: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:29.303: INFO: Number of nodes with available pods: 1
Dec  5 07:03:29.303: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:30.309: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:30.312: INFO: Number of nodes with available pods: 1
Dec  5 07:03:30.312: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:31.307: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:31.309: INFO: Number of nodes with available pods: 1
Dec  5 07:03:31.309: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:32.308: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:32.310: INFO: Number of nodes with available pods: 1
Dec  5 07:03:32.310: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:33.308: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:33.310: INFO: Number of nodes with available pods: 1
Dec  5 07:03:33.310: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:34.308: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:34.311: INFO: Number of nodes with available pods: 1
Dec  5 07:03:34.311: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:35.308: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:35.311: INFO: Number of nodes with available pods: 1
Dec  5 07:03:35.311: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:36.309: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:36.312: INFO: Number of nodes with available pods: 1
Dec  5 07:03:36.312: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:37.307: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:37.309: INFO: Number of nodes with available pods: 1
Dec  5 07:03:37.309: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:38.309: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:38.311: INFO: Number of nodes with available pods: 1
Dec  5 07:03:38.311: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:39.311: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:39.313: INFO: Number of nodes with available pods: 1
Dec  5 07:03:39.313: INFO: Node node1 is running more than one daemon pod
Dec  5 07:03:40.308: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:03:40.310: INFO: Number of nodes with available pods: 2
Dec  5 07:03:40.310: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7176, will wait for the garbage collector to delete the pods
Dec  5 07:03:40.374: INFO: Deleting DaemonSet.extensions daemon-set took: 8.742358ms
Dec  5 07:03:41.075: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.426893ms
Dec  5 07:03:53.077: INFO: Number of nodes with available pods: 0
Dec  5 07:03:53.077: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 07:03:53.079: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7176/daemonsets","resourceVersion":"40304"},"items":null}

Dec  5 07:03:53.080: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7176/pods","resourceVersion":"40304"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:03:53.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7176" for this suite.
Dec  5 07:03:59.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:03:59.198: INFO: namespace daemonsets-7176 deletion completed in 6.106905741s

• [SLOW TEST:32.964 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:03:59.199: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-6c616809-8878-44b2-b166-6e36b830f7ef
STEP: Creating a pod to test consume secrets
Dec  5 07:03:59.240: INFO: Waiting up to 5m0s for pod "pod-secrets-96b1e0d8-76fb-40f9-b45f-b7451382ef46" in namespace "secrets-3238" to be "success or failure"
Dec  5 07:03:59.243: INFO: Pod "pod-secrets-96b1e0d8-76fb-40f9-b45f-b7451382ef46": Phase="Pending", Reason="", readiness=false. Elapsed: 3.247599ms
Dec  5 07:04:01.247: INFO: Pod "pod-secrets-96b1e0d8-76fb-40f9-b45f-b7451382ef46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006606601s
STEP: Saw pod success
Dec  5 07:04:01.247: INFO: Pod "pod-secrets-96b1e0d8-76fb-40f9-b45f-b7451382ef46" satisfied condition "success or failure"
Dec  5 07:04:01.249: INFO: Trying to get logs from node node1 pod pod-secrets-96b1e0d8-76fb-40f9-b45f-b7451382ef46 container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 07:04:01.262: INFO: Waiting for pod pod-secrets-96b1e0d8-76fb-40f9-b45f-b7451382ef46 to disappear
Dec  5 07:04:01.278: INFO: Pod pod-secrets-96b1e0d8-76fb-40f9-b45f-b7451382ef46 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:04:01.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3238" for this suite.
Dec  5 07:04:07.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:04:07.399: INFO: namespace secrets-3238 deletion completed in 6.117599552s

• [SLOW TEST:8.200 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:04:07.400: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-51d61ef5-5447-4a46-a001-cd0d3b912402
STEP: Creating a pod to test consume secrets
Dec  5 07:04:07.438: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-949d64a9-f99d-42d7-b3ee-b98ae71c3d03" in namespace "projected-3192" to be "success or failure"
Dec  5 07:04:07.442: INFO: Pod "pod-projected-secrets-949d64a9-f99d-42d7-b3ee-b98ae71c3d03": Phase="Pending", Reason="", readiness=false. Elapsed: 3.895487ms
Dec  5 07:04:09.452: INFO: Pod "pod-projected-secrets-949d64a9-f99d-42d7-b3ee-b98ae71c3d03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014123432s
STEP: Saw pod success
Dec  5 07:04:09.452: INFO: Pod "pod-projected-secrets-949d64a9-f99d-42d7-b3ee-b98ae71c3d03" satisfied condition "success or failure"
Dec  5 07:04:09.455: INFO: Trying to get logs from node node2 pod pod-projected-secrets-949d64a9-f99d-42d7-b3ee-b98ae71c3d03 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 07:04:09.473: INFO: Waiting for pod pod-projected-secrets-949d64a9-f99d-42d7-b3ee-b98ae71c3d03 to disappear
Dec  5 07:04:09.475: INFO: Pod pod-projected-secrets-949d64a9-f99d-42d7-b3ee-b98ae71c3d03 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:04:09.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3192" for this suite.
Dec  5 07:04:15.486: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:04:15.595: INFO: namespace projected-3192 deletion completed in 6.117308579s

• [SLOW TEST:8.195 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:04:15.595: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 07:04:15.624: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d567e26-d27d-4520-8963-952351777b6c" in namespace "downward-api-7635" to be "success or failure"
Dec  5 07:04:15.627: INFO: Pod "downwardapi-volume-7d567e26-d27d-4520-8963-952351777b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.091016ms
Dec  5 07:04:17.631: INFO: Pod "downwardapi-volume-7d567e26-d27d-4520-8963-952351777b6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006333536s
STEP: Saw pod success
Dec  5 07:04:17.631: INFO: Pod "downwardapi-volume-7d567e26-d27d-4520-8963-952351777b6c" satisfied condition "success or failure"
Dec  5 07:04:17.633: INFO: Trying to get logs from node node1 pod downwardapi-volume-7d567e26-d27d-4520-8963-952351777b6c container client-container: <nil>
STEP: delete the pod
Dec  5 07:04:17.649: INFO: Waiting for pod downwardapi-volume-7d567e26-d27d-4520-8963-952351777b6c to disappear
Dec  5 07:04:17.652: INFO: Pod downwardapi-volume-7d567e26-d27d-4520-8963-952351777b6c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:04:17.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7635" for this suite.
Dec  5 07:04:23.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:04:23.754: INFO: namespace downward-api-7635 deletion completed in 6.099103989s

• [SLOW TEST:8.159 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:04:23.755: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:05:23.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5840" for this suite.
Dec  5 07:05:45.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:05:45.901: INFO: namespace container-probe-5840 deletion completed in 22.106665928s

• [SLOW TEST:82.147 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:05:45.901: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:05:49.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2608" for this suite.
Dec  5 07:05:55.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:05:56.040: INFO: namespace kubelet-test-2608 deletion completed in 6.100210342s

• [SLOW TEST:10.139 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:05:56.041: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec  5 07:05:56.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-9686'
Dec  5 07:05:56.479: INFO: stderr: ""
Dec  5 07:05:56.479: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  5 07:05:56.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9686'
Dec  5 07:05:56.557: INFO: stderr: ""
Dec  5 07:05:56.557: INFO: stdout: "update-demo-nautilus-6czq9 update-demo-nautilus-99w8s "
Dec  5 07:05:56.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-6czq9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9686'
Dec  5 07:05:56.640: INFO: stderr: ""
Dec  5 07:05:56.640: INFO: stdout: ""
Dec  5 07:05:56.640: INFO: update-demo-nautilus-6czq9 is created but not running
Dec  5 07:06:01.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9686'
Dec  5 07:06:01.742: INFO: stderr: ""
Dec  5 07:06:01.742: INFO: stdout: "update-demo-nautilus-6czq9 update-demo-nautilus-99w8s "
Dec  5 07:06:01.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-6czq9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9686'
Dec  5 07:06:01.853: INFO: stderr: ""
Dec  5 07:06:01.853: INFO: stdout: "true"
Dec  5 07:06:01.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-6czq9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9686'
Dec  5 07:06:01.954: INFO: stderr: ""
Dec  5 07:06:01.954: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 07:06:01.954: INFO: validating pod update-demo-nautilus-6czq9
Dec  5 07:06:01.958: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 07:06:01.958: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 07:06:01.958: INFO: update-demo-nautilus-6czq9 is verified up and running
Dec  5 07:06:01.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-99w8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9686'
Dec  5 07:06:02.049: INFO: stderr: ""
Dec  5 07:06:02.049: INFO: stdout: "true"
Dec  5 07:06:02.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods update-demo-nautilus-99w8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9686'
Dec  5 07:06:02.154: INFO: stderr: ""
Dec  5 07:06:02.154: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  5 07:06:02.154: INFO: validating pod update-demo-nautilus-99w8s
Dec  5 07:06:02.160: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  5 07:06:02.160: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  5 07:06:02.160: INFO: update-demo-nautilus-99w8s is verified up and running
STEP: using delete to clean up resources
Dec  5 07:06:02.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete --grace-period=0 --force -f - --namespace=kubectl-9686'
Dec  5 07:06:02.254: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 07:06:02.254: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  5 07:06:02.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9686'
Dec  5 07:06:02.361: INFO: stderr: "No resources found.\n"
Dec  5 07:06:02.361: INFO: stdout: ""
Dec  5 07:06:02.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -l name=update-demo --namespace=kubectl-9686 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 07:06:02.437: INFO: stderr: ""
Dec  5 07:06:02.437: INFO: stdout: "update-demo-nautilus-6czq9\nupdate-demo-nautilus-99w8s\n"
Dec  5 07:06:02.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9686'
Dec  5 07:06:03.025: INFO: stderr: "No resources found.\n"
Dec  5 07:06:03.025: INFO: stdout: ""
Dec  5 07:06:03.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -l name=update-demo --namespace=kubectl-9686 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 07:06:03.114: INFO: stderr: ""
Dec  5 07:06:03.114: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:06:03.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9686" for this suite.
Dec  5 07:06:19.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:06:19.220: INFO: namespace kubectl-9686 deletion completed in 16.102067794s

• [SLOW TEST:23.179 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:06:19.220: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-8347/configmap-test-b3b402ae-3b6f-461c-b4d3-353c597942d3
STEP: Creating a pod to test consume configMaps
Dec  5 07:06:19.257: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ceb5d83-8a59-4495-a36c-faf3e02fab38" in namespace "configmap-8347" to be "success or failure"
Dec  5 07:06:19.265: INFO: Pod "pod-configmaps-6ceb5d83-8a59-4495-a36c-faf3e02fab38": Phase="Pending", Reason="", readiness=false. Elapsed: 7.848423ms
Dec  5 07:06:21.269: INFO: Pod "pod-configmaps-6ceb5d83-8a59-4495-a36c-faf3e02fab38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011518896s
STEP: Saw pod success
Dec  5 07:06:21.269: INFO: Pod "pod-configmaps-6ceb5d83-8a59-4495-a36c-faf3e02fab38" satisfied condition "success or failure"
Dec  5 07:06:21.271: INFO: Trying to get logs from node node2 pod pod-configmaps-6ceb5d83-8a59-4495-a36c-faf3e02fab38 container env-test: <nil>
STEP: delete the pod
Dec  5 07:06:21.289: INFO: Waiting for pod pod-configmaps-6ceb5d83-8a59-4495-a36c-faf3e02fab38 to disappear
Dec  5 07:06:21.292: INFO: Pod pod-configmaps-6ceb5d83-8a59-4495-a36c-faf3e02fab38 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:06:21.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8347" for this suite.
Dec  5 07:06:27.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:06:27.410: INFO: namespace configmap-8347 deletion completed in 6.114227383s

• [SLOW TEST:8.189 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:06:27.410: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7285
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  5 07:06:27.437: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  5 07:06:51.497: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.90.107:8080/dial?request=hostName&protocol=udp&host=10.233.90.106&port=8081&tries=1'] Namespace:pod-network-test-7285 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 07:06:51.497: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 07:06:51.706: INFO: Waiting for endpoints: map[]
Dec  5 07:06:51.709: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.90.107:8080/dial?request=hostName&protocol=udp&host=10.233.96.109&port=8081&tries=1'] Namespace:pod-network-test-7285 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 07:06:51.709: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 07:06:51.934: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:06:51.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7285" for this suite.
Dec  5 07:07:13.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:07:14.053: INFO: namespace pod-network-test-7285 deletion completed in 22.11495733s

• [SLOW TEST:46.643 seconds]
[sig-network] Networking
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:07:14.054: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-9eba7214-d411-47bb-8f7b-0b43207f12a1
STEP: Creating a pod to test consume secrets
Dec  5 07:07:14.086: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9c129ec3-f888-4cdb-bf90-ca45d8ba15a6" in namespace "projected-985" to be "success or failure"
Dec  5 07:07:14.090: INFO: Pod "pod-projected-secrets-9c129ec3-f888-4cdb-bf90-ca45d8ba15a6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.599887ms
Dec  5 07:07:16.094: INFO: Pod "pod-projected-secrets-9c129ec3-f888-4cdb-bf90-ca45d8ba15a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007560851s
STEP: Saw pod success
Dec  5 07:07:16.094: INFO: Pod "pod-projected-secrets-9c129ec3-f888-4cdb-bf90-ca45d8ba15a6" satisfied condition "success or failure"
Dec  5 07:07:16.096: INFO: Trying to get logs from node node1 pod pod-projected-secrets-9c129ec3-f888-4cdb-bf90-ca45d8ba15a6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 07:07:16.113: INFO: Waiting for pod pod-projected-secrets-9c129ec3-f888-4cdb-bf90-ca45d8ba15a6 to disappear
Dec  5 07:07:16.115: INFO: Pod pod-projected-secrets-9c129ec3-f888-4cdb-bf90-ca45d8ba15a6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:07:16.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-985" for this suite.
Dec  5 07:07:22.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:07:22.222: INFO: namespace projected-985 deletion completed in 6.102308879s

• [SLOW TEST:8.168 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:07:22.222: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-fbf4a66e-a098-43a1-ad97-c312b95aad67
STEP: Creating a pod to test consume secrets
Dec  5 07:07:22.261: INFO: Waiting up to 5m0s for pod "pod-secrets-4faeb025-a5fe-46fe-8938-55c297b08081" in namespace "secrets-9535" to be "success or failure"
Dec  5 07:07:22.265: INFO: Pod "pod-secrets-4faeb025-a5fe-46fe-8938-55c297b08081": Phase="Pending", Reason="", readiness=false. Elapsed: 4.135504ms
Dec  5 07:07:24.268: INFO: Pod "pod-secrets-4faeb025-a5fe-46fe-8938-55c297b08081": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007282665s
Dec  5 07:07:26.270: INFO: Pod "pod-secrets-4faeb025-a5fe-46fe-8938-55c297b08081": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009699672s
STEP: Saw pod success
Dec  5 07:07:26.270: INFO: Pod "pod-secrets-4faeb025-a5fe-46fe-8938-55c297b08081" satisfied condition "success or failure"
Dec  5 07:07:26.272: INFO: Trying to get logs from node node2 pod pod-secrets-4faeb025-a5fe-46fe-8938-55c297b08081 container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 07:07:26.289: INFO: Waiting for pod pod-secrets-4faeb025-a5fe-46fe-8938-55c297b08081 to disappear
Dec  5 07:07:26.291: INFO: Pod pod-secrets-4faeb025-a5fe-46fe-8938-55c297b08081 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:07:26.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9535" for this suite.
Dec  5 07:07:32.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:07:32.406: INFO: namespace secrets-9535 deletion completed in 6.112259352s

• [SLOW TEST:10.184 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:07:32.407: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Dec  5 07:07:32.447: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2125" to be "success or failure"
Dec  5 07:07:32.450: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.399543ms
Dec  5 07:07:34.453: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006061875s
Dec  5 07:07:36.457: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010120603s
STEP: Saw pod success
Dec  5 07:07:36.457: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec  5 07:07:36.460: INFO: Trying to get logs from node node1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec  5 07:07:36.476: INFO: Waiting for pod pod-host-path-test to disappear
Dec  5 07:07:36.481: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:07:36.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2125" for this suite.
Dec  5 07:07:42.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:07:42.607: INFO: namespace hostpath-2125 deletion completed in 6.121990748s

• [SLOW TEST:10.200 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:07:42.607: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 07:07:42.647: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:07:44.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6893" for this suite.
Dec  5 07:08:22.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:08:22.821: INFO: namespace pods-6893 deletion completed in 38.131272819s

• [SLOW TEST:40.214 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:08:22.822: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-024ada35-f517-4dad-bd39-2cd45feac181
STEP: Creating a pod to test consume secrets
Dec  5 07:08:22.859: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9e02eca4-8cf2-4fa9-9b57-99833a49d455" in namespace "projected-2034" to be "success or failure"
Dec  5 07:08:22.862: INFO: Pod "pod-projected-secrets-9e02eca4-8cf2-4fa9-9b57-99833a49d455": Phase="Pending", Reason="", readiness=false. Elapsed: 2.318198ms
Dec  5 07:08:24.866: INFO: Pod "pod-projected-secrets-9e02eca4-8cf2-4fa9-9b57-99833a49d455": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006380955s
Dec  5 07:08:26.869: INFO: Pod "pod-projected-secrets-9e02eca4-8cf2-4fa9-9b57-99833a49d455": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009552885s
STEP: Saw pod success
Dec  5 07:08:26.869: INFO: Pod "pod-projected-secrets-9e02eca4-8cf2-4fa9-9b57-99833a49d455" satisfied condition "success or failure"
Dec  5 07:08:26.871: INFO: Trying to get logs from node node1 pod pod-projected-secrets-9e02eca4-8cf2-4fa9-9b57-99833a49d455 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 07:08:26.893: INFO: Waiting for pod pod-projected-secrets-9e02eca4-8cf2-4fa9-9b57-99833a49d455 to disappear
Dec  5 07:08:26.895: INFO: Pod pod-projected-secrets-9e02eca4-8cf2-4fa9-9b57-99833a49d455 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:08:26.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2034" for this suite.
Dec  5 07:08:32.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:08:33.008: INFO: namespace projected-2034 deletion completed in 6.109622809s

• [SLOW TEST:10.186 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:08:33.009: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 07:08:33.038: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0271956c-8f7b-4150-b8d1-6120b402e197" in namespace "projected-9410" to be "success or failure"
Dec  5 07:08:33.042: INFO: Pod "downwardapi-volume-0271956c-8f7b-4150-b8d1-6120b402e197": Phase="Pending", Reason="", readiness=false. Elapsed: 4.13748ms
Dec  5 07:08:35.045: INFO: Pod "downwardapi-volume-0271956c-8f7b-4150-b8d1-6120b402e197": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007289804s
STEP: Saw pod success
Dec  5 07:08:35.045: INFO: Pod "downwardapi-volume-0271956c-8f7b-4150-b8d1-6120b402e197" satisfied condition "success or failure"
Dec  5 07:08:35.047: INFO: Trying to get logs from node node2 pod downwardapi-volume-0271956c-8f7b-4150-b8d1-6120b402e197 container client-container: <nil>
STEP: delete the pod
Dec  5 07:08:35.072: INFO: Waiting for pod downwardapi-volume-0271956c-8f7b-4150-b8d1-6120b402e197 to disappear
Dec  5 07:08:35.076: INFO: Pod downwardapi-volume-0271956c-8f7b-4150-b8d1-6120b402e197 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:08:35.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9410" for this suite.
Dec  5 07:08:41.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:08:41.188: INFO: namespace projected-9410 deletion completed in 6.107478906s

• [SLOW TEST:8.179 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:08:41.188: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec  5 07:08:41.218: INFO: Waiting up to 5m0s for pod "pod-fc15ef29-5ec7-4759-8c9e-2078394affb4" in namespace "emptydir-9972" to be "success or failure"
Dec  5 07:08:41.221: INFO: Pod "pod-fc15ef29-5ec7-4759-8c9e-2078394affb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.591069ms
Dec  5 07:08:43.224: INFO: Pod "pod-fc15ef29-5ec7-4759-8c9e-2078394affb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005880736s
STEP: Saw pod success
Dec  5 07:08:43.224: INFO: Pod "pod-fc15ef29-5ec7-4759-8c9e-2078394affb4" satisfied condition "success or failure"
Dec  5 07:08:43.226: INFO: Trying to get logs from node node1 pod pod-fc15ef29-5ec7-4759-8c9e-2078394affb4 container test-container: <nil>
STEP: delete the pod
Dec  5 07:08:43.238: INFO: Waiting for pod pod-fc15ef29-5ec7-4759-8c9e-2078394affb4 to disappear
Dec  5 07:08:43.240: INFO: Pod pod-fc15ef29-5ec7-4759-8c9e-2078394affb4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:08:43.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9972" for this suite.
Dec  5 07:08:49.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:08:49.380: INFO: namespace emptydir-9972 deletion completed in 6.136032381s

• [SLOW TEST:8.192 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:08:49.380: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4303
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec  5 07:08:49.428: INFO: Found 0 stateful pods, waiting for 3
Dec  5 07:08:59.433: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 07:08:59.433: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 07:08:59.433: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 07:08:59.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-4303 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 07:08:59.754: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 07:08:59.754: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 07:08:59.754: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  5 07:09:09.784: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec  5 07:09:19.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-4303 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 07:09:20.070: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 07:09:20.070: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 07:09:20.070: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  5 07:09:30.086: INFO: Waiting for StatefulSet statefulset-4303/ss2 to complete update
Dec  5 07:09:30.086: INFO: Waiting for Pod statefulset-4303/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:09:30.086: INFO: Waiting for Pod statefulset-4303/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:09:30.086: INFO: Waiting for Pod statefulset-4303/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:09:40.094: INFO: Waiting for StatefulSet statefulset-4303/ss2 to complete update
Dec  5 07:09:40.094: INFO: Waiting for Pod statefulset-4303/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:09:40.094: INFO: Waiting for Pod statefulset-4303/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:09:50.094: INFO: Waiting for StatefulSet statefulset-4303/ss2 to complete update
Dec  5 07:09:50.094: INFO: Waiting for Pod statefulset-4303/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:09:50.094: INFO: Waiting for Pod statefulset-4303/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:10:00.094: INFO: Waiting for StatefulSet statefulset-4303/ss2 to complete update
Dec  5 07:10:00.094: INFO: Waiting for Pod statefulset-4303/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:10:00.094: INFO: Waiting for Pod statefulset-4303/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:10:10.093: INFO: Waiting for StatefulSet statefulset-4303/ss2 to complete update
Dec  5 07:10:10.093: INFO: Waiting for Pod statefulset-4303/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:10:10.093: INFO: Waiting for Pod statefulset-4303/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:10:20.092: INFO: Waiting for StatefulSet statefulset-4303/ss2 to complete update
Dec  5 07:10:20.092: INFO: Waiting for Pod statefulset-4303/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:10:20.092: INFO: Waiting for Pod statefulset-4303/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:10:30.093: INFO: Waiting for StatefulSet statefulset-4303/ss2 to complete update
Dec  5 07:10:30.093: INFO: Waiting for Pod statefulset-4303/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:10:40.095: INFO: Waiting for StatefulSet statefulset-4303/ss2 to complete update
Dec  5 07:10:40.095: INFO: Waiting for Pod statefulset-4303/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:10:50.093: INFO: Waiting for StatefulSet statefulset-4303/ss2 to complete update
Dec  5 07:10:50.093: INFO: Waiting for Pod statefulset-4303/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:11:00.093: INFO: Waiting for StatefulSet statefulset-4303/ss2 to complete update
Dec  5 07:11:00.093: INFO: Waiting for Pod statefulset-4303/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:11:10.093: INFO: Waiting for StatefulSet statefulset-4303/ss2 to complete update
Dec  5 07:11:10.093: INFO: Waiting for Pod statefulset-4303/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:11:20.093: INFO: Waiting for StatefulSet statefulset-4303/ss2 to complete update
Dec  5 07:11:20.093: INFO: Waiting for Pod statefulset-4303/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Dec  5 07:11:30.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-4303 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  5 07:11:30.383: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  5 07:11:30.383: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  5 07:11:30.383: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  5 07:11:40.411: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec  5 07:11:50.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 exec --namespace=statefulset-4303 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  5 07:11:50.708: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  5 07:11:50.708: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  5 07:11:50.708: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  5 07:12:10.725: INFO: Deleting all statefulset in ns statefulset-4303
Dec  5 07:12:10.727: INFO: Scaling statefulset ss2 to 0
Dec  5 07:12:30.740: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 07:12:30.742: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:12:30.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4303" for this suite.
Dec  5 07:12:36.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:12:36.854: INFO: namespace statefulset-4303 deletion completed in 6.096914854s

• [SLOW TEST:227.474 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:12:36.854: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  5 07:12:42.933: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 842
	[quantile=0.9] = 23944
	[quantile=0.99] = 179523
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 28420
	[quantile=0.9] = 610084
	[quantile=0.99] = 612257
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 4
	[quantile=0.9] = 6
	[quantile=0.99] = 21
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 15
	[quantile=0.9] = 28
	[quantile=0.99] = 48
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 13
	[quantile=0.9] = 15
	[quantile=0.99] = 21
For namespace_queue_latency_sum:
	[] = 3337
For namespace_queue_latency_count:
	[] = 235
For namespace_retries:
	[] = 376
For namespace_work_duration:
	[quantile=0.5] = 161188
	[quantile=0.9] = 204193
	[quantile=0.99] = 282866
For namespace_work_duration_sum:
	[] = 41861568
For namespace_work_duration_count:
	[] = 235
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:12:42.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3898" for this suite.
Dec  5 07:12:48.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:12:49.052: INFO: namespace gc-3898 deletion completed in 6.11426791s

• [SLOW TEST:12.198 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:12:49.052: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  5 07:12:59.154: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 842
	[quantile=0.9] = 179523
	[quantile=0.99] = 601171
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 28868
	[quantile=0.9] = 609990
	[quantile=0.99] = 753109
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = NaN
	[quantile=0.9] = NaN
	[quantile=0.99] = NaN
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 4
	[quantile=0.9] = 5
	[quantile=0.99] = 20
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 15
	[quantile=0.9] = 27
	[quantile=0.99] = 48
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 13
	[quantile=0.9] = 15
	[quantile=0.99] = 21
For namespace_queue_latency_sum:
	[] = 3349
For namespace_queue_latency_count:
	[] = 236
For namespace_retries:
	[] = 378
For namespace_work_duration:
	[quantile=0.5] = 161518
	[quantile=0.9] = 205903
	[quantile=0.99] = 297847
For namespace_work_duration_sum:
	[] = 42159415
For namespace_work_duration_count:
	[] = 236
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:12:59.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3258" for this suite.
Dec  5 07:13:05.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:13:05.275: INFO: namespace gc-3258 deletion completed in 6.116073034s

• [SLOW TEST:16.223 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:13:05.275: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-a1cc553a-34f5-4904-9e64-26e84c9f5643
STEP: Creating a pod to test consume configMaps
Dec  5 07:13:05.307: INFO: Waiting up to 5m0s for pod "pod-configmaps-131bcc36-d174-4f6d-aa5f-2a2751dce14e" in namespace "configmap-6868" to be "success or failure"
Dec  5 07:13:05.310: INFO: Pod "pod-configmaps-131bcc36-d174-4f6d-aa5f-2a2751dce14e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.152137ms
Dec  5 07:13:07.313: INFO: Pod "pod-configmaps-131bcc36-d174-4f6d-aa5f-2a2751dce14e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005452616s
STEP: Saw pod success
Dec  5 07:13:07.313: INFO: Pod "pod-configmaps-131bcc36-d174-4f6d-aa5f-2a2751dce14e" satisfied condition "success or failure"
Dec  5 07:13:07.315: INFO: Trying to get logs from node node1 pod pod-configmaps-131bcc36-d174-4f6d-aa5f-2a2751dce14e container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 07:13:07.329: INFO: Waiting for pod pod-configmaps-131bcc36-d174-4f6d-aa5f-2a2751dce14e to disappear
Dec  5 07:13:07.331: INFO: Pod pod-configmaps-131bcc36-d174-4f6d-aa5f-2a2751dce14e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:13:07.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6868" for this suite.
Dec  5 07:13:13.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:13:13.456: INFO: namespace configmap-6868 deletion completed in 6.121286333s

• [SLOW TEST:8.181 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:13:13.456: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:13:13.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-428" for this suite.
Dec  5 07:13:19.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:13:19.610: INFO: namespace services-428 deletion completed in 6.119403082s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.154 seconds]
[sig-network] Services
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:13:19.611: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-17d6ce7f-4ef3-4b43-bde9-3690e310fe6b
STEP: Creating a pod to test consume secrets
Dec  5 07:13:19.677: INFO: Waiting up to 5m0s for pod "pod-secrets-4de2626d-5686-42f2-a3af-a76014e592b7" in namespace "secrets-4672" to be "success or failure"
Dec  5 07:13:19.680: INFO: Pod "pod-secrets-4de2626d-5686-42f2-a3af-a76014e592b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.986954ms
Dec  5 07:13:21.684: INFO: Pod "pod-secrets-4de2626d-5686-42f2-a3af-a76014e592b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007006618s
STEP: Saw pod success
Dec  5 07:13:21.684: INFO: Pod "pod-secrets-4de2626d-5686-42f2-a3af-a76014e592b7" satisfied condition "success or failure"
Dec  5 07:13:21.685: INFO: Trying to get logs from node node2 pod pod-secrets-4de2626d-5686-42f2-a3af-a76014e592b7 container secret-volume-test: <nil>
STEP: delete the pod
Dec  5 07:13:21.703: INFO: Waiting for pod pod-secrets-4de2626d-5686-42f2-a3af-a76014e592b7 to disappear
Dec  5 07:13:21.704: INFO: Pod pod-secrets-4de2626d-5686-42f2-a3af-a76014e592b7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:13:21.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4672" for this suite.
Dec  5 07:13:27.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:13:27.836: INFO: namespace secrets-4672 deletion completed in 6.128277582s
STEP: Destroying namespace "secret-namespace-3412" for this suite.
Dec  5 07:13:33.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:13:33.941: INFO: namespace secret-namespace-3412 deletion completed in 6.105508236s

• [SLOW TEST:14.331 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:13:33.942: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-212b7dd0-5c52-4cf3-8970-272ec950932c in namespace container-probe-7467
Dec  5 07:13:35.979: INFO: Started pod busybox-212b7dd0-5c52-4cf3-8970-272ec950932c in namespace container-probe-7467
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 07:13:35.982: INFO: Initial restart count of pod busybox-212b7dd0-5c52-4cf3-8970-272ec950932c is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:17:36.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7467" for this suite.
Dec  5 07:17:42.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:17:42.577: INFO: namespace container-probe-7467 deletion completed in 6.119488595s

• [SLOW TEST:248.635 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:17:42.578: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-8645/secret-test-c7c0238a-d91d-417b-9a40-1649643e40d0
STEP: Creating a pod to test consume secrets
Dec  5 07:17:42.625: INFO: Waiting up to 5m0s for pod "pod-configmaps-1dafabd3-2801-4766-9117-b819ec4a95f2" in namespace "secrets-8645" to be "success or failure"
Dec  5 07:17:42.630: INFO: Pod "pod-configmaps-1dafabd3-2801-4766-9117-b819ec4a95f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.639845ms
Dec  5 07:17:44.633: INFO: Pod "pod-configmaps-1dafabd3-2801-4766-9117-b819ec4a95f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007399227s
STEP: Saw pod success
Dec  5 07:17:44.633: INFO: Pod "pod-configmaps-1dafabd3-2801-4766-9117-b819ec4a95f2" satisfied condition "success or failure"
Dec  5 07:17:44.634: INFO: Trying to get logs from node node2 pod pod-configmaps-1dafabd3-2801-4766-9117-b819ec4a95f2 container env-test: <nil>
STEP: delete the pod
Dec  5 07:17:44.651: INFO: Waiting for pod pod-configmaps-1dafabd3-2801-4766-9117-b819ec4a95f2 to disappear
Dec  5 07:17:44.654: INFO: Pod pod-configmaps-1dafabd3-2801-4766-9117-b819ec4a95f2 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:17:44.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8645" for this suite.
Dec  5 07:17:50.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:17:50.757: INFO: namespace secrets-8645 deletion completed in 6.098094438s

• [SLOW TEST:8.179 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:17:50.757: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-d5739595-a9ac-4c71-9ec3-8e8577ca2d8a
STEP: Creating secret with name secret-projected-all-test-volume-e00b2ec4-b1e1-47d8-8e15-468884a49919
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec  5 07:17:50.791: INFO: Waiting up to 5m0s for pod "projected-volume-0bdeb4f3-3c2f-4912-833b-1cb38fc76be0" in namespace "projected-8599" to be "success or failure"
Dec  5 07:17:50.794: INFO: Pod "projected-volume-0bdeb4f3-3c2f-4912-833b-1cb38fc76be0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.28451ms
Dec  5 07:17:52.798: INFO: Pod "projected-volume-0bdeb4f3-3c2f-4912-833b-1cb38fc76be0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007109752s
STEP: Saw pod success
Dec  5 07:17:52.798: INFO: Pod "projected-volume-0bdeb4f3-3c2f-4912-833b-1cb38fc76be0" satisfied condition "success or failure"
Dec  5 07:17:52.800: INFO: Trying to get logs from node node1 pod projected-volume-0bdeb4f3-3c2f-4912-833b-1cb38fc76be0 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec  5 07:17:52.816: INFO: Waiting for pod projected-volume-0bdeb4f3-3c2f-4912-833b-1cb38fc76be0 to disappear
Dec  5 07:17:52.819: INFO: Pod projected-volume-0bdeb4f3-3c2f-4912-833b-1cb38fc76be0 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:17:52.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8599" for this suite.
Dec  5 07:17:58.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:17:58.925: INFO: namespace projected-8599 deletion completed in 6.102073072s

• [SLOW TEST:8.168 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:17:58.928: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-06c009f4-19b5-4f39-9fb4-2bfe9b1a9892
STEP: Creating a pod to test consume configMaps
Dec  5 07:17:58.965: INFO: Waiting up to 5m0s for pod "pod-configmaps-100e26d1-5b7a-434f-9cb8-17cacbc1ca59" in namespace "configmap-6798" to be "success or failure"
Dec  5 07:17:58.968: INFO: Pod "pod-configmaps-100e26d1-5b7a-434f-9cb8-17cacbc1ca59": Phase="Pending", Reason="", readiness=false. Elapsed: 3.413227ms
Dec  5 07:18:00.971: INFO: Pod "pod-configmaps-100e26d1-5b7a-434f-9cb8-17cacbc1ca59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005774891s
STEP: Saw pod success
Dec  5 07:18:00.971: INFO: Pod "pod-configmaps-100e26d1-5b7a-434f-9cb8-17cacbc1ca59" satisfied condition "success or failure"
Dec  5 07:18:00.972: INFO: Trying to get logs from node node2 pod pod-configmaps-100e26d1-5b7a-434f-9cb8-17cacbc1ca59 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 07:18:00.991: INFO: Waiting for pod pod-configmaps-100e26d1-5b7a-434f-9cb8-17cacbc1ca59 to disappear
Dec  5 07:18:00.994: INFO: Pod pod-configmaps-100e26d1-5b7a-434f-9cb8-17cacbc1ca59 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:18:00.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6798" for this suite.
Dec  5 07:18:07.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:18:07.116: INFO: namespace configmap-6798 deletion completed in 6.11623834s

• [SLOW TEST:8.188 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:18:07.116: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Dec  5 07:18:07.662: INFO: created pod pod-service-account-defaultsa
Dec  5 07:18:07.662: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  5 07:18:07.665: INFO: created pod pod-service-account-mountsa
Dec  5 07:18:07.665: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  5 07:18:07.669: INFO: created pod pod-service-account-nomountsa
Dec  5 07:18:07.669: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  5 07:18:07.672: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  5 07:18:07.672: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  5 07:18:07.676: INFO: created pod pod-service-account-mountsa-mountspec
Dec  5 07:18:07.676: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  5 07:18:07.682: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  5 07:18:07.682: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  5 07:18:07.687: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  5 07:18:07.687: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  5 07:18:07.692: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  5 07:18:07.692: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  5 07:18:07.698: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  5 07:18:07.698: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:18:07.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9480" for this suite.
Dec  5 07:18:13.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:18:13.824: INFO: namespace svcaccounts-9480 deletion completed in 6.120789669s

• [SLOW TEST:6.708 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:18:13.824: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 07:18:13.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-9270'
Dec  5 07:18:14.265: INFO: stderr: ""
Dec  5 07:18:14.265: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec  5 07:18:14.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-9270'
Dec  5 07:18:14.457: INFO: stderr: ""
Dec  5 07:18:14.457: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  5 07:18:15.461: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 07:18:15.461: INFO: Found 0 / 1
Dec  5 07:18:16.463: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 07:18:16.463: INFO: Found 1 / 1
Dec  5 07:18:16.463: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  5 07:18:16.465: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 07:18:16.465: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  5 07:18:16.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 describe pod redis-master-cnmsk --namespace=kubectl-9270'
Dec  5 07:18:16.580: INFO: stderr: ""
Dec  5 07:18:16.580: INFO: stdout: "Name:           redis-master-cnmsk\nNamespace:      kubectl-9270\nPriority:       0\nNode:           node2/192.168.0.8\nStart Time:     Thu, 05 Dec 2019 07:18:14 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             10.233.96.135\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://1e316f9d28d86ec15f8293da3d747dc6c2eb90662e7ba18d945b473618aa2956\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 05 Dec 2019 07:18:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-g8hwk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-g8hwk:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-g8hwk\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9270/redis-master-cnmsk to node2\n  Normal  Pulled     1s    kubelet, node2     Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, node2     Created container redis-master\n  Normal  Started    1s    kubelet, node2     Started container redis-master\n"
Dec  5 07:18:16.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 describe rc redis-master --namespace=kubectl-9270'
Dec  5 07:18:16.714: INFO: stderr: ""
Dec  5 07:18:16.714: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9270\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-cnmsk\n"
Dec  5 07:18:16.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 describe service redis-master --namespace=kubectl-9270'
Dec  5 07:18:16.820: INFO: stderr: ""
Dec  5 07:18:16.820: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9270\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.233.8.154\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.233.96.135:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  5 07:18:16.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 describe node master'
Dec  5 07:18:16.939: INFO: stderr: ""
Dec  5 07:18:16.939: INFO: stdout: "Name:               master\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.0.6\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 05 Dec 2019 03:15:55 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 05 Dec 2019 03:17:07 +0000   Thu, 05 Dec 2019 03:17:07 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 05 Dec 2019 07:18:08 +0000   Thu, 05 Dec 2019 03:15:51 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 05 Dec 2019 07:18:08 +0000   Thu, 05 Dec 2019 03:15:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 05 Dec 2019 07:18:08 +0000   Thu, 05 Dec 2019 03:15:51 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 05 Dec 2019 07:18:08 +0000   Thu, 05 Dec 2019 03:17:03 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.0.6\n  Hostname:    master\nCapacity:\n cpu:                4\n ephemeral-storage:  103078888Ki\n hugepages-2Mi:      0\n memory:             8009424Ki\n pods:               110\nAllocatable:\n cpu:                3800m\n ephemeral-storage:  94997503024\n hugepages-2Mi:      0\n memory:             7407024Ki\n pods:               110\nSystem Info:\n Machine ID:                    8d6e3335d0073d34b2b0bcdb33cdef89\n System UUID:                   8D6E3335-D007-3D34-B2B0-BCDB33CDEF89\n Boot ID:                       1bd5374d-80a2-4755-b2de-e12ba3277429\n Kernel Version:                3.10.0-862.el7.x86_64\n OS Image:                      CentOS Linux 7 (Core)\n Operating System:              linux\n Architecture:                  amd64\n Container Runtime Version:     docker://18.9.7\n Kubelet Version:               v1.15.5\n Kube-Proxy Version:            v1.15.5\nPodCIDR:                        10.233.64.0/24\nNon-terminated Pods:            (18 in total)\n  Namespace                     Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                     ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                   calico-kube-controllers-5fd67f6788-qlblf                   30m (0%)      100m (2%)   64M (0%)         256M (3%)      4h\n  kube-system                   calico-node-prqtf                                          150m (3%)     300m (7%)   64M (0%)         500M (6%)      4h1m\n  kube-system                   coredns-647d4f64c4-c768m                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     4h\n  kube-system                   dns-autoscaler-77486c4479-7p9b7                            20m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h\n  kube-system                   kube-apiserver-master                                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         4h1m\n  kube-system                   kube-controller-manager-master                             200m (5%)     0 (0%)      0 (0%)           0 (0%)         4h1m\n  kube-system                   kube-proxy-5xjnx                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h2m\n  kube-system                   kube-scheduler-master                                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         4h1m\n  kube-system                   nodelocaldns-lwsvb                                         100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     4h\n  kubesphere-monitoring-system  node-exporter-hmrtz                                        20m (0%)      270m (7%)   200Mi (2%)       220Mi (3%)     3h16m\n  kubesphere-system             ks-account-d4c5cdf9d-mdbxz                                 20m (0%)      1 (26%)     100Mi (1%)       500Mi (6%)     3h16m\n  kubesphere-system             ks-apigateway-65dd54f989-xkqxx                             20m (0%)      1 (26%)     100Mi (1%)       500Mi (6%)     3h17m\n  kubesphere-system             ks-apiserver-6d7ddd7d-j2z59                                20m (0%)      1 (26%)     100Mi (1%)       1Gi (14%)      3h17m\n  kubesphere-system             ks-console-6f7f75bb48-sm6lq                                20m (0%)      1 (26%)     100Mi (1%)       512Mi (7%)     3h16m\n  kubesphere-system             ks-controller-manager-6dd9b76d75-bks76                     30m (0%)      500m (13%)  50Mi (0%)        500Mi (6%)     3h16m\n  kubesphere-system             openldap-0                                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h17m\n  kubesphere-system             redis-5d4844b947-dgkpd                                     20m (0%)      1 (26%)     100Mi (1%)       1000Mi (13%)   3h17m\n  sonobuoy                      sonobuoy-systemd-logs-daemon-set-4e7bfb56bab946ed-hxwrr    0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests          Limits\n  --------           --------          ------\n  cpu                1100m (28%)       6170m (162%)\n  memory             1071718400 (14%)  5575255296 (73%)\n  ephemeral-storage  0 (0%)            0 (0%)\nEvents:              <none>\n"
Dec  5 07:18:16.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 describe namespace kubectl-9270'
Dec  5 07:18:17.041: INFO: stderr: ""
Dec  5 07:18:17.041: INFO: stdout: "Name:         kubectl-9270\nLabels:       e2e-framework=kubectl\n              e2e-run=a461aeec-f126-42b0-aba5-4d4c7e0810c1\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:18:17.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9270" for this suite.
Dec  5 07:18:39.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:18:39.140: INFO: namespace kubectl-9270 deletion completed in 22.095258116s

• [SLOW TEST:25.316 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:18:39.141: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Dec  5 07:18:39.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 api-versions'
Dec  5 07:18:39.278: INFO: stderr: ""
Dec  5 07:18:39.278: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napp.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndevops.kubesphere.io/v1alpha1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nlogging.kubesphere.io/v1alpha1\nmonitoring.coreos.com/v1\nnetworking.istio.io/v1alpha3\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\nopenebs.io/v1alpha1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nservicemesh.kubesphere.io/v1alpha2\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntenant.kubesphere.io/v1alpha1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:18:39.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4530" for this suite.
Dec  5 07:18:45.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:18:45.459: INFO: namespace kubectl-4530 deletion completed in 6.176884953s

• [SLOW TEST:6.318 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:18:45.459: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-cdbf865d-774f-48ce-b1a0-0e8c87e3fda2
STEP: Creating secret with name s-test-opt-upd-229cb172-12d3-480a-b6f0-494051ad13bb
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-cdbf865d-774f-48ce-b1a0-0e8c87e3fda2
STEP: Updating secret s-test-opt-upd-229cb172-12d3-480a-b6f0-494051ad13bb
STEP: Creating secret with name s-test-opt-create-ad68f5b1-4b33-42f6-9f15-1a2933e0c0e9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:18:49.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8017" for this suite.
Dec  5 07:19:11.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:19:11.763: INFO: namespace secrets-8017 deletion completed in 22.115192285s

• [SLOW TEST:26.304 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:19:11.763: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec  5 07:19:11.791: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Dec  5 07:19:12.621: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec  5 07:19:14.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:16.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:18.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:20.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:22.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:24.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:26.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:28.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:30.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:32.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:34.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:36.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:38.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:40.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:42.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:44.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:46.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:48.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:50.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:52.654: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:54.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:56.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:19:58.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:00.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:02.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:04.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:06.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:08.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:10.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:12.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:14.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:16.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:18.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:20.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:22.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:24.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:26.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:28.651: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:30.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:32.650: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:34.652: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:36.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711127152, loc:(*time.Location)(0x7ed0a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  5 07:20:39.695: INFO: Waited 1.036956982s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:20:40.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5575" for this suite.
Dec  5 07:20:46.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:20:46.712: INFO: namespace aggregator-5575 deletion completed in 6.192229958s

• [SLOW TEST:94.949 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:20:46.712: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  5 07:20:52.780: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:20:52.782: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:20:54.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:20:54.785: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:20:56.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:20:56.786: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:20:58.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:20:58.786: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:00.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:00.786: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:02.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:02.785: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:04.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:04.786: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:06.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:06.785: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:08.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:08.786: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:10.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:10.785: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:12.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:12.788: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:14.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:14.786: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:16.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:16.786: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:18.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:18.786: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:20.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:20.786: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:22.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:22.786: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  5 07:21:24.782: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  5 07:21:24.785: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:21:24.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7297" for this suite.
Dec  5 07:21:46.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:21:46.895: INFO: namespace container-lifecycle-hook-7297 deletion completed in 22.106972782s

• [SLOW TEST:60.183 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:21:46.896: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  5 07:21:49.944: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:21:49.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8014" for this suite.
Dec  5 07:21:55.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:21:56.060: INFO: namespace container-runtime-8014 deletion completed in 6.103360441s

• [SLOW TEST:9.164 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:21:56.060: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-9287
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9287 to expose endpoints map[]
Dec  5 07:21:56.093: INFO: successfully validated that service endpoint-test2 in namespace services-9287 exposes endpoints map[] (2.488225ms elapsed)
STEP: Creating pod pod1 in namespace services-9287
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9287 to expose endpoints map[pod1:[80]]
Dec  5 07:21:58.116: INFO: successfully validated that service endpoint-test2 in namespace services-9287 exposes endpoints map[pod1:[80]] (2.018774491s elapsed)
STEP: Creating pod pod2 in namespace services-9287
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9287 to expose endpoints map[pod1:[80] pod2:[80]]
Dec  5 07:22:00.157: INFO: successfully validated that service endpoint-test2 in namespace services-9287 exposes endpoints map[pod1:[80] pod2:[80]] (2.037661975s elapsed)
STEP: Deleting pod pod1 in namespace services-9287
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9287 to expose endpoints map[pod2:[80]]
Dec  5 07:22:01.172: INFO: successfully validated that service endpoint-test2 in namespace services-9287 exposes endpoints map[pod2:[80]] (1.012366959s elapsed)
STEP: Deleting pod pod2 in namespace services-9287
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9287 to expose endpoints map[]
Dec  5 07:22:01.181: INFO: successfully validated that service endpoint-test2 in namespace services-9287 exposes endpoints map[] (3.873849ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:22:01.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9287" for this suite.
Dec  5 07:22:23.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:22:23.305: INFO: namespace services-9287 deletion completed in 22.108199204s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:27.245 seconds]
[sig-network] Services
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:22:23.306: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-wsjs
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 07:22:23.344: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wsjs" in namespace "subpath-7733" to be "success or failure"
Dec  5 07:22:23.347: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673788ms
Dec  5 07:22:25.350: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Running", Reason="", readiness=true. Elapsed: 2.006062335s
Dec  5 07:22:27.354: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Running", Reason="", readiness=true. Elapsed: 4.009747138s
Dec  5 07:22:29.357: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Running", Reason="", readiness=true. Elapsed: 6.012988512s
Dec  5 07:22:31.364: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Running", Reason="", readiness=true. Elapsed: 8.019815508s
Dec  5 07:22:33.368: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Running", Reason="", readiness=true. Elapsed: 10.023715588s
Dec  5 07:22:35.371: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Running", Reason="", readiness=true. Elapsed: 12.02705467s
Dec  5 07:22:37.374: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Running", Reason="", readiness=true. Elapsed: 14.029992858s
Dec  5 07:22:39.377: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Running", Reason="", readiness=true. Elapsed: 16.033301794s
Dec  5 07:22:41.380: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Running", Reason="", readiness=true. Elapsed: 18.036171684s
Dec  5 07:22:43.384: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Running", Reason="", readiness=true. Elapsed: 20.039796196s
Dec  5 07:22:45.388: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Running", Reason="", readiness=true. Elapsed: 22.043567298s
Dec  5 07:22:47.391: INFO: Pod "pod-subpath-test-secret-wsjs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.046957508s
STEP: Saw pod success
Dec  5 07:22:47.391: INFO: Pod "pod-subpath-test-secret-wsjs" satisfied condition "success or failure"
Dec  5 07:22:47.393: INFO: Trying to get logs from node node2 pod pod-subpath-test-secret-wsjs container test-container-subpath-secret-wsjs: <nil>
STEP: delete the pod
Dec  5 07:22:47.410: INFO: Waiting for pod pod-subpath-test-secret-wsjs to disappear
Dec  5 07:22:47.412: INFO: Pod pod-subpath-test-secret-wsjs no longer exists
STEP: Deleting pod pod-subpath-test-secret-wsjs
Dec  5 07:22:47.412: INFO: Deleting pod "pod-subpath-test-secret-wsjs" in namespace "subpath-7733"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:22:47.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7733" for this suite.
Dec  5 07:22:53.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:22:53.539: INFO: namespace subpath-7733 deletion completed in 6.123320856s

• [SLOW TEST:30.234 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:22:53.539: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 07:22:53.578: INFO: Waiting up to 5m0s for pod "downwardapi-volume-766a37cf-06ab-46e1-b5a2-dbebadc80059" in namespace "downward-api-2126" to be "success or failure"
Dec  5 07:22:53.581: INFO: Pod "downwardapi-volume-766a37cf-06ab-46e1-b5a2-dbebadc80059": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546411ms
Dec  5 07:22:55.585: INFO: Pod "downwardapi-volume-766a37cf-06ab-46e1-b5a2-dbebadc80059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006855331s
STEP: Saw pod success
Dec  5 07:22:55.585: INFO: Pod "downwardapi-volume-766a37cf-06ab-46e1-b5a2-dbebadc80059" satisfied condition "success or failure"
Dec  5 07:22:55.588: INFO: Trying to get logs from node node1 pod downwardapi-volume-766a37cf-06ab-46e1-b5a2-dbebadc80059 container client-container: <nil>
STEP: delete the pod
Dec  5 07:22:55.605: INFO: Waiting for pod downwardapi-volume-766a37cf-06ab-46e1-b5a2-dbebadc80059 to disappear
Dec  5 07:22:55.608: INFO: Pod downwardapi-volume-766a37cf-06ab-46e1-b5a2-dbebadc80059 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:22:55.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2126" for this suite.
Dec  5 07:23:01.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:23:01.735: INFO: namespace downward-api-2126 deletion completed in 6.123068155s

• [SLOW TEST:8.196 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:23:01.736: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec  5 07:23:01.791: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-378,SelfLink:/api/v1/namespaces/watch-378/configmaps/e2e-watch-test-resource-version,UID:ca715e9e-96d1-49dc-9fb4-b00e2532755c,ResourceVersion:45432,Generation:0,CreationTimestamp:2019-12-05 07:23:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 07:23:01.791: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-378,SelfLink:/api/v1/namespaces/watch-378/configmaps/e2e-watch-test-resource-version,UID:ca715e9e-96d1-49dc-9fb4-b00e2532755c,ResourceVersion:45433,Generation:0,CreationTimestamp:2019-12-05 07:23:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:23:01.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-378" for this suite.
Dec  5 07:23:07.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:23:07.904: INFO: namespace watch-378 deletion completed in 6.109373279s

• [SLOW TEST:6.168 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:23:07.904: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  5 07:23:07.941: INFO: Waiting up to 5m0s for pod "pod-4e68baec-1864-4120-a3ce-e6c7d6d23fda" in namespace "emptydir-9310" to be "success or failure"
Dec  5 07:23:07.949: INFO: Pod "pod-4e68baec-1864-4120-a3ce-e6c7d6d23fda": Phase="Pending", Reason="", readiness=false. Elapsed: 7.985235ms
Dec  5 07:23:09.955: INFO: Pod "pod-4e68baec-1864-4120-a3ce-e6c7d6d23fda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013627155s
Dec  5 07:23:11.959: INFO: Pod "pod-4e68baec-1864-4120-a3ce-e6c7d6d23fda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017525354s
STEP: Saw pod success
Dec  5 07:23:11.959: INFO: Pod "pod-4e68baec-1864-4120-a3ce-e6c7d6d23fda" satisfied condition "success or failure"
Dec  5 07:23:11.961: INFO: Trying to get logs from node node2 pod pod-4e68baec-1864-4120-a3ce-e6c7d6d23fda container test-container: <nil>
STEP: delete the pod
Dec  5 07:23:11.977: INFO: Waiting for pod pod-4e68baec-1864-4120-a3ce-e6c7d6d23fda to disappear
Dec  5 07:23:11.980: INFO: Pod pod-4e68baec-1864-4120-a3ce-e6c7d6d23fda no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:23:11.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9310" for this suite.
Dec  5 07:23:17.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:23:18.149: INFO: namespace emptydir-9310 deletion completed in 6.165792936s

• [SLOW TEST:10.246 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:23:18.151: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec  5 07:23:18.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-7029'
Dec  5 07:23:18.458: INFO: stderr: ""
Dec  5 07:23:18.458: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  5 07:23:19.462: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 07:23:19.462: INFO: Found 0 / 1
Dec  5 07:23:20.461: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 07:23:20.461: INFO: Found 1 / 1
Dec  5 07:23:20.461: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec  5 07:23:20.463: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 07:23:20.463: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  5 07:23:20.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 patch pod redis-master-78ncb --namespace=kubectl-7029 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  5 07:23:20.563: INFO: stderr: ""
Dec  5 07:23:20.563: INFO: stdout: "pod/redis-master-78ncb patched\n"
STEP: checking annotations
Dec  5 07:23:20.565: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 07:23:20.565: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:23:20.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7029" for this suite.
Dec  5 07:23:42.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:23:42.706: INFO: namespace kubectl-7029 deletion completed in 22.137443444s

• [SLOW TEST:24.556 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:23:42.707: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-94df1d35-3f83-4c81-99d1-e4024dac6f2a in namespace container-probe-2138
Dec  5 07:23:48.754: INFO: Started pod liveness-94df1d35-3f83-4c81-99d1-e4024dac6f2a in namespace container-probe-2138
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 07:23:48.757: INFO: Initial restart count of pod liveness-94df1d35-3f83-4c81-99d1-e4024dac6f2a is 0
Dec  5 07:24:04.793: INFO: Restart count of pod container-probe-2138/liveness-94df1d35-3f83-4c81-99d1-e4024dac6f2a is now 1 (16.035694873s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:24:04.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2138" for this suite.
Dec  5 07:24:10.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:24:10.906: INFO: namespace container-probe-2138 deletion completed in 6.10252924s

• [SLOW TEST:28.199 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:24:10.906: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec  5 07:24:10.940: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7603,SelfLink:/api/v1/namespaces/watch-7603/configmaps/e2e-watch-test-configmap-a,UID:495104a5-7d8b-45a1-8bf7-f2ce0cc54a0e,ResourceVersion:45727,Generation:0,CreationTimestamp:2019-12-05 07:24:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 07:24:10.940: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7603,SelfLink:/api/v1/namespaces/watch-7603/configmaps/e2e-watch-test-configmap-a,UID:495104a5-7d8b-45a1-8bf7-f2ce0cc54a0e,ResourceVersion:45727,Generation:0,CreationTimestamp:2019-12-05 07:24:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec  5 07:24:20.947: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7603,SelfLink:/api/v1/namespaces/watch-7603/configmaps/e2e-watch-test-configmap-a,UID:495104a5-7d8b-45a1-8bf7-f2ce0cc54a0e,ResourceVersion:45750,Generation:0,CreationTimestamp:2019-12-05 07:24:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  5 07:24:20.947: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7603,SelfLink:/api/v1/namespaces/watch-7603/configmaps/e2e-watch-test-configmap-a,UID:495104a5-7d8b-45a1-8bf7-f2ce0cc54a0e,ResourceVersion:45750,Generation:0,CreationTimestamp:2019-12-05 07:24:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec  5 07:24:30.955: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7603,SelfLink:/api/v1/namespaces/watch-7603/configmaps/e2e-watch-test-configmap-a,UID:495104a5-7d8b-45a1-8bf7-f2ce0cc54a0e,ResourceVersion:45773,Generation:0,CreationTimestamp:2019-12-05 07:24:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 07:24:30.956: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7603,SelfLink:/api/v1/namespaces/watch-7603/configmaps/e2e-watch-test-configmap-a,UID:495104a5-7d8b-45a1-8bf7-f2ce0cc54a0e,ResourceVersion:45773,Generation:0,CreationTimestamp:2019-12-05 07:24:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec  5 07:24:40.962: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7603,SelfLink:/api/v1/namespaces/watch-7603/configmaps/e2e-watch-test-configmap-a,UID:495104a5-7d8b-45a1-8bf7-f2ce0cc54a0e,ResourceVersion:45796,Generation:0,CreationTimestamp:2019-12-05 07:24:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 07:24:40.962: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7603,SelfLink:/api/v1/namespaces/watch-7603/configmaps/e2e-watch-test-configmap-a,UID:495104a5-7d8b-45a1-8bf7-f2ce0cc54a0e,ResourceVersion:45796,Generation:0,CreationTimestamp:2019-12-05 07:24:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec  5 07:24:50.968: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7603,SelfLink:/api/v1/namespaces/watch-7603/configmaps/e2e-watch-test-configmap-b,UID:c0638db8-59be-4245-bb65-bb61d894db27,ResourceVersion:45819,Generation:0,CreationTimestamp:2019-12-05 07:24:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 07:24:50.968: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7603,SelfLink:/api/v1/namespaces/watch-7603/configmaps/e2e-watch-test-configmap-b,UID:c0638db8-59be-4245-bb65-bb61d894db27,ResourceVersion:45819,Generation:0,CreationTimestamp:2019-12-05 07:24:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec  5 07:25:00.974: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7603,SelfLink:/api/v1/namespaces/watch-7603/configmaps/e2e-watch-test-configmap-b,UID:c0638db8-59be-4245-bb65-bb61d894db27,ResourceVersion:45842,Generation:0,CreationTimestamp:2019-12-05 07:24:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 07:25:00.974: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7603,SelfLink:/api/v1/namespaces/watch-7603/configmaps/e2e-watch-test-configmap-b,UID:c0638db8-59be-4245-bb65-bb61d894db27,ResourceVersion:45842,Generation:0,CreationTimestamp:2019-12-05 07:24:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:25:10.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7603" for this suite.
Dec  5 07:25:16.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:25:17.088: INFO: namespace watch-7603 deletion completed in 6.108418479s

• [SLOW TEST:66.182 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:25:17.088: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-4876d0ee-42c6-48d1-acdd-ac635248d32f
STEP: Creating a pod to test consume configMaps
Dec  5 07:25:17.123: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-71792888-5d90-4b4c-8d2c-4b22ce1a36e1" in namespace "projected-4424" to be "success or failure"
Dec  5 07:25:17.126: INFO: Pod "pod-projected-configmaps-71792888-5d90-4b4c-8d2c-4b22ce1a36e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.939067ms
Dec  5 07:25:19.130: INFO: Pod "pod-projected-configmaps-71792888-5d90-4b4c-8d2c-4b22ce1a36e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006443927s
STEP: Saw pod success
Dec  5 07:25:19.130: INFO: Pod "pod-projected-configmaps-71792888-5d90-4b4c-8d2c-4b22ce1a36e1" satisfied condition "success or failure"
Dec  5 07:25:19.132: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-71792888-5d90-4b4c-8d2c-4b22ce1a36e1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 07:25:19.148: INFO: Waiting for pod pod-projected-configmaps-71792888-5d90-4b4c-8d2c-4b22ce1a36e1 to disappear
Dec  5 07:25:19.150: INFO: Pod pod-projected-configmaps-71792888-5d90-4b4c-8d2c-4b22ce1a36e1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:25:19.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4424" for this suite.
Dec  5 07:25:25.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:25:25.259: INFO: namespace projected-4424 deletion completed in 6.106030866s

• [SLOW TEST:8.171 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:25:25.261: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-9b5124c3-8ab7-46ed-86b2-bb3370899612
STEP: Creating a pod to test consume configMaps
Dec  5 07:25:25.301: INFO: Waiting up to 5m0s for pod "pod-configmaps-5114f46b-16ad-4f57-9b9e-45a50cad70b5" in namespace "configmap-7671" to be "success or failure"
Dec  5 07:25:25.309: INFO: Pod "pod-configmaps-5114f46b-16ad-4f57-9b9e-45a50cad70b5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021505ms
Dec  5 07:25:27.312: INFO: Pod "pod-configmaps-5114f46b-16ad-4f57-9b9e-45a50cad70b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011440073s
STEP: Saw pod success
Dec  5 07:25:27.312: INFO: Pod "pod-configmaps-5114f46b-16ad-4f57-9b9e-45a50cad70b5" satisfied condition "success or failure"
Dec  5 07:25:27.314: INFO: Trying to get logs from node node2 pod pod-configmaps-5114f46b-16ad-4f57-9b9e-45a50cad70b5 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 07:25:27.327: INFO: Waiting for pod pod-configmaps-5114f46b-16ad-4f57-9b9e-45a50cad70b5 to disappear
Dec  5 07:25:27.329: INFO: Pod pod-configmaps-5114f46b-16ad-4f57-9b9e-45a50cad70b5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:25:27.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7671" for this suite.
Dec  5 07:25:33.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:25:33.430: INFO: namespace configmap-7671 deletion completed in 6.098354557s

• [SLOW TEST:8.170 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:25:33.431: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b41bd448-9d2e-4d2d-bf4c-ba490a41fb9b
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-b41bd448-9d2e-4d2d-bf4c-ba490a41fb9b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:25:37.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9591" for this suite.
Dec  5 07:25:59.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:25:59.624: INFO: namespace projected-9591 deletion completed in 22.104644783s

• [SLOW TEST:26.193 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:25:59.626: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 07:25:59.674: INFO: Waiting up to 5m0s for pod "downwardapi-volume-765ef967-5e2d-4793-9a63-51e7c61a1974" in namespace "downward-api-6659" to be "success or failure"
Dec  5 07:25:59.680: INFO: Pod "downwardapi-volume-765ef967-5e2d-4793-9a63-51e7c61a1974": Phase="Pending", Reason="", readiness=false. Elapsed: 5.438367ms
Dec  5 07:26:01.683: INFO: Pod "downwardapi-volume-765ef967-5e2d-4793-9a63-51e7c61a1974": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009200912s
Dec  5 07:26:03.687: INFO: Pod "downwardapi-volume-765ef967-5e2d-4793-9a63-51e7c61a1974": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013192877s
STEP: Saw pod success
Dec  5 07:26:03.687: INFO: Pod "downwardapi-volume-765ef967-5e2d-4793-9a63-51e7c61a1974" satisfied condition "success or failure"
Dec  5 07:26:03.690: INFO: Trying to get logs from node node2 pod downwardapi-volume-765ef967-5e2d-4793-9a63-51e7c61a1974 container client-container: <nil>
STEP: delete the pod
Dec  5 07:26:03.706: INFO: Waiting for pod downwardapi-volume-765ef967-5e2d-4793-9a63-51e7c61a1974 to disappear
Dec  5 07:26:03.709: INFO: Pod downwardapi-volume-765ef967-5e2d-4793-9a63-51e7c61a1974 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:26:03.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6659" for this suite.
Dec  5 07:26:09.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:26:09.819: INFO: namespace downward-api-6659 deletion completed in 6.106328091s

• [SLOW TEST:10.193 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:26:09.819: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  5 07:26:09.846: INFO: Waiting up to 5m0s for pod "downward-api-69b222c1-2b8c-4039-b402-13f791339f25" in namespace "downward-api-2444" to be "success or failure"
Dec  5 07:26:09.849: INFO: Pod "downward-api-69b222c1-2b8c-4039-b402-13f791339f25": Phase="Pending", Reason="", readiness=false. Elapsed: 3.079074ms
Dec  5 07:26:11.854: INFO: Pod "downward-api-69b222c1-2b8c-4039-b402-13f791339f25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00770592s
STEP: Saw pod success
Dec  5 07:26:11.854: INFO: Pod "downward-api-69b222c1-2b8c-4039-b402-13f791339f25" satisfied condition "success or failure"
Dec  5 07:26:11.857: INFO: Trying to get logs from node node1 pod downward-api-69b222c1-2b8c-4039-b402-13f791339f25 container dapi-container: <nil>
STEP: delete the pod
Dec  5 07:26:11.872: INFO: Waiting for pod downward-api-69b222c1-2b8c-4039-b402-13f791339f25 to disappear
Dec  5 07:26:11.875: INFO: Pod downward-api-69b222c1-2b8c-4039-b402-13f791339f25 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:26:11.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2444" for this suite.
Dec  5 07:26:17.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:26:17.996: INFO: namespace downward-api-2444 deletion completed in 6.116307064s

• [SLOW TEST:8.176 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:26:17.997: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-a42a65af-3593-4708-aae2-b60538535aad
STEP: Creating configMap with name cm-test-opt-upd-97bbe042-1626-4bdd-b5a3-c86193b340a5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a42a65af-3593-4708-aae2-b60538535aad
STEP: Updating configmap cm-test-opt-upd-97bbe042-1626-4bdd-b5a3-c86193b340a5
STEP: Creating configMap with name cm-test-opt-create-b5d5e423-581d-4c10-a151-0673a36a4566
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:27:46.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7856" for this suite.
Dec  5 07:28:08.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:28:08.726: INFO: namespace configmap-7856 deletion completed in 22.126571302s

• [SLOW TEST:110.729 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:28:08.726: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec  5 07:28:11.284: INFO: Successfully updated pod "annotationupdateebb2a166-b26c-45f8-b00a-3003d418277e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:28:13.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8061" for this suite.
Dec  5 07:28:35.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:28:35.409: INFO: namespace downward-api-8061 deletion completed in 22.1046386s

• [SLOW TEST:26.682 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:28:35.409: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 07:28:35.439: INFO: Creating ReplicaSet my-hostname-basic-92146c68-ad07-4835-96c6-cacf566b65e3
Dec  5 07:28:35.445: INFO: Pod name my-hostname-basic-92146c68-ad07-4835-96c6-cacf566b65e3: Found 0 pods out of 1
Dec  5 07:28:40.449: INFO: Pod name my-hostname-basic-92146c68-ad07-4835-96c6-cacf566b65e3: Found 1 pods out of 1
Dec  5 07:28:40.449: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-92146c68-ad07-4835-96c6-cacf566b65e3" is running
Dec  5 07:28:40.451: INFO: Pod "my-hostname-basic-92146c68-ad07-4835-96c6-cacf566b65e3-rzjfm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 07:28:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 07:28:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 07:28:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-05 07:28:35 +0000 UTC Reason: Message:}])
Dec  5 07:28:40.451: INFO: Trying to dial the pod
Dec  5 07:28:45.459: INFO: Controller my-hostname-basic-92146c68-ad07-4835-96c6-cacf566b65e3: Got expected result from replica 1 [my-hostname-basic-92146c68-ad07-4835-96c6-cacf566b65e3-rzjfm]: "my-hostname-basic-92146c68-ad07-4835-96c6-cacf566b65e3-rzjfm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:28:45.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1714" for this suite.
Dec  5 07:28:51.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:28:51.577: INFO: namespace replicaset-1714 deletion completed in 6.114887708s

• [SLOW TEST:16.168 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:28:51.578: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:28:57.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4163" for this suite.
Dec  5 07:29:03.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:29:03.785: INFO: namespace namespaces-4163 deletion completed in 6.104019653s
STEP: Destroying namespace "nsdeletetest-86" for this suite.
Dec  5 07:29:03.787: INFO: Namespace nsdeletetest-86 was already deleted
STEP: Destroying namespace "nsdeletetest-6961" for this suite.
Dec  5 07:29:09.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:29:09.893: INFO: namespace nsdeletetest-6961 deletion completed in 6.10558668s

• [SLOW TEST:18.314 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:29:09.893: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  5 07:29:09.926: INFO: Waiting up to 5m0s for pod "downward-api-9e491874-b65d-4704-a4ce-69b6923ac4ad" in namespace "downward-api-191" to be "success or failure"
Dec  5 07:29:09.929: INFO: Pod "downward-api-9e491874-b65d-4704-a4ce-69b6923ac4ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.523219ms
Dec  5 07:29:11.932: INFO: Pod "downward-api-9e491874-b65d-4704-a4ce-69b6923ac4ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006888302s
STEP: Saw pod success
Dec  5 07:29:11.933: INFO: Pod "downward-api-9e491874-b65d-4704-a4ce-69b6923ac4ad" satisfied condition "success or failure"
Dec  5 07:29:11.934: INFO: Trying to get logs from node node1 pod downward-api-9e491874-b65d-4704-a4ce-69b6923ac4ad container dapi-container: <nil>
STEP: delete the pod
Dec  5 07:29:11.952: INFO: Waiting for pod downward-api-9e491874-b65d-4704-a4ce-69b6923ac4ad to disappear
Dec  5 07:29:11.954: INFO: Pod downward-api-9e491874-b65d-4704-a4ce-69b6923ac4ad no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:29:11.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-191" for this suite.
Dec  5 07:29:17.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:29:18.067: INFO: namespace downward-api-191 deletion completed in 6.108722737s

• [SLOW TEST:8.174 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:29:18.068: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-jsc2
STEP: Creating a pod to test atomic-volume-subpath
Dec  5 07:29:18.104: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jsc2" in namespace "subpath-5407" to be "success or failure"
Dec  5 07:29:18.107: INFO: Pod "pod-subpath-test-configmap-jsc2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.265056ms
Dec  5 07:29:20.111: INFO: Pod "pod-subpath-test-configmap-jsc2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00756672s
Dec  5 07:29:22.115: INFO: Pod "pod-subpath-test-configmap-jsc2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011663805s
Dec  5 07:29:24.119: INFO: Pod "pod-subpath-test-configmap-jsc2": Phase="Running", Reason="", readiness=true. Elapsed: 6.015459963s
Dec  5 07:29:26.123: INFO: Pod "pod-subpath-test-configmap-jsc2": Phase="Running", Reason="", readiness=true. Elapsed: 8.019072271s
Dec  5 07:29:28.126: INFO: Pod "pod-subpath-test-configmap-jsc2": Phase="Running", Reason="", readiness=true. Elapsed: 10.022555711s
Dec  5 07:29:30.129: INFO: Pod "pod-subpath-test-configmap-jsc2": Phase="Running", Reason="", readiness=true. Elapsed: 12.025777131s
Dec  5 07:29:32.133: INFO: Pod "pod-subpath-test-configmap-jsc2": Phase="Running", Reason="", readiness=true. Elapsed: 14.029398584s
Dec  5 07:29:34.137: INFO: Pod "pod-subpath-test-configmap-jsc2": Phase="Running", Reason="", readiness=true. Elapsed: 16.033375737s
Dec  5 07:29:36.141: INFO: Pod "pod-subpath-test-configmap-jsc2": Phase="Running", Reason="", readiness=true. Elapsed: 18.03741271s
Dec  5 07:29:38.145: INFO: Pod "pod-subpath-test-configmap-jsc2": Phase="Running", Reason="", readiness=true. Elapsed: 20.041139443s
Dec  5 07:29:40.149: INFO: Pod "pod-subpath-test-configmap-jsc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.045727037s
STEP: Saw pod success
Dec  5 07:29:40.149: INFO: Pod "pod-subpath-test-configmap-jsc2" satisfied condition "success or failure"
Dec  5 07:29:40.151: INFO: Trying to get logs from node node2 pod pod-subpath-test-configmap-jsc2 container test-container-subpath-configmap-jsc2: <nil>
STEP: delete the pod
Dec  5 07:29:40.179: INFO: Waiting for pod pod-subpath-test-configmap-jsc2 to disappear
Dec  5 07:29:40.183: INFO: Pod pod-subpath-test-configmap-jsc2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jsc2
Dec  5 07:29:40.183: INFO: Deleting pod "pod-subpath-test-configmap-jsc2" in namespace "subpath-5407"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:29:40.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5407" for this suite.
Dec  5 07:29:46.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:29:46.306: INFO: namespace subpath-5407 deletion completed in 6.118354854s

• [SLOW TEST:28.238 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:29:46.307: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-493e7783-5ce0-4b8f-b4a1-c0d0dd870600
STEP: Creating a pod to test consume secrets
Dec  5 07:29:46.345: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3b63bf97-456a-4ed8-a966-7bc64be0b63d" in namespace "projected-142" to be "success or failure"
Dec  5 07:29:46.348: INFO: Pod "pod-projected-secrets-3b63bf97-456a-4ed8-a966-7bc64be0b63d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.650257ms
Dec  5 07:29:48.351: INFO: Pod "pod-projected-secrets-3b63bf97-456a-4ed8-a966-7bc64be0b63d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006359297s
STEP: Saw pod success
Dec  5 07:29:48.351: INFO: Pod "pod-projected-secrets-3b63bf97-456a-4ed8-a966-7bc64be0b63d" satisfied condition "success or failure"
Dec  5 07:29:48.354: INFO: Trying to get logs from node node1 pod pod-projected-secrets-3b63bf97-456a-4ed8-a966-7bc64be0b63d container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  5 07:29:48.371: INFO: Waiting for pod pod-projected-secrets-3b63bf97-456a-4ed8-a966-7bc64be0b63d to disappear
Dec  5 07:29:48.373: INFO: Pod pod-projected-secrets-3b63bf97-456a-4ed8-a966-7bc64be0b63d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:29:48.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-142" for this suite.
Dec  5 07:29:54.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:29:54.475: INFO: namespace projected-142 deletion completed in 6.097618014s

• [SLOW TEST:8.168 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:29:54.475: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 07:29:54.514: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec  5 07:29:54.522: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:29:54.524: INFO: Number of nodes with available pods: 0
Dec  5 07:29:54.524: INFO: Node node1 is running more than one daemon pod
Dec  5 07:29:55.529: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:29:55.531: INFO: Number of nodes with available pods: 0
Dec  5 07:29:55.531: INFO: Node node1 is running more than one daemon pod
Dec  5 07:29:56.529: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:29:56.532: INFO: Number of nodes with available pods: 2
Dec  5 07:29:56.532: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec  5 07:29:56.558: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:29:56.558: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:29:56.562: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:29:57.566: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:29:57.566: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:29:57.569: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:29:58.566: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:29:58.566: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:29:58.569: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:29:59.566: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:29:59.566: INFO: Pod daemon-set-8mq6j is not available
Dec  5 07:29:59.566: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:29:59.570: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:00.565: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:00.566: INFO: Pod daemon-set-8mq6j is not available
Dec  5 07:30:00.566: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:00.569: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:01.566: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:01.566: INFO: Pod daemon-set-8mq6j is not available
Dec  5 07:30:01.566: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:01.570: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:02.567: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:02.567: INFO: Pod daemon-set-8mq6j is not available
Dec  5 07:30:02.567: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:02.570: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:03.570: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:03.570: INFO: Pod daemon-set-8mq6j is not available
Dec  5 07:30:03.570: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:03.575: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:04.566: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:04.566: INFO: Pod daemon-set-8mq6j is not available
Dec  5 07:30:04.566: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:04.569: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:05.566: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:05.566: INFO: Pod daemon-set-8mq6j is not available
Dec  5 07:30:05.566: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:05.570: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:06.566: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:06.566: INFO: Pod daemon-set-8mq6j is not available
Dec  5 07:30:06.566: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:06.569: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:07.566: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:07.566: INFO: Pod daemon-set-8mq6j is not available
Dec  5 07:30:07.566: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:07.570: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:08.567: INFO: Wrong image for pod: daemon-set-8mq6j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:08.567: INFO: Pod daemon-set-8mq6j is not available
Dec  5 07:30:08.567: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:08.571: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:09.566: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:09.566: INFO: Pod daemon-set-pp57l is not available
Dec  5 07:30:09.571: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:10.565: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:10.565: INFO: Pod daemon-set-pp57l is not available
Dec  5 07:30:10.569: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:11.573: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:11.577: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:12.567: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:12.571: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:13.566: INFO: Wrong image for pod: daemon-set-bzlgt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  5 07:30:13.566: INFO: Pod daemon-set-bzlgt is not available
Dec  5 07:30:13.571: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:14.566: INFO: Pod daemon-set-w4wbt is not available
Dec  5 07:30:14.570: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec  5 07:30:14.574: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:14.575: INFO: Number of nodes with available pods: 1
Dec  5 07:30:14.575: INFO: Node node2 is running more than one daemon pod
Dec  5 07:30:15.581: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:15.583: INFO: Number of nodes with available pods: 1
Dec  5 07:30:15.583: INFO: Node node2 is running more than one daemon pod
Dec  5 07:30:16.580: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:30:16.583: INFO: Number of nodes with available pods: 2
Dec  5 07:30:16.583: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9723, will wait for the garbage collector to delete the pods
Dec  5 07:30:16.654: INFO: Deleting DaemonSet.extensions daemon-set took: 5.467981ms
Dec  5 07:30:17.355: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.511987ms
Dec  5 07:30:20.058: INFO: Number of nodes with available pods: 0
Dec  5 07:30:20.058: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 07:30:20.063: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9723/daemonsets","resourceVersion":"47155"},"items":null}

Dec  5 07:30:20.065: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9723/pods","resourceVersion":"47155"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:30:20.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9723" for this suite.
Dec  5 07:30:26.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:30:26.178: INFO: namespace daemonsets-9723 deletion completed in 6.099862346s

• [SLOW TEST:31.703 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:30:26.178: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 07:30:26.206: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec  5 07:30:28.230: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:30:28.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-123" for this suite.
Dec  5 07:30:34.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:30:34.343: INFO: namespace replication-controller-123 deletion completed in 6.104856125s

• [SLOW TEST:8.164 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:30:34.343: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Dec  5 07:31:04.430: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 11
	[quantile=0.9] = 304
	[quantile=0.99] = 304
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 604530
	[quantile=0.9] = 622241
	[quantile=0.99] = 622241
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 4
	[quantile=0.9] = 36553
	[quantile=0.99] = 36553
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 1316
	[quantile=0.9] = 36761
	[quantile=0.99] = 36761
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 4
	[quantile=0.9] = 6
	[quantile=0.99] = 18
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 15
	[quantile=0.9] = 27
	[quantile=0.99] = 54
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 13
	[quantile=0.9] = 19
	[quantile=0.99] = 22
For namespace_queue_latency_sum:
	[] = 4045
For namespace_queue_latency_count:
	[] = 284
For namespace_retries:
	[] = 467
For namespace_work_duration:
	[quantile=0.5] = 166817
	[quantile=0.9] = 234518
	[quantile=0.99] = 261954
For namespace_work_duration_sum:
	[] = 50676563
For namespace_work_duration_count:
	[] = 284
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:31:04.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7178" for this suite.
Dec  5 07:31:10.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:31:10.530: INFO: namespace gc-7178 deletion completed in 6.095977513s

• [SLOW TEST:36.187 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:31:10.530: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  5 07:31:10.562: INFO: Waiting up to 5m0s for pod "pod-e7f0e6f1-2d61-4755-bbac-953af4736710" in namespace "emptydir-2796" to be "success or failure"
Dec  5 07:31:10.565: INFO: Pod "pod-e7f0e6f1-2d61-4755-bbac-953af4736710": Phase="Pending", Reason="", readiness=false. Elapsed: 3.322604ms
Dec  5 07:31:12.572: INFO: Pod "pod-e7f0e6f1-2d61-4755-bbac-953af4736710": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01025207s
STEP: Saw pod success
Dec  5 07:31:12.572: INFO: Pod "pod-e7f0e6f1-2d61-4755-bbac-953af4736710" satisfied condition "success or failure"
Dec  5 07:31:12.574: INFO: Trying to get logs from node node2 pod pod-e7f0e6f1-2d61-4755-bbac-953af4736710 container test-container: <nil>
STEP: delete the pod
Dec  5 07:31:12.591: INFO: Waiting for pod pod-e7f0e6f1-2d61-4755-bbac-953af4736710 to disappear
Dec  5 07:31:12.593: INFO: Pod pod-e7f0e6f1-2d61-4755-bbac-953af4736710 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:31:12.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2796" for this suite.
Dec  5 07:31:18.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:31:18.725: INFO: namespace emptydir-2796 deletion completed in 6.12744677s

• [SLOW TEST:8.195 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:31:18.726: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:31:20.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3779" for this suite.
Dec  5 07:31:26.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:31:26.919: INFO: namespace emptydir-wrapper-3779 deletion completed in 6.104804954s

• [SLOW TEST:8.193 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:31:26.921: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-d4d067bd-8ff9-484f-8754-69367ee4ec91 in namespace container-probe-7486
Dec  5 07:31:28.955: INFO: Started pod liveness-d4d067bd-8ff9-484f-8754-69367ee4ec91 in namespace container-probe-7486
STEP: checking the pod's current state and verifying that restartCount is present
Dec  5 07:31:28.957: INFO: Initial restart count of pod liveness-d4d067bd-8ff9-484f-8754-69367ee4ec91 is 0
Dec  5 07:31:48.997: INFO: Restart count of pod container-probe-7486/liveness-d4d067bd-8ff9-484f-8754-69367ee4ec91 is now 1 (20.040249933s elapsed)
Dec  5 07:32:09.036: INFO: Restart count of pod container-probe-7486/liveness-d4d067bd-8ff9-484f-8754-69367ee4ec91 is now 2 (40.079024053s elapsed)
Dec  5 07:32:29.071: INFO: Restart count of pod container-probe-7486/liveness-d4d067bd-8ff9-484f-8754-69367ee4ec91 is now 3 (1m0.114129428s elapsed)
Dec  5 07:32:49.109: INFO: Restart count of pod container-probe-7486/liveness-d4d067bd-8ff9-484f-8754-69367ee4ec91 is now 4 (1m20.152198554s elapsed)
Dec  5 07:33:51.234: INFO: Restart count of pod container-probe-7486/liveness-d4d067bd-8ff9-484f-8754-69367ee4ec91 is now 5 (2m22.277042639s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:33:51.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7486" for this suite.
Dec  5 07:33:57.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:33:57.362: INFO: namespace container-probe-7486 deletion completed in 6.114677374s

• [SLOW TEST:150.441 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:33:57.362: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  5 07:33:57.394: INFO: Waiting up to 5m0s for pod "downward-api-c48d5754-3cc4-4232-b50f-2f03f1b8bada" in namespace "downward-api-4429" to be "success or failure"
Dec  5 07:33:57.398: INFO: Pod "downward-api-c48d5754-3cc4-4232-b50f-2f03f1b8bada": Phase="Pending", Reason="", readiness=false. Elapsed: 3.900463ms
Dec  5 07:33:59.402: INFO: Pod "downward-api-c48d5754-3cc4-4232-b50f-2f03f1b8bada": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007884142s
STEP: Saw pod success
Dec  5 07:33:59.402: INFO: Pod "downward-api-c48d5754-3cc4-4232-b50f-2f03f1b8bada" satisfied condition "success or failure"
Dec  5 07:33:59.404: INFO: Trying to get logs from node node1 pod downward-api-c48d5754-3cc4-4232-b50f-2f03f1b8bada container dapi-container: <nil>
STEP: delete the pod
Dec  5 07:33:59.420: INFO: Waiting for pod downward-api-c48d5754-3cc4-4232-b50f-2f03f1b8bada to disappear
Dec  5 07:33:59.422: INFO: Pod downward-api-c48d5754-3cc4-4232-b50f-2f03f1b8bada no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:33:59.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4429" for this suite.
Dec  5 07:34:05.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:34:05.531: INFO: namespace downward-api-4429 deletion completed in 6.104478059s

• [SLOW TEST:8.169 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:34:05.532: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:34:07.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6440" for this suite.
Dec  5 07:34:57.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:34:57.705: INFO: namespace kubelet-test-6440 deletion completed in 50.124610508s

• [SLOW TEST:52.173 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:34:57.706: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 07:35:13.746: INFO: Container started at 2019-12-05 07:34:58 +0000 UTC, pod became ready at 2019-12-05 07:35:13 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:35:13.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2362" for this suite.
Dec  5 07:35:35.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:35:35.853: INFO: namespace container-probe-2362 deletion completed in 22.103397499s

• [SLOW TEST:38.147 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:35:35.854: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Dec  5 07:35:35.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-188'
Dec  5 07:35:36.273: INFO: stderr: ""
Dec  5 07:35:36.273: INFO: stdout: "pod/pause created\n"
Dec  5 07:35:36.273: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  5 07:35:36.273: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-188" to be "running and ready"
Dec  5 07:35:36.626: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 353.093175ms
Dec  5 07:35:38.630: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.35649403s
Dec  5 07:35:38.630: INFO: Pod "pause" satisfied condition "running and ready"
Dec  5 07:35:38.630: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Dec  5 07:35:38.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 label pods pause testing-label=testing-label-value --namespace=kubectl-188'
Dec  5 07:35:38.732: INFO: stderr: ""
Dec  5 07:35:38.732: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec  5 07:35:38.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pod pause -L testing-label --namespace=kubectl-188'
Dec  5 07:35:38.822: INFO: stderr: ""
Dec  5 07:35:38.822: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec  5 07:35:38.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 label pods pause testing-label- --namespace=kubectl-188'
Dec  5 07:35:38.924: INFO: stderr: ""
Dec  5 07:35:38.924: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec  5 07:35:38.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pod pause -L testing-label --namespace=kubectl-188'
Dec  5 07:35:38.998: INFO: stderr: ""
Dec  5 07:35:38.998: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Dec  5 07:35:38.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete --grace-period=0 --force -f - --namespace=kubectl-188'
Dec  5 07:35:39.080: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 07:35:39.080: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  5 07:35:39.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get rc,svc -l name=pause --no-headers --namespace=kubectl-188'
Dec  5 07:35:39.183: INFO: stderr: "No resources found.\n"
Dec  5 07:35:39.183: INFO: stdout: ""
Dec  5 07:35:39.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -l name=pause --namespace=kubectl-188 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 07:35:39.256: INFO: stderr: ""
Dec  5 07:35:39.256: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:35:39.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-188" for this suite.
Dec  5 07:35:45.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:35:45.367: INFO: namespace kubectl-188 deletion completed in 6.106527595s

• [SLOW TEST:9.512 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:35:45.367: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec  5 07:35:45.449: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6072,SelfLink:/api/v1/namespaces/watch-6072/configmaps/e2e-watch-test-watch-closed,UID:0a45e1cb-4a19-4fb8-9eee-bffa5a4b9d17,ResourceVersion:48373,Generation:0,CreationTimestamp:2019-12-05 07:35:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  5 07:35:45.449: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6072,SelfLink:/api/v1/namespaces/watch-6072/configmaps/e2e-watch-test-watch-closed,UID:0a45e1cb-4a19-4fb8-9eee-bffa5a4b9d17,ResourceVersion:48374,Generation:0,CreationTimestamp:2019-12-05 07:35:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec  5 07:35:45.462: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6072,SelfLink:/api/v1/namespaces/watch-6072/configmaps/e2e-watch-test-watch-closed,UID:0a45e1cb-4a19-4fb8-9eee-bffa5a4b9d17,ResourceVersion:48375,Generation:0,CreationTimestamp:2019-12-05 07:35:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  5 07:35:45.463: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6072,SelfLink:/api/v1/namespaces/watch-6072/configmaps/e2e-watch-test-watch-closed,UID:0a45e1cb-4a19-4fb8-9eee-bffa5a4b9d17,ResourceVersion:48376,Generation:0,CreationTimestamp:2019-12-05 07:35:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:35:45.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6072" for this suite.
Dec  5 07:35:51.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:35:51.587: INFO: namespace watch-6072 deletion completed in 6.119639839s

• [SLOW TEST:6.220 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:35:51.588: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  5 07:35:55.687: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 07:35:55.689: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 07:35:57.690: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 07:35:57.693: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 07:35:59.690: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 07:35:59.694: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 07:36:01.690: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 07:36:01.693: INFO: Pod pod-with-poststart-http-hook still exists
Dec  5 07:36:03.690: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  5 07:36:03.693: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:36:03.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5760" for this suite.
Dec  5 07:36:35.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:36:35.796: INFO: namespace container-lifecycle-hook-5760 deletion completed in 32.099484829s

• [SLOW TEST:44.208 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:36:35.796: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 07:36:35.823: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f7f0a53-d182-4d63-818f-400dd4cda65a" in namespace "projected-1516" to be "success or failure"
Dec  5 07:36:35.825: INFO: Pod "downwardapi-volume-5f7f0a53-d182-4d63-818f-400dd4cda65a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.692117ms
Dec  5 07:36:37.829: INFO: Pod "downwardapi-volume-5f7f0a53-d182-4d63-818f-400dd4cda65a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00552609s
STEP: Saw pod success
Dec  5 07:36:37.829: INFO: Pod "downwardapi-volume-5f7f0a53-d182-4d63-818f-400dd4cda65a" satisfied condition "success or failure"
Dec  5 07:36:37.831: INFO: Trying to get logs from node node1 pod downwardapi-volume-5f7f0a53-d182-4d63-818f-400dd4cda65a container client-container: <nil>
STEP: delete the pod
Dec  5 07:36:37.845: INFO: Waiting for pod downwardapi-volume-5f7f0a53-d182-4d63-818f-400dd4cda65a to disappear
Dec  5 07:36:37.848: INFO: Pod downwardapi-volume-5f7f0a53-d182-4d63-818f-400dd4cda65a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:36:37.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1516" for this suite.
Dec  5 07:36:43.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:36:43.952: INFO: namespace projected-1516 deletion completed in 6.098948469s

• [SLOW TEST:8.156 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:36:43.952: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 07:36:43.982: INFO: (0) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 4.914648ms)
Dec  5 07:36:43.985: INFO: (1) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.03356ms)
Dec  5 07:36:43.988: INFO: (2) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.507317ms)
Dec  5 07:36:43.991: INFO: (3) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.966153ms)
Dec  5 07:36:43.995: INFO: (4) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.066637ms)
Dec  5 07:36:43.998: INFO: (5) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.990949ms)
Dec  5 07:36:44.001: INFO: (6) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.873787ms)
Dec  5 07:36:44.004: INFO: (7) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.487976ms)
Dec  5 07:36:44.007: INFO: (8) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.622061ms)
Dec  5 07:36:44.009: INFO: (9) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.552913ms)
Dec  5 07:36:44.013: INFO: (10) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.307687ms)
Dec  5 07:36:44.015: INFO: (11) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.398599ms)
Dec  5 07:36:44.018: INFO: (12) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.638049ms)
Dec  5 07:36:44.020: INFO: (13) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.501519ms)
Dec  5 07:36:44.023: INFO: (14) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.858982ms)
Dec  5 07:36:44.026: INFO: (15) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.410218ms)
Dec  5 07:36:44.029: INFO: (16) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 3.019491ms)
Dec  5 07:36:44.031: INFO: (17) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.531018ms)
Dec  5 07:36:44.034: INFO: (18) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.975039ms)
Dec  5 07:36:44.037: INFO: (19) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="NetworkManager">NetworkManager</a>
<a href="anaconda/">anaconda/</a>
<a href="audi... (200; 2.482125ms)
[AfterEach] version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:36:44.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1863" for this suite.
Dec  5 07:36:50.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:36:50.159: INFO: namespace proxy-1863 deletion completed in 6.118999614s

• [SLOW TEST:6.207 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:36:50.160: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 07:36:50.195: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fb5bf1a1-b615-4892-af0b-a3ddc064a20b" in namespace "projected-834" to be "success or failure"
Dec  5 07:36:50.198: INFO: Pod "downwardapi-volume-fb5bf1a1-b615-4892-af0b-a3ddc064a20b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.136244ms
Dec  5 07:36:52.202: INFO: Pod "downwardapi-volume-fb5bf1a1-b615-4892-af0b-a3ddc064a20b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006729759s
STEP: Saw pod success
Dec  5 07:36:52.202: INFO: Pod "downwardapi-volume-fb5bf1a1-b615-4892-af0b-a3ddc064a20b" satisfied condition "success or failure"
Dec  5 07:36:52.204: INFO: Trying to get logs from node node2 pod downwardapi-volume-fb5bf1a1-b615-4892-af0b-a3ddc064a20b container client-container: <nil>
STEP: delete the pod
Dec  5 07:36:52.224: INFO: Waiting for pod downwardapi-volume-fb5bf1a1-b615-4892-af0b-a3ddc064a20b to disappear
Dec  5 07:36:52.226: INFO: Pod downwardapi-volume-fb5bf1a1-b615-4892-af0b-a3ddc064a20b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:36:52.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-834" for this suite.
Dec  5 07:36:58.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:36:58.330: INFO: namespace projected-834 deletion completed in 6.100283795s

• [SLOW TEST:8.171 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:36:58.330: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec  5 07:37:00.886: INFO: Successfully updated pod "annotationupdatebeac4944-5a86-4c4a-93aa-5c28bd92812b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:37:02.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3461" for this suite.
Dec  5 07:37:24.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:37:25.029: INFO: namespace projected-3461 deletion completed in 22.11398031s

• [SLOW TEST:26.699 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:37:25.030: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 07:37:25.070: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1da0dbb9-3e37-4894-b5db-3561ce574134" in namespace "downward-api-4069" to be "success or failure"
Dec  5 07:37:25.073: INFO: Pod "downwardapi-volume-1da0dbb9-3e37-4894-b5db-3561ce574134": Phase="Pending", Reason="", readiness=false. Elapsed: 3.284687ms
Dec  5 07:37:27.076: INFO: Pod "downwardapi-volume-1da0dbb9-3e37-4894-b5db-3561ce574134": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006686469s
STEP: Saw pod success
Dec  5 07:37:27.076: INFO: Pod "downwardapi-volume-1da0dbb9-3e37-4894-b5db-3561ce574134" satisfied condition "success or failure"
Dec  5 07:37:27.079: INFO: Trying to get logs from node node2 pod downwardapi-volume-1da0dbb9-3e37-4894-b5db-3561ce574134 container client-container: <nil>
STEP: delete the pod
Dec  5 07:37:27.096: INFO: Waiting for pod downwardapi-volume-1da0dbb9-3e37-4894-b5db-3561ce574134 to disappear
Dec  5 07:37:27.099: INFO: Pod downwardapi-volume-1da0dbb9-3e37-4894-b5db-3561ce574134 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:37:27.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4069" for this suite.
Dec  5 07:37:33.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:37:33.223: INFO: namespace downward-api-4069 deletion completed in 6.120267979s

• [SLOW TEST:8.193 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:37:33.224: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 07:37:33.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 version'
Dec  5 07:37:33.367: INFO: stderr: ""
Dec  5 07:37:33.367: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.5\", GitCommit:\"20c265fef0741dd71a66480e35bd69f18351daea\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:16:51Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.5\", GitCommit:\"20c265fef0741dd71a66480e35bd69f18351daea\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:07:57Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:37:33.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-388" for this suite.
Dec  5 07:37:39.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:37:39.591: INFO: namespace kubectl-388 deletion completed in 6.219625799s

• [SLOW TEST:6.367 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:37:39.592: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec  5 07:37:39.619: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:37:42.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9657" for this suite.
Dec  5 07:37:48.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:37:48.712: INFO: namespace init-container-9657 deletion completed in 6.120557734s

• [SLOW TEST:9.120 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:37:48.713: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 07:37:48.752: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec  5 07:37:48.757: INFO: Number of nodes with available pods: 0
Dec  5 07:37:48.757: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec  5 07:37:48.778: INFO: Number of nodes with available pods: 0
Dec  5 07:37:48.778: INFO: Node node1 is running more than one daemon pod
Dec  5 07:37:49.783: INFO: Number of nodes with available pods: 0
Dec  5 07:37:49.783: INFO: Node node1 is running more than one daemon pod
Dec  5 07:37:50.782: INFO: Number of nodes with available pods: 0
Dec  5 07:37:50.782: INFO: Node node1 is running more than one daemon pod
Dec  5 07:37:51.782: INFO: Number of nodes with available pods: 1
Dec  5 07:37:51.782: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec  5 07:37:51.795: INFO: Number of nodes with available pods: 1
Dec  5 07:37:51.795: INFO: Number of running nodes: 0, number of available pods: 1
Dec  5 07:37:52.801: INFO: Number of nodes with available pods: 0
Dec  5 07:37:52.801: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec  5 07:37:52.807: INFO: Number of nodes with available pods: 0
Dec  5 07:37:52.807: INFO: Node node1 is running more than one daemon pod
Dec  5 07:37:53.810: INFO: Number of nodes with available pods: 0
Dec  5 07:37:53.811: INFO: Node node1 is running more than one daemon pod
Dec  5 07:37:54.810: INFO: Number of nodes with available pods: 0
Dec  5 07:37:54.810: INFO: Node node1 is running more than one daemon pod
Dec  5 07:37:55.810: INFO: Number of nodes with available pods: 0
Dec  5 07:37:55.810: INFO: Node node1 is running more than one daemon pod
Dec  5 07:37:56.811: INFO: Number of nodes with available pods: 0
Dec  5 07:37:56.811: INFO: Node node1 is running more than one daemon pod
Dec  5 07:37:57.811: INFO: Number of nodes with available pods: 1
Dec  5 07:37:57.811: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5316, will wait for the garbage collector to delete the pods
Dec  5 07:37:57.873: INFO: Deleting DaemonSet.extensions daemon-set took: 5.051769ms
Dec  5 07:38:00.674: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.800344779s
Dec  5 07:38:08.977: INFO: Number of nodes with available pods: 0
Dec  5 07:38:08.977: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 07:38:08.979: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5316/daemonsets","resourceVersion":"49052"},"items":null}

Dec  5 07:38:08.982: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5316/pods","resourceVersion":"49052"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:38:08.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5316" for this suite.
Dec  5 07:38:15.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:38:15.124: INFO: namespace daemonsets-5316 deletion completed in 6.122239241s

• [SLOW TEST:26.411 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:38:15.126: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-25e68140-51bb-4cc1-bbe8-270b24ef1ef1
STEP: Creating a pod to test consume configMaps
Dec  5 07:38:15.160: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64eb02fd-9247-4e14-a815-ada89152a4d9" in namespace "projected-3214" to be "success or failure"
Dec  5 07:38:15.162: INFO: Pod "pod-projected-configmaps-64eb02fd-9247-4e14-a815-ada89152a4d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.758722ms
Dec  5 07:38:17.169: INFO: Pod "pod-projected-configmaps-64eb02fd-9247-4e14-a815-ada89152a4d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008609968s
STEP: Saw pod success
Dec  5 07:38:17.169: INFO: Pod "pod-projected-configmaps-64eb02fd-9247-4e14-a815-ada89152a4d9" satisfied condition "success or failure"
Dec  5 07:38:17.171: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-64eb02fd-9247-4e14-a815-ada89152a4d9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 07:38:17.190: INFO: Waiting for pod pod-projected-configmaps-64eb02fd-9247-4e14-a815-ada89152a4d9 to disappear
Dec  5 07:38:17.193: INFO: Pod pod-projected-configmaps-64eb02fd-9247-4e14-a815-ada89152a4d9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:38:17.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3214" for this suite.
Dec  5 07:38:23.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:38:23.330: INFO: namespace projected-3214 deletion completed in 6.131941721s

• [SLOW TEST:8.204 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:38:23.330: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-v65sb in namespace proxy-5945
I1205 07:38:23.369610      16 runners.go:180] Created replication controller with name: proxy-service-v65sb, namespace: proxy-5945, replica count: 1
I1205 07:38:24.420062      16 runners.go:180] proxy-service-v65sb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 07:38:25.420451      16 runners.go:180] proxy-service-v65sb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 07:38:26.420739      16 runners.go:180] proxy-service-v65sb Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1205 07:38:27.420973      16 runners.go:180] proxy-service-v65sb Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1205 07:38:28.421183      16 runners.go:180] proxy-service-v65sb Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  5 07:38:28.424: INFO: setup took 5.064871744s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec  5 07:38:28.431: INFO: (0) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 7.047699ms)
Dec  5 07:38:28.431: INFO: (0) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 6.994022ms)
Dec  5 07:38:28.431: INFO: (0) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 6.712417ms)
Dec  5 07:38:28.431: INFO: (0) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 6.938843ms)
Dec  5 07:38:28.434: INFO: (0) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 9.536847ms)
Dec  5 07:38:28.434: INFO: (0) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 9.866075ms)
Dec  5 07:38:28.437: INFO: (0) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 12.893703ms)
Dec  5 07:38:28.437: INFO: (0) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 12.686972ms)
Dec  5 07:38:28.437: INFO: (0) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 13.079299ms)
Dec  5 07:38:28.437: INFO: (0) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 12.872647ms)
Dec  5 07:38:28.439: INFO: (0) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 14.95212ms)
Dec  5 07:38:28.439: INFO: (0) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 14.629981ms)
Dec  5 07:38:28.439: INFO: (0) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 14.421527ms)
Dec  5 07:38:28.442: INFO: (0) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 18.299437ms)
Dec  5 07:38:28.446: INFO: (0) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 21.712303ms)
Dec  5 07:38:28.447: INFO: (0) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 22.934254ms)
Dec  5 07:38:28.451: INFO: (1) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 4.025999ms)
Dec  5 07:38:28.452: INFO: (1) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 4.494296ms)
Dec  5 07:38:28.452: INFO: (1) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 4.189271ms)
Dec  5 07:38:28.452: INFO: (1) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 4.35766ms)
Dec  5 07:38:28.452: INFO: (1) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 5.098064ms)
Dec  5 07:38:28.452: INFO: (1) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 4.487006ms)
Dec  5 07:38:28.452: INFO: (1) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 4.20055ms)
Dec  5 07:38:28.452: INFO: (1) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 4.13977ms)
Dec  5 07:38:28.453: INFO: (1) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 5.373518ms)
Dec  5 07:38:28.453: INFO: (1) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 5.214593ms)
Dec  5 07:38:28.453: INFO: (1) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 5.186891ms)
Dec  5 07:38:28.453: INFO: (1) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 5.045035ms)
Dec  5 07:38:28.455: INFO: (1) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 6.963878ms)
Dec  5 07:38:28.455: INFO: (1) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 6.510258ms)
Dec  5 07:38:28.455: INFO: (1) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 6.595128ms)
Dec  5 07:38:28.455: INFO: (1) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 6.741492ms)
Dec  5 07:38:28.458: INFO: (2) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 2.96734ms)
Dec  5 07:38:28.459: INFO: (2) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 3.355024ms)
Dec  5 07:38:28.461: INFO: (2) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 5.522473ms)
Dec  5 07:38:28.464: INFO: (2) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 8.675939ms)
Dec  5 07:38:28.464: INFO: (2) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 8.528047ms)
Dec  5 07:38:28.464: INFO: (2) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 8.267131ms)
Dec  5 07:38:28.465: INFO: (2) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 9.026687ms)
Dec  5 07:38:28.465: INFO: (2) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 9.222988ms)
Dec  5 07:38:28.465: INFO: (2) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 8.967143ms)
Dec  5 07:38:28.465: INFO: (2) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 8.877739ms)
Dec  5 07:38:28.465: INFO: (2) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 8.811126ms)
Dec  5 07:38:28.465: INFO: (2) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 8.721345ms)
Dec  5 07:38:28.465: INFO: (2) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 9.161675ms)
Dec  5 07:38:28.465: INFO: (2) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 9.267833ms)
Dec  5 07:38:28.465: INFO: (2) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 9.105505ms)
Dec  5 07:38:28.465: INFO: (2) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 9.388256ms)
Dec  5 07:38:28.469: INFO: (3) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 3.126104ms)
Dec  5 07:38:28.473: INFO: (3) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 6.327879ms)
Dec  5 07:38:28.473: INFO: (3) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 6.515212ms)
Dec  5 07:38:28.473: INFO: (3) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 6.095567ms)
Dec  5 07:38:28.473: INFO: (3) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 7.180367ms)
Dec  5 07:38:28.473: INFO: (3) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 6.017444ms)
Dec  5 07:38:28.474: INFO: (3) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 7.734074ms)
Dec  5 07:38:28.474: INFO: (3) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 7.687307ms)
Dec  5 07:38:28.475: INFO: (3) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 7.446047ms)
Dec  5 07:38:28.475: INFO: (3) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 7.756013ms)
Dec  5 07:38:28.475: INFO: (3) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 7.884371ms)
Dec  5 07:38:28.475: INFO: (3) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 7.338762ms)
Dec  5 07:38:28.475: INFO: (3) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 8.452301ms)
Dec  5 07:38:28.475: INFO: (3) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 8.363477ms)
Dec  5 07:38:28.475: INFO: (3) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 9.303413ms)
Dec  5 07:38:28.475: INFO: (3) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 9.175415ms)
Dec  5 07:38:28.481: INFO: (4) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 5.322039ms)
Dec  5 07:38:28.481: INFO: (4) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 5.523406ms)
Dec  5 07:38:28.483: INFO: (4) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 7.367321ms)
Dec  5 07:38:28.483: INFO: (4) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 7.231313ms)
Dec  5 07:38:28.483: INFO: (4) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 7.149136ms)
Dec  5 07:38:28.483: INFO: (4) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 7.303916ms)
Dec  5 07:38:28.483: INFO: (4) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 7.385972ms)
Dec  5 07:38:28.484: INFO: (4) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 8.127549ms)
Dec  5 07:38:28.484: INFO: (4) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 7.454237ms)
Dec  5 07:38:28.484: INFO: (4) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 7.837673ms)
Dec  5 07:38:28.484: INFO: (4) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 7.567112ms)
Dec  5 07:38:28.484: INFO: (4) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 8.607155ms)
Dec  5 07:38:28.486: INFO: (4) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 10.075457ms)
Dec  5 07:38:28.486: INFO: (4) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 9.993095ms)
Dec  5 07:38:28.486: INFO: (4) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 10.26935ms)
Dec  5 07:38:28.486: INFO: (4) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 10.39223ms)
Dec  5 07:38:28.489: INFO: (5) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 2.914473ms)
Dec  5 07:38:28.490: INFO: (5) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 3.157644ms)
Dec  5 07:38:28.490: INFO: (5) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 2.973657ms)
Dec  5 07:38:28.491: INFO: (5) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 3.88618ms)
Dec  5 07:38:28.491: INFO: (5) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 3.86758ms)
Dec  5 07:38:28.492: INFO: (5) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 5.821602ms)
Dec  5 07:38:28.492: INFO: (5) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 5.225487ms)
Dec  5 07:38:28.492: INFO: (5) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 5.229398ms)
Dec  5 07:38:28.492: INFO: (5) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 5.245118ms)
Dec  5 07:38:28.492: INFO: (5) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 6.0048ms)
Dec  5 07:38:28.493: INFO: (5) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 6.288074ms)
Dec  5 07:38:28.493: INFO: (5) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 5.765693ms)
Dec  5 07:38:28.493: INFO: (5) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 5.671933ms)
Dec  5 07:38:28.493: INFO: (5) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 6.290927ms)
Dec  5 07:38:28.493: INFO: (5) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 6.823905ms)
Dec  5 07:38:28.494: INFO: (5) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 7.36847ms)
Dec  5 07:38:28.498: INFO: (6) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 3.897621ms)
Dec  5 07:38:28.498: INFO: (6) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 3.702803ms)
Dec  5 07:38:28.498: INFO: (6) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 3.87145ms)
Dec  5 07:38:28.499: INFO: (6) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 3.568901ms)
Dec  5 07:38:28.499: INFO: (6) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 4.160362ms)
Dec  5 07:38:28.501: INFO: (6) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 5.799709ms)
Dec  5 07:38:28.501: INFO: (6) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 5.493547ms)
Dec  5 07:38:28.501: INFO: (6) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 5.20304ms)
Dec  5 07:38:28.501: INFO: (6) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 5.146868ms)
Dec  5 07:38:28.501: INFO: (6) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 5.936442ms)
Dec  5 07:38:28.502: INFO: (6) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 6.334866ms)
Dec  5 07:38:28.502: INFO: (6) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 5.937086ms)
Dec  5 07:38:28.502: INFO: (6) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 5.83005ms)
Dec  5 07:38:28.502: INFO: (6) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 6.605762ms)
Dec  5 07:38:28.502: INFO: (6) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 7.250355ms)
Dec  5 07:38:28.502: INFO: (6) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 6.345292ms)
Dec  5 07:38:28.508: INFO: (7) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 5.295828ms)
Dec  5 07:38:28.508: INFO: (7) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 5.49784ms)
Dec  5 07:38:28.508: INFO: (7) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 5.647917ms)
Dec  5 07:38:28.508: INFO: (7) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 5.950094ms)
Dec  5 07:38:28.509: INFO: (7) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 5.47162ms)
Dec  5 07:38:28.509: INFO: (7) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 5.693603ms)
Dec  5 07:38:28.509: INFO: (7) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 6.126081ms)
Dec  5 07:38:28.509: INFO: (7) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 5.970852ms)
Dec  5 07:38:28.509: INFO: (7) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 6.12743ms)
Dec  5 07:38:28.511: INFO: (7) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 7.645008ms)
Dec  5 07:38:28.511: INFO: (7) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 8.153893ms)
Dec  5 07:38:28.512: INFO: (7) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 8.510113ms)
Dec  5 07:38:28.512: INFO: (7) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 8.333739ms)
Dec  5 07:38:28.512: INFO: (7) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 8.572209ms)
Dec  5 07:38:28.512: INFO: (7) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 8.227169ms)
Dec  5 07:38:28.512: INFO: (7) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 8.829081ms)
Dec  5 07:38:28.515: INFO: (8) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 2.931096ms)
Dec  5 07:38:28.515: INFO: (8) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 3.556278ms)
Dec  5 07:38:28.518: INFO: (8) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 5.311245ms)
Dec  5 07:38:28.518: INFO: (8) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 5.302854ms)
Dec  5 07:38:28.519: INFO: (8) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 5.497435ms)
Dec  5 07:38:28.519: INFO: (8) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 5.883417ms)
Dec  5 07:38:28.519: INFO: (8) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 6.808871ms)
Dec  5 07:38:28.519: INFO: (8) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 5.859653ms)
Dec  5 07:38:28.519: INFO: (8) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 6.636939ms)
Dec  5 07:38:28.519: INFO: (8) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 6.267046ms)
Dec  5 07:38:28.520: INFO: (8) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 6.501116ms)
Dec  5 07:38:28.520: INFO: (8) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 6.656394ms)
Dec  5 07:38:28.520: INFO: (8) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 7.173871ms)
Dec  5 07:38:28.520: INFO: (8) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 7.155911ms)
Dec  5 07:38:28.520: INFO: (8) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 7.138439ms)
Dec  5 07:38:28.520: INFO: (8) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 7.479696ms)
Dec  5 07:38:28.526: INFO: (9) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 5.669015ms)
Dec  5 07:38:28.527: INFO: (9) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 6.332779ms)
Dec  5 07:38:28.527: INFO: (9) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 6.623962ms)
Dec  5 07:38:28.527: INFO: (9) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 5.964572ms)
Dec  5 07:38:28.527: INFO: (9) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 6.490592ms)
Dec  5 07:38:28.527: INFO: (9) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 5.9251ms)
Dec  5 07:38:28.527: INFO: (9) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 6.189325ms)
Dec  5 07:38:28.527: INFO: (9) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 6.357893ms)
Dec  5 07:38:28.527: INFO: (9) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 5.908393ms)
Dec  5 07:38:28.527: INFO: (9) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 5.932925ms)
Dec  5 07:38:28.527: INFO: (9) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 6.233997ms)
Dec  5 07:38:28.529: INFO: (9) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 8.372445ms)
Dec  5 07:38:28.529: INFO: (9) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 8.294657ms)
Dec  5 07:38:28.529: INFO: (9) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 8.463689ms)
Dec  5 07:38:28.530: INFO: (9) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 8.750128ms)
Dec  5 07:38:28.530: INFO: (9) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 8.904576ms)
Dec  5 07:38:28.534: INFO: (10) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 4.665366ms)
Dec  5 07:38:28.535: INFO: (10) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 4.902675ms)
Dec  5 07:38:28.535: INFO: (10) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 5.258811ms)
Dec  5 07:38:28.536: INFO: (10) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 5.43079ms)
Dec  5 07:38:28.536: INFO: (10) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 5.838656ms)
Dec  5 07:38:28.537: INFO: (10) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 7.039371ms)
Dec  5 07:38:28.537: INFO: (10) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 6.382635ms)
Dec  5 07:38:28.537: INFO: (10) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 6.093526ms)
Dec  5 07:38:28.537: INFO: (10) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 6.475484ms)
Dec  5 07:38:28.537: INFO: (10) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 6.509973ms)
Dec  5 07:38:28.537: INFO: (10) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 6.377814ms)
Dec  5 07:38:28.538: INFO: (10) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 7.759188ms)
Dec  5 07:38:28.538: INFO: (10) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 7.928681ms)
Dec  5 07:38:28.538: INFO: (10) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 7.889043ms)
Dec  5 07:38:28.538: INFO: (10) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 7.648806ms)
Dec  5 07:38:28.538: INFO: (10) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 8.075133ms)
Dec  5 07:38:28.546: INFO: (11) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 6.149053ms)
Dec  5 07:38:28.546: INFO: (11) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 6.545775ms)
Dec  5 07:38:28.546: INFO: (11) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 7.19713ms)
Dec  5 07:38:28.546: INFO: (11) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 7.705459ms)
Dec  5 07:38:28.546: INFO: (11) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 7.636797ms)
Dec  5 07:38:28.546: INFO: (11) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 7.351095ms)
Dec  5 07:38:28.546: INFO: (11) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 7.467803ms)
Dec  5 07:38:28.546: INFO: (11) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 7.587424ms)
Dec  5 07:38:28.546: INFO: (11) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 7.249184ms)
Dec  5 07:38:28.546: INFO: (11) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 8.098314ms)
Dec  5 07:38:28.547: INFO: (11) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 7.341871ms)
Dec  5 07:38:28.547: INFO: (11) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 7.729657ms)
Dec  5 07:38:28.547: INFO: (11) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 7.747283ms)
Dec  5 07:38:28.547: INFO: (11) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 7.580108ms)
Dec  5 07:38:28.547: INFO: (11) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 8.746586ms)
Dec  5 07:38:28.547: INFO: (11) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 7.58174ms)
Dec  5 07:38:28.550: INFO: (12) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 2.93577ms)
Dec  5 07:38:28.551: INFO: (12) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 3.553883ms)
Dec  5 07:38:28.551: INFO: (12) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 3.967547ms)
Dec  5 07:38:28.552: INFO: (12) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 4.348986ms)
Dec  5 07:38:28.552: INFO: (12) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 4.138018ms)
Dec  5 07:38:28.554: INFO: (12) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 5.348249ms)
Dec  5 07:38:28.554: INFO: (12) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 6.080791ms)
Dec  5 07:38:28.554: INFO: (12) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 5.237464ms)
Dec  5 07:38:28.554: INFO: (12) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 6.7412ms)
Dec  5 07:38:28.555: INFO: (12) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 6.697365ms)
Dec  5 07:38:28.555: INFO: (12) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 6.92203ms)
Dec  5 07:38:28.556: INFO: (12) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 7.541162ms)
Dec  5 07:38:28.556: INFO: (12) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 8.260108ms)
Dec  5 07:38:28.557: INFO: (12) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 7.803587ms)
Dec  5 07:38:28.557: INFO: (12) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 8.354864ms)
Dec  5 07:38:28.557: INFO: (12) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 9.051621ms)
Dec  5 07:38:28.562: INFO: (13) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 4.774962ms)
Dec  5 07:38:28.563: INFO: (13) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 4.461751ms)
Dec  5 07:38:28.563: INFO: (13) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 4.870607ms)
Dec  5 07:38:28.563: INFO: (13) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 5.344735ms)
Dec  5 07:38:28.563: INFO: (13) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 4.777886ms)
Dec  5 07:38:28.567: INFO: (13) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 8.644386ms)
Dec  5 07:38:28.567: INFO: (13) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 9.0035ms)
Dec  5 07:38:28.568: INFO: (13) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 9.101584ms)
Dec  5 07:38:28.568: INFO: (13) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 9.612793ms)
Dec  5 07:38:28.568: INFO: (13) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 9.259144ms)
Dec  5 07:38:28.568: INFO: (13) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 9.850985ms)
Dec  5 07:38:28.568: INFO: (13) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 9.172641ms)
Dec  5 07:38:28.568: INFO: (13) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 9.014234ms)
Dec  5 07:38:28.568: INFO: (13) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 9.121748ms)
Dec  5 07:38:28.568: INFO: (13) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 10.206916ms)
Dec  5 07:38:28.568: INFO: (13) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 9.51555ms)
Dec  5 07:38:28.573: INFO: (14) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 4.314364ms)
Dec  5 07:38:28.573: INFO: (14) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 3.903443ms)
Dec  5 07:38:28.573: INFO: (14) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 4.059685ms)
Dec  5 07:38:28.573: INFO: (14) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 4.173515ms)
Dec  5 07:38:28.573: INFO: (14) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 4.365433ms)
Dec  5 07:38:28.574: INFO: (14) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 4.543931ms)
Dec  5 07:38:28.574: INFO: (14) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 4.967457ms)
Dec  5 07:38:28.574: INFO: (14) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 5.115432ms)
Dec  5 07:38:28.575: INFO: (14) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 5.713503ms)
Dec  5 07:38:28.575: INFO: (14) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 5.958751ms)
Dec  5 07:38:28.582: INFO: (14) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 12.890893ms)
Dec  5 07:38:28.582: INFO: (14) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 13.535291ms)
Dec  5 07:38:28.582: INFO: (14) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 12.707273ms)
Dec  5 07:38:28.582: INFO: (14) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 12.814835ms)
Dec  5 07:38:28.582: INFO: (14) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 12.678483ms)
Dec  5 07:38:28.582: INFO: (14) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 13.115861ms)
Dec  5 07:38:28.592: INFO: (15) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 9.198277ms)
Dec  5 07:38:28.592: INFO: (15) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 9.319909ms)
Dec  5 07:38:28.592: INFO: (15) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 9.48851ms)
Dec  5 07:38:28.592: INFO: (15) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 9.820057ms)
Dec  5 07:38:28.592: INFO: (15) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 9.463898ms)
Dec  5 07:38:28.592: INFO: (15) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 9.838968ms)
Dec  5 07:38:28.593: INFO: (15) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 10.187074ms)
Dec  5 07:38:28.593: INFO: (15) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 10.030403ms)
Dec  5 07:38:28.593: INFO: (15) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 9.932479ms)
Dec  5 07:38:28.593: INFO: (15) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 9.856162ms)
Dec  5 07:38:28.593: INFO: (15) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 9.913831ms)
Dec  5 07:38:28.593: INFO: (15) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 9.930361ms)
Dec  5 07:38:28.593: INFO: (15) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 10.122099ms)
Dec  5 07:38:28.593: INFO: (15) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 10.295141ms)
Dec  5 07:38:28.593: INFO: (15) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 10.159417ms)
Dec  5 07:38:28.593: INFO: (15) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 10.121491ms)
Dec  5 07:38:28.603: INFO: (16) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 9.262632ms)
Dec  5 07:38:28.603: INFO: (16) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 10.086808ms)
Dec  5 07:38:28.604: INFO: (16) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 10.021887ms)
Dec  5 07:38:28.604: INFO: (16) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 9.704379ms)
Dec  5 07:38:28.604: INFO: (16) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 10.02686ms)
Dec  5 07:38:28.604: INFO: (16) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 9.909195ms)
Dec  5 07:38:28.604: INFO: (16) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 10.220559ms)
Dec  5 07:38:28.604: INFO: (16) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 10.552421ms)
Dec  5 07:38:28.605: INFO: (16) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 11.172633ms)
Dec  5 07:38:28.605: INFO: (16) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 10.453432ms)
Dec  5 07:38:28.605: INFO: (16) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 10.427634ms)
Dec  5 07:38:28.605: INFO: (16) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 10.715594ms)
Dec  5 07:38:28.605: INFO: (16) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 10.821642ms)
Dec  5 07:38:28.605: INFO: (16) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 10.928708ms)
Dec  5 07:38:28.605: INFO: (16) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 11.819298ms)
Dec  5 07:38:28.605: INFO: (16) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 11.797394ms)
Dec  5 07:38:28.612: INFO: (17) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 6.457556ms)
Dec  5 07:38:28.612: INFO: (17) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 6.731042ms)
Dec  5 07:38:28.612: INFO: (17) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 6.451952ms)
Dec  5 07:38:28.612: INFO: (17) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 6.38141ms)
Dec  5 07:38:28.612: INFO: (17) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 6.703613ms)
Dec  5 07:38:28.612: INFO: (17) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 6.585579ms)
Dec  5 07:38:28.612: INFO: (17) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 6.500769ms)
Dec  5 07:38:28.613: INFO: (17) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 7.046697ms)
Dec  5 07:38:28.613: INFO: (17) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 6.561834ms)
Dec  5 07:38:28.613: INFO: (17) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 7.042281ms)
Dec  5 07:38:28.613: INFO: (17) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 7.521696ms)
Dec  5 07:38:28.615: INFO: (17) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 8.877862ms)
Dec  5 07:38:28.615: INFO: (17) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 8.646957ms)
Dec  5 07:38:28.615: INFO: (17) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 8.993156ms)
Dec  5 07:38:28.615: INFO: (17) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 9.268414ms)
Dec  5 07:38:28.615: INFO: (17) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 9.158274ms)
Dec  5 07:38:28.619: INFO: (18) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 4.016754ms)
Dec  5 07:38:28.624: INFO: (18) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 8.247448ms)
Dec  5 07:38:28.624: INFO: (18) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 8.342536ms)
Dec  5 07:38:28.624: INFO: (18) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 8.510736ms)
Dec  5 07:38:28.624: INFO: (18) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 8.541428ms)
Dec  5 07:38:28.624: INFO: (18) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 8.550063ms)
Dec  5 07:38:28.625: INFO: (18) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 9.13807ms)
Dec  5 07:38:28.625: INFO: (18) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 9.107292ms)
Dec  5 07:38:28.625: INFO: (18) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 9.255395ms)
Dec  5 07:38:28.625: INFO: (18) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 9.298026ms)
Dec  5 07:38:28.625: INFO: (18) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 9.497337ms)
Dec  5 07:38:28.625: INFO: (18) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 9.366954ms)
Dec  5 07:38:28.625: INFO: (18) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 9.568857ms)
Dec  5 07:38:28.626: INFO: (18) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 10.108059ms)
Dec  5 07:38:28.626: INFO: (18) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 10.271512ms)
Dec  5 07:38:28.626: INFO: (18) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 10.35833ms)
Dec  5 07:38:28.636: INFO: (19) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:1080/proxy/rewriteme">... (200; 9.600913ms)
Dec  5 07:38:28.639: INFO: (19) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:160/proxy/: foo (200; 12.214871ms)
Dec  5 07:38:28.639: INFO: (19) /api/v1/namespaces/proxy-5945/pods/http:proxy-service-v65sb-88bp7:162/proxy/: bar (200; 12.09291ms)
Dec  5 07:38:28.641: INFO: (19) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:460/proxy/: tls baz (200; 13.114261ms)
Dec  5 07:38:28.641: INFO: (19) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname2/proxy/: bar (200; 14.338196ms)
Dec  5 07:38:28.641: INFO: (19) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:162/proxy/: bar (200; 13.377045ms)
Dec  5 07:38:28.641: INFO: (19) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7/proxy/rewriteme">test</a> (200; 14.039361ms)
Dec  5 07:38:28.641: INFO: (19) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname2/proxy/: bar (200; 14.93311ms)
Dec  5 07:38:28.642: INFO: (19) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:1080/proxy/rewriteme">test<... (200; 13.886952ms)
Dec  5 07:38:28.642: INFO: (19) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/: <a href="/api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:443/proxy/tlsrewritem... (200; 14.21573ms)
Dec  5 07:38:28.642: INFO: (19) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname1/proxy/: tls baz (200; 13.373635ms)
Dec  5 07:38:28.642: INFO: (19) /api/v1/namespaces/proxy-5945/pods/https:proxy-service-v65sb-88bp7:462/proxy/: tls qux (200; 13.716545ms)
Dec  5 07:38:28.642: INFO: (19) /api/v1/namespaces/proxy-5945/pods/proxy-service-v65sb-88bp7:160/proxy/: foo (200; 14.756934ms)
Dec  5 07:38:28.642: INFO: (19) /api/v1/namespaces/proxy-5945/services/https:proxy-service-v65sb:tlsportname2/proxy/: tls qux (200; 15.40078ms)
Dec  5 07:38:28.642: INFO: (19) /api/v1/namespaces/proxy-5945/services/http:proxy-service-v65sb:portname1/proxy/: foo (200; 15.030231ms)
Dec  5 07:38:28.646: INFO: (19) /api/v1/namespaces/proxy-5945/services/proxy-service-v65sb:portname1/proxy/: foo (200; 17.426412ms)
STEP: deleting ReplicationController proxy-service-v65sb in namespace proxy-5945, will wait for the garbage collector to delete the pods
Dec  5 07:38:28.703: INFO: Deleting ReplicationController proxy-service-v65sb took: 5.221103ms
Dec  5 07:38:29.403: INFO: Terminating ReplicationController proxy-service-v65sb pods took: 700.266731ms
[AfterEach] version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:38:39.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5945" for this suite.
Dec  5 07:38:45.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:38:45.119: INFO: namespace proxy-5945 deletion completed in 6.110269117s

• [SLOW TEST:21.790 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:38:45.120: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  5 07:38:47.710: INFO: Successfully updated pod "pod-update-5b49848c-60b4-4f4b-8d3c-59e58e5ef9bd"
STEP: verifying the updated pod is in kubernetes
Dec  5 07:38:47.731: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:38:47.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9841" for this suite.
Dec  5 07:39:09.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:39:09.899: INFO: namespace pods-9841 deletion completed in 22.160487329s

• [SLOW TEST:24.780 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:39:09.900: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Dec  5 07:39:09.937: INFO: Waiting up to 5m0s for pod "client-containers-6a493262-2617-4237-885b-cbf2275572bd" in namespace "containers-3313" to be "success or failure"
Dec  5 07:39:09.941: INFO: Pod "client-containers-6a493262-2617-4237-885b-cbf2275572bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.222476ms
Dec  5 07:39:11.944: INFO: Pod "client-containers-6a493262-2617-4237-885b-cbf2275572bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006699687s
STEP: Saw pod success
Dec  5 07:39:11.944: INFO: Pod "client-containers-6a493262-2617-4237-885b-cbf2275572bd" satisfied condition "success or failure"
Dec  5 07:39:11.946: INFO: Trying to get logs from node node1 pod client-containers-6a493262-2617-4237-885b-cbf2275572bd container test-container: <nil>
STEP: delete the pod
Dec  5 07:39:11.963: INFO: Waiting for pod client-containers-6a493262-2617-4237-885b-cbf2275572bd to disappear
Dec  5 07:39:11.964: INFO: Pod client-containers-6a493262-2617-4237-885b-cbf2275572bd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:39:11.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3313" for this suite.
Dec  5 07:39:17.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:39:18.076: INFO: namespace containers-3313 deletion completed in 6.107828185s

• [SLOW TEST:8.176 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:39:18.076: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Dec  5 07:39:58.155: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
	[quantile=0.5] = 9
	[quantile=0.9] = 298
	[quantile=0.99] = 304
For garbage_collector_attempt_to_delete_work_duration:
	[quantile=0.5] = 604376
	[quantile=0.9] = 2706593
	[quantile=0.99] = 2709678
For garbage_collector_attempt_to_orphan_queue_latency:
	[quantile=0.5] = 6
	[quantile=0.9] = 36553
	[quantile=0.99] = 36553
For garbage_collector_attempt_to_orphan_work_duration:
	[quantile=0.5] = 36761
	[quantile=0.9] = 646130
	[quantile=0.99] = 646130
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
	[quantile=0.5] = 5
	[quantile=0.9] = 6
	[quantile=0.99] = 20
For garbage_collector_graph_changes_work_duration:
	[quantile=0.5] = 15
	[quantile=0.9] = 28
	[quantile=0.99] = 59
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
	[quantile=0.5] = 14
	[quantile=0.9] = 20
	[quantile=0.99] = 23
For namespace_queue_latency_sum:
	[] = 4616
For namespace_queue_latency_count:
	[] = 324
For namespace_retries:
	[] = 531
For namespace_work_duration:
	[quantile=0.5] = 158937
	[quantile=0.9] = 199609
	[quantile=0.99] = 252526
For namespace_work_duration_sum:
	[] = 57038028
For namespace_work_duration_count:
	[] = 324
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:39:58.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2145" for this suite.
Dec  5 07:40:04.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:40:04.324: INFO: namespace gc-2145 deletion completed in 6.165196327s

• [SLOW TEST:46.249 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:40:04.325: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec  5 07:40:04.356: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec  5 07:40:13.395: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:40:13.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8279" for this suite.
Dec  5 07:40:19.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:40:19.527: INFO: namespace pods-8279 deletion completed in 6.124004853s

• [SLOW TEST:15.202 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:40:19.527: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2443
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec  5 07:40:19.565: INFO: Found 0 stateful pods, waiting for 3
Dec  5 07:40:29.569: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 07:40:29.569: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 07:40:29.569: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  5 07:40:29.596: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec  5 07:40:39.623: INFO: Updating stateful set ss2
Dec  5 07:40:39.628: INFO: Waiting for Pod statefulset-2443/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Dec  5 07:40:49.669: INFO: Found 2 stateful pods, waiting for 3
Dec  5 07:40:59.673: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 07:40:59.673: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  5 07:40:59.673: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec  5 07:40:59.696: INFO: Updating stateful set ss2
Dec  5 07:40:59.705: INFO: Waiting for Pod statefulset-2443/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:41:09.711: INFO: Waiting for Pod statefulset-2443/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:41:19.728: INFO: Updating stateful set ss2
Dec  5 07:41:19.738: INFO: Waiting for StatefulSet statefulset-2443/ss2 to complete update
Dec  5 07:41:19.738: INFO: Waiting for Pod statefulset-2443/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  5 07:41:29.744: INFO: Waiting for StatefulSet statefulset-2443/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  5 07:41:39.747: INFO: Deleting all statefulset in ns statefulset-2443
Dec  5 07:41:39.750: INFO: Scaling statefulset ss2 to 0
Dec  5 07:42:09.762: INFO: Waiting for statefulset status.replicas updated to 0
Dec  5 07:42:09.764: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:42:09.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2443" for this suite.
Dec  5 07:42:15.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:42:15.896: INFO: namespace statefulset-2443 deletion completed in 6.120001181s

• [SLOW TEST:116.369 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:42:15.899: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Dec  5 07:42:15.930: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-771445370 proxy --unix-socket=/tmp/kubectl-proxy-unix860007253/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:42:16.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3205" for this suite.
Dec  5 07:42:22.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:42:22.141: INFO: namespace kubectl-3205 deletion completed in 6.124734742s

• [SLOW TEST:6.242 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:42:22.141: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec  5 07:42:22.404: INFO: Pod name wrapped-volume-race-94780487-8a0a-43b5-9fd8-f8529219cbde: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-94780487-8a0a-43b5-9fd8-f8529219cbde in namespace emptydir-wrapper-2945, will wait for the garbage collector to delete the pods
Dec  5 07:42:36.520: INFO: Deleting ReplicationController wrapped-volume-race-94780487-8a0a-43b5-9fd8-f8529219cbde took: 5.957857ms
Dec  5 07:42:37.220: INFO: Terminating ReplicationController wrapped-volume-race-94780487-8a0a-43b5-9fd8-f8529219cbde pods took: 700.453796ms
STEP: Creating RC which spawns configmap-volume pods
Dec  5 07:43:11.944: INFO: Pod name wrapped-volume-race-5f156a76-c1a5-4dba-a837-876f2a89fa33: Found 0 pods out of 5
Dec  5 07:43:16.954: INFO: Pod name wrapped-volume-race-5f156a76-c1a5-4dba-a837-876f2a89fa33: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5f156a76-c1a5-4dba-a837-876f2a89fa33 in namespace emptydir-wrapper-2945, will wait for the garbage collector to delete the pods
Dec  5 07:43:27.041: INFO: Deleting ReplicationController wrapped-volume-race-5f156a76-c1a5-4dba-a837-876f2a89fa33 took: 14.013306ms
Dec  5 07:43:27.741: INFO: Terminating ReplicationController wrapped-volume-race-5f156a76-c1a5-4dba-a837-876f2a89fa33 pods took: 700.39306ms
STEP: Creating RC which spawns configmap-volume pods
Dec  5 07:44:09.456: INFO: Pod name wrapped-volume-race-95ad5e98-4627-47cc-8b31-9d8cdd0bf302: Found 0 pods out of 5
Dec  5 07:44:14.465: INFO: Pod name wrapped-volume-race-95ad5e98-4627-47cc-8b31-9d8cdd0bf302: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-95ad5e98-4627-47cc-8b31-9d8cdd0bf302 in namespace emptydir-wrapper-2945, will wait for the garbage collector to delete the pods
Dec  5 07:44:24.541: INFO: Deleting ReplicationController wrapped-volume-race-95ad5e98-4627-47cc-8b31-9d8cdd0bf302 took: 5.69412ms
Dec  5 07:44:25.241: INFO: Terminating ReplicationController wrapped-volume-race-95ad5e98-4627-47cc-8b31-9d8cdd0bf302 pods took: 700.387746ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:44:59.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2945" for this suite.
Dec  5 07:45:05.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:45:05.741: INFO: namespace emptydir-wrapper-2945 deletion completed in 6.102954052s

• [SLOW TEST:163.600 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:45:05.742: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-11108ee8-ca43-448a-81a8-0fdd7540e150
STEP: Creating a pod to test consume configMaps
Dec  5 07:45:05.777: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cce06c6a-009a-4ee8-9c3a-938b4ba13534" in namespace "projected-7657" to be "success or failure"
Dec  5 07:45:05.779: INFO: Pod "pod-projected-configmaps-cce06c6a-009a-4ee8-9c3a-938b4ba13534": Phase="Pending", Reason="", readiness=false. Elapsed: 1.839402ms
Dec  5 07:45:07.783: INFO: Pod "pod-projected-configmaps-cce06c6a-009a-4ee8-9c3a-938b4ba13534": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005393091s
Dec  5 07:45:09.786: INFO: Pod "pod-projected-configmaps-cce06c6a-009a-4ee8-9c3a-938b4ba13534": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008871448s
STEP: Saw pod success
Dec  5 07:45:09.786: INFO: Pod "pod-projected-configmaps-cce06c6a-009a-4ee8-9c3a-938b4ba13534" satisfied condition "success or failure"
Dec  5 07:45:09.788: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-cce06c6a-009a-4ee8-9c3a-938b4ba13534 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  5 07:45:09.811: INFO: Waiting for pod pod-projected-configmaps-cce06c6a-009a-4ee8-9c3a-938b4ba13534 to disappear
Dec  5 07:45:09.813: INFO: Pod pod-projected-configmaps-cce06c6a-009a-4ee8-9c3a-938b4ba13534 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:45:09.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7657" for this suite.
Dec  5 07:45:15.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:45:15.934: INFO: namespace projected-7657 deletion completed in 6.116635212s

• [SLOW TEST:10.192 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:45:15.934: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-5315
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5315 to expose endpoints map[]
Dec  5 07:45:15.967: INFO: successfully validated that service multi-endpoint-test in namespace services-5315 exposes endpoints map[] (2.301128ms elapsed)
STEP: Creating pod pod1 in namespace services-5315
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5315 to expose endpoints map[pod1:[100]]
Dec  5 07:45:17.998: INFO: successfully validated that service multi-endpoint-test in namespace services-5315 exposes endpoints map[pod1:[100]] (2.026708418s elapsed)
STEP: Creating pod pod2 in namespace services-5315
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5315 to expose endpoints map[pod1:[100] pod2:[101]]
Dec  5 07:45:20.023: INFO: successfully validated that service multi-endpoint-test in namespace services-5315 exposes endpoints map[pod1:[100] pod2:[101]] (2.02233139s elapsed)
STEP: Deleting pod pod1 in namespace services-5315
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5315 to expose endpoints map[pod2:[101]]
Dec  5 07:45:21.040: INFO: successfully validated that service multi-endpoint-test in namespace services-5315 exposes endpoints map[pod2:[101]] (1.012232613s elapsed)
STEP: Deleting pod pod2 in namespace services-5315
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5315 to expose endpoints map[]
Dec  5 07:45:22.048: INFO: successfully validated that service multi-endpoint-test in namespace services-5315 exposes endpoints map[] (1.005710782s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:45:22.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5315" for this suite.
Dec  5 07:45:44.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:45:44.159: INFO: namespace services-5315 deletion completed in 22.096639319s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:28.225 seconds]
[sig-network] Services
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:45:44.160: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Dec  5 07:45:44.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 create -f - --namespace=kubectl-5829'
Dec  5 07:45:44.588: INFO: stderr: ""
Dec  5 07:45:44.588: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Dec  5 07:45:45.591: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 07:45:45.591: INFO: Found 0 / 1
Dec  5 07:45:46.593: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 07:45:46.593: INFO: Found 1 / 1
Dec  5 07:45:46.593: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  5 07:45:46.595: INFO: Selector matched 1 pods for map[app:redis]
Dec  5 07:45:46.595: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec  5 07:45:46.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 logs redis-master-28rc5 redis-master --namespace=kubectl-5829'
Dec  5 07:45:46.695: INFO: stderr: ""
Dec  5 07:45:46.696: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Dec 07:45:45.592 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Dec 07:45:45.592 # Server started, Redis version 3.2.12\n1:M 05 Dec 07:45:45.592 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Dec 07:45:45.592 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec  5 07:45:46.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 logs redis-master-28rc5 redis-master --namespace=kubectl-5829 --tail=1'
Dec  5 07:45:46.802: INFO: stderr: ""
Dec  5 07:45:46.802: INFO: stdout: "1:M 05 Dec 07:45:45.592 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec  5 07:45:46.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 logs redis-master-28rc5 redis-master --namespace=kubectl-5829 --limit-bytes=1'
Dec  5 07:45:46.896: INFO: stderr: ""
Dec  5 07:45:46.896: INFO: stdout: " "
STEP: exposing timestamps
Dec  5 07:45:46.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 logs redis-master-28rc5 redis-master --namespace=kubectl-5829 --tail=1 --timestamps'
Dec  5 07:45:47.023: INFO: stderr: ""
Dec  5 07:45:47.023: INFO: stdout: "2019-12-05T07:45:45.594619069Z 1:M 05 Dec 07:45:45.592 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec  5 07:45:49.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 logs redis-master-28rc5 redis-master --namespace=kubectl-5829 --since=1s'
Dec  5 07:45:49.629: INFO: stderr: ""
Dec  5 07:45:49.629: INFO: stdout: ""
Dec  5 07:45:49.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 logs redis-master-28rc5 redis-master --namespace=kubectl-5829 --since=24h'
Dec  5 07:45:49.740: INFO: stderr: ""
Dec  5 07:45:49.740: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 05 Dec 07:45:45.592 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 05 Dec 07:45:45.592 # Server started, Redis version 3.2.12\n1:M 05 Dec 07:45:45.592 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 05 Dec 07:45:45.592 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Dec  5 07:45:49.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 delete --grace-period=0 --force -f - --namespace=kubectl-5829'
Dec  5 07:45:49.829: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  5 07:45:49.829: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec  5 07:45:49.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get rc,svc -l name=nginx --no-headers --namespace=kubectl-5829'
Dec  5 07:45:49.932: INFO: stderr: "No resources found.\n"
Dec  5 07:45:49.932: INFO: stdout: ""
Dec  5 07:45:49.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 get pods -l name=nginx --namespace=kubectl-5829 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  5 07:45:50.035: INFO: stderr: ""
Dec  5 07:45:50.035: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:45:50.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5829" for this suite.
Dec  5 07:46:12.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:46:12.158: INFO: namespace kubectl-5829 deletion completed in 22.118795847s

• [SLOW TEST:27.998 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:46:12.159: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  5 07:46:12.198: INFO: Waiting up to 5m0s for pod "pod-1f0ab098-c6a4-4c26-b8fb-dd27e5c7d514" in namespace "emptydir-217" to be "success or failure"
Dec  5 07:46:12.201: INFO: Pod "pod-1f0ab098-c6a4-4c26-b8fb-dd27e5c7d514": Phase="Pending", Reason="", readiness=false. Elapsed: 3.357925ms
Dec  5 07:46:14.205: INFO: Pod "pod-1f0ab098-c6a4-4c26-b8fb-dd27e5c7d514": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007430128s
Dec  5 07:46:16.209: INFO: Pod "pod-1f0ab098-c6a4-4c26-b8fb-dd27e5c7d514": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010925618s
STEP: Saw pod success
Dec  5 07:46:16.209: INFO: Pod "pod-1f0ab098-c6a4-4c26-b8fb-dd27e5c7d514" satisfied condition "success or failure"
Dec  5 07:46:16.211: INFO: Trying to get logs from node node1 pod pod-1f0ab098-c6a4-4c26-b8fb-dd27e5c7d514 container test-container: <nil>
STEP: delete the pod
Dec  5 07:46:16.225: INFO: Waiting for pod pod-1f0ab098-c6a4-4c26-b8fb-dd27e5c7d514 to disappear
Dec  5 07:46:16.227: INFO: Pod pod-1f0ab098-c6a4-4c26-b8fb-dd27e5c7d514 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:46:16.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-217" for this suite.
Dec  5 07:46:22.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:46:22.352: INFO: namespace emptydir-217 deletion completed in 6.120712053s

• [SLOW TEST:10.193 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:46:22.352: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5759
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  5 07:46:22.383: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  5 07:46:40.435: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.96.173:8080/dial?request=hostName&protocol=http&host=10.233.96.172&port=8080&tries=1'] Namespace:pod-network-test-5759 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 07:46:40.435: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 07:46:40.653: INFO: Waiting for endpoints: map[]
Dec  5 07:46:40.657: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.96.173:8080/dial?request=hostName&protocol=http&host=10.233.90.188&port=8080&tries=1'] Namespace:pod-network-test-5759 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  5 07:46:40.657: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
Dec  5 07:46:40.847: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:46:40.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5759" for this suite.
Dec  5 07:47:02.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:47:02.957: INFO: namespace pod-network-test-5759 deletion completed in 22.105290215s

• [SLOW TEST:40.605 seconds]
[sig-network] Networking
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:47:02.958: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:47:25.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-609" for this suite.
Dec  5 07:47:31.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:47:31.280: INFO: namespace container-runtime-609 deletion completed in 6.110999342s

• [SLOW TEST:28.323 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:47:31.280: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 07:47:31.320: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:47:32.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7278" for this suite.
Dec  5 07:47:38.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:47:38.519: INFO: namespace custom-resource-definition-7278 deletion completed in 6.102254093s

• [SLOW TEST:7.238 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:47:38.520: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Dec  5 07:47:38.554: INFO: Waiting up to 5m0s for pod "var-expansion-680e6c41-ef2e-499c-932f-130619696fdd" in namespace "var-expansion-5903" to be "success or failure"
Dec  5 07:47:38.561: INFO: Pod "var-expansion-680e6c41-ef2e-499c-932f-130619696fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.516551ms
Dec  5 07:47:40.565: INFO: Pod "var-expansion-680e6c41-ef2e-499c-932f-130619696fdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011334486s
STEP: Saw pod success
Dec  5 07:47:40.565: INFO: Pod "var-expansion-680e6c41-ef2e-499c-932f-130619696fdd" satisfied condition "success or failure"
Dec  5 07:47:40.568: INFO: Trying to get logs from node node1 pod var-expansion-680e6c41-ef2e-499c-932f-130619696fdd container dapi-container: <nil>
STEP: delete the pod
Dec  5 07:47:40.583: INFO: Waiting for pod var-expansion-680e6c41-ef2e-499c-932f-130619696fdd to disappear
Dec  5 07:47:40.586: INFO: Pod var-expansion-680e6c41-ef2e-499c-932f-130619696fdd no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:47:40.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5903" for this suite.
Dec  5 07:47:46.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:47:46.706: INFO: namespace var-expansion-5903 deletion completed in 6.116236181s

• [SLOW TEST:8.186 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:47:46.707: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Dec  5 07:47:46.738: INFO: Waiting up to 5m0s for pod "client-containers-2025aeb4-8704-4617-aa5a-53f55c1050f5" in namespace "containers-3161" to be "success or failure"
Dec  5 07:47:46.740: INFO: Pod "client-containers-2025aeb4-8704-4617-aa5a-53f55c1050f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255194ms
Dec  5 07:47:48.744: INFO: Pod "client-containers-2025aeb4-8704-4617-aa5a-53f55c1050f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005886212s
Dec  5 07:47:50.747: INFO: Pod "client-containers-2025aeb4-8704-4617-aa5a-53f55c1050f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0090247s
STEP: Saw pod success
Dec  5 07:47:50.747: INFO: Pod "client-containers-2025aeb4-8704-4617-aa5a-53f55c1050f5" satisfied condition "success or failure"
Dec  5 07:47:50.749: INFO: Trying to get logs from node node2 pod client-containers-2025aeb4-8704-4617-aa5a-53f55c1050f5 container test-container: <nil>
STEP: delete the pod
Dec  5 07:47:50.767: INFO: Waiting for pod client-containers-2025aeb4-8704-4617-aa5a-53f55c1050f5 to disappear
Dec  5 07:47:50.769: INFO: Pod client-containers-2025aeb4-8704-4617-aa5a-53f55c1050f5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:47:50.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3161" for this suite.
Dec  5 07:47:56.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:47:56.882: INFO: namespace containers-3161 deletion completed in 6.105309884s

• [SLOW TEST:10.174 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:47:56.883: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Dec  5 07:47:56.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-771445370 --namespace=kubectl-8984 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec  5 07:47:58.935: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec  5 07:47:58.935: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:48:00.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8984" for this suite.
Dec  5 07:48:10.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:48:11.045: INFO: namespace kubectl-8984 deletion completed in 10.098261425s

• [SLOW TEST:14.162 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:48:11.045: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  5 07:48:11.095: INFO: Create a RollingUpdate DaemonSet
Dec  5 07:48:11.098: INFO: Check that daemon pods launch on every node of the cluster
Dec  5 07:48:11.103: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:48:11.106: INFO: Number of nodes with available pods: 0
Dec  5 07:48:11.106: INFO: Node node1 is running more than one daemon pod
Dec  5 07:48:12.113: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:48:12.115: INFO: Number of nodes with available pods: 0
Dec  5 07:48:12.115: INFO: Node node1 is running more than one daemon pod
Dec  5 07:48:13.110: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:48:13.113: INFO: Number of nodes with available pods: 1
Dec  5 07:48:13.113: INFO: Node node1 is running more than one daemon pod
Dec  5 07:48:14.111: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:48:14.114: INFO: Number of nodes with available pods: 2
Dec  5 07:48:14.114: INFO: Number of running nodes: 2, number of available pods: 2
Dec  5 07:48:14.114: INFO: Update the DaemonSet to trigger a rollout
Dec  5 07:48:14.120: INFO: Updating DaemonSet daemon-set
Dec  5 07:48:23.136: INFO: Roll back the DaemonSet before rollout is complete
Dec  5 07:48:23.141: INFO: Updating DaemonSet daemon-set
Dec  5 07:48:23.141: INFO: Make sure DaemonSet rollback is complete
Dec  5 07:48:23.143: INFO: Wrong image for pod: daemon-set-55pfd. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  5 07:48:23.143: INFO: Pod daemon-set-55pfd is not available
Dec  5 07:48:23.148: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:48:24.151: INFO: Wrong image for pod: daemon-set-55pfd. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  5 07:48:24.151: INFO: Pod daemon-set-55pfd is not available
Dec  5 07:48:24.155: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:48:25.153: INFO: Wrong image for pod: daemon-set-55pfd. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  5 07:48:25.153: INFO: Pod daemon-set-55pfd is not available
Dec  5 07:48:25.157: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:48:26.152: INFO: Wrong image for pod: daemon-set-55pfd. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  5 07:48:26.152: INFO: Pod daemon-set-55pfd is not available
Dec  5 07:48:26.155: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:48:27.152: INFO: Wrong image for pod: daemon-set-55pfd. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  5 07:48:27.152: INFO: Pod daemon-set-55pfd is not available
Dec  5 07:48:27.155: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:48:28.159: INFO: Wrong image for pod: daemon-set-55pfd. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  5 07:48:28.159: INFO: Pod daemon-set-55pfd is not available
Dec  5 07:48:28.163: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  5 07:48:29.152: INFO: Pod daemon-set-4cbbr is not available
Dec  5 07:48:29.157: INFO: DaemonSet pods can't tolerate node master with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3725, will wait for the garbage collector to delete the pods
Dec  5 07:48:29.218: INFO: Deleting DaemonSet.extensions daemon-set took: 3.801017ms
Dec  5 07:48:29.918: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.231127ms
Dec  5 07:49:41.721: INFO: Number of nodes with available pods: 0
Dec  5 07:49:41.721: INFO: Number of running nodes: 0, number of available pods: 0
Dec  5 07:49:41.724: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3725/daemonsets","resourceVersion":"52994"},"items":null}

Dec  5 07:49:41.725: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3725/pods","resourceVersion":"52994"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:49:41.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3725" for this suite.
Dec  5 07:49:47.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:49:47.839: INFO: namespace daemonsets-3725 deletion completed in 6.101724987s

• [SLOW TEST:96.794 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:49:47.839: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 07:49:47.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72cad59c-92bf-4cd3-809b-ab319fed0a5f" in namespace "projected-6786" to be "success or failure"
Dec  5 07:49:47.896: INFO: Pod "downwardapi-volume-72cad59c-92bf-4cd3-809b-ab319fed0a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.972265ms
Dec  5 07:49:49.899: INFO: Pod "downwardapi-volume-72cad59c-92bf-4cd3-809b-ab319fed0a5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006545221s
Dec  5 07:49:51.903: INFO: Pod "downwardapi-volume-72cad59c-92bf-4cd3-809b-ab319fed0a5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010351011s
STEP: Saw pod success
Dec  5 07:49:51.903: INFO: Pod "downwardapi-volume-72cad59c-92bf-4cd3-809b-ab319fed0a5f" satisfied condition "success or failure"
Dec  5 07:49:51.905: INFO: Trying to get logs from node node2 pod downwardapi-volume-72cad59c-92bf-4cd3-809b-ab319fed0a5f container client-container: <nil>
STEP: delete the pod
Dec  5 07:49:51.919: INFO: Waiting for pod downwardapi-volume-72cad59c-92bf-4cd3-809b-ab319fed0a5f to disappear
Dec  5 07:49:51.921: INFO: Pod downwardapi-volume-72cad59c-92bf-4cd3-809b-ab319fed0a5f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:49:51.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6786" for this suite.
Dec  5 07:49:57.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:49:58.034: INFO: namespace projected-6786 deletion completed in 6.108941901s

• [SLOW TEST:10.195 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:49:58.034: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  5 07:49:58.065: INFO: Waiting up to 5m0s for pod "pod-43032dad-903f-47d2-bdee-63a3d40982df" in namespace "emptydir-5600" to be "success or failure"
Dec  5 07:49:58.069: INFO: Pod "pod-43032dad-903f-47d2-bdee-63a3d40982df": Phase="Pending", Reason="", readiness=false. Elapsed: 3.663335ms
Dec  5 07:50:00.073: INFO: Pod "pod-43032dad-903f-47d2-bdee-63a3d40982df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007344029s
STEP: Saw pod success
Dec  5 07:50:00.073: INFO: Pod "pod-43032dad-903f-47d2-bdee-63a3d40982df" satisfied condition "success or failure"
Dec  5 07:50:00.075: INFO: Trying to get logs from node node1 pod pod-43032dad-903f-47d2-bdee-63a3d40982df container test-container: <nil>
STEP: delete the pod
Dec  5 07:50:00.089: INFO: Waiting for pod pod-43032dad-903f-47d2-bdee-63a3d40982df to disappear
Dec  5 07:50:00.093: INFO: Pod pod-43032dad-903f-47d2-bdee-63a3d40982df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:50:00.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5600" for this suite.
Dec  5 07:50:06.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:50:06.197: INFO: namespace emptydir-5600 deletion completed in 6.100111194s

• [SLOW TEST:8.163 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:50:06.197: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Dec  5 07:50:06.234: INFO: Waiting up to 5m0s for pod "client-containers-60a639bf-6b39-463d-835b-63cba0b82a5b" in namespace "containers-6169" to be "success or failure"
Dec  5 07:50:06.238: INFO: Pod "client-containers-60a639bf-6b39-463d-835b-63cba0b82a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.67859ms
Dec  5 07:50:08.241: INFO: Pod "client-containers-60a639bf-6b39-463d-835b-63cba0b82a5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006969118s
Dec  5 07:50:10.245: INFO: Pod "client-containers-60a639bf-6b39-463d-835b-63cba0b82a5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010630511s
STEP: Saw pod success
Dec  5 07:50:10.245: INFO: Pod "client-containers-60a639bf-6b39-463d-835b-63cba0b82a5b" satisfied condition "success or failure"
Dec  5 07:50:10.247: INFO: Trying to get logs from node node2 pod client-containers-60a639bf-6b39-463d-835b-63cba0b82a5b container test-container: <nil>
STEP: delete the pod
Dec  5 07:50:10.261: INFO: Waiting for pod client-containers-60a639bf-6b39-463d-835b-63cba0b82a5b to disappear
Dec  5 07:50:10.263: INFO: Pod client-containers-60a639bf-6b39-463d-835b-63cba0b82a5b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:50:10.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6169" for this suite.
Dec  5 07:50:16.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:50:16.377: INFO: namespace containers-6169 deletion completed in 6.109945821s

• [SLOW TEST:10.180 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  5 07:50:16.378: INFO: >>> kubeConfig: /tmp/kubeconfig-771445370
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  5 07:50:16.408: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ef6dd1d-4be1-4311-940f-3673041f0123" in namespace "downward-api-5445" to be "success or failure"
Dec  5 07:50:16.411: INFO: Pod "downwardapi-volume-0ef6dd1d-4be1-4311-940f-3673041f0123": Phase="Pending", Reason="", readiness=false. Elapsed: 3.025715ms
Dec  5 07:50:18.414: INFO: Pod "downwardapi-volume-0ef6dd1d-4be1-4311-940f-3673041f0123": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006534681s
STEP: Saw pod success
Dec  5 07:50:18.414: INFO: Pod "downwardapi-volume-0ef6dd1d-4be1-4311-940f-3673041f0123" satisfied condition "success or failure"
Dec  5 07:50:18.417: INFO: Trying to get logs from node node1 pod downwardapi-volume-0ef6dd1d-4be1-4311-940f-3673041f0123 container client-container: <nil>
STEP: delete the pod
Dec  5 07:50:18.432: INFO: Waiting for pod downwardapi-volume-0ef6dd1d-4be1-4311-940f-3673041f0123 to disappear
Dec  5 07:50:18.434: INFO: Pod downwardapi-volume-0ef6dd1d-4be1-4311-940f-3673041f0123 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  5 07:50:18.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5445" for this suite.
Dec  5 07:50:24.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  5 07:50:24.541: INFO: namespace downward-api-5445 deletion completed in 6.10425663s

• [SLOW TEST:8.164 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.5-beta.0.35+20c265fef0741d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSDec  5 07:50:24.541: INFO: Running AfterSuite actions on all nodes
Dec  5 07:50:24.542: INFO: Running AfterSuite actions on node 1
Dec  5 07:50:24.542: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 6376.942 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h46m18.200196701s
Test Suite Passed
