Conformance test: not doing test setup.
I1203 14:40:02.567781    5065 e2e.go:243] Starting e2e run "2dd116ee-353c-4a76-b427-d334ebb975bf" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1575384001 - Will randomize all specs
Will run 215 of 4413 specs

Dec  3 14:40:38.872: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Deleting namespaces
STEP: Waiting for namespaces to vanish
I1203 14:40:39.247364    5065 e2e.go:98] Waiting for deletion of the following namespaces: []
Dec  3 14:40:41.338: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  3 14:40:41.609: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  3 14:40:41.997: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  3 14:40:42.047: INFO: expected 12 pod replicas in namespace 'kube-system', 12 are Running and Ready.
Dec  3 14:40:42.047: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  3 14:40:42.144: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec  3 14:40:42.144: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec  3 14:40:42.144: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Dec  3 14:40:42.144: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Dec  3 14:40:42.144: INFO: e2e test version: v1.15.6
Dec  3 14:40:42.233: INFO: kube-apiserver version: v1.15.6
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:40:42.234: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
Dec  3 14:40:42.595: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Dec  3 14:40:42.867: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  3 14:40:43.417: INFO: Waiting up to 5m0s for pod "pod-7e16b3e8-2bb3-4075-8437-758b77ba9069" in namespace "emptydir-9576" to be "success or failure"
Dec  3 14:40:43.507: INFO: Pod "pod-7e16b3e8-2bb3-4075-8437-758b77ba9069": Phase="Pending", Reason="", readiness=false. Elapsed: 89.889645ms
Dec  3 14:40:45.599: INFO: Pod "pod-7e16b3e8-2bb3-4075-8437-758b77ba9069": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.181278792s
STEP: Saw pod success
Dec  3 14:40:45.599: INFO: Pod "pod-7e16b3e8-2bb3-4075-8437-758b77ba9069" satisfied condition "success or failure"
Dec  3 14:40:45.688: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-7e16b3e8-2bb3-4075-8437-758b77ba9069 container test-container: <nil>
STEP: delete the pod
Dec  3 14:40:46.014: INFO: Waiting for pod pod-7e16b3e8-2bb3-4075-8437-758b77ba9069 to disappear
Dec  3 14:40:46.103: INFO: Pod pod-7e16b3e8-2bb3-4075-8437-758b77ba9069 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:40:46.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9576" for this suite.
Dec  3 14:40:52.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:40:55.960: INFO: namespace emptydir-9576 deletion completed in 9.766704067s
•SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:40:55.961: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1895
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 14:40:56.600: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  3 14:40:56.781: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  3 14:41:00.961: INFO: Creating deployment "test-rolling-update-deployment"
Dec  3 14:41:01.051: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  3 14:41:01.232: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  3 14:41:01.321: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710980861, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710980861, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710980861, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710980861, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 14:41:03.412: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710980861, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710980861, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710980861, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710980861, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 14:41:05.411: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  3 14:41:05.790: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-1895,SelfLink:/apis/apps/v1/namespaces/deployment-1895/deployments/test-rolling-update-deployment,UID:498a5c96-aba7-4651-aa29-c19ff69c69d6,ResourceVersion:2542,Generation:1,CreationTimestamp:2019-12-03 14:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-03 14:41:01 +0000 UTC 2019-12-03 14:41:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-03 14:41:03 +0000 UTC 2019-12-03 14:41:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  3 14:41:05.880: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-1895,SelfLink:/apis/apps/v1/namespaces/deployment-1895/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:1a6daaff-f8a2-4b63-9205-3b30f3301e3f,ResourceVersion:2535,Generation:1,CreationTimestamp:2019-12-03 14:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 498a5c96-aba7-4651-aa29-c19ff69c69d6 0xc0010c79b7 0xc0010c79b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  3 14:41:05.880: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  3 14:41:05.881: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-1895,SelfLink:/apis/apps/v1/namespaces/deployment-1895/replicasets/test-rolling-update-controller,UID:1de15125-f0ef-4209-b5f1-46c6f8da1f84,ResourceVersion:2541,Generation:2,CreationTimestamp:2019-12-03 14:40:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 498a5c96-aba7-4651-aa29-c19ff69c69d6 0xc0010c78df 0xc0010c78f0}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 14:41:05.971: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-6sf77" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-6sf77,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-1895,SelfLink:/api/v1/namespaces/deployment-1895/pods/test-rolling-update-deployment-79f6b9d75c-6sf77,UID:1a537241-26cb-46b4-b262-853260ee2303,ResourceVersion:2534,Generation:0,CreationTimestamp:2019-12-03 14:41:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.6/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 1a6daaff-f8a2-4b63-9205-3b30f3301e3f 0xc00223c287 0xc00223c288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-zjstc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-zjstc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-zjstc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00223c2f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00223c310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:41:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:41:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:41:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:41:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:100.64.0.6,StartTime:2019-12-03 14:41:01 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-03 14:41:02 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://23c9c800dceb2bc9724d3b37f74bc2dec9783decd6ab2ca065b936e06302ab89}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:41:05.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1895" for this suite.
Dec  3 14:41:12.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:41:15.833: INFO: namespace deployment-1895 deletion completed in 9.771210652s
•SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:41:15.833: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4778
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 14:41:16.559: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version'
Dec  3 14:41:21.129: INFO: stderr: ""
Dec  3 14:41:21.203: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.6\", GitCommit:\"7015f71e75f670eb9e7ebd4b5749639d42e20079\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:20:18Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.6\", GitCommit:\"7015f71e75f670eb9e7ebd4b5749639d42e20079\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:11:50Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:41:21.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4778" for this suite.
Dec  3 14:41:27.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:41:31.078: INFO: namespace kubectl-4778 deletion completed in 9.784288004s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:41:31.079: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1967
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec  3 14:41:35.026: INFO: Successfully updated pod "annotationupdatecc8c330d-7c9b-4fd3-bd42-20af47d1e0a6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:41:39.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1967" for this suite.
Dec  3 14:42:01.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:42:05.243: INFO: namespace projected-1967 deletion completed in 25.824784813s
•SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:42:05.243: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 14:42:05.972: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7aed05bd-d7ed-48a3-8036-675f9836ba7e" in namespace "projected-2963" to be "success or failure"
Dec  3 14:42:06.062: INFO: Pod "downwardapi-volume-7aed05bd-d7ed-48a3-8036-675f9836ba7e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.372243ms
Dec  3 14:42:08.152: INFO: Pod "downwardapi-volume-7aed05bd-d7ed-48a3-8036-675f9836ba7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179308642s
STEP: Saw pod success
Dec  3 14:42:08.152: INFO: Pod "downwardapi-volume-7aed05bd-d7ed-48a3-8036-675f9836ba7e" satisfied condition "success or failure"
Dec  3 14:42:08.241: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-7aed05bd-d7ed-48a3-8036-675f9836ba7e container client-container: <nil>
STEP: delete the pod
Dec  3 14:42:08.430: INFO: Waiting for pod downwardapi-volume-7aed05bd-d7ed-48a3-8036-675f9836ba7e to disappear
Dec  3 14:42:08.520: INFO: Pod downwardapi-volume-7aed05bd-d7ed-48a3-8036-675f9836ba7e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:42:08.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2963" for this suite.
Dec  3 14:42:14.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:42:18.387: INFO: namespace projected-2963 deletion completed in 9.776901414s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:42:18.387: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7087
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-fx2h
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 14:42:19.308: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fx2h" in namespace "subpath-7087" to be "success or failure"
Dec  3 14:42:19.398: INFO: Pod "pod-subpath-test-downwardapi-fx2h": Phase="Pending", Reason="", readiness=false. Elapsed: 89.940927ms
Dec  3 14:42:21.489: INFO: Pod "pod-subpath-test-downwardapi-fx2h": Phase="Running", Reason="", readiness=true. Elapsed: 2.180313668s
Dec  3 14:42:23.579: INFO: Pod "pod-subpath-test-downwardapi-fx2h": Phase="Running", Reason="", readiness=true. Elapsed: 4.270446264s
Dec  3 14:42:25.669: INFO: Pod "pod-subpath-test-downwardapi-fx2h": Phase="Running", Reason="", readiness=true. Elapsed: 6.360437784s
Dec  3 14:42:27.759: INFO: Pod "pod-subpath-test-downwardapi-fx2h": Phase="Running", Reason="", readiness=true. Elapsed: 8.450736771s
Dec  3 14:42:29.849: INFO: Pod "pod-subpath-test-downwardapi-fx2h": Phase="Running", Reason="", readiness=true. Elapsed: 10.540619007s
Dec  3 14:42:31.939: INFO: Pod "pod-subpath-test-downwardapi-fx2h": Phase="Running", Reason="", readiness=true. Elapsed: 12.630806866s
Dec  3 14:42:34.029: INFO: Pod "pod-subpath-test-downwardapi-fx2h": Phase="Running", Reason="", readiness=true. Elapsed: 14.720789078s
Dec  3 14:42:36.120: INFO: Pod "pod-subpath-test-downwardapi-fx2h": Phase="Running", Reason="", readiness=true. Elapsed: 16.811967181s
Dec  3 14:42:38.210: INFO: Pod "pod-subpath-test-downwardapi-fx2h": Phase="Running", Reason="", readiness=true. Elapsed: 18.901913342s
Dec  3 14:42:40.300: INFO: Pod "pod-subpath-test-downwardapi-fx2h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.99218432s
STEP: Saw pod success
Dec  3 14:42:40.301: INFO: Pod "pod-subpath-test-downwardapi-fx2h" satisfied condition "success or failure"
Dec  3 14:42:40.391: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-subpath-test-downwardapi-fx2h container test-container-subpath-downwardapi-fx2h: <nil>
STEP: delete the pod
Dec  3 14:42:40.582: INFO: Waiting for pod pod-subpath-test-downwardapi-fx2h to disappear
Dec  3 14:42:40.672: INFO: Pod pod-subpath-test-downwardapi-fx2h no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fx2h
Dec  3 14:42:40.672: INFO: Deleting pod "pod-subpath-test-downwardapi-fx2h" in namespace "subpath-7087"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:42:40.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7087" for this suite.
Dec  3 14:42:47.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:42:50.629: INFO: namespace subpath-7087 deletion completed in 9.776842504s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:42:50.630: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 14:42:51.360: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8539760-5471-4eb8-9f10-b6986c1b9b2c" in namespace "downward-api-1053" to be "success or failure"
Dec  3 14:42:51.450: INFO: Pod "downwardapi-volume-a8539760-5471-4eb8-9f10-b6986c1b9b2c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.762752ms
Dec  3 14:42:53.540: INFO: Pod "downwardapi-volume-a8539760-5471-4eb8-9f10-b6986c1b9b2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17993758s
STEP: Saw pod success
Dec  3 14:42:53.540: INFO: Pod "downwardapi-volume-a8539760-5471-4eb8-9f10-b6986c1b9b2c" satisfied condition "success or failure"
Dec  3 14:42:53.630: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-a8539760-5471-4eb8-9f10-b6986c1b9b2c container client-container: <nil>
STEP: delete the pod
Dec  3 14:42:53.820: INFO: Waiting for pod downwardapi-volume-a8539760-5471-4eb8-9f10-b6986c1b9b2c to disappear
Dec  3 14:42:53.909: INFO: Pod downwardapi-volume-a8539760-5471-4eb8-9f10-b6986c1b9b2c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:42:53.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1053" for this suite.
Dec  3 14:43:00.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:43:03.768: INFO: namespace downward-api-1053 deletion completed in 9.768457659s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:43:03.769: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-ee558b10-7150-4564-aaf0-48ab8a59901f in namespace container-probe-6917
Dec  3 14:43:06.678: INFO: Started pod liveness-ee558b10-7150-4564-aaf0-48ab8a59901f in namespace container-probe-6917
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 14:43:06.768: INFO: Initial restart count of pod liveness-ee558b10-7150-4564-aaf0-48ab8a59901f is 0
Dec  3 14:43:23.582: INFO: Restart count of pod container-probe-6917/liveness-ee558b10-7150-4564-aaf0-48ab8a59901f is now 1 (16.814031885s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:43:23.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6917" for this suite.
Dec  3 14:43:30.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:43:33.540: INFO: namespace container-probe-6917 deletion completed in 9.775412948s
•SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:43:33.540: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 14:43:34.271: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1deef57e-be42-4611-b3cb-672c86f4d38f" in namespace "projected-6394" to be "success or failure"
Dec  3 14:43:34.361: INFO: Pod "downwardapi-volume-1deef57e-be42-4611-b3cb-672c86f4d38f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.741325ms
Dec  3 14:43:36.452: INFO: Pod "downwardapi-volume-1deef57e-be42-4611-b3cb-672c86f4d38f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180160045s
STEP: Saw pod success
Dec  3 14:43:36.452: INFO: Pod "downwardapi-volume-1deef57e-be42-4611-b3cb-672c86f4d38f" satisfied condition "success or failure"
Dec  3 14:43:36.542: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-1deef57e-be42-4611-b3cb-672c86f4d38f container client-container: <nil>
STEP: delete the pod
Dec  3 14:43:36.730: INFO: Waiting for pod downwardapi-volume-1deef57e-be42-4611-b3cb-672c86f4d38f to disappear
Dec  3 14:43:36.820: INFO: Pod downwardapi-volume-1deef57e-be42-4611-b3cb-672c86f4d38f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:43:36.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6394" for this suite.
Dec  3 14:43:43.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:43:46.696: INFO: namespace projected-6394 deletion completed in 9.786291778s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:43:46.697: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9198
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  3 14:43:49.790: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:43:49.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9198" for this suite.
Dec  3 14:43:56.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:43:59.827: INFO: namespace container-runtime-9198 deletion completed in 9.763815321s
•SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:43:59.827: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7872.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7872.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 147.104.111.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.111.104.147_udp@PTR;check="$$(dig +tcp +noall +answer +search 147.104.111.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.111.104.147_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7872.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7872.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7872.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7872.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7872.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 147.104.111.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.111.104.147_udp@PTR;check="$$(dig +tcp +noall +answer +search 147.104.111.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.111.104.147_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 14:44:11.193: INFO: Unable to read wheezy_udp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:11.286: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:11.378: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:11.470: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:12.130: INFO: Unable to read jessie_udp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:12.227: INFO: Unable to read jessie_tcp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:12.319: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:12.412: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:12.973: INFO: Lookups using dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19 failed for: [wheezy_udp@dns-test-service.dns-7872.svc.cluster.local wheezy_tcp@dns-test-service.dns-7872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local jessie_udp@dns-test-service.dns-7872.svc.cluster.local jessie_tcp@dns-test-service.dns-7872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local]

Dec  3 14:44:18.066: INFO: Unable to read wheezy_udp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:18.158: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:18.250: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:18.342: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:19.000: INFO: Unable to read jessie_udp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:19.110: INFO: Unable to read jessie_tcp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:19.203: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:19.295: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:19.857: INFO: Lookups using dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19 failed for: [wheezy_udp@dns-test-service.dns-7872.svc.cluster.local wheezy_tcp@dns-test-service.dns-7872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local jessie_udp@dns-test-service.dns-7872.svc.cluster.local jessie_tcp@dns-test-service.dns-7872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local]

Dec  3 14:44:23.066: INFO: Unable to read wheezy_udp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:23.158: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:23.251: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:23.343: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:23.997: INFO: Unable to read jessie_udp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:24.089: INFO: Unable to read jessie_tcp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:24.182: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:24.274: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:24.837: INFO: Lookups using dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19 failed for: [wheezy_udp@dns-test-service.dns-7872.svc.cluster.local wheezy_tcp@dns-test-service.dns-7872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local jessie_udp@dns-test-service.dns-7872.svc.cluster.local jessie_tcp@dns-test-service.dns-7872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local]

Dec  3 14:44:28.066: INFO: Unable to read wheezy_udp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:28.158: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:28.251: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:28.343: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:28.998: INFO: Unable to read jessie_udp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:29.090: INFO: Unable to read jessie_tcp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:29.184: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:29.277: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:29.841: INFO: Lookups using dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19 failed for: [wheezy_udp@dns-test-service.dns-7872.svc.cluster.local wheezy_tcp@dns-test-service.dns-7872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local jessie_udp@dns-test-service.dns-7872.svc.cluster.local jessie_tcp@dns-test-service.dns-7872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local]

Dec  3 14:44:33.066: INFO: Unable to read wheezy_udp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:33.159: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:33.251: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:33.344: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:34.186: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:34.278: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local from pod dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19: the server could not find the requested resource (get pods dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19)
Dec  3 14:44:34.844: INFO: Lookups using dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19 failed for: [wheezy_udp@dns-test-service.dns-7872.svc.cluster.local wheezy_tcp@dns-test-service.dns-7872.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7872.svc.cluster.local]

Dec  3 14:44:39.902: INFO: DNS probes using dns-7872/dns-test-aaf5b819-1b53-41c7-a37b-28f53f1bfa19 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:44:40.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7872" for this suite.
Dec  3 14:44:46.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:44:50.048: INFO: namespace dns-7872 deletion completed in 9.772879556s
•SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:44:50.048: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4514
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec  3 14:44:50.686: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 14:44:50.867: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 14:44:50.957: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-31-164.ec2.internal before test
Dec  3 14:44:51.054: INFO: kube-proxy-8xnnr from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.054: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 14:44:51.054: INFO: node-problem-detector-hnrtk from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.054: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 14:44:51.054: INFO: calico-typha-deploy-5547c4cdc6-zvk82 from kube-system started at 2019-12-03 14:32:23 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.054: INFO: 	Container calico-typha ready: true, restart count 0
Dec  3 14:44:51.054: INFO: calico-node-w5f9r from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.054: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 14:44:51.054: INFO: blackbox-exporter-c87bdd467-vscd2 from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.054: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 14:44:51.054: INFO: node-exporter-68n8d from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.054: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 14:44:51.054: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-9-228.ec2.internal before test
Dec  3 14:44:51.162: INFO: calico-typha-horizontal-autoscaler-554dfbfdd7-r9vld from kube-system started at 2019-12-03 14:30:28 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 14:44:51.162: INFO: coredns-858b686868-r84d9 from kube-system started at 2019-12-03 14:30:28 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container coredns ready: true, restart count 0
Dec  3 14:44:51.162: INFO: calico-typha-vertical-autoscaler-656557779f-zrt7j from kube-system started at 2019-12-03 14:30:32 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container autoscaler ready: true, restart count 3
Dec  3 14:44:51.162: INFO: addons-kubernetes-dashboard-5c8d9945bc-5mvxv from kube-system started at 2019-12-03 14:30:32 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 14:44:51.162: INFO: node-problem-detector-bpps6 from kube-system started at 2019-12-03 14:30:18 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 14:44:51.162: INFO: vpn-shoot-859fb5c977-s5ldm from kube-system started at 2019-12-03 14:30:30 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 14:44:51.162: INFO: calico-kube-controllers-5d785bc598-746p6 from kube-system started at 2019-12-03 14:30:30 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  3 14:44:51.162: INFO: coredns-858b686868-lbh4p from kube-system started at 2019-12-03 14:30:32 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container coredns ready: true, restart count 0
Dec  3 14:44:51.162: INFO: calico-node-hvhtd from kube-system started at 2019-12-03 14:30:18 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 14:44:51.162: INFO: node-exporter-kklbs from kube-system started at 2019-12-03 14:30:18 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 14:44:51.162: INFO: addons-nginx-ingress-controller-8468678b64-x6f59 from kube-system started at 2019-12-03 14:30:32 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 14:44:51.162: INFO: kube-proxy-8j96c from kube-system started at 2019-12-03 14:30:18 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 14:44:51.162: INFO: metrics-server-bf696d85c-7v9h8 from kube-system started at 2019-12-03 14:30:28 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 14:44:51.162: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-nwxw7 from kube-system started at 2019-12-03 14:30:28 +0000 UTC (1 container statuses recorded)
Dec  3 14:44:51.162: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15dce3aa96b311b7], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:44:52.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4514" for this suite.
Dec  3 14:44:58.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:45:02.499: INFO: namespace sched-pred-4514 deletion completed in 9.793557559s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:45:02.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 14:45:03.138: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-6667'
Dec  3 14:45:03.577: INFO: stderr: ""
Dec  3 14:45:03.577: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec  3 14:45:08.680: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod e2e-test-nginx-pod --namespace=kubectl-6667 -o json'
Dec  3 14:45:09.106: INFO: stderr: ""
Dec  3 14:45:09.106: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.64.0.15/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-12-03T14:45:03Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-6667\",\n        \"resourceVersion\": \"3252\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6667/pods/e2e-test-nginx-pod\",\n        \"uid\": \"3ab8fe1f-9742-4f84-b1b9-69dd1bfcf6e9\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-4ctk7\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-250-31-164.ec2.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-4ctk7\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-4ctk7\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T14:45:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T14:45:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T14:45:04Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T14:45:03Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://c58fe645c33dbfc11a384032e79b7edd1530ca1fa8f2bd151a5347d24deecebf\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-03T14:45:04Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.31.164\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.64.0.15\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-03T14:45:03Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec  3 14:45:09.106: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config replace -f - --namespace=kubectl-6667'
Dec  3 14:45:10.053: INFO: stderr: ""
Dec  3 14:45:10.053: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Dec  3 14:45:10.142: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-6667'
Dec  3 14:45:11.747: INFO: stderr: ""
Dec  3 14:45:11.747: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:45:11.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6667" for this suite.
Dec  3 14:45:18.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:45:21.650: INFO: namespace kubectl-6667 deletion completed in 9.813084584s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:45:21.651: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec  3 14:45:22.288: INFO: PodSpec: initContainers in spec.initContainers
Dec  3 14:46:05.297: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-64a8f7f5-8ec1-4d77-828c-06e9f35b0060", GenerateName:"", Namespace:"init-container-1208", SelfLink:"/api/v1/namespaces/init-container-1208/pods/pod-init-64a8f7f5-8ec1-4d77-828c-06e9f35b0060", UID:"2f2758c4-5373-42b3-a27a-e52299269d73", ResourceVersion:"3405", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63710981122, loc:(*time.Location)(0x7ed1a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"288514495"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.64.0.16/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-ncf8z", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0019056c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ncf8z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ncf8z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-ncf8z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001a1ab78), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-250-31-164.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001d53860), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a1acf0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a1ad10)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001a1ad18), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001a1ad1c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981122, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981122, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981122, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981122, loc:(*time.Location)(0x7ed1a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.31.164", PodIP:"100.64.0.16", StartTime:(*v1.Time)(0xc001b02340), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002976700)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002976770)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://1522c95561ba89fb4517a6ffdde466a1337818001787b22da6b615fd4e6468f6"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b023a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001b02360), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:46:05.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1208" for this suite.
Dec  3 14:46:27.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:46:31.156: INFO: namespace init-container-1208 deletion completed in 25.767755309s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:46:31.156: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-895
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 14:46:36.341: INFO: Waiting up to 5m0s for pod "client-envvars-4a283988-bc82-419d-ba48-cd5ae419c7b1" in namespace "pods-895" to be "success or failure"
Dec  3 14:46:36.431: INFO: Pod "client-envvars-4a283988-bc82-419d-ba48-cd5ae419c7b1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.912652ms
Dec  3 14:46:38.523: INFO: Pod "client-envvars-4a283988-bc82-419d-ba48-cd5ae419c7b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.181414754s
STEP: Saw pod success
Dec  3 14:46:38.523: INFO: Pod "client-envvars-4a283988-bc82-419d-ba48-cd5ae419c7b1" satisfied condition "success or failure"
Dec  3 14:46:38.613: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod client-envvars-4a283988-bc82-419d-ba48-cd5ae419c7b1 container env3cont: <nil>
STEP: delete the pod
Dec  3 14:46:38.804: INFO: Waiting for pod client-envvars-4a283988-bc82-419d-ba48-cd5ae419c7b1 to disappear
Dec  3 14:46:38.893: INFO: Pod client-envvars-4a283988-bc82-419d-ba48-cd5ae419c7b1 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:46:38.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-895" for this suite.
Dec  3 14:47:17.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:47:20.754: INFO: namespace pods-895 deletion completed in 41.769553903s
•SSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:47:20.754: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-7742
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec  3 14:47:28.114: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7742 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:47:28.114: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:47:29.068: INFO: Exec stderr: ""
Dec  3 14:47:29.068: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7742 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:47:29.068: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:47:29.883: INFO: Exec stderr: ""
Dec  3 14:47:29.883: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7742 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:47:29.883: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:47:30.716: INFO: Exec stderr: ""
Dec  3 14:47:30.716: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7742 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:47:30.716: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:47:31.541: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec  3 14:47:31.541: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7742 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:47:31.541: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:47:32.359: INFO: Exec stderr: ""
Dec  3 14:47:32.359: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7742 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:47:32.359: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:47:33.221: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec  3 14:47:33.221: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7742 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:47:33.221: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:47:34.041: INFO: Exec stderr: ""
Dec  3 14:47:34.041: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7742 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:47:34.041: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:47:34.855: INFO: Exec stderr: ""
Dec  3 14:47:34.855: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7742 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:47:34.855: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:47:35.674: INFO: Exec stderr: ""
Dec  3 14:47:35.674: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7742 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:47:35.674: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:47:36.534: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:47:36.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7742" for this suite.
Dec  3 14:48:26.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:48:30.399: INFO: namespace e2e-kubelet-etc-hosts-7742 deletion completed in 53.774882599s
•SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:48:30.400: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4087
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  3 14:48:31.135: INFO: Waiting up to 5m0s for pod "pod-ba878a82-b573-4c1b-952b-7a02faa84a12" in namespace "emptydir-4087" to be "success or failure"
Dec  3 14:48:31.225: INFO: Pod "pod-ba878a82-b573-4c1b-952b-7a02faa84a12": Phase="Pending", Reason="", readiness=false. Elapsed: 89.758501ms
Dec  3 14:48:33.315: INFO: Pod "pod-ba878a82-b573-4c1b-952b-7a02faa84a12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180114099s
STEP: Saw pod success
Dec  3 14:48:33.315: INFO: Pod "pod-ba878a82-b573-4c1b-952b-7a02faa84a12" satisfied condition "success or failure"
Dec  3 14:48:33.405: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-ba878a82-b573-4c1b-952b-7a02faa84a12 container test-container: <nil>
STEP: delete the pod
Dec  3 14:48:33.594: INFO: Waiting for pod pod-ba878a82-b573-4c1b-952b-7a02faa84a12 to disappear
Dec  3 14:48:33.684: INFO: Pod pod-ba878a82-b573-4c1b-952b-7a02faa84a12 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:48:33.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4087" for this suite.
Dec  3 14:48:40.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:48:43.545: INFO: namespace emptydir-4087 deletion completed in 9.77037388s
•
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:48:43.545: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7465
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-7465/secret-test-7b3f44e9-6a86-4493-9255-a0de8bbe2443
STEP: Creating a pod to test consume secrets
Dec  3 14:48:44.366: INFO: Waiting up to 5m0s for pod "pod-configmaps-f86c1ec7-d8b0-46cf-a668-9d6778d00892" in namespace "secrets-7465" to be "success or failure"
Dec  3 14:48:44.456: INFO: Pod "pod-configmaps-f86c1ec7-d8b0-46cf-a668-9d6778d00892": Phase="Pending", Reason="", readiness=false. Elapsed: 89.652734ms
Dec  3 14:48:46.546: INFO: Pod "pod-configmaps-f86c1ec7-d8b0-46cf-a668-9d6778d00892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17987806s
STEP: Saw pod success
Dec  3 14:48:46.546: INFO: Pod "pod-configmaps-f86c1ec7-d8b0-46cf-a668-9d6778d00892" satisfied condition "success or failure"
Dec  3 14:48:46.636: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-configmaps-f86c1ec7-d8b0-46cf-a668-9d6778d00892 container env-test: <nil>
STEP: delete the pod
Dec  3 14:48:46.826: INFO: Waiting for pod pod-configmaps-f86c1ec7-d8b0-46cf-a668-9d6778d00892 to disappear
Dec  3 14:48:46.916: INFO: Pod pod-configmaps-f86c1ec7-d8b0-46cf-a668-9d6778d00892 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:48:46.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7465" for this suite.
Dec  3 14:48:53.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:48:56.785: INFO: namespace secrets-7465 deletion completed in 9.777994769s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:48:56.785: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9407
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-7481
STEP: Creating secret with name secret-test-4a1a0a20-94d0-459b-9948-d2aa7cfb05c2
STEP: Creating a pod to test consume secrets
Dec  3 14:48:58.250: INFO: Waiting up to 5m0s for pod "pod-secrets-c1d0fa94-9361-42d9-b2ba-0bfed7617df8" in namespace "secrets-9407" to be "success or failure"
Dec  3 14:48:58.340: INFO: Pod "pod-secrets-c1d0fa94-9361-42d9-b2ba-0bfed7617df8": Phase="Pending", Reason="", readiness=false. Elapsed: 89.760964ms
Dec  3 14:49:00.430: INFO: Pod "pod-secrets-c1d0fa94-9361-42d9-b2ba-0bfed7617df8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179852989s
STEP: Saw pod success
Dec  3 14:49:00.430: INFO: Pod "pod-secrets-c1d0fa94-9361-42d9-b2ba-0bfed7617df8" satisfied condition "success or failure"
Dec  3 14:49:00.520: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-secrets-c1d0fa94-9361-42d9-b2ba-0bfed7617df8 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 14:49:00.713: INFO: Waiting for pod pod-secrets-c1d0fa94-9361-42d9-b2ba-0bfed7617df8 to disappear
Dec  3 14:49:00.803: INFO: Pod pod-secrets-c1d0fa94-9361-42d9-b2ba-0bfed7617df8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:49:00.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9407" for this suite.
Dec  3 14:49:07.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:49:10.657: INFO: namespace secrets-9407 deletion completed in 9.763059809s
STEP: Destroying namespace "secret-namespace-7481" for this suite.
Dec  3 14:49:16.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:49:20.428: INFO: namespace secret-namespace-7481 deletion completed in 9.771192795s
•
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:49:20.428: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-cd43e8e4-2fa5-4143-ae23-3472b863793e in namespace container-probe-534
Dec  3 14:49:23.338: INFO: Started pod liveness-cd43e8e4-2fa5-4143-ae23-3472b863793e in namespace container-probe-534
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 14:49:23.429: INFO: Initial restart count of pod liveness-cd43e8e4-2fa5-4143-ae23-3472b863793e is 0
Dec  3 14:49:36.059: INFO: Restart count of pod container-probe-534/liveness-cd43e8e4-2fa5-4143-ae23-3472b863793e is now 1 (12.630267359s elapsed)
Dec  3 14:49:56.961: INFO: Restart count of pod container-probe-534/liveness-cd43e8e4-2fa5-4143-ae23-3472b863793e is now 2 (33.532615637s elapsed)
Dec  3 14:50:17.867: INFO: Restart count of pod container-probe-534/liveness-cd43e8e4-2fa5-4143-ae23-3472b863793e is now 3 (54.438043179s elapsed)
Dec  3 14:50:36.678: INFO: Restart count of pod container-probe-534/liveness-cd43e8e4-2fa5-4143-ae23-3472b863793e is now 4 (1m13.249290228s elapsed)
Dec  3 14:51:47.745: INFO: Restart count of pod container-probe-534/liveness-cd43e8e4-2fa5-4143-ae23-3472b863793e is now 5 (2m24.316852454s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:51:47.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-534" for this suite.
Dec  3 14:51:54.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:51:57.693: INFO: namespace container-probe-534 deletion completed in 9.763439095s
•SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:51:57.694: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-3d51e11a-e8b5-439c-bb44-a6a154ba958a in namespace container-probe-8537
Dec  3 14:52:00.603: INFO: Started pod busybox-3d51e11a-e8b5-439c-bb44-a6a154ba958a in namespace container-probe-8537
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 14:52:00.693: INFO: Initial restart count of pod busybox-3d51e11a-e8b5-439c-bb44-a6a154ba958a is 0
Dec  3 14:52:48.857: INFO: Restart count of pod container-probe-8537/busybox-3d51e11a-e8b5-439c-bb44-a6a154ba958a is now 1 (48.16384108s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:52:48.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8537" for this suite.
Dec  3 14:52:55.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:52:58.807: INFO: namespace container-probe-8537 deletion completed in 9.765288483s
•SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:52:58.807: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3126
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 14:52:59.624: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  3 14:53:01.805: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  3 14:53:03.895: INFO: Creating deployment "test-rollover-deployment"
Dec  3 14:53:04.075: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  3 14:53:04.165: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  3 14:53:04.344: INFO: Ensure that both replica sets have 1 created replica
Dec  3 14:53:04.536: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  3 14:53:04.717: INFO: Updating deployment test-rollover-deployment
Dec  3 14:53:04.717: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  3 14:53:04.807: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  3 14:53:04.987: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  3 14:53:05.167: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 14:53:05.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981584, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 14:53:07.347: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 14:53:07.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981586, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 14:53:09.347: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 14:53:09.348: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981586, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 14:53:11.347: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 14:53:11.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981586, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 14:53:13.347: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 14:53:13.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981586, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 14:53:15.348: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 14:53:15.348: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981586, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981583, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 14:53:17.348: INFO: 
Dec  3 14:53:17.348: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  3 14:53:17.618: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-3126,SelfLink:/apis/apps/v1/namespaces/deployment-3126/deployments/test-rollover-deployment,UID:e6f82468-4baa-46cc-bdf5-010674cfceb5,ResourceVersion:4533,Generation:2,CreationTimestamp:2019-12-03 14:53:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-03 14:53:03 +0000 UTC 2019-12-03 14:53:03 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-03 14:53:16 +0000 UTC 2019-12-03 14:53:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  3 14:53:17.708: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-3126,SelfLink:/apis/apps/v1/namespaces/deployment-3126/replicasets/test-rollover-deployment-854595fc44,UID:9b8f7525-d3af-42cd-aef0-ee2373b5ba89,ResourceVersion:4526,Generation:2,CreationTimestamp:2019-12-03 14:53:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e6f82468-4baa-46cc-bdf5-010674cfceb5 0xc0015b1547 0xc0015b1548}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  3 14:53:17.709: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  3 14:53:17.709: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-3126,SelfLink:/apis/apps/v1/namespaces/deployment-3126/replicasets/test-rollover-controller,UID:7b8383bc-769e-47ec-8849-7a52cc7272b0,ResourceVersion:4532,Generation:2,CreationTimestamp:2019-12-03 14:52:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e6f82468-4baa-46cc-bdf5-010674cfceb5 0xc0015b146f 0xc0015b1480}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 14:53:17.709: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-3126,SelfLink:/apis/apps/v1/namespaces/deployment-3126/replicasets/test-rollover-deployment-9b8b997cf,UID:1e3ef21f-e6bb-4c15-95d2-0c3616b71c2c,ResourceVersion:4489,Generation:2,CreationTimestamp:2019-12-03 14:53:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e6f82468-4baa-46cc-bdf5-010674cfceb5 0xc0015b1600 0xc0015b1601}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 14:53:17.799: INFO: Pod "test-rollover-deployment-854595fc44-22qkb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-22qkb,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-3126,SelfLink:/api/v1/namespaces/deployment-3126/pods/test-rollover-deployment-854595fc44-22qkb,UID:06f92b6b-e928-4af5-8e22-9d1fc9f91012,ResourceVersion:4502,Generation:0,CreationTimestamp:2019-12-03 14:53:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.27/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 9b8f7525-d3af-42cd-aef0-ee2373b5ba89 0xc0019a6d67 0xc0019a6d68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-drvcn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-drvcn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-drvcn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019a6ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019a6ee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:53:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:53:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:53:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:53:04 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:100.64.0.27,StartTime:2019-12-03 14:53:04 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-03 14:53:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://13f0a2097ef7be42c36d78f82eb172913df072c6f59a1f32424c7a890a9b1df5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:53:17.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3126" for this suite.
Dec  3 14:53:24.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:53:27.662: INFO: namespace deployment-3126 deletion completed in 9.772018878s
•SSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:53:27.662: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-8510
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-8510
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8510
STEP: Deleting pre-stop pod
Dec  3 14:53:38.204: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:53:38.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8510" for this suite.
Dec  3 14:54:16.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:54:20.163: INFO: namespace prestop-8510 deletion completed in 41.776447036s
•SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:54:20.164: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2964, will wait for the garbage collector to delete the pods
Dec  3 14:54:23.267: INFO: Deleting Job.batch foo took: 91.348557ms
Dec  3 14:54:23.667: INFO: Terminating Job.batch foo pods took: 400.379053ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:55:03.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2964" for this suite.
Dec  3 14:55:09.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:55:13.320: INFO: namespace job-2964 deletion completed in 9.772512583s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:55:13.320: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec  3 14:55:13.959: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:55:16.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2872" for this suite.
Dec  3 14:55:22.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:55:26.490: INFO: namespace init-container-2872 deletion completed in 9.766026848s
•SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:55:26.491: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-c2dee2d4-6fcc-444f-88f1-ee61f632e3ba
STEP: Creating a pod to test consume configMaps
Dec  3 14:55:27.313: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eebc28c6-98f5-490f-8c0e-06aa99128803" in namespace "projected-4653" to be "success or failure"
Dec  3 14:55:27.403: INFO: Pod "pod-projected-configmaps-eebc28c6-98f5-490f-8c0e-06aa99128803": Phase="Pending", Reason="", readiness=false. Elapsed: 89.940421ms
Dec  3 14:55:29.493: INFO: Pod "pod-projected-configmaps-eebc28c6-98f5-490f-8c0e-06aa99128803": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180130332s
STEP: Saw pod success
Dec  3 14:55:29.493: INFO: Pod "pod-projected-configmaps-eebc28c6-98f5-490f-8c0e-06aa99128803" satisfied condition "success or failure"
Dec  3 14:55:29.583: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-configmaps-eebc28c6-98f5-490f-8c0e-06aa99128803 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 14:55:29.776: INFO: Waiting for pod pod-projected-configmaps-eebc28c6-98f5-490f-8c0e-06aa99128803 to disappear
Dec  3 14:55:29.866: INFO: Pod pod-projected-configmaps-eebc28c6-98f5-490f-8c0e-06aa99128803 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:55:29.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4653" for this suite.
Dec  3 14:55:36.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:55:39.721: INFO: namespace projected-4653 deletion completed in 9.764436863s
•
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:55:39.721: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9366
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-9d88a2e8-21dd-4138-bc98-d5e5d9508b61
STEP: Creating secret with name s-test-opt-upd-d6212722-387f-4dec-97b6-44ed4815f579
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9d88a2e8-21dd-4138-bc98-d5e5d9508b61
STEP: Updating secret s-test-opt-upd-d6212722-387f-4dec-97b6-44ed4815f579
STEP: Creating secret with name s-test-opt-create-9cc6ee3b-1716-4b87-9b58-6b381ba40410
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:55:48.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9366" for this suite.
Dec  3 14:56:10.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:56:13.896: INFO: namespace secrets-9366 deletion completed in 25.765567187s
•SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:56:13.896: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2254
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  3 14:56:21.167: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W1203 14:56:21.167055    5065 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  3 14:56:21.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2254" for this suite.
Dec  3 14:56:27.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:56:31.021: INFO: namespace gc-2254 deletion completed in 9.764345462s
•SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:56:31.021: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4241
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec  3 14:56:31.660: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4241'
Dec  3 14:56:32.820: INFO: stderr: ""
Dec  3 14:56:32.820: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 14:56:32.820: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4241'
Dec  3 14:56:33.246: INFO: stderr: ""
Dec  3 14:56:33.246: INFO: stdout: "update-demo-nautilus-2x5r7 update-demo-nautilus-nkjx7 "
Dec  3 14:56:33.246: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-2x5r7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4241'
Dec  3 14:56:33.670: INFO: stderr: ""
Dec  3 14:56:33.670: INFO: stdout: ""
Dec  3 14:56:33.670: INFO: update-demo-nautilus-2x5r7 is created but not running
Dec  3 14:56:38.670: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4241'
Dec  3 14:56:39.099: INFO: stderr: ""
Dec  3 14:56:39.099: INFO: stdout: "update-demo-nautilus-2x5r7 update-demo-nautilus-nkjx7 "
Dec  3 14:56:39.100: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-2x5r7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4241'
Dec  3 14:56:39.525: INFO: stderr: ""
Dec  3 14:56:39.525: INFO: stdout: "true"
Dec  3 14:56:39.525: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-2x5r7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4241'
Dec  3 14:56:39.947: INFO: stderr: ""
Dec  3 14:56:39.947: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 14:56:39.947: INFO: validating pod update-demo-nautilus-2x5r7
Dec  3 14:56:40.129: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 14:56:40.129: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 14:56:40.129: INFO: update-demo-nautilus-2x5r7 is verified up and running
Dec  3 14:56:40.129: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-nkjx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4241'
Dec  3 14:56:40.554: INFO: stderr: ""
Dec  3 14:56:40.554: INFO: stdout: "true"
Dec  3 14:56:40.554: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-nkjx7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4241'
Dec  3 14:56:40.975: INFO: stderr: ""
Dec  3 14:56:40.975: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 14:56:40.975: INFO: validating pod update-demo-nautilus-nkjx7
Dec  3 14:56:41.156: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 14:56:41.156: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 14:56:41.156: INFO: update-demo-nautilus-nkjx7 is verified up and running
STEP: scaling down the replication controller
Dec  3 14:56:41.158: INFO: scanned /root for discovery docs: <nil>
Dec  3 14:56:41.158: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4241'
Dec  3 14:56:41.768: INFO: stderr: ""
Dec  3 14:56:41.768: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 14:56:41.768: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4241'
Dec  3 14:56:42.198: INFO: stderr: ""
Dec  3 14:56:42.198: INFO: stdout: "update-demo-nautilus-2x5r7 update-demo-nautilus-nkjx7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  3 14:56:47.198: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4241'
Dec  3 14:56:47.627: INFO: stderr: ""
Dec  3 14:56:47.627: INFO: stdout: "update-demo-nautilus-2x5r7 update-demo-nautilus-nkjx7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  3 14:56:52.627: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4241'
Dec  3 14:56:53.049: INFO: stderr: ""
Dec  3 14:56:53.049: INFO: stdout: "update-demo-nautilus-2x5r7 update-demo-nautilus-nkjx7 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  3 14:56:58.049: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4241'
Dec  3 14:56:58.472: INFO: stderr: ""
Dec  3 14:56:58.472: INFO: stdout: "update-demo-nautilus-nkjx7 "
Dec  3 14:56:58.472: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-nkjx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4241'
Dec  3 14:56:58.919: INFO: stderr: ""
Dec  3 14:56:58.919: INFO: stdout: "true"
Dec  3 14:56:58.919: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-nkjx7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4241'
Dec  3 14:56:59.341: INFO: stderr: ""
Dec  3 14:56:59.341: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 14:56:59.341: INFO: validating pod update-demo-nautilus-nkjx7
Dec  3 14:56:59.434: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 14:56:59.434: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 14:56:59.434: INFO: update-demo-nautilus-nkjx7 is verified up and running
STEP: scaling up the replication controller
Dec  3 14:56:59.436: INFO: scanned /root for discovery docs: <nil>
Dec  3 14:56:59.436: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4241'
Dec  3 14:57:00.057: INFO: stderr: ""
Dec  3 14:57:00.057: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 14:57:00.057: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4241'
Dec  3 14:57:00.483: INFO: stderr: ""
Dec  3 14:57:00.483: INFO: stdout: "update-demo-nautilus-kqr2c update-demo-nautilus-nkjx7 "
Dec  3 14:57:00.483: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-kqr2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4241'
Dec  3 14:57:00.903: INFO: stderr: ""
Dec  3 14:57:00.903: INFO: stdout: ""
Dec  3 14:57:00.903: INFO: update-demo-nautilus-kqr2c is created but not running
Dec  3 14:57:05.903: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4241'
Dec  3 14:57:06.335: INFO: stderr: ""
Dec  3 14:57:06.336: INFO: stdout: "update-demo-nautilus-kqr2c update-demo-nautilus-nkjx7 "
Dec  3 14:57:06.336: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-kqr2c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4241'
Dec  3 14:57:06.758: INFO: stderr: ""
Dec  3 14:57:06.758: INFO: stdout: "true"
Dec  3 14:57:06.758: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-kqr2c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4241'
Dec  3 14:57:07.183: INFO: stderr: ""
Dec  3 14:57:07.183: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 14:57:07.183: INFO: validating pod update-demo-nautilus-kqr2c
Dec  3 14:57:07.365: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 14:57:07.365: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 14:57:07.365: INFO: update-demo-nautilus-kqr2c is verified up and running
Dec  3 14:57:07.365: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-nkjx7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4241'
Dec  3 14:57:07.788: INFO: stderr: ""
Dec  3 14:57:07.788: INFO: stdout: "true"
Dec  3 14:57:07.788: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-nkjx7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4241'
Dec  3 14:57:08.209: INFO: stderr: ""
Dec  3 14:57:08.209: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 14:57:08.209: INFO: validating pod update-demo-nautilus-nkjx7
Dec  3 14:57:08.303: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 14:57:08.303: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 14:57:08.303: INFO: update-demo-nautilus-nkjx7 is verified up and running
STEP: using delete to clean up resources
Dec  3 14:57:08.303: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4241'
Dec  3 14:57:08.815: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 14:57:08.815: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  3 14:57:08.815: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4241'
Dec  3 14:57:09.330: INFO: stderr: "No resources found.\n"
Dec  3 14:57:09.330: INFO: stdout: ""
Dec  3 14:57:09.330: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-4241 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 14:57:09.755: INFO: stderr: ""
Dec  3 14:57:09.755: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:57:09.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4241" for this suite.
Dec  3 14:57:32.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:57:35.611: INFO: namespace kubectl-4241 deletion completed in 25.763717114s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:57:35.612: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Dec  3 14:57:36.250: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:57:36.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2411" for this suite.
Dec  3 14:57:43.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:57:46.535: INFO: namespace kubectl-2411 deletion completed in 9.775438059s
•SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:57:46.535: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7328
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-bbae686c-b620-48b9-86a8-d635ecba7f78
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-bbae686c-b620-48b9-86a8-d635ecba7f78
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:57:52.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7328" for this suite.
Dec  3 14:58:14.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:58:17.955: INFO: namespace configmap-7328 deletion completed in 25.771147669s
•SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:58:17.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-807
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 14:58:18.596: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:58:21.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-807" for this suite.
Dec  3 14:59:05.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:59:09.402: INFO: namespace pods-807 deletion completed in 47.76986717s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:59:09.403: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4287
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 14:59:10.135: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5f628ff-60b3-461e-be35-d7ba7b7a866a" in namespace "projected-4287" to be "success or failure"
Dec  3 14:59:10.226: INFO: Pod "downwardapi-volume-c5f628ff-60b3-461e-be35-d7ba7b7a866a": Phase="Pending", Reason="", readiness=false. Elapsed: 90.812452ms
Dec  3 14:59:12.317: INFO: Pod "downwardapi-volume-c5f628ff-60b3-461e-be35-d7ba7b7a866a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.181613655s
STEP: Saw pod success
Dec  3 14:59:12.317: INFO: Pod "downwardapi-volume-c5f628ff-60b3-461e-be35-d7ba7b7a866a" satisfied condition "success or failure"
Dec  3 14:59:12.406: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-c5f628ff-60b3-461e-be35-d7ba7b7a866a container client-container: <nil>
STEP: delete the pod
Dec  3 14:59:12.597: INFO: Waiting for pod downwardapi-volume-c5f628ff-60b3-461e-be35-d7ba7b7a866a to disappear
Dec  3 14:59:12.687: INFO: Pod downwardapi-volume-c5f628ff-60b3-461e-be35-d7ba7b7a866a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:59:12.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4287" for this suite.
Dec  3 14:59:19.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:59:22.545: INFO: namespace projected-4287 deletion completed in 9.767797897s
•SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:59:22.545: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 14:59:23.278: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff73b09e-2246-461c-909b-d1fb1f627833" in namespace "projected-6544" to be "success or failure"
Dec  3 14:59:23.368: INFO: Pod "downwardapi-volume-ff73b09e-2246-461c-909b-d1fb1f627833": Phase="Pending", Reason="", readiness=false. Elapsed: 89.784708ms
Dec  3 14:59:25.458: INFO: Pod "downwardapi-volume-ff73b09e-2246-461c-909b-d1fb1f627833": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17992274s
STEP: Saw pod success
Dec  3 14:59:25.458: INFO: Pod "downwardapi-volume-ff73b09e-2246-461c-909b-d1fb1f627833" satisfied condition "success or failure"
Dec  3 14:59:25.548: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-ff73b09e-2246-461c-909b-d1fb1f627833 container client-container: <nil>
STEP: delete the pod
Dec  3 14:59:25.737: INFO: Waiting for pod downwardapi-volume-ff73b09e-2246-461c-909b-d1fb1f627833 to disappear
Dec  3 14:59:25.827: INFO: Pod downwardapi-volume-ff73b09e-2246-461c-909b-d1fb1f627833 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:59:25.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6544" for this suite.
Dec  3 14:59:32.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:59:35.715: INFO: namespace projected-6544 deletion completed in 9.797185851s
•SSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 14:59:35.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-488
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec  3 14:59:38.803: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-698554cd-ec5a-49ea-b5a2-29595f6d46c0,GenerateName:,Namespace:events-488,SelfLink:/api/v1/namespaces/events-488/pods/send-events-698554cd-ec5a-49ea-b5a2-29595f6d46c0,UID:3880ecee-dffc-4c7a-9444-9985a6cc283d,ResourceVersion:5709,Generation:0,CreationTimestamp:2019-12-03 14:59:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 352393583,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.48/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-l42xd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-l42xd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-l42xd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c70ab0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c70ad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:59:36 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:59:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:59:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:59:36 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:100.64.0.48,StartTime:2019-12-03 14:59:36 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-12-03 14:59:37 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://a739d50b2188810be5ec319ebbdda40d4b59cf4fbe80fb618b9882fa8a5c1ace}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec  3 14:59:40.894: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec  3 14:59:42.984: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 14:59:43.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-488" for this suite.
Dec  3 15:00:21.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:00:24.931: INFO: namespace events-488 deletion completed in 41.765190731s
•SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:00:24.931: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4053
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-a762c141-d33d-40aa-bedb-1e565f85e809
STEP: Creating secret with name secret-projected-all-test-volume-3fe03d71-0f9f-4acb-9160-ff026037d1d1
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec  3 15:00:25.842: INFO: Waiting up to 5m0s for pod "projected-volume-df5588d2-2e9f-40f1-8c3f-4e3194793e49" in namespace "projected-4053" to be "success or failure"
Dec  3 15:00:25.932: INFO: Pod "projected-volume-df5588d2-2e9f-40f1-8c3f-4e3194793e49": Phase="Pending", Reason="", readiness=false. Elapsed: 89.494297ms
Dec  3 15:00:28.022: INFO: Pod "projected-volume-df5588d2-2e9f-40f1-8c3f-4e3194793e49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179904623s
STEP: Saw pod success
Dec  3 15:00:28.023: INFO: Pod "projected-volume-df5588d2-2e9f-40f1-8c3f-4e3194793e49" satisfied condition "success or failure"
Dec  3 15:00:28.113: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod projected-volume-df5588d2-2e9f-40f1-8c3f-4e3194793e49 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec  3 15:00:28.303: INFO: Waiting for pod projected-volume-df5588d2-2e9f-40f1-8c3f-4e3194793e49 to disappear
Dec  3 15:00:28.393: INFO: Pod projected-volume-df5588d2-2e9f-40f1-8c3f-4e3194793e49 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:00:28.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4053" for this suite.
Dec  3 15:00:34.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:00:38.250: INFO: namespace projected-4053 deletion completed in 9.766288145s
•SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:00:38.250: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec  3 15:00:41.521: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:00:41.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4005" for this suite.
Dec  3 15:01:04.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:01:07.660: INFO: namespace replicaset-4005 deletion completed in 25.777997366s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:01:07.660: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4503
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  3 15:01:08.391: INFO: Waiting up to 5m0s for pod "downward-api-b95b7bc2-0491-4eab-b6a1-16a16bf08a74" in namespace "downward-api-4503" to be "success or failure"
Dec  3 15:01:08.481: INFO: Pod "downward-api-b95b7bc2-0491-4eab-b6a1-16a16bf08a74": Phase="Pending", Reason="", readiness=false. Elapsed: 89.545792ms
Dec  3 15:01:10.571: INFO: Pod "downward-api-b95b7bc2-0491-4eab-b6a1-16a16bf08a74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179492675s
STEP: Saw pod success
Dec  3 15:01:10.571: INFO: Pod "downward-api-b95b7bc2-0491-4eab-b6a1-16a16bf08a74" satisfied condition "success or failure"
Dec  3 15:01:10.660: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downward-api-b95b7bc2-0491-4eab-b6a1-16a16bf08a74 container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:01:10.851: INFO: Waiting for pod downward-api-b95b7bc2-0491-4eab-b6a1-16a16bf08a74 to disappear
Dec  3 15:01:10.941: INFO: Pod downward-api-b95b7bc2-0491-4eab-b6a1-16a16bf08a74 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:01:10.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4503" for this suite.
Dec  3 15:01:17.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:01:20.799: INFO: namespace downward-api-4503 deletion completed in 9.767305918s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:01:20.799: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2210
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-b8a46e5e-2888-4377-84fd-20305e24aef9
STEP: Creating configMap with name cm-test-opt-upd-8d5bacc7-3bb2-4e22-b39e-1e0bce901aab
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b8a46e5e-2888-4377-84fd-20305e24aef9
STEP: Updating configmap cm-test-opt-upd-8d5bacc7-3bb2-4e22-b39e-1e0bce901aab
STEP: Creating configMap with name cm-test-opt-create-ca39f0e9-1230-4c0e-b1b2-080bf0ed2c40
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:02:44.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2210" for this suite.
Dec  3 15:03:06.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:03:10.534: INFO: namespace configmap-2210 deletion completed in 25.813022236s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:03:10.534: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Dec  3 15:03:11.172: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config cluster-info'
Dec  3 15:03:11.601: INFO: stderr: ""
Dec  3 15:03:11.601: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.tm2em-9re.it.internal.staging.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.tm2em-9re.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.tm2em-9re.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:03:11.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2672" for this suite.
Dec  3 15:03:17.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:03:21.460: INFO: namespace kubectl-2672 deletion completed in 9.768092832s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:03:21.460: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4578
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:03:22.549: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec  3 15:03:22.728: INFO: Number of nodes with available pods: 0
Dec  3 15:03:22.728: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec  3 15:03:23.092: INFO: Number of nodes with available pods: 0
Dec  3 15:03:23.092: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:03:24.182: INFO: Number of nodes with available pods: 0
Dec  3 15:03:24.182: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:03:25.182: INFO: Number of nodes with available pods: 1
Dec  3 15:03:25.182: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec  3 15:03:25.542: INFO: Number of nodes with available pods: 0
Dec  3 15:03:25.542: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec  3 15:03:25.725: INFO: Number of nodes with available pods: 0
Dec  3 15:03:25.725: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:03:26.815: INFO: Number of nodes with available pods: 0
Dec  3 15:03:26.815: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:03:27.815: INFO: Number of nodes with available pods: 0
Dec  3 15:03:27.815: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:03:28.815: INFO: Number of nodes with available pods: 0
Dec  3 15:03:28.815: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:03:29.815: INFO: Number of nodes with available pods: 0
Dec  3 15:03:29.815: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:03:30.815: INFO: Number of nodes with available pods: 0
Dec  3 15:03:30.815: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:03:31.815: INFO: Number of nodes with available pods: 0
Dec  3 15:03:31.815: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:03:32.815: INFO: Number of nodes with available pods: 0
Dec  3 15:03:32.815: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:03:33.815: INFO: Number of nodes with available pods: 0
Dec  3 15:03:33.815: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:03:34.816: INFO: Number of nodes with available pods: 0
Dec  3 15:03:34.816: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:03:35.815: INFO: Number of nodes with available pods: 1
Dec  3 15:03:35.815: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4578, will wait for the garbage collector to delete the pods
Dec  3 15:03:36.276: INFO: Deleting DaemonSet.extensions daemon-set took: 91.333745ms
Dec  3 15:03:36.376: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.269996ms
Dec  3 15:03:43.466: INFO: Number of nodes with available pods: 0
Dec  3 15:03:43.466: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 15:03:43.558: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4578/daemonsets","resourceVersion":"6377"},"items":null}

Dec  3 15:03:43.647: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4578/pods","resourceVersion":"6378"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:03:44.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4578" for this suite.
Dec  3 15:03:50.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:03:53.868: INFO: namespace daemonsets-4578 deletion completed in 9.770459455s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:03:53.869: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5960
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Dec  3 15:03:54.613: INFO: Waiting up to 5m0s for pod "var-expansion-836eadd6-6418-4191-b342-0f5dc07c2ff7" in namespace "var-expansion-5960" to be "success or failure"
Dec  3 15:03:54.703: INFO: Pod "var-expansion-836eadd6-6418-4191-b342-0f5dc07c2ff7": Phase="Pending", Reason="", readiness=false. Elapsed: 90.343867ms
Dec  3 15:03:56.793: INFO: Pod "var-expansion-836eadd6-6418-4191-b342-0f5dc07c2ff7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180821195s
STEP: Saw pod success
Dec  3 15:03:56.793: INFO: Pod "var-expansion-836eadd6-6418-4191-b342-0f5dc07c2ff7" satisfied condition "success or failure"
Dec  3 15:03:56.883: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod var-expansion-836eadd6-6418-4191-b342-0f5dc07c2ff7 container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:03:57.074: INFO: Waiting for pod var-expansion-836eadd6-6418-4191-b342-0f5dc07c2ff7 to disappear
Dec  3 15:03:57.164: INFO: Pod var-expansion-836eadd6-6418-4191-b342-0f5dc07c2ff7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:03:57.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5960" for this suite.
Dec  3 15:04:03.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:04:07.021: INFO: namespace var-expansion-5960 deletion completed in 9.767036357s
•S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:04:07.021: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-5260
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5260 to expose endpoints map[]
Dec  3 15:04:07.844: INFO: successfully validated that service multi-endpoint-test in namespace services-5260 exposes endpoints map[] (89.419032ms elapsed)
STEP: Creating pod pod1 in namespace services-5260
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5260 to expose endpoints map[pod1:[100]]
Dec  3 15:04:10.474: INFO: successfully validated that service multi-endpoint-test in namespace services-5260 exposes endpoints map[pod1:[100]] (2.537876056s elapsed)
STEP: Creating pod pod2 in namespace services-5260
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5260 to expose endpoints map[pod1:[100] pod2:[101]]
Dec  3 15:04:13.373: INFO: successfully validated that service multi-endpoint-test in namespace services-5260 exposes endpoints map[pod1:[100] pod2:[101]] (2.808073755s elapsed)
STEP: Deleting pod pod1 in namespace services-5260
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5260 to expose endpoints map[pod2:[101]]
Dec  3 15:04:13.644: INFO: successfully validated that service multi-endpoint-test in namespace services-5260 exposes endpoints map[pod2:[101]] (179.526103ms elapsed)
STEP: Deleting pod pod2 in namespace services-5260
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5260 to expose endpoints map[]
Dec  3 15:04:13.824: INFO: successfully validated that service multi-endpoint-test in namespace services-5260 exposes endpoints map[] (89.350215ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:04:13.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5260" for this suite.
Dec  3 15:04:20.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:04:23.785: INFO: namespace services-5260 deletion completed in 9.775261294s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:04:23.785: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9027
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Dec  3 15:04:24.516: INFO: Waiting up to 5m0s for pod "client-containers-c96cfaa6-a4af-4c75-8056-b98713cca6f7" in namespace "containers-9027" to be "success or failure"
Dec  3 15:04:24.606: INFO: Pod "client-containers-c96cfaa6-a4af-4c75-8056-b98713cca6f7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.728629ms
Dec  3 15:04:26.696: INFO: Pod "client-containers-c96cfaa6-a4af-4c75-8056-b98713cca6f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179719703s
STEP: Saw pod success
Dec  3 15:04:26.696: INFO: Pod "client-containers-c96cfaa6-a4af-4c75-8056-b98713cca6f7" satisfied condition "success or failure"
Dec  3 15:04:26.786: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod client-containers-c96cfaa6-a4af-4c75-8056-b98713cca6f7 container test-container: <nil>
STEP: delete the pod
Dec  3 15:04:26.978: INFO: Waiting for pod client-containers-c96cfaa6-a4af-4c75-8056-b98713cca6f7 to disappear
Dec  3 15:04:27.068: INFO: Pod client-containers-c96cfaa6-a4af-4c75-8056-b98713cca6f7 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:04:27.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9027" for this suite.
Dec  3 15:04:33.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:04:36.925: INFO: namespace containers-9027 deletion completed in 9.765647177s
•
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:04:36.925: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-856
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:04:38.015: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec  3 15:04:38.288: INFO: Number of nodes with available pods: 0
Dec  3 15:04:38.288: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:04:39.468: INFO: Number of nodes with available pods: 1
Dec  3 15:04:39.468: INFO: Node ip-10-250-9-228.ec2.internal is running more than one daemon pod
Dec  3 15:04:40.468: INFO: Number of nodes with available pods: 2
Dec  3 15:04:40.468: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec  3 15:04:41.101: INFO: Wrong image for pod: daemon-set-l54nq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:41.101: INFO: Wrong image for pod: daemon-set-m9xsz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:42.281: INFO: Wrong image for pod: daemon-set-l54nq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:42.281: INFO: Wrong image for pod: daemon-set-m9xsz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:43.281: INFO: Wrong image for pod: daemon-set-l54nq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:43.281: INFO: Pod daemon-set-l54nq is not available
Dec  3 15:04:43.281: INFO: Wrong image for pod: daemon-set-m9xsz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:44.281: INFO: Wrong image for pod: daemon-set-l54nq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:44.281: INFO: Pod daemon-set-l54nq is not available
Dec  3 15:04:44.282: INFO: Wrong image for pod: daemon-set-m9xsz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:45.282: INFO: Wrong image for pod: daemon-set-l54nq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:45.282: INFO: Pod daemon-set-l54nq is not available
Dec  3 15:04:45.282: INFO: Wrong image for pod: daemon-set-m9xsz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:46.282: INFO: Wrong image for pod: daemon-set-l54nq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:46.282: INFO: Pod daemon-set-l54nq is not available
Dec  3 15:04:46.282: INFO: Wrong image for pod: daemon-set-m9xsz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:47.283: INFO: Wrong image for pod: daemon-set-l54nq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:47.283: INFO: Pod daemon-set-l54nq is not available
Dec  3 15:04:47.283: INFO: Wrong image for pod: daemon-set-m9xsz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:48.282: INFO: Wrong image for pod: daemon-set-m9xsz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:48.282: INFO: Pod daemon-set-sjpm8 is not available
Dec  3 15:04:49.281: INFO: Wrong image for pod: daemon-set-m9xsz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:49.281: INFO: Pod daemon-set-sjpm8 is not available
Dec  3 15:04:50.282: INFO: Wrong image for pod: daemon-set-m9xsz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:51.281: INFO: Wrong image for pod: daemon-set-m9xsz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  3 15:04:51.282: INFO: Pod daemon-set-m9xsz is not available
Dec  3 15:04:52.282: INFO: Pod daemon-set-ct4d7 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec  3 15:04:52.552: INFO: Number of nodes with available pods: 2
Dec  3 15:04:52.552: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-856, will wait for the garbage collector to delete the pods
Dec  3 15:04:53.284: INFO: Deleting DaemonSet.extensions daemon-set took: 91.587202ms
Dec  3 15:04:53.684: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.432233ms
Dec  3 15:05:03.374: INFO: Number of nodes with available pods: 0
Dec  3 15:05:03.374: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 15:05:03.463: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-856/daemonsets","resourceVersion":"6683"},"items":null}

Dec  3 15:05:03.553: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-856/pods","resourceVersion":"6683"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:05:03.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-856" for this suite.
Dec  3 15:05:10.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:13.680: INFO: namespace daemonsets-856 deletion completed in 9.766585752s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:05:13.681: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:05:14.411: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79ea3ace-4c0d-408d-9170-ce55fef799cf" in namespace "downward-api-4753" to be "success or failure"
Dec  3 15:05:14.501: INFO: Pod "downwardapi-volume-79ea3ace-4c0d-408d-9170-ce55fef799cf": Phase="Pending", Reason="", readiness=false. Elapsed: 89.875651ms
Dec  3 15:05:16.592: INFO: Pod "downwardapi-volume-79ea3ace-4c0d-408d-9170-ce55fef799cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180176551s
STEP: Saw pod success
Dec  3 15:05:16.592: INFO: Pod "downwardapi-volume-79ea3ace-4c0d-408d-9170-ce55fef799cf" satisfied condition "success or failure"
Dec  3 15:05:16.682: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-79ea3ace-4c0d-408d-9170-ce55fef799cf container client-container: <nil>
STEP: delete the pod
Dec  3 15:05:16.872: INFO: Waiting for pod downwardapi-volume-79ea3ace-4c0d-408d-9170-ce55fef799cf to disappear
Dec  3 15:05:16.962: INFO: Pod downwardapi-volume-79ea3ace-4c0d-408d-9170-ce55fef799cf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:05:16.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4753" for this suite.
Dec  3 15:05:23.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:26.818: INFO: namespace downward-api-4753 deletion completed in 9.765615088s
•SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:05:26.819: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec  3 15:05:27.457: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:05:30.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6598" for this suite.
Dec  3 15:05:37.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:40.665: INFO: namespace init-container-6598 deletion completed in 9.769506349s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:05:40.665: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5793
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-5793/configmap-test-d0d39cb3-1062-45d5-86c7-1acccbbf9b39
STEP: Creating a pod to test consume configMaps
Dec  3 15:05:41.486: INFO: Waiting up to 5m0s for pod "pod-configmaps-f812973c-0bdd-404a-a7b1-d52d00332dc6" in namespace "configmap-5793" to be "success or failure"
Dec  3 15:05:41.576: INFO: Pod "pod-configmaps-f812973c-0bdd-404a-a7b1-d52d00332dc6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.554924ms
Dec  3 15:05:43.666: INFO: Pod "pod-configmaps-f812973c-0bdd-404a-a7b1-d52d00332dc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179840267s
STEP: Saw pod success
Dec  3 15:05:43.666: INFO: Pod "pod-configmaps-f812973c-0bdd-404a-a7b1-d52d00332dc6" satisfied condition "success or failure"
Dec  3 15:05:43.757: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-configmaps-f812973c-0bdd-404a-a7b1-d52d00332dc6 container env-test: <nil>
STEP: delete the pod
Dec  3 15:05:43.945: INFO: Waiting for pod pod-configmaps-f812973c-0bdd-404a-a7b1-d52d00332dc6 to disappear
Dec  3 15:05:44.035: INFO: Pod pod-configmaps-f812973c-0bdd-404a-a7b1-d52d00332dc6 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:05:44.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5793" for this suite.
Dec  3 15:05:50.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:53.896: INFO: namespace configmap-5793 deletion completed in 9.770200475s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:05:53.896: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4274
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec  3 15:05:55.165: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4274,SelfLink:/api/v1/namespaces/watch-4274/configmaps/e2e-watch-test-resource-version,UID:a0034192-f7f2-4221-a05a-e59ce8ef1595,ResourceVersion:6858,Generation:0,CreationTimestamp:2019-12-03 15:05:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 15:05:55.165: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4274,SelfLink:/api/v1/namespaces/watch-4274/configmaps/e2e-watch-test-resource-version,UID:a0034192-f7f2-4221-a05a-e59ce8ef1595,ResourceVersion:6859,Generation:0,CreationTimestamp:2019-12-03 15:05:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:05:55.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4274" for this suite.
Dec  3 15:06:01.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:06:05.215: INFO: namespace watch-4274 deletion completed in 9.959832678s
•
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:06:05.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6082
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-de04d2b8-5e40-4c2e-a663-d2c9dbaac3e4
STEP: Creating a pod to test consume configMaps
Dec  3 15:06:06.036: INFO: Waiting up to 5m0s for pod "pod-configmaps-72e30526-f330-42db-86b1-3a92f3edb21b" in namespace "configmap-6082" to be "success or failure"
Dec  3 15:06:06.126: INFO: Pod "pod-configmaps-72e30526-f330-42db-86b1-3a92f3edb21b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.891784ms
Dec  3 15:06:08.217: INFO: Pod "pod-configmaps-72e30526-f330-42db-86b1-3a92f3edb21b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180064626s
STEP: Saw pod success
Dec  3 15:06:08.217: INFO: Pod "pod-configmaps-72e30526-f330-42db-86b1-3a92f3edb21b" satisfied condition "success or failure"
Dec  3 15:06:08.306: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-configmaps-72e30526-f330-42db-86b1-3a92f3edb21b container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:06:08.496: INFO: Waiting for pod pod-configmaps-72e30526-f330-42db-86b1-3a92f3edb21b to disappear
Dec  3 15:06:08.586: INFO: Pod pod-configmaps-72e30526-f330-42db-86b1-3a92f3edb21b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:06:08.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6082" for this suite.
Dec  3 15:06:14.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:06:18.445: INFO: namespace configmap-6082 deletion completed in 9.768363738s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:06:18.445: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: Gathering metrics
Dec  3 15:06:19.719: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1203 15:06:19.719563    5065 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:06:19.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-990" for this suite.
Dec  3 15:06:26.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:06:29.576: INFO: namespace gc-990 deletion completed in 9.766086722s
•SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:06:29.577: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1016
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec  3 15:06:30.215: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 15:06:30.396: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 15:06:30.486: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-31-164.ec2.internal before test
Dec  3 15:06:30.585: INFO: calico-node-w5f9r from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.585: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:06:30.585: INFO: node-exporter-68n8d from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.585: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:06:30.585: INFO: node-problem-detector-hnrtk from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.585: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:06:30.585: INFO: calico-typha-deploy-5547c4cdc6-zvk82 from kube-system started at 2019-12-03 14:32:23 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.585: INFO: 	Container calico-typha ready: true, restart count 0
Dec  3 15:06:30.585: INFO: blackbox-exporter-c87bdd467-vscd2 from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.585: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 15:06:30.585: INFO: kube-proxy-8xnnr from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.585: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:06:30.585: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-9-228.ec2.internal before test
Dec  3 15:06:30.688: INFO: calico-node-hvhtd from kube-system started at 2019-12-03 14:30:18 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:06:30.688: INFO: node-exporter-kklbs from kube-system started at 2019-12-03 14:30:18 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:06:30.688: INFO: addons-nginx-ingress-controller-8468678b64-x6f59 from kube-system started at 2019-12-03 14:30:32 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 15:06:30.688: INFO: kube-proxy-8j96c from kube-system started at 2019-12-03 14:30:18 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:06:30.688: INFO: metrics-server-bf696d85c-7v9h8 from kube-system started at 2019-12-03 14:30:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 15:06:30.688: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-nwxw7 from kube-system started at 2019-12-03 14:30:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 15:06:30.688: INFO: calico-typha-horizontal-autoscaler-554dfbfdd7-r9vld from kube-system started at 2019-12-03 14:30:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 15:06:30.688: INFO: coredns-858b686868-r84d9 from kube-system started at 2019-12-03 14:30:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:06:30.688: INFO: calico-typha-vertical-autoscaler-656557779f-zrt7j from kube-system started at 2019-12-03 14:30:32 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container autoscaler ready: true, restart count 3
Dec  3 15:06:30.688: INFO: addons-kubernetes-dashboard-5c8d9945bc-5mvxv from kube-system started at 2019-12-03 14:30:32 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 15:06:30.688: INFO: node-problem-detector-bpps6 from kube-system started at 2019-12-03 14:30:18 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:06:30.688: INFO: vpn-shoot-859fb5c977-s5ldm from kube-system started at 2019-12-03 14:30:30 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 15:06:30.688: INFO: calico-kube-controllers-5d785bc598-746p6 from kube-system started at 2019-12-03 14:30:30 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  3 15:06:30.688: INFO: coredns-858b686868-lbh4p from kube-system started at 2019-12-03 14:30:32 +0000 UTC (1 container statuses recorded)
Dec  3 15:06:30.688: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e7567be5-d546-4e59-9b60-f32f34f2b052 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e7567be5-d546-4e59-9b60-f32f34f2b052 off the node ip-10-250-31-164.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e7567be5-d546-4e59-9b60-f32f34f2b052
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:06:35.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1016" for this suite.
Dec  3 15:07:04.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:07:07.808: INFO: namespace sched-pred-1016 deletion completed in 31.763002512s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:07:07.808: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  3 15:07:09.169: INFO: Number of nodes with available pods: 0
Dec  3 15:07:09.169: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:07:10.350: INFO: Number of nodes with available pods: 1
Dec  3 15:07:10.350: INFO: Node ip-10-250-9-228.ec2.internal is running more than one daemon pod
Dec  3 15:07:11.352: INFO: Number of nodes with available pods: 2
Dec  3 15:07:11.352: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec  3 15:07:11.803: INFO: Number of nodes with available pods: 1
Dec  3 15:07:11.803: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:07:12.983: INFO: Number of nodes with available pods: 1
Dec  3 15:07:12.983: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:07:13.983: INFO: Number of nodes with available pods: 1
Dec  3 15:07:13.984: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:07:14.983: INFO: Number of nodes with available pods: 2
Dec  3 15:07:14.983: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5222, will wait for the garbage collector to delete the pods
Dec  3 15:07:15.354: INFO: Deleting DaemonSet.extensions daemon-set took: 91.601934ms
Dec  3 15:07:15.455: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.377725ms
Dec  3 15:07:28.046: INFO: Number of nodes with available pods: 0
Dec  3 15:07:28.046: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 15:07:28.135: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5222/daemonsets","resourceVersion":"7177"},"items":null}

Dec  3 15:07:28.225: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5222/pods","resourceVersion":"7177"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:07:28.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5222" for this suite.
Dec  3 15:07:34.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:07:38.345: INFO: namespace daemonsets-5222 deletion completed in 9.760233528s
•SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:07:38.345: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6380
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-cfwp8 in namespace proxy-6380
I1203 15:07:39.167975    5065 runners.go:180] Created replication controller with name: proxy-service-cfwp8, namespace: proxy-6380, replica count: 1
I1203 15:07:40.268571    5065 runners.go:180] proxy-service-cfwp8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:07:41.268787    5065 runners.go:180] proxy-service-cfwp8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:07:42.269045    5065 runners.go:180] proxy-service-cfwp8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:07:43.269380    5065 runners.go:180] proxy-service-cfwp8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:07:44.269672    5065 runners.go:180] proxy-service-cfwp8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:07:45.269924    5065 runners.go:180] proxy-service-cfwp8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:07:46.270149    5065 runners.go:180] proxy-service-cfwp8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:07:47.270439    5065 runners.go:180] proxy-service-cfwp8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:07:48.270799    5065 runners.go:180] proxy-service-cfwp8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 15:07:49.271069    5065 runners.go:180] proxy-service-cfwp8 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  3 15:07:49.361: INFO: setup took 10.377319251s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec  3 15:07:49.458: INFO: (0) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 96.481963ms)
Dec  3 15:07:49.458: INFO: (0) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 97.349899ms)
Dec  3 15:07:49.458: INFO: (0) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 97.1763ms)
Dec  3 15:07:49.458: INFO: (0) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 97.369265ms)
Dec  3 15:07:49.458: INFO: (0) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 97.26188ms)
Dec  3 15:07:49.458: INFO: (0) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 97.43223ms)
Dec  3 15:07:49.464: INFO: (0) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 102.588677ms)
Dec  3 15:07:49.464: INFO: (0) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 102.523331ms)
Dec  3 15:07:49.514: INFO: (0) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 153.318653ms)
Dec  3 15:07:49.514: INFO: (0) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 153.341638ms)
Dec  3 15:07:49.601: INFO: (0) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 239.432603ms)
Dec  3 15:07:49.601: INFO: (0) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 239.408016ms)
Dec  3 15:07:49.601: INFO: (0) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 239.453334ms)
Dec  3 15:07:49.603: INFO: (0) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 241.861183ms)
Dec  3 15:07:49.603: INFO: (0) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 241.966332ms)
Dec  3 15:07:49.615: INFO: (0) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 253.70186ms)
Dec  3 15:07:49.708: INFO: (1) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 93.015801ms)
Dec  3 15:07:49.708: INFO: (1) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.979705ms)
Dec  3 15:07:49.708: INFO: (1) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 93.24056ms)
Dec  3 15:07:49.708: INFO: (1) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 93.064075ms)
Dec  3 15:07:49.708: INFO: (1) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 93.081341ms)
Dec  3 15:07:49.708: INFO: (1) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 92.995899ms)
Dec  3 15:07:49.708: INFO: (1) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 93.043529ms)
Dec  3 15:07:49.709: INFO: (1) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 93.908211ms)
Dec  3 15:07:49.709: INFO: (1) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 94.247071ms)
Dec  3 15:07:49.709: INFO: (1) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 93.950228ms)
Dec  3 15:07:49.709: INFO: (1) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 94.054847ms)
Dec  3 15:07:49.709: INFO: (1) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 94.02299ms)
Dec  3 15:07:49.710: INFO: (1) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 94.966564ms)
Dec  3 15:07:49.712: INFO: (1) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 96.543266ms)
Dec  3 15:07:49.712: INFO: (1) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 96.50619ms)
Dec  3 15:07:49.713: INFO: (1) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 97.951715ms)
Dec  3 15:07:49.805: INFO: (2) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 92.035077ms)
Dec  3 15:07:49.805: INFO: (2) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.118308ms)
Dec  3 15:07:49.806: INFO: (2) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 92.210198ms)
Dec  3 15:07:49.806: INFO: (2) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.354754ms)
Dec  3 15:07:49.806: INFO: (2) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.388513ms)
Dec  3 15:07:49.806: INFO: (2) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 92.439864ms)
Dec  3 15:07:49.806: INFO: (2) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 92.465951ms)
Dec  3 15:07:49.806: INFO: (2) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 92.489126ms)
Dec  3 15:07:49.806: INFO: (2) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 92.597199ms)
Dec  3 15:07:49.806: INFO: (2) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 92.509043ms)
Dec  3 15:07:49.806: INFO: (2) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.441047ms)
Dec  3 15:07:49.806: INFO: (2) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 92.611956ms)
Dec  3 15:07:49.808: INFO: (2) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 94.900286ms)
Dec  3 15:07:49.808: INFO: (2) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 94.991225ms)
Dec  3 15:07:49.808: INFO: (2) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 94.978971ms)
Dec  3 15:07:49.808: INFO: (2) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 94.908436ms)
Dec  3 15:07:49.901: INFO: (3) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.878695ms)
Dec  3 15:07:49.901: INFO: (3) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 92.74951ms)
Dec  3 15:07:49.901: INFO: (3) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 92.800369ms)
Dec  3 15:07:49.901: INFO: (3) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.76147ms)
Dec  3 15:07:49.901: INFO: (3) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 92.705344ms)
Dec  3 15:07:49.901: INFO: (3) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.754014ms)
Dec  3 15:07:49.901: INFO: (3) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 92.731236ms)
Dec  3 15:07:49.901: INFO: (3) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 92.857571ms)
Dec  3 15:07:49.901: INFO: (3) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 92.760478ms)
Dec  3 15:07:49.901: INFO: (3) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.840035ms)
Dec  3 15:07:49.903: INFO: (3) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 94.064038ms)
Dec  3 15:07:49.903: INFO: (3) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 94.03731ms)
Dec  3 15:07:49.904: INFO: (3) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 95.159466ms)
Dec  3 15:07:49.905: INFO: (3) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 96.675269ms)
Dec  3 15:07:49.905: INFO: (3) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 96.725844ms)
Dec  3 15:07:49.905: INFO: (3) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 96.685087ms)
Dec  3 15:07:49.998: INFO: (4) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 92.805718ms)
Dec  3 15:07:49.998: INFO: (4) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 92.835209ms)
Dec  3 15:07:49.998: INFO: (4) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 92.853989ms)
Dec  3 15:07:49.998: INFO: (4) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 92.951067ms)
Dec  3 15:07:49.998: INFO: (4) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.79312ms)
Dec  3 15:07:49.998: INFO: (4) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.895499ms)
Dec  3 15:07:49.998: INFO: (4) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 93.026763ms)
Dec  3 15:07:49.998: INFO: (4) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 92.916375ms)
Dec  3 15:07:49.998: INFO: (4) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 92.898021ms)
Dec  3 15:07:49.998: INFO: (4) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 92.93298ms)
Dec  3 15:07:50.000: INFO: (4) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 94.202398ms)
Dec  3 15:07:50.000: INFO: (4) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 94.190975ms)
Dec  3 15:07:50.000: INFO: (4) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 95.005904ms)
Dec  3 15:07:50.001: INFO: (4) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 95.688198ms)
Dec  3 15:07:50.001: INFO: (4) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 95.692099ms)
Dec  3 15:07:50.002: INFO: (4) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 96.758762ms)
Dec  3 15:07:50.095: INFO: (5) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 92.502956ms)
Dec  3 15:07:50.095: INFO: (5) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.29907ms)
Dec  3 15:07:50.095: INFO: (5) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 92.287884ms)
Dec  3 15:07:50.095: INFO: (5) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 92.328012ms)
Dec  3 15:07:50.095: INFO: (5) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 92.506057ms)
Dec  3 15:07:50.095: INFO: (5) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 92.513895ms)
Dec  3 15:07:50.095: INFO: (5) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 92.394664ms)
Dec  3 15:07:50.095: INFO: (5) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.485013ms)
Dec  3 15:07:50.096: INFO: (5) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 93.582605ms)
Dec  3 15:07:50.096: INFO: (5) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 93.764937ms)
Dec  3 15:07:50.096: INFO: (5) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 93.611384ms)
Dec  3 15:07:50.096: INFO: (5) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 93.628311ms)
Dec  3 15:07:50.097: INFO: (5) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 94.195221ms)
Dec  3 15:07:50.097: INFO: (5) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 94.919765ms)
Dec  3 15:07:50.099: INFO: (5) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 96.115493ms)
Dec  3 15:07:50.100: INFO: (5) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 97.595002ms)
Dec  3 15:07:50.193: INFO: (6) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.71591ms)
Dec  3 15:07:50.193: INFO: (6) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.821682ms)
Dec  3 15:07:50.193: INFO: (6) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 92.795049ms)
Dec  3 15:07:50.193: INFO: (6) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 92.7845ms)
Dec  3 15:07:50.193: INFO: (6) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.760891ms)
Dec  3 15:07:50.193: INFO: (6) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 92.720691ms)
Dec  3 15:07:50.193: INFO: (6) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 92.800086ms)
Dec  3 15:07:50.193: INFO: (6) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 92.407847ms)
Dec  3 15:07:50.193: INFO: (6) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.791726ms)
Dec  3 15:07:50.194: INFO: (6) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 94.024627ms)
Dec  3 15:07:50.194: INFO: (6) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 93.893166ms)
Dec  3 15:07:50.194: INFO: (6) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 94.073596ms)
Dec  3 15:07:50.195: INFO: (6) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 95.176033ms)
Dec  3 15:07:50.197: INFO: (6) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 96.662856ms)
Dec  3 15:07:50.197: INFO: (6) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 96.667248ms)
Dec  3 15:07:50.197: INFO: (6) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 96.644923ms)
Dec  3 15:07:50.292: INFO: (7) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 94.313945ms)
Dec  3 15:07:50.292: INFO: (7) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 94.272921ms)
Dec  3 15:07:50.292: INFO: (7) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 95.125109ms)
Dec  3 15:07:50.292: INFO: (7) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 95.013047ms)
Dec  3 15:07:50.292: INFO: (7) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 94.992943ms)
Dec  3 15:07:50.292: INFO: (7) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 95.03706ms)
Dec  3 15:07:50.293: INFO: (7) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 95.440927ms)
Dec  3 15:07:50.293: INFO: (7) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 95.497125ms)
Dec  3 15:07:50.294: INFO: (7) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 96.706082ms)
Dec  3 15:07:50.294: INFO: (7) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 96.896064ms)
Dec  3 15:07:50.294: INFO: (7) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 96.821994ms)
Dec  3 15:07:50.294: INFO: (7) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 96.805651ms)
Dec  3 15:07:50.295: INFO: (7) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 98.058469ms)
Dec  3 15:07:50.295: INFO: (7) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 98.163476ms)
Dec  3 15:07:50.295: INFO: (7) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 98.20909ms)
Dec  3 15:07:50.297: INFO: (7) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 99.456541ms)
Dec  3 15:07:50.389: INFO: (8) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 92.26498ms)
Dec  3 15:07:50.390: INFO: (8) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.899383ms)
Dec  3 15:07:50.390: INFO: (8) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.946768ms)
Dec  3 15:07:50.390: INFO: (8) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 93.015672ms)
Dec  3 15:07:50.390: INFO: (8) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.77531ms)
Dec  3 15:07:50.390: INFO: (8) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 92.772871ms)
Dec  3 15:07:50.390: INFO: (8) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 92.857438ms)
Dec  3 15:07:50.390: INFO: (8) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 92.899696ms)
Dec  3 15:07:50.391: INFO: (8) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 94.185628ms)
Dec  3 15:07:50.391: INFO: (8) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 94.017556ms)
Dec  3 15:07:50.391: INFO: (8) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 94.019668ms)
Dec  3 15:07:50.391: INFO: (8) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 94.059155ms)
Dec  3 15:07:50.392: INFO: (8) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 95.058376ms)
Dec  3 15:07:50.394: INFO: (8) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 96.657655ms)
Dec  3 15:07:50.394: INFO: (8) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 96.611396ms)
Dec  3 15:07:50.394: INFO: (8) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 96.63519ms)
Dec  3 15:07:50.487: INFO: (9) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.453113ms)
Dec  3 15:07:50.487: INFO: (9) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 92.50704ms)
Dec  3 15:07:50.487: INFO: (9) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 92.544255ms)
Dec  3 15:07:50.487: INFO: (9) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 92.683844ms)
Dec  3 15:07:50.487: INFO: (9) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 92.607545ms)
Dec  3 15:07:50.487: INFO: (9) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.692983ms)
Dec  3 15:07:50.487: INFO: (9) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 92.552235ms)
Dec  3 15:07:50.487: INFO: (9) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 92.563992ms)
Dec  3 15:07:50.488: INFO: (9) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 94.001249ms)
Dec  3 15:07:50.488: INFO: (9) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 93.807341ms)
Dec  3 15:07:50.488: INFO: (9) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 94.040305ms)
Dec  3 15:07:50.488: INFO: (9) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 93.97599ms)
Dec  3 15:07:50.489: INFO: (9) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 95.097198ms)
Dec  3 15:07:50.489: INFO: (9) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 95.43901ms)
Dec  3 15:07:50.490: INFO: (9) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 96.472571ms)
Dec  3 15:07:50.492: INFO: (9) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 97.921297ms)
Dec  3 15:07:50.585: INFO: (10) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 92.886498ms)
Dec  3 15:07:50.585: INFO: (10) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 92.77244ms)
Dec  3 15:07:50.585: INFO: (10) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 93.11026ms)
Dec  3 15:07:50.585: INFO: (10) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 92.91548ms)
Dec  3 15:07:50.585: INFO: (10) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.952968ms)
Dec  3 15:07:50.585: INFO: (10) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 92.955672ms)
Dec  3 15:07:50.585: INFO: (10) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 93.019531ms)
Dec  3 15:07:50.585: INFO: (10) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 93.003258ms)
Dec  3 15:07:50.586: INFO: (10) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 94.060201ms)
Dec  3 15:07:50.586: INFO: (10) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 94.269427ms)
Dec  3 15:07:50.586: INFO: (10) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 94.142596ms)
Dec  3 15:07:50.586: INFO: (10) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 94.224791ms)
Dec  3 15:07:50.587: INFO: (10) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 95.146401ms)
Dec  3 15:07:50.589: INFO: (10) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 96.620592ms)
Dec  3 15:07:50.589: INFO: (10) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 96.462118ms)
Dec  3 15:07:50.590: INFO: (10) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 98.024777ms)
Dec  3 15:07:50.683: INFO: (11) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.75854ms)
Dec  3 15:07:50.683: INFO: (11) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.862008ms)
Dec  3 15:07:50.683: INFO: (11) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 92.955717ms)
Dec  3 15:07:50.683: INFO: (11) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 92.866047ms)
Dec  3 15:07:50.683: INFO: (11) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 92.857944ms)
Dec  3 15:07:50.683: INFO: (11) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 92.808558ms)
Dec  3 15:07:50.683: INFO: (11) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 92.899107ms)
Dec  3 15:07:50.683: INFO: (11) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.78035ms)
Dec  3 15:07:50.683: INFO: (11) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.911682ms)
Dec  3 15:07:50.684: INFO: (11) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 93.898264ms)
Dec  3 15:07:50.684: INFO: (11) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 94.018567ms)
Dec  3 15:07:50.684: INFO: (11) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 94.076691ms)
Dec  3 15:07:50.686: INFO: (11) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 95.315824ms)
Dec  3 15:07:50.688: INFO: (11) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 97.096727ms)
Dec  3 15:07:50.688: INFO: (11) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 97.176051ms)
Dec  3 15:07:50.688: INFO: (11) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 97.184191ms)
Dec  3 15:07:50.781: INFO: (12) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.975876ms)
Dec  3 15:07:50.782: INFO: (12) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 94.191714ms)
Dec  3 15:07:50.782: INFO: (12) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 94.248877ms)
Dec  3 15:07:50.782: INFO: (12) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 94.250925ms)
Dec  3 15:07:50.782: INFO: (12) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 94.35436ms)
Dec  3 15:07:50.782: INFO: (12) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 94.217357ms)
Dec  3 15:07:50.782: INFO: (12) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 94.422727ms)
Dec  3 15:07:50.782: INFO: (12) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 94.285541ms)
Dec  3 15:07:50.782: INFO: (12) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 94.337732ms)
Dec  3 15:07:50.782: INFO: (12) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 94.32652ms)
Dec  3 15:07:50.782: INFO: (12) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 94.291509ms)
Dec  3 15:07:50.782: INFO: (12) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 94.236916ms)
Dec  3 15:07:50.784: INFO: (12) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 96.447412ms)
Dec  3 15:07:50.784: INFO: (12) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 96.400631ms)
Dec  3 15:07:50.784: INFO: (12) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 96.401629ms)
Dec  3 15:07:50.784: INFO: (12) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 96.405691ms)
Dec  3 15:07:50.877: INFO: (13) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 92.19558ms)
Dec  3 15:07:50.877: INFO: (13) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 92.609714ms)
Dec  3 15:07:50.877: INFO: (13) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.63502ms)
Dec  3 15:07:50.877: INFO: (13) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 92.787414ms)
Dec  3 15:07:50.877: INFO: (13) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.845078ms)
Dec  3 15:07:50.877: INFO: (13) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 92.731423ms)
Dec  3 15:07:50.878: INFO: (13) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 93.910801ms)
Dec  3 15:07:50.878: INFO: (13) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 94.020712ms)
Dec  3 15:07:50.878: INFO: (13) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 94.002432ms)
Dec  3 15:07:50.879: INFO: (13) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 94.056866ms)
Dec  3 15:07:50.880: INFO: (13) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 95.290008ms)
Dec  3 15:07:50.880: INFO: (13) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 95.405922ms)
Dec  3 15:07:50.880: INFO: (13) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 95.347684ms)
Dec  3 15:07:50.881: INFO: (13) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 96.623904ms)
Dec  3 15:07:50.883: INFO: (13) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 98.162716ms)
Dec  3 15:07:50.883: INFO: (13) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 98.376347ms)
Dec  3 15:07:50.975: INFO: (14) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 92.099821ms)
Dec  3 15:07:50.975: INFO: (14) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 91.86031ms)
Dec  3 15:07:50.975: INFO: (14) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 92.125682ms)
Dec  3 15:07:50.975: INFO: (14) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 91.88341ms)
Dec  3 15:07:50.975: INFO: (14) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 92.128666ms)
Dec  3 15:07:50.975: INFO: (14) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 91.749657ms)
Dec  3 15:07:50.975: INFO: (14) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.033237ms)
Dec  3 15:07:50.975: INFO: (14) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.147399ms)
Dec  3 15:07:50.976: INFO: (14) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 93.253833ms)
Dec  3 15:07:50.976: INFO: (14) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 93.490671ms)
Dec  3 15:07:50.976: INFO: (14) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 93.424306ms)
Dec  3 15:07:50.978: INFO: (14) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 94.499677ms)
Dec  3 15:07:50.978: INFO: (14) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 94.688738ms)
Dec  3 15:07:50.979: INFO: (14) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 95.954852ms)
Dec  3 15:07:50.979: INFO: (14) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 95.907976ms)
Dec  3 15:07:50.979: INFO: (14) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 95.753749ms)
Dec  3 15:07:51.071: INFO: (15) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 92.190201ms)
Dec  3 15:07:51.072: INFO: (15) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 93.130101ms)
Dec  3 15:07:51.072: INFO: (15) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 93.118031ms)
Dec  3 15:07:51.072: INFO: (15) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 93.143586ms)
Dec  3 15:07:51.072: INFO: (15) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 93.185493ms)
Dec  3 15:07:51.072: INFO: (15) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 93.20754ms)
Dec  3 15:07:51.072: INFO: (15) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 93.320168ms)
Dec  3 15:07:51.072: INFO: (15) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 93.187173ms)
Dec  3 15:07:51.074: INFO: (15) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 94.373291ms)
Dec  3 15:07:51.074: INFO: (15) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 94.500025ms)
Dec  3 15:07:51.074: INFO: (15) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 94.461513ms)
Dec  3 15:07:51.074: INFO: (15) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 94.457383ms)
Dec  3 15:07:51.075: INFO: (15) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 95.380504ms)
Dec  3 15:07:51.075: INFO: (15) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 95.837004ms)
Dec  3 15:07:51.076: INFO: (15) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 97.02139ms)
Dec  3 15:07:51.077: INFO: (15) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 98.260376ms)
Dec  3 15:07:51.171: INFO: (16) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 92.878607ms)
Dec  3 15:07:51.171: INFO: (16) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 93.026838ms)
Dec  3 15:07:51.171: INFO: (16) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 92.921935ms)
Dec  3 15:07:51.171: INFO: (16) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 92.888194ms)
Dec  3 15:07:51.171: INFO: (16) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.981291ms)
Dec  3 15:07:51.171: INFO: (16) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 93.007736ms)
Dec  3 15:07:51.171: INFO: (16) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 92.974562ms)
Dec  3 15:07:51.171: INFO: (16) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 92.998704ms)
Dec  3 15:07:51.172: INFO: (16) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 94.473222ms)
Dec  3 15:07:51.172: INFO: (16) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 94.410208ms)
Dec  3 15:07:51.172: INFO: (16) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 94.481194ms)
Dec  3 15:07:51.172: INFO: (16) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 94.66818ms)
Dec  3 15:07:51.173: INFO: (16) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 95.529985ms)
Dec  3 15:07:51.174: INFO: (16) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 96.790034ms)
Dec  3 15:07:51.174: INFO: (16) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 96.727347ms)
Dec  3 15:07:51.176: INFO: (16) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 98.106245ms)
Dec  3 15:07:51.272: INFO: (17) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 95.669367ms)
Dec  3 15:07:51.272: INFO: (17) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 95.721201ms)
Dec  3 15:07:51.272: INFO: (17) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 95.777984ms)
Dec  3 15:07:51.272: INFO: (17) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 95.685428ms)
Dec  3 15:07:51.272: INFO: (17) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 95.722705ms)
Dec  3 15:07:51.272: INFO: (17) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 95.632461ms)
Dec  3 15:07:51.272: INFO: (17) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 95.776786ms)
Dec  3 15:07:51.272: INFO: (17) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 95.817855ms)
Dec  3 15:07:51.272: INFO: (17) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 95.682097ms)
Dec  3 15:07:51.272: INFO: (17) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 95.724122ms)
Dec  3 15:07:51.272: INFO: (17) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 95.680344ms)
Dec  3 15:07:51.273: INFO: (17) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 97.053998ms)
Dec  3 15:07:51.274: INFO: (17) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 98.054647ms)
Dec  3 15:07:51.274: INFO: (17) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 97.96114ms)
Dec  3 15:07:51.274: INFO: (17) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 97.98581ms)
Dec  3 15:07:51.274: INFO: (17) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 98.00795ms)
Dec  3 15:07:51.366: INFO: (18) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.073868ms)
Dec  3 15:07:51.367: INFO: (18) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.962155ms)
Dec  3 15:07:51.367: INFO: (18) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 93.081914ms)
Dec  3 15:07:51.367: INFO: (18) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 93.04594ms)
Dec  3 15:07:51.367: INFO: (18) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 93.020156ms)
Dec  3 15:07:51.367: INFO: (18) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 93.070507ms)
Dec  3 15:07:51.368: INFO: (18) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 93.257932ms)
Dec  3 15:07:51.368: INFO: (18) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 93.342424ms)
Dec  3 15:07:51.369: INFO: (18) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 95.094485ms)
Dec  3 15:07:51.369: INFO: (18) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 95.048917ms)
Dec  3 15:07:51.369: INFO: (18) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 95.086232ms)
Dec  3 15:07:51.369: INFO: (18) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 95.176378ms)
Dec  3 15:07:51.370: INFO: (18) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 95.824156ms)
Dec  3 15:07:51.370: INFO: (18) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 95.710493ms)
Dec  3 15:07:51.372: INFO: (18) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 97.105367ms)
Dec  3 15:07:51.372: INFO: (18) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 97.129358ms)
Dec  3 15:07:51.465: INFO: (19) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 93.00165ms)
Dec  3 15:07:51.465: INFO: (19) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">... (200; 92.850471ms)
Dec  3 15:07:51.465: INFO: (19) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.977057ms)
Dec  3 15:07:51.465: INFO: (19) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:160/proxy/: foo (200; 92.902807ms)
Dec  3 15:07:51.465: INFO: (19) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr/proxy/rewriteme">test</a> (200; 92.989712ms)
Dec  3 15:07:51.465: INFO: (19) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:462/proxy/: tls qux (200; 92.999642ms)
Dec  3 15:07:51.465: INFO: (19) /api/v1/namespaces/proxy-6380/pods/http:proxy-service-cfwp8-rncjr:162/proxy/: bar (200; 92.98904ms)
Dec  3 15:07:51.465: INFO: (19) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname2/proxy/: tls qux (200; 93.018096ms)
Dec  3 15:07:51.465: INFO: (19) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:460/proxy/: tls baz (200; 92.993015ms)
Dec  3 15:07:51.465: INFO: (19) /api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/https:proxy-service-cfwp8-rncjr:443/proxy/tlsrewritem... (200; 93.131179ms)
Dec  3 15:07:51.465: INFO: (19) /api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/: <a href="/api/v1/namespaces/proxy-6380/pods/proxy-service-cfwp8-rncjr:1080/proxy/rewriteme">test<... (200; 93.059502ms)
Dec  3 15:07:51.465: INFO: (19) /api/v1/namespaces/proxy-6380/services/https:proxy-service-cfwp8:tlsportname1/proxy/: tls baz (200; 93.044767ms)
Dec  3 15:07:51.467: INFO: (19) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname1/proxy/: foo (200; 95.499665ms)
Dec  3 15:07:51.467: INFO: (19) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname1/proxy/: foo (200; 95.627857ms)
Dec  3 15:07:51.467: INFO: (19) /api/v1/namespaces/proxy-6380/services/proxy-service-cfwp8:portname2/proxy/: bar (200; 95.500426ms)
Dec  3 15:07:51.467: INFO: (19) /api/v1/namespaces/proxy-6380/services/http:proxy-service-cfwp8:portname2/proxy/: bar (200; 95.517184ms)
STEP: deleting ReplicationController proxy-service-cfwp8 in namespace proxy-6380, will wait for the garbage collector to delete the pods
Dec  3 15:07:51.748: INFO: Deleting ReplicationController proxy-service-cfwp8 took: 90.907499ms
Dec  3 15:07:51.849: INFO: Terminating ReplicationController proxy-service-cfwp8 pods took: 100.323616ms
[AfterEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:07:54.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6380" for this suite.
Dec  3 15:08:00.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:08:04.106: INFO: namespace proxy-6380 deletion completed in 9.766866532s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:08:04.107: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:08:04.838: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e0831f5-8013-4360-8a2d-c517839aee3a" in namespace "projected-1309" to be "success or failure"
Dec  3 15:08:04.928: INFO: Pod "downwardapi-volume-3e0831f5-8013-4360-8a2d-c517839aee3a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.93228ms
Dec  3 15:08:07.019: INFO: Pod "downwardapi-volume-3e0831f5-8013-4360-8a2d-c517839aee3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180254027s
STEP: Saw pod success
Dec  3 15:08:07.019: INFO: Pod "downwardapi-volume-3e0831f5-8013-4360-8a2d-c517839aee3a" satisfied condition "success or failure"
Dec  3 15:08:07.109: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-3e0831f5-8013-4360-8a2d-c517839aee3a container client-container: <nil>
STEP: delete the pod
Dec  3 15:08:07.299: INFO: Waiting for pod downwardapi-volume-3e0831f5-8013-4360-8a2d-c517839aee3a to disappear
Dec  3 15:08:07.389: INFO: Pod downwardapi-volume-3e0831f5-8013-4360-8a2d-c517839aee3a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:08:07.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1309" for this suite.
Dec  3 15:08:13.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:08:17.287: INFO: namespace projected-1309 deletion completed in 9.806272411s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:08:17.287: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2915
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-ded0fc71-315c-47f5-a4b3-d02a45432d52
STEP: Creating a pod to test consume secrets
Dec  3 15:08:18.109: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6dd96b7a-f16d-454f-a58d-30897fd88858" in namespace "projected-2915" to be "success or failure"
Dec  3 15:08:18.198: INFO: Pod "pod-projected-secrets-6dd96b7a-f16d-454f-a58d-30897fd88858": Phase="Pending", Reason="", readiness=false. Elapsed: 89.792452ms
Dec  3 15:08:20.289: INFO: Pod "pod-projected-secrets-6dd96b7a-f16d-454f-a58d-30897fd88858": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180148932s
STEP: Saw pod success
Dec  3 15:08:20.289: INFO: Pod "pod-projected-secrets-6dd96b7a-f16d-454f-a58d-30897fd88858" satisfied condition "success or failure"
Dec  3 15:08:20.391: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-secrets-6dd96b7a-f16d-454f-a58d-30897fd88858 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:08:20.580: INFO: Waiting for pod pod-projected-secrets-6dd96b7a-f16d-454f-a58d-30897fd88858 to disappear
Dec  3 15:08:20.670: INFO: Pod pod-projected-secrets-6dd96b7a-f16d-454f-a58d-30897fd88858 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:08:20.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2915" for this suite.
Dec  3 15:08:27.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:08:30.537: INFO: namespace projected-2915 deletion completed in 9.777067152s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:08:30.538: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:08:31.177: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2316'
Dec  3 15:08:32.172: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 15:08:32.172: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Dec  3 15:08:32.262: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete jobs e2e-test-nginx-job --namespace=kubectl-2316'
Dec  3 15:08:32.784: INFO: stderr: ""
Dec  3 15:08:32.784: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:08:32.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2316" for this suite.
Dec  3 15:08:39.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:08:42.652: INFO: namespace kubectl-2316 deletion completed in 9.776784549s
•SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:08:42.652: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6050
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-58e2a444-7b0e-4413-b270-229927541561
STEP: Creating a pod to test consume secrets
Dec  3 15:08:43.474: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-95d9e20d-8717-41f0-a948-5d7810ea16d9" in namespace "projected-6050" to be "success or failure"
Dec  3 15:08:43.564: INFO: Pod "pod-projected-secrets-95d9e20d-8717-41f0-a948-5d7810ea16d9": Phase="Pending", Reason="", readiness=false. Elapsed: 89.799972ms
Dec  3 15:08:45.655: INFO: Pod "pod-projected-secrets-95d9e20d-8717-41f0-a948-5d7810ea16d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.181145865s
STEP: Saw pod success
Dec  3 15:08:45.655: INFO: Pod "pod-projected-secrets-95d9e20d-8717-41f0-a948-5d7810ea16d9" satisfied condition "success or failure"
Dec  3 15:08:45.745: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-secrets-95d9e20d-8717-41f0-a948-5d7810ea16d9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:08:45.935: INFO: Waiting for pod pod-projected-secrets-95d9e20d-8717-41f0-a948-5d7810ea16d9 to disappear
Dec  3 15:08:46.024: INFO: Pod pod-projected-secrets-95d9e20d-8717-41f0-a948-5d7810ea16d9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:08:46.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6050" for this suite.
Dec  3 15:08:52.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:08:55.880: INFO: namespace projected-6050 deletion completed in 9.765198051s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:08:55.881: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2999
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Dec  3 15:08:56.519: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix426498099/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:08:56.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2999" for this suite.
Dec  3 15:09:02.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:09:06.434: INFO: namespace kubectl-2999 deletion completed in 9.769027999s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:09:06.434: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6049
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-6760b426-846c-41be-b883-0a65fd51b9cc
STEP: Creating a pod to test consume configMaps
Dec  3 15:09:07.256: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ab5564d6-e6f7-40a7-b89b-725f51914e3b" in namespace "projected-6049" to be "success or failure"
Dec  3 15:09:07.346: INFO: Pod "pod-projected-configmaps-ab5564d6-e6f7-40a7-b89b-725f51914e3b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.81025ms
Dec  3 15:09:09.436: INFO: Pod "pod-projected-configmaps-ab5564d6-e6f7-40a7-b89b-725f51914e3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180052593s
STEP: Saw pod success
Dec  3 15:09:09.437: INFO: Pod "pod-projected-configmaps-ab5564d6-e6f7-40a7-b89b-725f51914e3b" satisfied condition "success or failure"
Dec  3 15:09:09.526: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-configmaps-ab5564d6-e6f7-40a7-b89b-725f51914e3b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:09:09.724: INFO: Waiting for pod pod-projected-configmaps-ab5564d6-e6f7-40a7-b89b-725f51914e3b to disappear
Dec  3 15:09:09.813: INFO: Pod pod-projected-configmaps-ab5564d6-e6f7-40a7-b89b-725f51914e3b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:09:09.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6049" for this suite.
Dec  3 15:09:16.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:09:19.679: INFO: namespace projected-6049 deletion completed in 9.774853849s
•SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:09:19.679: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8217
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec  3 15:09:30.867: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:09:30.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1203 15:09:30.867819    5065 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-8217" for this suite.
Dec  3 15:09:37.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:09:40.725: INFO: namespace gc-8217 deletion completed in 9.767790813s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:09:40.727: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:09:43.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5003" for this suite.
Dec  3 15:10:22.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:10:25.680: INFO: namespace kubelet-test-5003 deletion completed in 41.76570167s
•SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:10:25.681: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7914
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:10:28.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7914" for this suite.
Dec  3 15:11:07.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:11:10.640: INFO: namespace kubelet-test-7914 deletion completed in 41.771707087s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:11:10.641: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6304
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:11:11.278: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6304'
Dec  3 15:11:11.723: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 15:11:11.723: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Dec  3 15:11:11.904: INFO: scanned /root for discovery docs: <nil>
Dec  3 15:11:11.905: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6304'
Dec  3 15:11:24.152: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  3 15:11:24.152: INFO: stdout: "Created e2e-test-nginx-rc-f768fe459a88dce01c37a29c3f749f80\nScaling up e2e-test-nginx-rc-f768fe459a88dce01c37a29c3f749f80 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f768fe459a88dce01c37a29c3f749f80 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f768fe459a88dce01c37a29c3f749f80 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec  3 15:11:24.152: INFO: stdout: "Created e2e-test-nginx-rc-f768fe459a88dce01c37a29c3f749f80\nScaling up e2e-test-nginx-rc-f768fe459a88dce01c37a29c3f749f80 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f768fe459a88dce01c37a29c3f749f80 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f768fe459a88dce01c37a29c3f749f80 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec  3 15:11:24.152: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-6304'
Dec  3 15:11:24.579: INFO: stderr: ""
Dec  3 15:11:24.579: INFO: stdout: "e2e-test-nginx-rc-f768fe459a88dce01c37a29c3f749f80-d6jww "
Dec  3 15:11:24.579: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-f768fe459a88dce01c37a29c3f749f80-d6jww -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6304'
Dec  3 15:11:25.008: INFO: stderr: ""
Dec  3 15:11:25.008: INFO: stdout: "true"
Dec  3 15:11:25.008: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-f768fe459a88dce01c37a29c3f749f80-d6jww -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6304'
Dec  3 15:11:25.431: INFO: stderr: ""
Dec  3 15:11:25.431: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec  3 15:11:25.431: INFO: e2e-test-nginx-rc-f768fe459a88dce01c37a29c3f749f80-d6jww is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Dec  3 15:11:25.431: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-6304'
Dec  3 15:11:25.944: INFO: stderr: ""
Dec  3 15:11:25.944: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:11:25.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6304" for this suite.
Dec  3 15:11:32.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:11:35.794: INFO: namespace kubectl-6304 deletion completed in 9.759322302s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:11:35.794: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9494
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec  3 15:11:36.524: INFO: Waiting up to 5m0s for pod "pod-85e3e4a6-bd97-4e59-ae0a-8ab40c1fffa2" in namespace "emptydir-9494" to be "success or failure"
Dec  3 15:11:36.614: INFO: Pod "pod-85e3e4a6-bd97-4e59-ae0a-8ab40c1fffa2": Phase="Pending", Reason="", readiness=false. Elapsed: 89.317549ms
Dec  3 15:11:38.704: INFO: Pod "pod-85e3e4a6-bd97-4e59-ae0a-8ab40c1fffa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179231763s
STEP: Saw pod success
Dec  3 15:11:38.704: INFO: Pod "pod-85e3e4a6-bd97-4e59-ae0a-8ab40c1fffa2" satisfied condition "success or failure"
Dec  3 15:11:38.794: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-85e3e4a6-bd97-4e59-ae0a-8ab40c1fffa2 container test-container: <nil>
STEP: delete the pod
Dec  3 15:11:38.986: INFO: Waiting for pod pod-85e3e4a6-bd97-4e59-ae0a-8ab40c1fffa2 to disappear
Dec  3 15:11:39.075: INFO: Pod pod-85e3e4a6-bd97-4e59-ae0a-8ab40c1fffa2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:11:39.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9494" for this suite.
Dec  3 15:11:45.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:11:48.936: INFO: namespace emptydir-9494 deletion completed in 9.771055697s
•SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:11:48.937: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5299
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-kj88
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 15:11:49.855: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kj88" in namespace "subpath-5299" to be "success or failure"
Dec  3 15:11:49.944: INFO: Pod "pod-subpath-test-configmap-kj88": Phase="Pending", Reason="", readiness=false. Elapsed: 89.63603ms
Dec  3 15:11:52.035: INFO: Pod "pod-subpath-test-configmap-kj88": Phase="Running", Reason="", readiness=true. Elapsed: 2.180320269s
Dec  3 15:11:54.128: INFO: Pod "pod-subpath-test-configmap-kj88": Phase="Running", Reason="", readiness=true. Elapsed: 4.273167796s
Dec  3 15:11:56.218: INFO: Pod "pod-subpath-test-configmap-kj88": Phase="Running", Reason="", readiness=true. Elapsed: 6.363761453s
Dec  3 15:11:58.309: INFO: Pod "pod-subpath-test-configmap-kj88": Phase="Running", Reason="", readiness=true. Elapsed: 8.45418614s
Dec  3 15:12:00.399: INFO: Pod "pod-subpath-test-configmap-kj88": Phase="Running", Reason="", readiness=true. Elapsed: 10.54478431s
Dec  3 15:12:02.489: INFO: Pod "pod-subpath-test-configmap-kj88": Phase="Running", Reason="", readiness=true. Elapsed: 12.634810357s
Dec  3 15:12:04.580: INFO: Pod "pod-subpath-test-configmap-kj88": Phase="Running", Reason="", readiness=true. Elapsed: 14.724983574s
Dec  3 15:12:06.670: INFO: Pod "pod-subpath-test-configmap-kj88": Phase="Running", Reason="", readiness=true. Elapsed: 16.815262761s
Dec  3 15:12:08.760: INFO: Pod "pod-subpath-test-configmap-kj88": Phase="Running", Reason="", readiness=true. Elapsed: 18.905168371s
Dec  3 15:12:10.850: INFO: Pod "pod-subpath-test-configmap-kj88": Phase="Running", Reason="", readiness=true. Elapsed: 20.995289855s
Dec  3 15:12:12.940: INFO: Pod "pod-subpath-test-configmap-kj88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.085432512s
STEP: Saw pod success
Dec  3 15:12:12.940: INFO: Pod "pod-subpath-test-configmap-kj88" satisfied condition "success or failure"
Dec  3 15:12:13.030: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-subpath-test-configmap-kj88 container test-container-subpath-configmap-kj88: <nil>
STEP: delete the pod
Dec  3 15:12:13.223: INFO: Waiting for pod pod-subpath-test-configmap-kj88 to disappear
Dec  3 15:12:13.313: INFO: Pod pod-subpath-test-configmap-kj88 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kj88
Dec  3 15:12:13.313: INFO: Deleting pod "pod-subpath-test-configmap-kj88" in namespace "subpath-5299"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:12:13.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5299" for this suite.
Dec  3 15:12:19.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:12:23.265: INFO: namespace subpath-5299 deletion completed in 9.772011303s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:12:23.266: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2803
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2803
STEP: Creating statefulset with conflicting port in namespace statefulset-2803
STEP: Waiting until pod test-pod will start running in namespace statefulset-2803
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2803
Dec  3 15:12:26.536: INFO: Observed stateful pod in namespace: statefulset-2803, name: ss-0, uid: caf9b3d5-17d4-4284-97b1-395101214e3a, status phase: Failed. Waiting for statefulset controller to delete.
Dec  3 15:12:26.571: INFO: Observed stateful pod in namespace: statefulset-2803, name: ss-0, uid: caf9b3d5-17d4-4284-97b1-395101214e3a, status phase: Failed. Waiting for statefulset controller to delete.
Dec  3 15:12:26.573: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2803
STEP: Removing pod with conflicting port in namespace statefulset-2803
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2803 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  3 15:12:28.846: INFO: Deleting all statefulset in ns statefulset-2803
Dec  3 15:12:28.936: INFO: Scaling statefulset ss to 0
Dec  3 15:12:49.297: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:12:49.387: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:12:49.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2803" for this suite.
Dec  3 15:12:56.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:12:59.517: INFO: namespace statefulset-2803 deletion completed in 9.768855722s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:12:59.517: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2700
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  3 15:13:06.974: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:13:07.064: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:13:09.065: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:13:09.155: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:13:11.065: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:13:11.155: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:13:13.065: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:13:13.155: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:13:15.065: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:13:15.155: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:13:17.065: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:13:17.155: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:13:19.065: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:13:19.154: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:13:21.065: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:13:21.155: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:13:23.065: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:13:23.155: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:13:25.065: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:13:25.155: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:13:25.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2700" for this suite.
Dec  3 15:13:47.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:13:51.122: INFO: namespace container-lifecycle-hook-2700 deletion completed in 25.780040457s
•SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:13:51.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-4699ceee-3c57-4085-8297-7031fcc2d8a9 in namespace container-probe-1454
Dec  3 15:13:54.034: INFO: Started pod busybox-4699ceee-3c57-4085-8297-7031fcc2d8a9 in namespace container-probe-1454
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 15:13:54.124: INFO: Initial restart count of pod busybox-4699ceee-3c57-4085-8297-7031fcc2d8a9 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:17:54.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1454" for this suite.
Dec  3 15:18:00.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:18:04.492: INFO: namespace container-probe-1454 deletion completed in 9.810752367s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:18:04.493: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec  3 15:18:08.273: INFO: Successfully updated pod "labelsupdate14a93c6e-0778-4872-816c-cd2ea735981e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:18:10.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1843" for this suite.
Dec  3 15:18:32.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:18:36.321: INFO: namespace projected-1843 deletion completed in 25.763879852s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:18:36.322: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8915
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-h7rn
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 15:18:37.232: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-h7rn" in namespace "subpath-8915" to be "success or failure"
Dec  3 15:18:37.322: INFO: Pod "pod-subpath-test-projected-h7rn": Phase="Pending", Reason="", readiness=false. Elapsed: 89.611715ms
Dec  3 15:18:39.412: INFO: Pod "pod-subpath-test-projected-h7rn": Phase="Running", Reason="", readiness=true. Elapsed: 2.179939951s
Dec  3 15:18:41.503: INFO: Pod "pod-subpath-test-projected-h7rn": Phase="Running", Reason="", readiness=true. Elapsed: 4.270174112s
Dec  3 15:18:43.624: INFO: Pod "pod-subpath-test-projected-h7rn": Phase="Running", Reason="", readiness=true. Elapsed: 6.391632558s
Dec  3 15:18:45.714: INFO: Pod "pod-subpath-test-projected-h7rn": Phase="Running", Reason="", readiness=true. Elapsed: 8.481651931s
Dec  3 15:18:47.804: INFO: Pod "pod-subpath-test-projected-h7rn": Phase="Running", Reason="", readiness=true. Elapsed: 10.571909552s
Dec  3 15:18:49.894: INFO: Pod "pod-subpath-test-projected-h7rn": Phase="Running", Reason="", readiness=true. Elapsed: 12.661992826s
Dec  3 15:18:51.985: INFO: Pod "pod-subpath-test-projected-h7rn": Phase="Running", Reason="", readiness=true. Elapsed: 14.752742363s
Dec  3 15:18:54.075: INFO: Pod "pod-subpath-test-projected-h7rn": Phase="Running", Reason="", readiness=true. Elapsed: 16.842756375s
Dec  3 15:18:56.165: INFO: Pod "pod-subpath-test-projected-h7rn": Phase="Running", Reason="", readiness=true. Elapsed: 18.932997661s
Dec  3 15:18:58.256: INFO: Pod "pod-subpath-test-projected-h7rn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 21.023131833s
STEP: Saw pod success
Dec  3 15:18:58.256: INFO: Pod "pod-subpath-test-projected-h7rn" satisfied condition "success or failure"
Dec  3 15:18:58.346: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-subpath-test-projected-h7rn container test-container-subpath-projected-h7rn: <nil>
STEP: delete the pod
Dec  3 15:18:58.534: INFO: Waiting for pod pod-subpath-test-projected-h7rn to disappear
Dec  3 15:18:58.623: INFO: Pod pod-subpath-test-projected-h7rn no longer exists
STEP: Deleting pod pod-subpath-test-projected-h7rn
Dec  3 15:18:58.623: INFO: Deleting pod "pod-subpath-test-projected-h7rn" in namespace "subpath-8915"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:18:58.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8915" for this suite.
Dec  3 15:19:05.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:19:08.577: INFO: namespace subpath-8915 deletion completed in 9.773117075s
•S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:19:08.577: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:19:09.308: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0ff4520-9a49-43e9-bc6a-65539f7588a4" in namespace "projected-9410" to be "success or failure"
Dec  3 15:19:09.398: INFO: Pod "downwardapi-volume-d0ff4520-9a49-43e9-bc6a-65539f7588a4": Phase="Pending", Reason="", readiness=false. Elapsed: 89.784336ms
Dec  3 15:19:11.488: INFO: Pod "downwardapi-volume-d0ff4520-9a49-43e9-bc6a-65539f7588a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180100705s
STEP: Saw pod success
Dec  3 15:19:11.489: INFO: Pod "downwardapi-volume-d0ff4520-9a49-43e9-bc6a-65539f7588a4" satisfied condition "success or failure"
Dec  3 15:19:11.578: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-d0ff4520-9a49-43e9-bc6a-65539f7588a4 container client-container: <nil>
STEP: delete the pod
Dec  3 15:19:11.767: INFO: Waiting for pod downwardapi-volume-d0ff4520-9a49-43e9-bc6a-65539f7588a4 to disappear
Dec  3 15:19:11.857: INFO: Pod downwardapi-volume-d0ff4520-9a49-43e9-bc6a-65539f7588a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:19:11.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9410" for this suite.
Dec  3 15:19:18.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:19:21.713: INFO: namespace projected-9410 deletion completed in 9.765703754s
•SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:19:21.713: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1341
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  3 15:19:27.280: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:19:27.370: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:19:29.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:19:29.460: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:19:31.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:19:31.460: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:19:33.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:19:33.460: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:19:35.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:19:35.460: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:19:37.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:19:37.460: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:19:39.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:19:39.460: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:19:41.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:19:41.461: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:19:43.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:19:43.460: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:19:45.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:19:45.460: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 15:19:47.370: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 15:19:47.460: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:19:47.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1341" for this suite.
Dec  3 15:20:09.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:13.321: INFO: namespace container-lifecycle-hook-1341 deletion completed in 25.770480649s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:20:13.322: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6362
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-d024be6d-797b-49ee-8e80-d3a675dc3d2f
STEP: Creating a pod to test consume secrets
Dec  3 15:20:14.142: INFO: Waiting up to 5m0s for pod "pod-secrets-43c78285-61d1-491a-86f0-729eacbe8126" in namespace "secrets-6362" to be "success or failure"
Dec  3 15:20:14.232: INFO: Pod "pod-secrets-43c78285-61d1-491a-86f0-729eacbe8126": Phase="Pending", Reason="", readiness=false. Elapsed: 89.904179ms
Dec  3 15:20:16.322: INFO: Pod "pod-secrets-43c78285-61d1-491a-86f0-729eacbe8126": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180165194s
STEP: Saw pod success
Dec  3 15:20:16.322: INFO: Pod "pod-secrets-43c78285-61d1-491a-86f0-729eacbe8126" satisfied condition "success or failure"
Dec  3 15:20:16.413: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-secrets-43c78285-61d1-491a-86f0-729eacbe8126 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:20:16.602: INFO: Waiting for pod pod-secrets-43c78285-61d1-491a-86f0-729eacbe8126 to disappear
Dec  3 15:20:16.691: INFO: Pod pod-secrets-43c78285-61d1-491a-86f0-729eacbe8126 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:20:16.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6362" for this suite.
Dec  3 15:20:23.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:26.556: INFO: namespace secrets-6362 deletion completed in 9.774085545s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:20:26.557: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-125
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec  3 15:20:27.197: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Dec  3 15:20:28.770: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:20:30.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:20:33.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:20:34.861: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:20:36.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:20:38.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983228, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:20:41.759: INFO: Waited 807.868568ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:20:44.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-125" for this suite.
Dec  3 15:20:51.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:54.591: INFO: namespace aggregator-125 deletion completed in 9.781851745s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:20:54.592: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8715
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  3 15:20:55.323: INFO: Waiting up to 5m0s for pod "pod-e2c0cfe9-8c8f-49e5-ba55-f6988a69dadc" in namespace "emptydir-8715" to be "success or failure"
Dec  3 15:20:55.412: INFO: Pod "pod-e2c0cfe9-8c8f-49e5-ba55-f6988a69dadc": Phase="Pending", Reason="", readiness=false. Elapsed: 89.263078ms
Dec  3 15:20:57.502: INFO: Pod "pod-e2c0cfe9-8c8f-49e5-ba55-f6988a69dadc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179379546s
STEP: Saw pod success
Dec  3 15:20:57.502: INFO: Pod "pod-e2c0cfe9-8c8f-49e5-ba55-f6988a69dadc" satisfied condition "success or failure"
Dec  3 15:20:57.592: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-e2c0cfe9-8c8f-49e5-ba55-f6988a69dadc container test-container: <nil>
STEP: delete the pod
Dec  3 15:20:57.915: INFO: Waiting for pod pod-e2c0cfe9-8c8f-49e5-ba55-f6988a69dadc to disappear
Dec  3 15:20:58.005: INFO: Pod pod-e2c0cfe9-8c8f-49e5-ba55-f6988a69dadc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:20:58.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8715" for this suite.
Dec  3 15:21:04.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:21:07.854: INFO: namespace emptydir-8715 deletion completed in 9.758533793s
•SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:21:07.854: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5875
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:21:08.493: INFO: Creating ReplicaSet my-hostname-basic-5c59821e-4699-430f-b100-f9cb86c41b9e
Dec  3 15:21:08.673: INFO: Pod name my-hostname-basic-5c59821e-4699-430f-b100-f9cb86c41b9e: Found 1 pods out of 1
Dec  3 15:21:08.673: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-5c59821e-4699-430f-b100-f9cb86c41b9e" is running
Dec  3 15:21:10.852: INFO: Pod "my-hostname-basic-5c59821e-4699-430f-b100-f9cb86c41b9e-7xvjd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:21:08 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:21:08 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-5c59821e-4699-430f-b100-f9cb86c41b9e]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:21:08 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-5c59821e-4699-430f-b100-f9cb86c41b9e]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 15:21:08 +0000 UTC Reason: Message:}])
Dec  3 15:21:10.852: INFO: Trying to dial the pod
Dec  3 15:21:16.252: INFO: Controller my-hostname-basic-5c59821e-4699-430f-b100-f9cb86c41b9e: Got expected result from replica 1 [my-hostname-basic-5c59821e-4699-430f-b100-f9cb86c41b9e-7xvjd]: "my-hostname-basic-5c59821e-4699-430f-b100-f9cb86c41b9e-7xvjd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:21:16.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5875" for this suite.
Dec  3 15:21:22.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:21:26.100: INFO: namespace replicaset-5875 deletion completed in 9.757362795s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:21:26.101: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4881
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-238e9d36-e1ca-4d96-bd47-f72ba93956d9
STEP: Creating a pod to test consume secrets
Dec  3 15:21:26.920: INFO: Waiting up to 5m0s for pod "pod-secrets-643bf3c2-2710-456c-908e-33a86fd9bb1a" in namespace "secrets-4881" to be "success or failure"
Dec  3 15:21:27.010: INFO: Pod "pod-secrets-643bf3c2-2710-456c-908e-33a86fd9bb1a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.490362ms
Dec  3 15:21:29.100: INFO: Pod "pod-secrets-643bf3c2-2710-456c-908e-33a86fd9bb1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179535732s
STEP: Saw pod success
Dec  3 15:21:29.100: INFO: Pod "pod-secrets-643bf3c2-2710-456c-908e-33a86fd9bb1a" satisfied condition "success or failure"
Dec  3 15:21:29.190: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-secrets-643bf3c2-2710-456c-908e-33a86fd9bb1a container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:21:29.380: INFO: Waiting for pod pod-secrets-643bf3c2-2710-456c-908e-33a86fd9bb1a to disappear
Dec  3 15:21:29.469: INFO: Pod pod-secrets-643bf3c2-2710-456c-908e-33a86fd9bb1a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:21:29.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4881" for this suite.
Dec  3 15:21:35.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:21:39.316: INFO: namespace secrets-4881 deletion completed in 9.757573196s
•SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:21:39.317: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:21:40.492: INFO: Create a RollingUpdate DaemonSet
Dec  3 15:21:40.582: INFO: Check that daemon pods launch on every node of the cluster
Dec  3 15:21:40.761: INFO: Number of nodes with available pods: 0
Dec  3 15:21:40.761: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 15:21:41.941: INFO: Number of nodes with available pods: 2
Dec  3 15:21:41.941: INFO: Number of running nodes: 2, number of available pods: 2
Dec  3 15:21:41.941: INFO: Update the DaemonSet to trigger a rollout
Dec  3 15:21:42.121: INFO: Updating DaemonSet daemon-set
Dec  3 15:21:46.484: INFO: Roll back the DaemonSet before rollout is complete
Dec  3 15:21:46.662: INFO: Updating DaemonSet daemon-set
Dec  3 15:21:46.662: INFO: Make sure DaemonSet rollback is complete
Dec  3 15:21:46.752: INFO: Wrong image for pod: daemon-set-v8lkj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 15:21:46.752: INFO: Pod daemon-set-v8lkj is not available
Dec  3 15:21:47.932: INFO: Wrong image for pod: daemon-set-v8lkj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 15:21:47.932: INFO: Pod daemon-set-v8lkj is not available
Dec  3 15:21:48.932: INFO: Wrong image for pod: daemon-set-v8lkj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 15:21:48.932: INFO: Pod daemon-set-v8lkj is not available
Dec  3 15:21:49.932: INFO: Wrong image for pod: daemon-set-v8lkj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 15:21:49.932: INFO: Pod daemon-set-v8lkj is not available
Dec  3 15:21:50.932: INFO: Wrong image for pod: daemon-set-v8lkj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 15:21:50.932: INFO: Pod daemon-set-v8lkj is not available
Dec  3 15:21:51.932: INFO: Wrong image for pod: daemon-set-v8lkj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 15:21:51.932: INFO: Pod daemon-set-v8lkj is not available
Dec  3 15:21:52.933: INFO: Wrong image for pod: daemon-set-v8lkj. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  3 15:21:52.933: INFO: Pod daemon-set-v8lkj is not available
Dec  3 15:21:53.932: INFO: Pod daemon-set-s5h59 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4849, will wait for the garbage collector to delete the pods
Dec  3 15:21:54.483: INFO: Deleting DaemonSet.extensions daemon-set took: 90.888358ms
Dec  3 15:21:54.884: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.385912ms
Dec  3 15:22:08.074: INFO: Number of nodes with available pods: 0
Dec  3 15:22:08.074: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 15:22:08.164: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4849/daemonsets","resourceVersion":"9796"},"items":null}

Dec  3 15:22:08.254: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4849/pods","resourceVersion":"9796"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:22:08.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4849" for this suite.
Dec  3 15:22:14.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:22:18.370: INFO: namespace daemonsets-4849 deletion completed in 9.75499767s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:22:18.371: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Dec  3 15:22:19.100: INFO: Waiting up to 5m0s for pod "client-containers-b1ee2bc0-e3ef-4e3d-b159-673788518702" in namespace "containers-7834" to be "success or failure"
Dec  3 15:22:19.189: INFO: Pod "client-containers-b1ee2bc0-e3ef-4e3d-b159-673788518702": Phase="Pending", Reason="", readiness=false. Elapsed: 89.567595ms
Dec  3 15:22:21.279: INFO: Pod "client-containers-b1ee2bc0-e3ef-4e3d-b159-673788518702": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179719206s
STEP: Saw pod success
Dec  3 15:22:21.279: INFO: Pod "client-containers-b1ee2bc0-e3ef-4e3d-b159-673788518702" satisfied condition "success or failure"
Dec  3 15:22:21.369: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod client-containers-b1ee2bc0-e3ef-4e3d-b159-673788518702 container test-container: <nil>
STEP: delete the pod
Dec  3 15:22:21.559: INFO: Waiting for pod client-containers-b1ee2bc0-e3ef-4e3d-b159-673788518702 to disappear
Dec  3 15:22:21.648: INFO: Pod client-containers-b1ee2bc0-e3ef-4e3d-b159-673788518702 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:22:21.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7834" for this suite.
Dec  3 15:22:28.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:22:31.503: INFO: namespace containers-7834 deletion completed in 9.765026063s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:22:31.503: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3249
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Dec  3 15:22:32.143: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3249'
Dec  3 15:22:33.609: INFO: stderr: ""
Dec  3 15:22:33.609: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 15:22:33.609: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3249'
Dec  3 15:22:34.033: INFO: stderr: ""
Dec  3 15:22:34.033: INFO: stdout: "update-demo-nautilus-d7r8s update-demo-nautilus-jfrp5 "
Dec  3 15:22:34.033: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-d7r8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3249'
Dec  3 15:22:34.482: INFO: stderr: ""
Dec  3 15:22:34.482: INFO: stdout: ""
Dec  3 15:22:34.482: INFO: update-demo-nautilus-d7r8s is created but not running
Dec  3 15:22:39.482: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3249'
Dec  3 15:22:39.911: INFO: stderr: ""
Dec  3 15:22:39.911: INFO: stdout: "update-demo-nautilus-d7r8s update-demo-nautilus-jfrp5 "
Dec  3 15:22:39.911: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-d7r8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3249'
Dec  3 15:22:40.361: INFO: stderr: ""
Dec  3 15:22:40.361: INFO: stdout: "true"
Dec  3 15:22:40.361: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-d7r8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3249'
Dec  3 15:22:40.781: INFO: stderr: ""
Dec  3 15:22:40.781: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 15:22:40.781: INFO: validating pod update-demo-nautilus-d7r8s
Dec  3 15:22:40.961: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 15:22:40.961: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 15:22:40.961: INFO: update-demo-nautilus-d7r8s is verified up and running
Dec  3 15:22:40.961: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-jfrp5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3249'
Dec  3 15:22:41.386: INFO: stderr: ""
Dec  3 15:22:41.387: INFO: stdout: "true"
Dec  3 15:22:41.387: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-jfrp5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3249'
Dec  3 15:22:41.817: INFO: stderr: ""
Dec  3 15:22:41.817: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 15:22:41.817: INFO: validating pod update-demo-nautilus-jfrp5
Dec  3 15:22:41.996: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 15:22:41.997: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 15:22:41.997: INFO: update-demo-nautilus-jfrp5 is verified up and running
STEP: rolling-update to new replication controller
Dec  3 15:22:41.999: INFO: scanned /root for discovery docs: <nil>
Dec  3 15:22:41.999: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3249'
Dec  3 15:22:56.798: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  3 15:22:56.798: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 15:22:56.798: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3249'
Dec  3 15:22:57.227: INFO: stderr: ""
Dec  3 15:22:57.227: INFO: stdout: "update-demo-kitten-pflsg update-demo-kitten-vgbwv update-demo-nautilus-jfrp5 "
STEP: Replicas for name=update-demo: expected=2 actual=3
Dec  3 15:23:02.227: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3249'
Dec  3 15:23:02.657: INFO: stderr: ""
Dec  3 15:23:02.657: INFO: stdout: "update-demo-kitten-pflsg update-demo-kitten-vgbwv "
Dec  3 15:23:02.657: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-pflsg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3249'
Dec  3 15:23:03.084: INFO: stderr: ""
Dec  3 15:23:03.084: INFO: stdout: "true"
Dec  3 15:23:03.085: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-pflsg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3249'
Dec  3 15:23:03.504: INFO: stderr: ""
Dec  3 15:23:03.504: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  3 15:23:03.504: INFO: validating pod update-demo-kitten-pflsg
Dec  3 15:23:03.684: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  3 15:23:03.684: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  3 15:23:03.684: INFO: update-demo-kitten-pflsg is verified up and running
Dec  3 15:23:03.685: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-vgbwv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3249'
Dec  3 15:23:04.105: INFO: stderr: ""
Dec  3 15:23:04.105: INFO: stdout: "true"
Dec  3 15:23:04.105: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-vgbwv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3249'
Dec  3 15:23:04.528: INFO: stderr: ""
Dec  3 15:23:04.528: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  3 15:23:04.528: INFO: validating pod update-demo-kitten-vgbwv
Dec  3 15:23:04.682: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  3 15:23:04.682: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  3 15:23:04.682: INFO: update-demo-kitten-vgbwv is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:23:04.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3249" for this suite.
Dec  3 15:23:27.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:23:30.543: INFO: namespace kubectl-3249 deletion completed in 25.77069616s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:23:30.544: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6472
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5593
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4281
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:23:56.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6472" for this suite.
Dec  3 15:24:03.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:24:06.753: INFO: namespace namespaces-6472 deletion completed in 9.759059241s
STEP: Destroying namespace "nsdeletetest-5593" for this suite.
Dec  3 15:24:06.843: INFO: Namespace nsdeletetest-5593 was already deleted
STEP: Destroying namespace "nsdeletetest-4281" for this suite.
Dec  3 15:24:13.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:24:16.611: INFO: namespace nsdeletetest-4281 deletion completed in 9.768232328s
•
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:24:16.611: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Dec  3 15:24:17.252: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3261'
Dec  3 15:24:17.870: INFO: stderr: ""
Dec  3 15:24:17.870: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Dec  3 15:24:18.961: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:24:18.961: INFO: Found 0 / 1
Dec  3 15:24:19.961: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:24:19.961: INFO: Found 1 / 1
Dec  3 15:24:19.961: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  3 15:24:20.053: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:24:20.053: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec  3 15:24:20.053: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-qmptm redis-master --namespace=kubectl-3261'
Dec  3 15:24:20.582: INFO: stderr: ""
Dec  3 15:24:20.582: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 Dec 15:24:18.606 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Dec 15:24:18.606 # Server started, Redis version 3.2.12\n1:M 03 Dec 15:24:18.606 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Dec 15:24:18.607 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec  3 15:24:20.582: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-qmptm redis-master --namespace=kubectl-3261 --tail=1'
Dec  3 15:24:21.245: INFO: stderr: ""
Dec  3 15:24:21.245: INFO: stdout: "1:M 03 Dec 15:24:18.607 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec  3 15:24:21.246: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-qmptm redis-master --namespace=kubectl-3261 --limit-bytes=1'
Dec  3 15:24:21.908: INFO: stderr: ""
Dec  3 15:24:21.908: INFO: stdout: " "
STEP: exposing timestamps
Dec  3 15:24:21.908: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-qmptm redis-master --namespace=kubectl-3261 --tail=1 --timestamps'
Dec  3 15:24:22.433: INFO: stderr: ""
Dec  3 15:24:22.433: INFO: stdout: "2019-12-03T15:24:18.607087342Z 1:M 03 Dec 15:24:18.607 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec  3 15:24:24.933: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-qmptm redis-master --namespace=kubectl-3261 --since=1s'
Dec  3 15:24:25.450: INFO: stderr: ""
Dec  3 15:24:25.450: INFO: stdout: ""
Dec  3 15:24:25.450: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-qmptm redis-master --namespace=kubectl-3261 --since=24h'
Dec  3 15:24:25.973: INFO: stderr: ""
Dec  3 15:24:25.973: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 Dec 15:24:18.606 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Dec 15:24:18.606 # Server started, Redis version 3.2.12\n1:M 03 Dec 15:24:18.606 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Dec 15:24:18.607 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Dec  3 15:24:25.973: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-3261'
Dec  3 15:24:26.492: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:24:26.492: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec  3 15:24:26.492: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=nginx --no-headers --namespace=kubectl-3261'
Dec  3 15:24:27.011: INFO: stderr: "No resources found.\n"
Dec  3 15:24:27.011: INFO: stdout: ""
Dec  3 15:24:27.011: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=nginx --namespace=kubectl-3261 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 15:24:27.434: INFO: stderr: ""
Dec  3 15:24:27.434: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:24:27.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3261" for this suite.
Dec  3 15:24:49.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:24:53.284: INFO: namespace kubectl-3261 deletion completed in 25.759427937s
•SSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:24:53.284: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec  3 15:24:54.100: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:25:03.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9547" for this suite.
Dec  3 15:25:09.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:25:13.215: INFO: namespace pods-9547 deletion completed in 9.755491761s
•SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:25:13.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Dec  3 15:25:13.944: INFO: Waiting up to 5m0s for pod "client-containers-032fe37a-5237-41ca-8ed9-168ac31fd74a" in namespace "containers-493" to be "success or failure"
Dec  3 15:25:14.034: INFO: Pod "client-containers-032fe37a-5237-41ca-8ed9-168ac31fd74a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.675692ms
Dec  3 15:25:16.125: INFO: Pod "client-containers-032fe37a-5237-41ca-8ed9-168ac31fd74a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.18020993s
STEP: Saw pod success
Dec  3 15:25:16.125: INFO: Pod "client-containers-032fe37a-5237-41ca-8ed9-168ac31fd74a" satisfied condition "success or failure"
Dec  3 15:25:16.214: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod client-containers-032fe37a-5237-41ca-8ed9-168ac31fd74a container test-container: <nil>
STEP: delete the pod
Dec  3 15:25:16.404: INFO: Waiting for pod client-containers-032fe37a-5237-41ca-8ed9-168ac31fd74a to disappear
Dec  3 15:25:16.494: INFO: Pod client-containers-032fe37a-5237-41ca-8ed9-168ac31fd74a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:25:16.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-493" for this suite.
Dec  3 15:25:22.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:25:26.341: INFO: namespace containers-493 deletion completed in 9.756267968s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:25:26.342: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-31a8ac8e-49a5-4eb3-bc5a-6cf24854c1b5
STEP: Creating a pod to test consume configMaps
Dec  3 15:25:27.163: INFO: Waiting up to 5m0s for pod "pod-configmaps-87d7b363-44eb-494e-94bc-867fcdba6c60" in namespace "configmap-2730" to be "success or failure"
Dec  3 15:25:27.252: INFO: Pod "pod-configmaps-87d7b363-44eb-494e-94bc-867fcdba6c60": Phase="Pending", Reason="", readiness=false. Elapsed: 89.297788ms
Dec  3 15:25:29.342: INFO: Pod "pod-configmaps-87d7b363-44eb-494e-94bc-867fcdba6c60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179482107s
STEP: Saw pod success
Dec  3 15:25:29.343: INFO: Pod "pod-configmaps-87d7b363-44eb-494e-94bc-867fcdba6c60" satisfied condition "success or failure"
Dec  3 15:25:29.432: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-configmaps-87d7b363-44eb-494e-94bc-867fcdba6c60 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:25:29.621: INFO: Waiting for pod pod-configmaps-87d7b363-44eb-494e-94bc-867fcdba6c60 to disappear
Dec  3 15:25:29.710: INFO: Pod pod-configmaps-87d7b363-44eb-494e-94bc-867fcdba6c60 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:25:29.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2730" for this suite.
Dec  3 15:25:36.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:25:39.562: INFO: namespace configmap-2730 deletion completed in 9.761151556s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:25:39.563: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9748
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-8b2aa64c-e09b-4529-8b7e-92a603adb9d6
STEP: Creating a pod to test consume configMaps
Dec  3 15:25:40.381: INFO: Waiting up to 5m0s for pod "pod-configmaps-ccdc8a15-3dbf-4d04-bff4-10fb6b0a14a8" in namespace "configmap-9748" to be "success or failure"
Dec  3 15:25:40.470: INFO: Pod "pod-configmaps-ccdc8a15-3dbf-4d04-bff4-10fb6b0a14a8": Phase="Pending", Reason="", readiness=false. Elapsed: 89.45663ms
Dec  3 15:25:42.560: INFO: Pod "pod-configmaps-ccdc8a15-3dbf-4d04-bff4-10fb6b0a14a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179108052s
STEP: Saw pod success
Dec  3 15:25:42.560: INFO: Pod "pod-configmaps-ccdc8a15-3dbf-4d04-bff4-10fb6b0a14a8" satisfied condition "success or failure"
Dec  3 15:25:42.650: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-configmaps-ccdc8a15-3dbf-4d04-bff4-10fb6b0a14a8 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:25:42.840: INFO: Waiting for pod pod-configmaps-ccdc8a15-3dbf-4d04-bff4-10fb6b0a14a8 to disappear
Dec  3 15:25:42.930: INFO: Pod pod-configmaps-ccdc8a15-3dbf-4d04-bff4-10fb6b0a14a8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:25:42.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9748" for this suite.
Dec  3 15:25:49.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:25:52.775: INFO: namespace configmap-9748 deletion completed in 9.754594582s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:25:52.775: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6409
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-43670e51-7e50-4670-8716-248b33717f66
STEP: Creating a pod to test consume secrets
Dec  3 15:25:53.597: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-99a8f9e5-b486-4439-b9e2-77995340deed" in namespace "projected-6409" to be "success or failure"
Dec  3 15:25:53.686: INFO: Pod "pod-projected-secrets-99a8f9e5-b486-4439-b9e2-77995340deed": Phase="Pending", Reason="", readiness=false. Elapsed: 89.326668ms
Dec  3 15:25:55.776: INFO: Pod "pod-projected-secrets-99a8f9e5-b486-4439-b9e2-77995340deed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179233909s
STEP: Saw pod success
Dec  3 15:25:55.776: INFO: Pod "pod-projected-secrets-99a8f9e5-b486-4439-b9e2-77995340deed" satisfied condition "success or failure"
Dec  3 15:25:55.866: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-secrets-99a8f9e5-b486-4439-b9e2-77995340deed container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:25:56.056: INFO: Waiting for pod pod-projected-secrets-99a8f9e5-b486-4439-b9e2-77995340deed to disappear
Dec  3 15:25:56.146: INFO: Pod pod-projected-secrets-99a8f9e5-b486-4439-b9e2-77995340deed no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:25:56.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6409" for this suite.
Dec  3 15:26:02.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:05.988: INFO: namespace projected-6409 deletion completed in 9.752391346s
•SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:26:05.988: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec  3 15:26:09.864: INFO: Successfully updated pod "annotationupdate45c9ca50-9c06-4824-87a6-aee98a06d1d5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:26:12.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4545" for this suite.
Dec  3 15:26:34.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:37.904: INFO: namespace downward-api-4545 deletion completed in 25.757232505s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:26:37.904: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7992
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ee705316-6c5a-4672-a5a3-3eed0de135d9
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-ee705316-6c5a-4672-a5a3-3eed0de135d9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:26:45.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7992" for this suite.
Dec  3 15:27:07.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:11.404: INFO: namespace projected-7992 deletion completed in 25.754047133s
•SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:27:11.404: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6558
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1203 15:27:42.773354    5065 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  3 15:27:42.773: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:27:42.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6558" for this suite.
Dec  3 15:27:49.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:52.617: INFO: namespace gc-6558 deletion completed in 9.753420981s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:27:52.617: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-669
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-669
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 15:27:53.254: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 15:28:14.872: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.1.25 8081 | grep -v '^\s*$'] Namespace:pod-network-test-669 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:28:14.872: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:28:16.725: INFO: Found all expected endpoints: [netserver-0]
Dec  3 15:28:16.815: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.0.114 8081 | grep -v '^\s*$'] Namespace:pod-network-test-669 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:28:16.815: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:28:18.635: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:28:18.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-669" for this suite.
Dec  3 15:28:40.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:28:44.485: INFO: namespace pod-network-test-669 deletion completed in 25.759035422s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:28:44.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-5cb4718c-2900-4436-af8d-e48ab63001d1
STEP: Creating a pod to test consume configMaps
Dec  3 15:28:45.305: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9d0a5203-9ef1-438f-ac59-0473039ec163" in namespace "projected-6702" to be "success or failure"
Dec  3 15:28:45.395: INFO: Pod "pod-projected-configmaps-9d0a5203-9ef1-438f-ac59-0473039ec163": Phase="Pending", Reason="", readiness=false. Elapsed: 89.580192ms
Dec  3 15:28:47.485: INFO: Pod "pod-projected-configmaps-9d0a5203-9ef1-438f-ac59-0473039ec163": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180022467s
STEP: Saw pod success
Dec  3 15:28:47.485: INFO: Pod "pod-projected-configmaps-9d0a5203-9ef1-438f-ac59-0473039ec163" satisfied condition "success or failure"
Dec  3 15:28:47.574: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-configmaps-9d0a5203-9ef1-438f-ac59-0473039ec163 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:28:47.764: INFO: Waiting for pod pod-projected-configmaps-9d0a5203-9ef1-438f-ac59-0473039ec163 to disappear
Dec  3 15:28:47.854: INFO: Pod pod-projected-configmaps-9d0a5203-9ef1-438f-ac59-0473039ec163 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:28:47.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6702" for this suite.
Dec  3 15:28:54.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:28:57.701: INFO: namespace projected-6702 deletion completed in 9.756918659s
•S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:28:57.701: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9997
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3550
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-540
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:29:05.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9997" for this suite.
Dec  3 15:29:12.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:15.744: INFO: namespace namespaces-9997 deletion completed in 9.761019786s
STEP: Destroying namespace "nsdeletetest-3550" for this suite.
Dec  3 15:29:15.834: INFO: Namespace nsdeletetest-3550 was already deleted
STEP: Destroying namespace "nsdeletetest-540" for this suite.
Dec  3 15:29:22.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:25.585: INFO: namespace nsdeletetest-540 deletion completed in 9.751666852s
•SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:29:25.586: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7894
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  3 15:29:26.317: INFO: Waiting up to 5m0s for pod "pod-ad6af855-5fd8-4ce1-9c2d-3fc6563d1148" in namespace "emptydir-7894" to be "success or failure"
Dec  3 15:29:26.406: INFO: Pod "pod-ad6af855-5fd8-4ce1-9c2d-3fc6563d1148": Phase="Pending", Reason="", readiness=false. Elapsed: 89.826996ms
Dec  3 15:29:28.497: INFO: Pod "pod-ad6af855-5fd8-4ce1-9c2d-3fc6563d1148": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179931103s
STEP: Saw pod success
Dec  3 15:29:28.497: INFO: Pod "pod-ad6af855-5fd8-4ce1-9c2d-3fc6563d1148" satisfied condition "success or failure"
Dec  3 15:29:28.586: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-ad6af855-5fd8-4ce1-9c2d-3fc6563d1148 container test-container: <nil>
STEP: delete the pod
Dec  3 15:29:28.778: INFO: Waiting for pod pod-ad6af855-5fd8-4ce1-9c2d-3fc6563d1148 to disappear
Dec  3 15:29:28.871: INFO: Pod pod-ad6af855-5fd8-4ce1-9c2d-3fc6563d1148 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:29:28.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7894" for this suite.
Dec  3 15:29:35.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:38.713: INFO: namespace emptydir-7894 deletion completed in 9.750755368s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:29:38.716: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec  3 15:29:39.892: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1523,SelfLink:/api/v1/namespaces/watch-1523/configmaps/e2e-watch-test-label-changed,UID:c4f827e1-d570-408c-b5bb-30be7aa7c13f,ResourceVersion:11204,Generation:0,CreationTimestamp:2019-12-03 15:29:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 15:29:39.892: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1523,SelfLink:/api/v1/namespaces/watch-1523/configmaps/e2e-watch-test-label-changed,UID:c4f827e1-d570-408c-b5bb-30be7aa7c13f,ResourceVersion:11205,Generation:0,CreationTimestamp:2019-12-03 15:29:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  3 15:29:39.892: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1523,SelfLink:/api/v1/namespaces/watch-1523/configmaps/e2e-watch-test-label-changed,UID:c4f827e1-d570-408c-b5bb-30be7aa7c13f,ResourceVersion:11206,Generation:0,CreationTimestamp:2019-12-03 15:29:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec  3 15:29:50.530: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1523,SelfLink:/api/v1/namespaces/watch-1523/configmaps/e2e-watch-test-label-changed,UID:c4f827e1-d570-408c-b5bb-30be7aa7c13f,ResourceVersion:11229,Generation:0,CreationTimestamp:2019-12-03 15:29:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 15:29:50.530: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1523,SelfLink:/api/v1/namespaces/watch-1523/configmaps/e2e-watch-test-label-changed,UID:c4f827e1-d570-408c-b5bb-30be7aa7c13f,ResourceVersion:11230,Generation:0,CreationTimestamp:2019-12-03 15:29:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec  3 15:29:50.530: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-1523,SelfLink:/api/v1/namespaces/watch-1523/configmaps/e2e-watch-test-label-changed,UID:c4f827e1-d570-408c-b5bb-30be7aa7c13f,ResourceVersion:11232,Generation:0,CreationTimestamp:2019-12-03 15:29:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:29:50.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1523" for this suite.
Dec  3 15:29:56.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:30:00.380: INFO: namespace watch-1523 deletion completed in 9.759137025s
•SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:30:00.380: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9473
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-e5a84707-2303-4fd9-aad5-b291fd459ef0
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:30:01.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9473" for this suite.
Dec  3 15:30:07.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:30:10.950: INFO: namespace configmap-9473 deletion completed in 9.754210031s
•SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:30:10.950: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-525
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Dec  3 15:30:11.588: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config api-versions'
Dec  3 15:30:12.285: INFO: stderr: ""
Dec  3 15:30:12.285: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:30:12.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-525" for this suite.
Dec  3 15:30:18.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:30:22.128: INFO: namespace kubectl-525 deletion completed in 9.752454844s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:30:22.128: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4279
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:30:22.858: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1081dd30-8f51-4932-bd69-28e225eae62f" in namespace "downward-api-4279" to be "success or failure"
Dec  3 15:30:22.948: INFO: Pod "downwardapi-volume-1081dd30-8f51-4932-bd69-28e225eae62f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.584624ms
Dec  3 15:30:25.038: INFO: Pod "downwardapi-volume-1081dd30-8f51-4932-bd69-28e225eae62f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179457298s
STEP: Saw pod success
Dec  3 15:30:25.038: INFO: Pod "downwardapi-volume-1081dd30-8f51-4932-bd69-28e225eae62f" satisfied condition "success or failure"
Dec  3 15:30:25.128: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-1081dd30-8f51-4932-bd69-28e225eae62f container client-container: <nil>
STEP: delete the pod
Dec  3 15:30:25.318: INFO: Waiting for pod downwardapi-volume-1081dd30-8f51-4932-bd69-28e225eae62f to disappear
Dec  3 15:30:25.407: INFO: Pod downwardapi-volume-1081dd30-8f51-4932-bd69-28e225eae62f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:30:25.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4279" for this suite.
Dec  3 15:30:31.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:30:35.250: INFO: namespace downward-api-4279 deletion completed in 9.751385462s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:30:35.251: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8143
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  3 15:30:39.022: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d93a6235-1b9f-4ec6-8863-5a1f545e274e"
Dec  3 15:30:39.022: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d93a6235-1b9f-4ec6-8863-5a1f545e274e" in namespace "pods-8143" to be "terminated due to deadline exceeded"
Dec  3 15:30:39.111: INFO: Pod "pod-update-activedeadlineseconds-d93a6235-1b9f-4ec6-8863-5a1f545e274e": Phase="Running", Reason="", readiness=true. Elapsed: 89.077843ms
Dec  3 15:30:41.201: INFO: Pod "pod-update-activedeadlineseconds-d93a6235-1b9f-4ec6-8863-5a1f545e274e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.179203161s
Dec  3 15:30:41.201: INFO: Pod "pod-update-activedeadlineseconds-d93a6235-1b9f-4ec6-8863-5a1f545e274e" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:30:41.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8143" for this suite.
Dec  3 15:30:47.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:30:51.046: INFO: namespace pods-8143 deletion completed in 9.754477552s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:30:51.047: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8741
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 15:30:51.685: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 15:31:15.214: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.0.121:8080/dial?request=hostName&protocol=http&host=100.64.0.120&port=8080&tries=1'] Namespace:pod-network-test-8741 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:31:15.214: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:31:16.085: INFO: Waiting for endpoints: map[]
Dec  3 15:31:16.174: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.0.121:8080/dial?request=hostName&protocol=http&host=100.64.1.26&port=8080&tries=1'] Namespace:pod-network-test-8741 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:31:16.174: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:31:17.034: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:31:17.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8741" for this suite.
Dec  3 15:31:39.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:31:42.886: INFO: namespace pod-network-test-8741 deletion completed in 25.760899945s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:31:42.887: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec  3 15:31:43.525: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 15:31:43.705: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 15:31:43.794: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-31-164.ec2.internal before test
Dec  3 15:31:43.893: INFO: node-problem-detector-hnrtk from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:43.893: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:31:43.893: INFO: calico-typha-deploy-5547c4cdc6-zvk82 from kube-system started at 2019-12-03 14:32:23 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:43.893: INFO: 	Container calico-typha ready: true, restart count 0
Dec  3 15:31:43.893: INFO: calico-node-w5f9r from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:43.893: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:31:43.893: INFO: node-exporter-68n8d from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:43.893: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:31:43.893: INFO: blackbox-exporter-c87bdd467-vscd2 from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:43.893: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 15:31:43.893: INFO: kube-proxy-8xnnr from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:43.893: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:31:43.893: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-9-228.ec2.internal before test
Dec  3 15:31:44.180: INFO: calico-node-hvhtd from kube-system started at 2019-12-03 14:30:18 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.180: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:31:44.181: INFO: node-exporter-kklbs from kube-system started at 2019-12-03 14:30:18 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:31:44.181: INFO: addons-nginx-ingress-controller-8468678b64-x6f59 from kube-system started at 2019-12-03 14:30:32 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 15:31:44.181: INFO: kube-proxy-8j96c from kube-system started at 2019-12-03 14:30:18 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:31:44.181: INFO: metrics-server-bf696d85c-7v9h8 from kube-system started at 2019-12-03 14:30:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 15:31:44.181: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-nwxw7 from kube-system started at 2019-12-03 14:30:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 15:31:44.181: INFO: calico-typha-horizontal-autoscaler-554dfbfdd7-r9vld from kube-system started at 2019-12-03 14:30:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 15:31:44.181: INFO: coredns-858b686868-r84d9 from kube-system started at 2019-12-03 14:30:28 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:31:44.181: INFO: calico-typha-vertical-autoscaler-656557779f-zrt7j from kube-system started at 2019-12-03 14:30:32 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container autoscaler ready: true, restart count 3
Dec  3 15:31:44.181: INFO: addons-kubernetes-dashboard-5c8d9945bc-5mvxv from kube-system started at 2019-12-03 14:30:32 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 15:31:44.181: INFO: coredns-858b686868-lbh4p from kube-system started at 2019-12-03 14:30:32 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:31:44.181: INFO: node-problem-detector-bpps6 from kube-system started at 2019-12-03 14:30:18 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:31:44.181: INFO: vpn-shoot-859fb5c977-s5ldm from kube-system started at 2019-12-03 14:30:30 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 15:31:44.181: INFO: calico-kube-controllers-5d785bc598-746p6 from kube-system started at 2019-12-03 14:30:30 +0000 UTC (1 container statuses recorded)
Dec  3 15:31:44.181: INFO: 	Container calico-kube-controllers ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node ip-10-250-31-164.ec2.internal
STEP: verifying the node has the label node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod addons-kubernetes-dashboard-5c8d9945bc-5mvxv requesting resource cpu=50m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod addons-nginx-ingress-controller-8468678b64-x6f59 requesting resource cpu=100m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-nwxw7 requesting resource cpu=0m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod blackbox-exporter-c87bdd467-vscd2 requesting resource cpu=5m on Node ip-10-250-31-164.ec2.internal
Dec  3 15:31:44.729: INFO: Pod calico-kube-controllers-5d785bc598-746p6 requesting resource cpu=0m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod calico-node-hvhtd requesting resource cpu=100m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod calico-node-w5f9r requesting resource cpu=100m on Node ip-10-250-31-164.ec2.internal
Dec  3 15:31:44.729: INFO: Pod calico-typha-deploy-5547c4cdc6-zvk82 requesting resource cpu=0m on Node ip-10-250-31-164.ec2.internal
Dec  3 15:31:44.729: INFO: Pod calico-typha-horizontal-autoscaler-554dfbfdd7-r9vld requesting resource cpu=10m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod calico-typha-vertical-autoscaler-656557779f-zrt7j requesting resource cpu=0m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod coredns-858b686868-lbh4p requesting resource cpu=50m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod coredns-858b686868-r84d9 requesting resource cpu=50m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod kube-proxy-8j96c requesting resource cpu=20m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod kube-proxy-8xnnr requesting resource cpu=20m on Node ip-10-250-31-164.ec2.internal
Dec  3 15:31:44.729: INFO: Pod metrics-server-bf696d85c-7v9h8 requesting resource cpu=20m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod node-exporter-68n8d requesting resource cpu=5m on Node ip-10-250-31-164.ec2.internal
Dec  3 15:31:44.729: INFO: Pod node-exporter-kklbs requesting resource cpu=5m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod node-problem-detector-bpps6 requesting resource cpu=20m on Node ip-10-250-9-228.ec2.internal
Dec  3 15:31:44.729: INFO: Pod node-problem-detector-hnrtk requesting resource cpu=20m on Node ip-10-250-31-164.ec2.internal
Dec  3 15:31:44.729: INFO: Pod vpn-shoot-859fb5c977-s5ldm requesting resource cpu=100m on Node ip-10-250-9-228.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77b8c1d6-7928-4e96-8344-8d268696136d.15dce6399c6add47], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1208/filler-pod-77b8c1d6-7928-4e96-8344-8d268696136d to ip-10-250-31-164.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77b8c1d6-7928-4e96-8344-8d268696136d.15dce639c1d4370b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77b8c1d6-7928-4e96-8344-8d268696136d.15dce639c4656aa5], Reason = [Created], Message = [Created container filler-pod-77b8c1d6-7928-4e96-8344-8d268696136d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77b8c1d6-7928-4e96-8344-8d268696136d.15dce639c9c4a734], Reason = [Started], Message = [Started container filler-pod-77b8c1d6-7928-4e96-8344-8d268696136d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d30f6710-ca20-478c-b608-39bf01bfeb02.15dce639a1cc71b5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1208/filler-pod-d30f6710-ca20-478c-b608-39bf01bfeb02 to ip-10-250-9-228.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d30f6710-ca20-478c-b608-39bf01bfeb02.15dce639c6d3c2f6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d30f6710-ca20-478c-b608-39bf01bfeb02.15dce639caab251d], Reason = [Created], Message = [Created container filler-pod-d30f6710-ca20-478c-b608-39bf01bfeb02]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d30f6710-ca20-478c-b608-39bf01bfeb02.15dce639cfe3a702], Reason = [Started], Message = [Started container filler-pod-d30f6710-ca20-478c-b608-39bf01bfeb02]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15dce63a38fcf0cc], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node ip-10-250-31-164.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-250-9-228.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:31:49.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1208" for this suite.
Dec  3 15:31:55.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:31:58.925: INFO: namespace sched-pred-1208 deletion completed in 9.754911454s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:31:58.926: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-ac704eac-4bbb-4796-ab49-ef2f0182f656
STEP: Creating a pod to test consume secrets
Dec  3 15:31:59.745: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eee023bd-122e-45aa-8f13-4210d21e6412" in namespace "projected-343" to be "success or failure"
Dec  3 15:31:59.835: INFO: Pod "pod-projected-secrets-eee023bd-122e-45aa-8f13-4210d21e6412": Phase="Pending", Reason="", readiness=false. Elapsed: 89.221153ms
Dec  3 15:32:01.927: INFO: Pod "pod-projected-secrets-eee023bd-122e-45aa-8f13-4210d21e6412": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.181203607s
STEP: Saw pod success
Dec  3 15:32:01.927: INFO: Pod "pod-projected-secrets-eee023bd-122e-45aa-8f13-4210d21e6412" satisfied condition "success or failure"
Dec  3 15:32:02.016: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-secrets-eee023bd-122e-45aa-8f13-4210d21e6412 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:32:02.214: INFO: Waiting for pod pod-projected-secrets-eee023bd-122e-45aa-8f13-4210d21e6412 to disappear
Dec  3 15:32:02.303: INFO: Pod pod-projected-secrets-eee023bd-122e-45aa-8f13-4210d21e6412 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:32:02.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-343" for this suite.
Dec  3 15:32:08.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:32:12.144: INFO: namespace projected-343 deletion completed in 9.750968964s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:32:12.145: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:32:12.973: INFO: (0) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 95.022026ms)
Dec  3 15:32:13.065: INFO: (1) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.538877ms)
Dec  3 15:32:13.158: INFO: (2) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.032914ms)
Dec  3 15:32:13.249: INFO: (3) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.868428ms)
Dec  3 15:32:13.342: INFO: (4) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.277118ms)
Dec  3 15:32:13.435: INFO: (5) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.675782ms)
Dec  3 15:32:13.527: INFO: (6) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.205795ms)
Dec  3 15:32:13.619: INFO: (7) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.325886ms)
Dec  3 15:32:13.714: INFO: (8) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 94.9131ms)
Dec  3 15:32:13.806: INFO: (9) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.955356ms)
Dec  3 15:32:13.898: INFO: (10) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.937348ms)
Dec  3 15:32:13.990: INFO: (11) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.939391ms)
Dec  3 15:32:14.083: INFO: (12) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.442853ms)
Dec  3 15:32:14.176: INFO: (13) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.605524ms)
Dec  3 15:32:14.268: INFO: (14) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.105461ms)
Dec  3 15:32:14.360: INFO: (15) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.957473ms)
Dec  3 15:32:14.452: INFO: (16) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.528815ms)
Dec  3 15:32:14.544: INFO: (17) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.072597ms)
Dec  3 15:32:14.637: INFO: (18) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.526491ms)
Dec  3 15:32:14.730: INFO: (19) /api/v1/nodes/ip-10-250-31-164.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 93.104015ms)
[AfterEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:32:14.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6252" for this suite.
Dec  3 15:32:21.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:32:24.575: INFO: namespace proxy-6252 deletion completed in 9.7545361s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:32:24.575: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  3 15:32:28.344: INFO: Successfully updated pod "pod-update-4eb39dd0-29b3-44f9-a65d-76332a6e8281"
STEP: verifying the updated pod is in kubernetes
Dec  3 15:32:28.523: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:32:28.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-527" for this suite.
Dec  3 15:32:50.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:32:54.370: INFO: namespace pods-527 deletion completed in 25.755940487s
•SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:32:54.370: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec  3 15:32:55.010: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-2327'
Dec  3 15:32:56.279: INFO: stderr: ""
Dec  3 15:32:56.279: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 15:32:56.279: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2327'
Dec  3 15:32:56.703: INFO: stderr: ""
Dec  3 15:32:56.703: INFO: stdout: "update-demo-nautilus-dg8r5 update-demo-nautilus-s6zrc "
Dec  3 15:32:56.703: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-dg8r5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2327'
Dec  3 15:32:57.124: INFO: stderr: ""
Dec  3 15:32:57.124: INFO: stdout: ""
Dec  3 15:32:57.124: INFO: update-demo-nautilus-dg8r5 is created but not running
Dec  3 15:33:02.124: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2327'
Dec  3 15:33:02.551: INFO: stderr: ""
Dec  3 15:33:02.551: INFO: stdout: "update-demo-nautilus-dg8r5 update-demo-nautilus-s6zrc "
Dec  3 15:33:02.551: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-dg8r5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2327'
Dec  3 15:33:02.973: INFO: stderr: ""
Dec  3 15:33:02.973: INFO: stdout: "true"
Dec  3 15:33:02.973: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-dg8r5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2327'
Dec  3 15:33:03.407: INFO: stderr: ""
Dec  3 15:33:03.407: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 15:33:03.407: INFO: validating pod update-demo-nautilus-dg8r5
Dec  3 15:33:03.588: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 15:33:03.588: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 15:33:03.588: INFO: update-demo-nautilus-dg8r5 is verified up and running
Dec  3 15:33:03.588: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-s6zrc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2327'
Dec  3 15:33:04.009: INFO: stderr: ""
Dec  3 15:33:04.009: INFO: stdout: "true"
Dec  3 15:33:04.009: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-s6zrc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2327'
Dec  3 15:33:04.436: INFO: stderr: ""
Dec  3 15:33:04.437: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 15:33:04.437: INFO: validating pod update-demo-nautilus-s6zrc
Dec  3 15:33:04.616: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 15:33:04.616: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 15:33:04.616: INFO: update-demo-nautilus-s6zrc is verified up and running
STEP: using delete to clean up resources
Dec  3 15:33:04.616: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-2327'
Dec  3 15:33:05.129: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:33:05.129: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  3 15:33:05.129: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2327'
Dec  3 15:33:05.643: INFO: stderr: "No resources found.\n"
Dec  3 15:33:05.643: INFO: stdout: ""
Dec  3 15:33:05.643: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-2327 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 15:33:06.075: INFO: stderr: ""
Dec  3 15:33:06.075: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:33:06.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2327" for this suite.
Dec  3 15:33:28.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:33:31.918: INFO: namespace kubectl-2327 deletion completed in 25.75370283s
•SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:33:31.919: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-w8bg
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 15:33:32.830: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w8bg" in namespace "subpath-1268" to be "success or failure"
Dec  3 15:33:32.919: INFO: Pod "pod-subpath-test-configmap-w8bg": Phase="Pending", Reason="", readiness=false. Elapsed: 89.018636ms
Dec  3 15:33:35.009: INFO: Pod "pod-subpath-test-configmap-w8bg": Phase="Running", Reason="", readiness=true. Elapsed: 2.178750359s
Dec  3 15:33:37.099: INFO: Pod "pod-subpath-test-configmap-w8bg": Phase="Running", Reason="", readiness=true. Elapsed: 4.268457619s
Dec  3 15:33:39.189: INFO: Pod "pod-subpath-test-configmap-w8bg": Phase="Running", Reason="", readiness=true. Elapsed: 6.358419303s
Dec  3 15:33:41.279: INFO: Pod "pod-subpath-test-configmap-w8bg": Phase="Running", Reason="", readiness=true. Elapsed: 8.4487353s
Dec  3 15:33:43.369: INFO: Pod "pod-subpath-test-configmap-w8bg": Phase="Running", Reason="", readiness=true. Elapsed: 10.538750281s
Dec  3 15:33:45.459: INFO: Pod "pod-subpath-test-configmap-w8bg": Phase="Running", Reason="", readiness=true. Elapsed: 12.628615148s
Dec  3 15:33:47.549: INFO: Pod "pod-subpath-test-configmap-w8bg": Phase="Running", Reason="", readiness=true. Elapsed: 14.718662043s
Dec  3 15:33:49.639: INFO: Pod "pod-subpath-test-configmap-w8bg": Phase="Running", Reason="", readiness=true. Elapsed: 16.808631163s
Dec  3 15:33:51.729: INFO: Pod "pod-subpath-test-configmap-w8bg": Phase="Running", Reason="", readiness=true. Elapsed: 18.898776405s
Dec  3 15:33:53.819: INFO: Pod "pod-subpath-test-configmap-w8bg": Phase="Running", Reason="", readiness=true. Elapsed: 20.988919691s
Dec  3 15:33:55.909: INFO: Pod "pod-subpath-test-configmap-w8bg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.079047465s
STEP: Saw pod success
Dec  3 15:33:55.910: INFO: Pod "pod-subpath-test-configmap-w8bg" satisfied condition "success or failure"
Dec  3 15:33:55.999: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-subpath-test-configmap-w8bg container test-container-subpath-configmap-w8bg: <nil>
STEP: delete the pod
Dec  3 15:33:56.188: INFO: Waiting for pod pod-subpath-test-configmap-w8bg to disappear
Dec  3 15:33:56.277: INFO: Pod pod-subpath-test-configmap-w8bg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-w8bg
Dec  3 15:33:56.277: INFO: Deleting pod "pod-subpath-test-configmap-w8bg" in namespace "subpath-1268"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:33:56.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1268" for this suite.
Dec  3 15:34:02.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:06.233: INFO: namespace subpath-1268 deletion completed in 9.775813871s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:34:06.234: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  3 15:34:08.233: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:34:08.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8622" for this suite.
Dec  3 15:34:14.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:18.261: INFO: namespace container-runtime-8622 deletion completed in 9.755295024s
•SSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:34:18.261: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  3 15:34:18.991: INFO: Waiting up to 5m0s for pod "downward-api-3108d16b-2281-48ac-bd55-8a3a6905fcb9" in namespace "downward-api-9914" to be "success or failure"
Dec  3 15:34:19.081: INFO: Pod "downward-api-3108d16b-2281-48ac-bd55-8a3a6905fcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 89.249167ms
Dec  3 15:34:21.171: INFO: Pod "downward-api-3108d16b-2281-48ac-bd55-8a3a6905fcb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179212528s
STEP: Saw pod success
Dec  3 15:34:21.171: INFO: Pod "downward-api-3108d16b-2281-48ac-bd55-8a3a6905fcb9" satisfied condition "success or failure"
Dec  3 15:34:21.260: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downward-api-3108d16b-2281-48ac-bd55-8a3a6905fcb9 container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:34:21.449: INFO: Waiting for pod downward-api-3108d16b-2281-48ac-bd55-8a3a6905fcb9 to disappear
Dec  3 15:34:21.539: INFO: Pod downward-api-3108d16b-2281-48ac-bd55-8a3a6905fcb9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:34:21.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9914" for this suite.
Dec  3 15:34:27.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:31.387: INFO: namespace downward-api-9914 deletion completed in 9.757134939s
•SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:34:31.387: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1565
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec  3 15:34:34.566: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec  3 15:34:45.088: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:34:45.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1565" for this suite.
Dec  3 15:34:51.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:55.029: INFO: namespace pods-1565 deletion completed in 9.761546575s
•SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:34:55.030: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2402
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-8d13ddf7-694a-41a9-a91a-9492613c8b67
STEP: Creating a pod to test consume configMaps
Dec  3 15:34:55.848: INFO: Waiting up to 5m0s for pod "pod-configmaps-b0c0a79a-4945-4f09-ac17-6afa4ebad685" in namespace "configmap-2402" to be "success or failure"
Dec  3 15:34:55.938: INFO: Pod "pod-configmaps-b0c0a79a-4945-4f09-ac17-6afa4ebad685": Phase="Pending", Reason="", readiness=false. Elapsed: 89.362581ms
Dec  3 15:34:58.028: INFO: Pod "pod-configmaps-b0c0a79a-4945-4f09-ac17-6afa4ebad685": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179524651s
STEP: Saw pod success
Dec  3 15:34:58.028: INFO: Pod "pod-configmaps-b0c0a79a-4945-4f09-ac17-6afa4ebad685" satisfied condition "success or failure"
Dec  3 15:34:58.117: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-configmaps-b0c0a79a-4945-4f09-ac17-6afa4ebad685 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:34:58.307: INFO: Waiting for pod pod-configmaps-b0c0a79a-4945-4f09-ac17-6afa4ebad685 to disappear
Dec  3 15:34:58.397: INFO: Pod pod-configmaps-b0c0a79a-4945-4f09-ac17-6afa4ebad685 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:34:58.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2402" for this suite.
Dec  3 15:35:04.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:35:08.250: INFO: namespace configmap-2402 deletion completed in 9.762353936s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:35:08.251: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3591
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:35:08.981: INFO: Waiting up to 5m0s for pod "downwardapi-volume-048c5bb1-f8cd-42b0-8ef5-df5d48ee619f" in namespace "downward-api-3591" to be "success or failure"
Dec  3 15:35:09.070: INFO: Pod "downwardapi-volume-048c5bb1-f8cd-42b0-8ef5-df5d48ee619f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.733261ms
Dec  3 15:35:11.160: INFO: Pod "downwardapi-volume-048c5bb1-f8cd-42b0-8ef5-df5d48ee619f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179673726s
STEP: Saw pod success
Dec  3 15:35:11.160: INFO: Pod "downwardapi-volume-048c5bb1-f8cd-42b0-8ef5-df5d48ee619f" satisfied condition "success or failure"
Dec  3 15:35:11.250: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-048c5bb1-f8cd-42b0-8ef5-df5d48ee619f container client-container: <nil>
STEP: delete the pod
Dec  3 15:35:11.440: INFO: Waiting for pod downwardapi-volume-048c5bb1-f8cd-42b0-8ef5-df5d48ee619f to disappear
Dec  3 15:35:11.530: INFO: Pod downwardapi-volume-048c5bb1-f8cd-42b0-8ef5-df5d48ee619f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:35:11.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3591" for this suite.
Dec  3 15:35:17.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:35:21.375: INFO: namespace downward-api-3591 deletion completed in 9.75495282s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:35:21.376: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6506
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Dec  3 15:35:22.106: INFO: Waiting up to 5m0s for pod "client-containers-86cca7c1-6b51-4c3f-b50a-04d642b65282" in namespace "containers-6506" to be "success or failure"
Dec  3 15:35:22.195: INFO: Pod "client-containers-86cca7c1-6b51-4c3f-b50a-04d642b65282": Phase="Pending", Reason="", readiness=false. Elapsed: 89.507467ms
Dec  3 15:35:24.285: INFO: Pod "client-containers-86cca7c1-6b51-4c3f-b50a-04d642b65282": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17959771s
STEP: Saw pod success
Dec  3 15:35:24.285: INFO: Pod "client-containers-86cca7c1-6b51-4c3f-b50a-04d642b65282" satisfied condition "success or failure"
Dec  3 15:35:24.375: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod client-containers-86cca7c1-6b51-4c3f-b50a-04d642b65282 container test-container: <nil>
STEP: delete the pod
Dec  3 15:35:24.564: INFO: Waiting for pod client-containers-86cca7c1-6b51-4c3f-b50a-04d642b65282 to disappear
Dec  3 15:35:24.653: INFO: Pod client-containers-86cca7c1-6b51-4c3f-b50a-04d642b65282 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:35:24.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6506" for this suite.
Dec  3 15:35:31.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:35:34.498: INFO: namespace containers-6506 deletion completed in 9.754063521s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:35:34.498: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-7a395275-bd9e-40e0-8195-d5e72b93aaf6
STEP: Creating a pod to test consume configMaps
Dec  3 15:35:35.320: INFO: Waiting up to 5m0s for pod "pod-configmaps-8c1a2751-2e34-4f76-883c-082509605cb2" in namespace "configmap-8005" to be "success or failure"
Dec  3 15:35:35.409: INFO: Pod "pod-configmaps-8c1a2751-2e34-4f76-883c-082509605cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 88.990134ms
Dec  3 15:35:37.500: INFO: Pod "pod-configmaps-8c1a2751-2e34-4f76-883c-082509605cb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179864724s
STEP: Saw pod success
Dec  3 15:35:37.500: INFO: Pod "pod-configmaps-8c1a2751-2e34-4f76-883c-082509605cb2" satisfied condition "success or failure"
Dec  3 15:35:37.590: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-configmaps-8c1a2751-2e34-4f76-883c-082509605cb2 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:35:37.778: INFO: Waiting for pod pod-configmaps-8c1a2751-2e34-4f76-883c-082509605cb2 to disappear
Dec  3 15:35:37.867: INFO: Pod pod-configmaps-8c1a2751-2e34-4f76-883c-082509605cb2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:35:37.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8005" for this suite.
Dec  3 15:35:44.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:35:47.715: INFO: namespace configmap-8005 deletion completed in 9.757495526s
•S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:35:47.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6131
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:35:52.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6131" for this suite.
Dec  3 15:35:58.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:36:02.481: INFO: namespace kubelet-test-6131 deletion completed in 9.757064614s
•SS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:36:02.481: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8324
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-b49ba17b-2818-4c15-8e36-4050c45028c9
STEP: Creating configMap with name cm-test-opt-upd-7208342c-a4c0-44d5-9680-5e40a2b0a8e5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b49ba17b-2818-4c15-8e36-4050c45028c9
STEP: Updating configmap cm-test-opt-upd-7208342c-a4c0-44d5-9680-5e40a2b0a8e5
STEP: Creating configMap with name cm-test-opt-create-995474e3-2707-4320-b240-cd84d42fa15a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:37:13.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8324" for this suite.
Dec  3 15:37:36.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:37:39.544: INFO: namespace projected-8324 deletion completed in 25.750746548s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:37:39.545: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9006
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  3 15:37:51.449: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1203 15:37:51.449230    5065 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:37:51.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9006" for this suite.
Dec  3 15:37:57.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:38:01.296: INFO: namespace gc-9006 deletion completed in 9.757043446s
•SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:38:01.433: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec  3 15:38:05.221: INFO: Successfully updated pod "labelsupdate63602f8d-326f-4641-92a4-77a4e4954b3a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:38:07.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5166" for this suite.
Dec  3 15:38:29.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:38:33.265: INFO: namespace downward-api-5166 deletion completed in 25.760479833s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:38:33.267: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-0d18c64a-5166-4e3b-896d-b4a2bdc79ba3
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:38:33.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7404" for this suite.
Dec  3 15:38:40.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:38:43.838: INFO: namespace secrets-7404 deletion completed in 9.75489039s
•SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:38:43.838: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:38:44.568: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e592b665-3876-4db9-a35f-1c9e32d9266c" in namespace "projected-3153" to be "success or failure"
Dec  3 15:38:44.658: INFO: Pod "downwardapi-volume-e592b665-3876-4db9-a35f-1c9e32d9266c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.684661ms
Dec  3 15:38:46.748: INFO: Pod "downwardapi-volume-e592b665-3876-4db9-a35f-1c9e32d9266c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179550949s
STEP: Saw pod success
Dec  3 15:38:46.748: INFO: Pod "downwardapi-volume-e592b665-3876-4db9-a35f-1c9e32d9266c" satisfied condition "success or failure"
Dec  3 15:38:46.838: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-e592b665-3876-4db9-a35f-1c9e32d9266c container client-container: <nil>
STEP: delete the pod
Dec  3 15:38:47.028: INFO: Waiting for pod downwardapi-volume-e592b665-3876-4db9-a35f-1c9e32d9266c to disappear
Dec  3 15:38:47.120: INFO: Pod downwardapi-volume-e592b665-3876-4db9-a35f-1c9e32d9266c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:38:47.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3153" for this suite.
Dec  3 15:38:53.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:38:56.962: INFO: namespace projected-3153 deletion completed in 9.751638909s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:38:56.962: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3665
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-dc505ba5-4247-4736-bb7f-05e2041f602d
STEP: Creating a pod to test consume configMaps
Dec  3 15:38:57.782: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-665d4cca-86cb-40ea-9358-713a76a32cf8" in namespace "projected-3665" to be "success or failure"
Dec  3 15:38:57.872: INFO: Pod "pod-projected-configmaps-665d4cca-86cb-40ea-9358-713a76a32cf8": Phase="Pending", Reason="", readiness=false. Elapsed: 89.45577ms
Dec  3 15:38:59.962: INFO: Pod "pod-projected-configmaps-665d4cca-86cb-40ea-9358-713a76a32cf8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179659325s
STEP: Saw pod success
Dec  3 15:38:59.962: INFO: Pod "pod-projected-configmaps-665d4cca-86cb-40ea-9358-713a76a32cf8" satisfied condition "success or failure"
Dec  3 15:39:00.054: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-configmaps-665d4cca-86cb-40ea-9358-713a76a32cf8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:39:00.244: INFO: Waiting for pod pod-projected-configmaps-665d4cca-86cb-40ea-9358-713a76a32cf8 to disappear
Dec  3 15:39:00.333: INFO: Pod pod-projected-configmaps-665d4cca-86cb-40ea-9358-713a76a32cf8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:39:00.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3665" for this suite.
Dec  3 15:39:06.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:39:10.182: INFO: namespace projected-3665 deletion completed in 9.758116686s
•SSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:39:10.182: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-2919
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Dec  3 15:39:10.912: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2919" to be "success or failure"
Dec  3 15:39:11.002: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 89.25447ms
Dec  3 15:39:13.092: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179276415s
STEP: Saw pod success
Dec  3 15:39:13.092: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec  3 15:39:13.182: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec  3 15:39:13.370: INFO: Waiting for pod pod-host-path-test to disappear
Dec  3 15:39:13.459: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:39:13.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2919" for this suite.
Dec  3 15:39:19.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:39:23.328: INFO: namespace hostpath-2919 deletion completed in 9.778332419s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:39:23.330: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7072
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec  3 15:39:23.967: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7072'
Dec  3 15:39:24.591: INFO: stderr: ""
Dec  3 15:39:24.591: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  3 15:39:25.681: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:39:25.682: INFO: Found 1 / 1
Dec  3 15:39:25.682: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec  3 15:39:25.771: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:39:25.771: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  3 15:39:25.771: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config patch pod redis-master-btjq5 --namespace=kubectl-7072 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  3 15:39:26.288: INFO: stderr: ""
Dec  3 15:39:26.288: INFO: stdout: "pod/redis-master-btjq5 patched\n"
STEP: checking annotations
Dec  3 15:39:26.379: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:39:26.379: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:39:26.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7072" for this suite.
Dec  3 15:39:48.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:39:52.229: INFO: namespace kubectl-7072 deletion completed in 25.759782941s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:39:52.229: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  3 15:39:57.679: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 15:39:57.768: INFO: Pod pod-with-prestop-http-hook still exists
Dec  3 15:39:59.769: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 15:39:59.858: INFO: Pod pod-with-prestop-http-hook still exists
Dec  3 15:40:01.769: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 15:40:01.858: INFO: Pod pod-with-prestop-http-hook still exists
Dec  3 15:40:03.769: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 15:40:03.859: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:40:03.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9373" for this suite.
Dec  3 15:40:26.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:29.806: INFO: namespace container-lifecycle-hook-9373 deletion completed in 25.752742873s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:40:29.807: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec  3 15:40:30.451: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:40:34.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-135" for this suite.
Dec  3 15:40:56.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:41:00.002: INFO: namespace init-container-135 deletion completed in 25.763196674s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:41:00.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec  3 15:41:05.423: INFO: Pod name wrapped-volume-race-17864bfe-cebc-429b-a33a-01ed2541b56c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-17864bfe-cebc-429b-a33a-01ed2541b56c in namespace emptydir-wrapper-8333, will wait for the garbage collector to delete the pods
Dec  3 15:41:10.264: INFO: Deleting ReplicationController wrapped-volume-race-17864bfe-cebc-429b-a33a-01ed2541b56c took: 91.418253ms
Dec  3 15:41:10.665: INFO: Terminating ReplicationController wrapped-volume-race-17864bfe-cebc-429b-a33a-01ed2541b56c pods took: 400.327918ms
STEP: Creating RC which spawns configmap-volume pods
Dec  3 15:41:53.740: INFO: Pod name wrapped-volume-race-91127d03-8264-4170-a8d9-bd38660b0a69: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-91127d03-8264-4170-a8d9-bd38660b0a69 in namespace emptydir-wrapper-8333, will wait for the garbage collector to delete the pods
Dec  3 15:41:58.573: INFO: Deleting ReplicationController wrapped-volume-race-91127d03-8264-4170-a8d9-bd38660b0a69 took: 91.524622ms
Dec  3 15:41:58.974: INFO: Terminating ReplicationController wrapped-volume-race-91127d03-8264-4170-a8d9-bd38660b0a69 pods took: 400.392534ms
STEP: Creating RC which spawns configmap-volume pods
Dec  3 15:42:43.648: INFO: Pod name wrapped-volume-race-a63274f9-a1b7-4fc1-8f81-c54174787540: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a63274f9-a1b7-4fc1-8f81-c54174787540 in namespace emptydir-wrapper-8333, will wait for the garbage collector to delete the pods
Dec  3 15:42:46.480: INFO: Deleting ReplicationController wrapped-volume-race-a63274f9-a1b7-4fc1-8f81-c54174787540 took: 91.683665ms
Dec  3 15:42:46.881: INFO: Terminating ReplicationController wrapped-volume-race-a63274f9-a1b7-4fc1-8f81-c54174787540 pods took: 400.40725ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:43:28.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8333" for this suite.
Dec  3 15:43:34.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:43:37.884: INFO: namespace emptydir-wrapper-8333 deletion completed in 9.754251739s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:43:37.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  3 15:43:38.614: INFO: Waiting up to 5m0s for pod "pod-24c04c38-0827-4c55-b637-f354c8356820" in namespace "emptydir-5357" to be "success or failure"
Dec  3 15:43:38.704: INFO: Pod "pod-24c04c38-0827-4c55-b637-f354c8356820": Phase="Pending", Reason="", readiness=false. Elapsed: 89.366578ms
Dec  3 15:43:40.794: INFO: Pod "pod-24c04c38-0827-4c55-b637-f354c8356820": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179447051s
STEP: Saw pod success
Dec  3 15:43:40.794: INFO: Pod "pod-24c04c38-0827-4c55-b637-f354c8356820" satisfied condition "success or failure"
Dec  3 15:43:40.884: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-24c04c38-0827-4c55-b637-f354c8356820 container test-container: <nil>
STEP: delete the pod
Dec  3 15:43:41.072: INFO: Waiting for pod pod-24c04c38-0827-4c55-b637-f354c8356820 to disappear
Dec  3 15:43:41.162: INFO: Pod pod-24c04c38-0827-4c55-b637-f354c8356820 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:43:41.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5357" for this suite.
Dec  3 15:43:47.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:43:51.005: INFO: namespace emptydir-5357 deletion completed in 9.753514867s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:43:51.006: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3317
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3317
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec  3 15:43:51.914: INFO: Found 1 stateful pods, waiting for 3
Dec  3 15:44:02.006: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:44:02.006: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:44:02.006: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:44:02.277: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3317 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 15:44:04.173: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 15:44:04.173: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 15:44:04.173: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  3 15:44:14.723: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec  3 15:44:14.992: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3317 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:44:16.270: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 15:44:16.270: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 15:44:16.270: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 15:44:26.811: INFO: Waiting for StatefulSet statefulset-3317/ss2 to complete update
Dec  3 15:44:26.811: INFO: Waiting for Pod statefulset-3317/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 15:44:26.811: INFO: Waiting for Pod statefulset-3317/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Dec  3 15:44:36.992: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3317 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 15:44:38.286: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 15:44:38.287: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 15:44:38.287: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 15:44:48.836: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec  3 15:44:49.105: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3317 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 15:44:50.378: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 15:44:50.378: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 15:44:50.378: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 15:45:00.919: INFO: Waiting for StatefulSet statefulset-3317/ss2 to complete update
Dec  3 15:45:00.919: INFO: Waiting for Pod statefulset-3317/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  3 15:45:00.919: INFO: Waiting for Pod statefulset-3317/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  3 15:45:00.919: INFO: Waiting for Pod statefulset-3317/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  3 15:45:11.100: INFO: Waiting for StatefulSet statefulset-3317/ss2 to complete update
Dec  3 15:45:11.100: INFO: Waiting for Pod statefulset-3317/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  3 15:45:21.101: INFO: Deleting all statefulset in ns statefulset-3317
Dec  3 15:45:21.190: INFO: Scaling statefulset ss2 to 0
Dec  3 15:45:31.551: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:45:31.641: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:45:31.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3317" for this suite.
Dec  3 15:45:38.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:45:41.770: INFO: namespace statefulset-3317 deletion completed in 9.76613721s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:45:41.770: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9944
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-a0364c78-1805-4820-bc55-8efbb0364879
STEP: Creating a pod to test consume secrets
Dec  3 15:45:42.595: INFO: Waiting up to 5m0s for pod "pod-secrets-cdfdd912-ceeb-40bd-b950-62af8b627256" in namespace "secrets-9944" to be "success or failure"
Dec  3 15:45:42.685: INFO: Pod "pod-secrets-cdfdd912-ceeb-40bd-b950-62af8b627256": Phase="Pending", Reason="", readiness=false. Elapsed: 89.815137ms
Dec  3 15:45:44.775: INFO: Pod "pod-secrets-cdfdd912-ceeb-40bd-b950-62af8b627256": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179972672s
STEP: Saw pod success
Dec  3 15:45:44.775: INFO: Pod "pod-secrets-cdfdd912-ceeb-40bd-b950-62af8b627256" satisfied condition "success or failure"
Dec  3 15:45:44.865: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-secrets-cdfdd912-ceeb-40bd-b950-62af8b627256 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:45:45.059: INFO: Waiting for pod pod-secrets-cdfdd912-ceeb-40bd-b950-62af8b627256 to disappear
Dec  3 15:45:45.148: INFO: Pod pod-secrets-cdfdd912-ceeb-40bd-b950-62af8b627256 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:45:45.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9944" for this suite.
Dec  3 15:45:51.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:45:55.050: INFO: namespace secrets-9944 deletion completed in 9.808291165s
•SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:45:55.051: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9224
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  3 15:45:57.050: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:45:57.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9224" for this suite.
Dec  3 15:46:03.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:46:07.084: INFO: namespace container-runtime-9224 deletion completed in 9.759316915s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:46:07.084: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 15:46:07.723: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4968'
Dec  3 15:46:08.368: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 15:46:08.368: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Dec  3 15:46:08.458: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-4968'
Dec  3 15:46:09.057: INFO: stderr: ""
Dec  3 15:46:09.057: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:46:09.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4968" for this suite.
Dec  3 15:46:31.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:46:34.903: INFO: namespace kubectl-4968 deletion completed in 25.756179657s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:46:34.904: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5219
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-d56b
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 15:46:35.816: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-d56b" in namespace "subpath-5219" to be "success or failure"
Dec  3 15:46:35.905: INFO: Pod "pod-subpath-test-secret-d56b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.069542ms
Dec  3 15:46:37.995: INFO: Pod "pod-subpath-test-secret-d56b": Phase="Running", Reason="", readiness=true. Elapsed: 2.179316032s
Dec  3 15:46:40.085: INFO: Pod "pod-subpath-test-secret-d56b": Phase="Running", Reason="", readiness=true. Elapsed: 4.268988188s
Dec  3 15:46:42.175: INFO: Pod "pod-subpath-test-secret-d56b": Phase="Running", Reason="", readiness=true. Elapsed: 6.358863918s
Dec  3 15:46:44.265: INFO: Pod "pod-subpath-test-secret-d56b": Phase="Running", Reason="", readiness=true. Elapsed: 8.448632178s
Dec  3 15:46:46.355: INFO: Pod "pod-subpath-test-secret-d56b": Phase="Running", Reason="", readiness=true. Elapsed: 10.538898988s
Dec  3 15:46:48.445: INFO: Pod "pod-subpath-test-secret-d56b": Phase="Running", Reason="", readiness=true. Elapsed: 12.628828854s
Dec  3 15:46:50.535: INFO: Pod "pod-subpath-test-secret-d56b": Phase="Running", Reason="", readiness=true. Elapsed: 14.718881798s
Dec  3 15:46:52.627: INFO: Pod "pod-subpath-test-secret-d56b": Phase="Running", Reason="", readiness=true. Elapsed: 16.811320731s
Dec  3 15:46:54.717: INFO: Pod "pod-subpath-test-secret-d56b": Phase="Running", Reason="", readiness=true. Elapsed: 18.901299941s
Dec  3 15:46:56.807: INFO: Pod "pod-subpath-test-secret-d56b": Phase="Running", Reason="", readiness=true. Elapsed: 20.991231323s
Dec  3 15:46:58.898: INFO: Pod "pod-subpath-test-secret-d56b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.081714526s
STEP: Saw pod success
Dec  3 15:46:58.898: INFO: Pod "pod-subpath-test-secret-d56b" satisfied condition "success or failure"
Dec  3 15:46:58.987: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-subpath-test-secret-d56b container test-container-subpath-secret-d56b: <nil>
STEP: delete the pod
Dec  3 15:46:59.176: INFO: Waiting for pod pod-subpath-test-secret-d56b to disappear
Dec  3 15:46:59.265: INFO: Pod pod-subpath-test-secret-d56b no longer exists
STEP: Deleting pod pod-subpath-test-secret-d56b
Dec  3 15:46:59.265: INFO: Deleting pod "pod-subpath-test-secret-d56b" in namespace "subpath-5219"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:46:59.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5219" for this suite.
Dec  3 15:47:05.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:47:09.206: INFO: namespace subpath-5219 deletion completed in 9.760217568s
•SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:47:09.206: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6367
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec  3 15:47:10.025: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:47:10.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6367" for this suite.
Dec  3 15:47:16.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:47:20.145: INFO: namespace replication-controller-6367 deletion completed in 9.760375276s
•S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:47:20.145: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  3 15:47:23.234: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:47:23.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1169" for this suite.
Dec  3 15:47:29.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:47:33.264: INFO: namespace container-runtime-1169 deletion completed in 9.757497964s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:47:33.264: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4250
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4250.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4250.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 15:47:36.630: INFO: DNS probes using dns-test-3573830b-bbe9-4f8b-ac31-d3e5ab602893 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4250.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4250.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 15:47:39.444: INFO: File wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local from pod  dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 15:47:39.538: INFO: File jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local from pod  dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 15:47:39.538: INFO: Lookups using dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 failed for: [wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local]

Dec  3 15:47:44.633: INFO: File wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local from pod  dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 15:47:44.726: INFO: File jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local from pod  dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 15:47:44.726: INFO: Lookups using dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 failed for: [wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local]

Dec  3 15:47:49.633: INFO: File wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local from pod  dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 15:47:49.727: INFO: File jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local from pod  dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 15:47:49.727: INFO: Lookups using dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 failed for: [wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local]

Dec  3 15:47:54.632: INFO: File wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local from pod  dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 15:47:54.727: INFO: File jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local from pod  dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 15:47:54.727: INFO: Lookups using dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 failed for: [wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local]

Dec  3 15:47:59.631: INFO: File wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local from pod  dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 15:47:59.725: INFO: File jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local from pod  dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 15:47:59.725: INFO: Lookups using dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 failed for: [wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local]

Dec  3 15:48:04.631: INFO: File wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local from pod  dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 15:48:04.766: INFO: File jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local from pod  dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 15:48:04.766: INFO: Lookups using dns-4250/dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 failed for: [wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local]

Dec  3 15:48:09.725: INFO: DNS probes using dns-test-cbd36fa6-8428-4d82-83f7-2748888b1884 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4250.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4250.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4250.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4250.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 15:48:12.726: INFO: DNS probes using dns-test-2c29274e-c4db-4c2c-8b41-e32c5b69c280 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:48:12.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4250" for this suite.
Dec  3 15:48:19.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:48:22.761: INFO: namespace dns-4250 deletion completed in 9.75696786s
•
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:48:22.761: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2661
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  3 15:48:23.491: INFO: Waiting up to 5m0s for pod "pod-0919fee0-e51e-468d-b3c2-9face256ff42" in namespace "emptydir-2661" to be "success or failure"
Dec  3 15:48:23.581: INFO: Pod "pod-0919fee0-e51e-468d-b3c2-9face256ff42": Phase="Pending", Reason="", readiness=false. Elapsed: 89.388025ms
Dec  3 15:48:25.671: INFO: Pod "pod-0919fee0-e51e-468d-b3c2-9face256ff42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179215216s
STEP: Saw pod success
Dec  3 15:48:25.671: INFO: Pod "pod-0919fee0-e51e-468d-b3c2-9face256ff42" satisfied condition "success or failure"
Dec  3 15:48:25.760: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-0919fee0-e51e-468d-b3c2-9face256ff42 container test-container: <nil>
STEP: delete the pod
Dec  3 15:48:25.950: INFO: Waiting for pod pod-0919fee0-e51e-468d-b3c2-9face256ff42 to disappear
Dec  3 15:48:26.039: INFO: Pod pod-0919fee0-e51e-468d-b3c2-9face256ff42 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:48:26.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2661" for this suite.
Dec  3 15:48:32.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:48:35.890: INFO: namespace emptydir-2661 deletion completed in 9.760259933s
•SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:48:35.891: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Dec  3 15:48:36.529: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9881 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec  3 15:48:39.992: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec  3 15:48:39.992: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:48:42.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9881" for this suite.
Dec  3 15:48:48.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:48:52.018: INFO: namespace kubectl-9881 deletion completed in 9.754906737s
•SSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:48:52.018: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6531
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:48:52.656: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:48:55.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6531" for this suite.
Dec  3 15:49:45.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:49:49.232: INFO: namespace pods-6531 deletion completed in 53.755393738s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:49:49.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4555
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Dec  3 15:49:49.871: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4555'
Dec  3 15:49:50.483: INFO: stderr: ""
Dec  3 15:49:50.483: INFO: stdout: "pod/pause created\n"
Dec  3 15:49:50.483: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  3 15:49:50.484: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4555" to be "running and ready"
Dec  3 15:49:50.573: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 89.725902ms
Dec  3 15:49:52.663: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.179682346s
Dec  3 15:49:52.663: INFO: Pod "pause" satisfied condition "running and ready"
Dec  3 15:49:52.663: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Dec  3 15:49:52.663: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-4555'
Dec  3 15:49:53.180: INFO: stderr: ""
Dec  3 15:49:53.180: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec  3 15:49:53.180: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-4555'
Dec  3 15:49:53.607: INFO: stderr: ""
Dec  3 15:49:53.607: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec  3 15:49:53.607: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label- --namespace=kubectl-4555'
Dec  3 15:49:54.131: INFO: stderr: ""
Dec  3 15:49:54.131: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec  3 15:49:54.131: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-4555'
Dec  3 15:49:54.551: INFO: stderr: ""
Dec  3 15:49:54.551: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Dec  3 15:49:54.551: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4555'
Dec  3 15:49:55.065: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:49:55.065: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  3 15:49:55.065: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-4555'
Dec  3 15:49:55.581: INFO: stderr: "No resources found.\n"
Dec  3 15:49:55.581: INFO: stdout: ""
Dec  3 15:49:55.581: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=pause --namespace=kubectl-4555 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 15:49:56.005: INFO: stderr: ""
Dec  3 15:49:56.005: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:49:56.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4555" for this suite.
Dec  3 15:50:02.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:50:05.855: INFO: namespace kubectl-4555 deletion completed in 9.759278125s
•SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:50:05.855: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-49b4d844-e503-4434-80ef-5c85aa582d1e
STEP: Creating a pod to test consume secrets
Dec  3 15:50:06.675: INFO: Waiting up to 5m0s for pod "pod-secrets-fd0da31d-82c6-46bf-9977-bb6ee6fe13f2" in namespace "secrets-2325" to be "success or failure"
Dec  3 15:50:06.765: INFO: Pod "pod-secrets-fd0da31d-82c6-46bf-9977-bb6ee6fe13f2": Phase="Pending", Reason="", readiness=false. Elapsed: 89.416505ms
Dec  3 15:50:08.855: INFO: Pod "pod-secrets-fd0da31d-82c6-46bf-9977-bb6ee6fe13f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179848168s
STEP: Saw pod success
Dec  3 15:50:08.855: INFO: Pod "pod-secrets-fd0da31d-82c6-46bf-9977-bb6ee6fe13f2" satisfied condition "success or failure"
Dec  3 15:50:08.945: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-secrets-fd0da31d-82c6-46bf-9977-bb6ee6fe13f2 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:50:09.134: INFO: Waiting for pod pod-secrets-fd0da31d-82c6-46bf-9977-bb6ee6fe13f2 to disappear
Dec  3 15:50:09.223: INFO: Pod pod-secrets-fd0da31d-82c6-46bf-9977-bb6ee6fe13f2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:50:09.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2325" for this suite.
Dec  3 15:50:15.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:50:19.074: INFO: namespace secrets-2325 deletion completed in 9.759695219s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:50:19.075: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9632
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-85928dc9-b428-40a4-8f74-4dcb0d841e1b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:50:22.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9632" for this suite.
Dec  3 15:50:44.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:50:48.289: INFO: namespace configmap-9632 deletion completed in 25.751802147s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:50:48.290: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:50:49.111: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  3 15:50:51.290: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  3 15:50:54.007: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-5671,SelfLink:/apis/apps/v1/namespaces/deployment-5671/deployments/test-cleanup-deployment,UID:b0354173-868a-442f-9eea-619f80e8a612,ResourceVersion:15366,Generation:1,CreationTimestamp:2019-12-03 15:50:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-03 15:50:51 +0000 UTC 2019-12-03 15:50:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-03 15:50:52 +0000 UTC 2019-12-03 15:50:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  3 15:50:54.098: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-5671,SelfLink:/apis/apps/v1/namespaces/deployment-5671/replicasets/test-cleanup-deployment-55bbcbc84c,UID:eeb09e00-e539-410e-9330-36af309541c0,ResourceVersion:15359,Generation:1,CreationTimestamp:2019-12-03 15:50:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment b0354173-868a-442f-9eea-619f80e8a612 0xc002e6edd7 0xc002e6edd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  3 15:50:54.189: INFO: Pod "test-cleanup-deployment-55bbcbc84c-l4zml" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-l4zml,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-5671,SelfLink:/api/v1/namespaces/deployment-5671/pods/test-cleanup-deployment-55bbcbc84c-l4zml,UID:5fb690e2-4e00-47f3-91b0-2abd7f558708,ResourceVersion:15358,Generation:0,CreationTimestamp:2019-12-03 15:50:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.190/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c eeb09e00-e539-410e-9330-36af309541c0 0xc001abc007 0xc001abc008}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pl6qq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pl6qq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-pl6qq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001abc0d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001abc160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:51 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:50:51 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:100.64.0.190,StartTime:2019-12-03 15:50:51 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-03 15:50:52 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://ecc84b7980ee8828b8f2a649e70424644138b4baccae546bb808cf63ab575e03}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:50:54.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5671" for this suite.
Dec  3 15:51:00.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:04.036: INFO: namespace deployment-5671 deletion completed in 9.757075982s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:51:04.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-2438/configmap-test-fd47c8ed-d9d3-4523-95f4-e2f1059011b0
STEP: Creating a pod to test consume configMaps
Dec  3 15:51:04.856: INFO: Waiting up to 5m0s for pod "pod-configmaps-4f795cb7-f4f7-4656-9f0c-160fcc88d53d" in namespace "configmap-2438" to be "success or failure"
Dec  3 15:51:04.946: INFO: Pod "pod-configmaps-4f795cb7-f4f7-4656-9f0c-160fcc88d53d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.687966ms
Dec  3 15:51:07.036: INFO: Pod "pod-configmaps-4f795cb7-f4f7-4656-9f0c-160fcc88d53d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179601336s
STEP: Saw pod success
Dec  3 15:51:07.036: INFO: Pod "pod-configmaps-4f795cb7-f4f7-4656-9f0c-160fcc88d53d" satisfied condition "success or failure"
Dec  3 15:51:07.125: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-configmaps-4f795cb7-f4f7-4656-9f0c-160fcc88d53d container env-test: <nil>
STEP: delete the pod
Dec  3 15:51:07.391: INFO: Waiting for pod pod-configmaps-4f795cb7-f4f7-4656-9f0c-160fcc88d53d to disappear
Dec  3 15:51:07.480: INFO: Pod pod-configmaps-4f795cb7-f4f7-4656-9f0c-160fcc88d53d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:51:07.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2438" for this suite.
Dec  3 15:51:13.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:17.328: INFO: namespace configmap-2438 deletion completed in 9.757942431s
•SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:51:17.329: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:51:17.967: INFO: Creating deployment "nginx-deployment"
Dec  3 15:51:18.057: INFO: Waiting for observed generation 1
Dec  3 15:51:18.146: INFO: Waiting for all required pods to come up
Dec  3 15:51:18.236: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec  3 15:51:22.416: INFO: Waiting for deployment "nginx-deployment" to complete
Dec  3 15:51:22.596: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec  3 15:51:22.775: INFO: Updating deployment nginx-deployment
Dec  3 15:51:22.775: INFO: Waiting for observed generation 2
Dec  3 15:51:22.864: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  3 15:51:22.954: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  3 15:51:23.043: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  3 15:51:23.312: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  3 15:51:23.312: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  3 15:51:23.401: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  3 15:51:23.579: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec  3 15:51:23.579: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec  3 15:51:23.759: INFO: Updating deployment nginx-deployment
Dec  3 15:51:23.759: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec  3 15:51:23.937: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  3 15:51:24.026: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  3 15:51:24.205: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-577,SelfLink:/apis/apps/v1/namespaces/deployment-577/deployments/nginx-deployment,UID:86dc8714-cbe3-4fd4-9bb4-70bae7f3a9bb,ResourceVersion:15634,Generation:3,CreationTimestamp:2019-12-03 15:51:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-12-03 15:51:23 +0000 UTC 2019-12-03 15:51:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-03 15:51:23 +0000 UTC 2019-12-03 15:51:18 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec  3 15:51:24.296: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-577,SelfLink:/apis/apps/v1/namespaces/deployment-577/replicasets/nginx-deployment-55fb7cb77f,UID:94b9f0f6-1c8d-43cf-86fc-f3587284a902,ResourceVersion:15632,Generation:3,CreationTimestamp:2019-12-03 15:51:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 86dc8714-cbe3-4fd4-9bb4-70bae7f3a9bb 0xc0019a64f7 0xc0019a64f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 15:51:24.296: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec  3 15:51:24.296: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-577,SelfLink:/apis/apps/v1/namespaces/deployment-577/replicasets/nginx-deployment-7b8c6f4498,UID:b7db3305-b071-42e4-8240-0e2171805115,ResourceVersion:15627,Generation:3,CreationTimestamp:2019-12-03 15:51:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 86dc8714-cbe3-4fd4-9bb4-70bae7f3a9bb 0xc0019a6667 0xc0019a6668}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec  3 15:51:24.389: INFO: Pod "nginx-deployment-55fb7cb77f-2dtwf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2dtwf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-2dtwf,UID:06cf9bd7-0683-4f5a-9c29-e647232c25f3,ResourceVersion:15650,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243a9b7 0xc00243a9b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243aa20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243aa40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.389: INFO: Pod "nginx-deployment-55fb7cb77f-855r8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-855r8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-855r8,UID:87d8d623-ff7d-437c-8577-3c45ed0d5d46,ResourceVersion:15656,Generation:0,CreationTimestamp:2019-12-03 15:51:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.201/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243ab20 0xc00243ab21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243aba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243abc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.389: INFO: Pod "nginx-deployment-55fb7cb77f-8prgn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8prgn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-8prgn,UID:6822be85-07fe-403d-98e5-4622a004900f,ResourceVersion:15648,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243ac90 0xc00243ac91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243ad00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243ad20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.389: INFO: Pod "nginx-deployment-55fb7cb77f-c62dd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-c62dd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-c62dd,UID:f8f84361-b175-4f29-8032-5b59f21ad256,ResourceVersion:15635,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243adf0 0xc00243adf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243ae70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243ae90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.389: INFO: Pod "nginx-deployment-55fb7cb77f-cgn9b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cgn9b,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-cgn9b,UID:89a5afa6-7d2e-41a0-97f1-f390b2ad60d3,ResourceVersion:15654,Generation:0,CreationTimestamp:2019-12-03 15:51:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.200/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243af70 0xc00243af71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243afe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243b000}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.389: INFO: Pod "nginx-deployment-55fb7cb77f-chmmf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-chmmf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-chmmf,UID:daeb0457-a5b2-4269-a82f-108c41ba2621,ResourceVersion:15649,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243b0d0 0xc00243b0d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243b140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243b160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.389: INFO: Pod "nginx-deployment-55fb7cb77f-cj6qv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cj6qv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-cj6qv,UID:05f9cda3-f0f7-4c1a-a700-ebf68ca3c1e0,ResourceVersion:15642,Generation:0,CreationTimestamp:2019-12-03 15:51:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.199/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243b240 0xc00243b241}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243b2b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243b2d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.389: INFO: Pod "nginx-deployment-55fb7cb77f-dfcjz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-dfcjz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-dfcjz,UID:58ed6771-3033-48ff-8702-e754a71c4397,ResourceVersion:15651,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243b3a0 0xc00243b3a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243b410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243b430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.390: INFO: Pod "nginx-deployment-55fb7cb77f-j7xmv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-j7xmv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-j7xmv,UID:8f0bd5ae-38d6-4f57-bd4d-89985cb8ceb0,ResourceVersion:15638,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243b500 0xc00243b501}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243b570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243b590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.390: INFO: Pod "nginx-deployment-55fb7cb77f-pdbnx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pdbnx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-pdbnx,UID:d3f049bd-f477-42d3-80e7-e66939a00981,ResourceVersion:15645,Generation:0,CreationTimestamp:2019-12-03 15:51:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.38/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243b670 0xc00243b671}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243b6e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243b700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:100.64.1.38,StartTime:2019-12-03 15:51:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.390: INFO: Pod "nginx-deployment-55fb7cb77f-wp8jk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wp8jk,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-wp8jk,UID:91870aa5-4d45-4fc0-9d16-ab645aad247d,ResourceVersion:15631,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243b7f0 0xc00243b7f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243b860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243b890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.390: INFO: Pod "nginx-deployment-55fb7cb77f-xj5pv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xj5pv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-xj5pv,UID:346f0d64-e00e-45a1-b5e7-a98b1451b6fb,ResourceVersion:15633,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243b970 0xc00243b971}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243b9e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243ba00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.390: INFO: Pod "nginx-deployment-55fb7cb77f-z9sdv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-z9sdv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-55fb7cb77f-z9sdv,UID:fac00d8d-8aaf-4b86-a27c-7b7f830db7ff,ResourceVersion:15644,Generation:0,CreationTimestamp:2019-12-03 15:51:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.39/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 94b9f0f6-1c8d-43cf-86fc-f3587284a902 0xc00243bae0 0xc00243bae1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243bb50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243bb70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:22 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:100.64.1.39,StartTime:2019-12-03 15:51:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.390: INFO: Pod "nginx-deployment-7b8c6f4498-7t2l9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7t2l9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-7t2l9,UID:bd52b820-1de0-45ac-975a-eade8fd6b860,ResourceVersion:15643,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc00243bc60 0xc00243bc61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243bcc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243bce0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.390: INFO: Pod "nginx-deployment-7b8c6f4498-dptzp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dptzp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-dptzp,UID:ddfdf657-a944-407a-a07c-485731b23d16,ResourceVersion:15630,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc00243bda7 0xc00243bda8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243be10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243be30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.391: INFO: Pod "nginx-deployment-7b8c6f4498-fgcpk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fgcpk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-fgcpk,UID:5c7225de-8cae-4fe4-b85e-0c5be799efa9,ResourceVersion:15534,Generation:0,CreationTimestamp:2019-12-03 15:51:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.198/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc00243bf07 0xc00243bf08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00243bf70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00243bfb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:100.64.0.198,StartTime:2019-12-03 15:51:18 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:51:20 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1ed5807b4cc97f027b8fecdcf011636154682257f4400e1cfc2517e82e1caa62}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.391: INFO: Pod "nginx-deployment-7b8c6f4498-gfqmj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gfqmj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-gfqmj,UID:b96eaeab-31ce-42da-8ca6-9b648dae293c,ResourceVersion:15531,Generation:0,CreationTimestamp:2019-12-03 15:51:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.192/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af2097 0xc001af2098}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af2100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af2120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:100.64.0.192,StartTime:2019-12-03 15:51:18 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:51:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9a45b4218f777fd95c7e0dfaf9ec62c83d99b527bdab0b83a784d2b76808a5db}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.391: INFO: Pod "nginx-deployment-7b8c6f4498-kw8fk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kw8fk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-kw8fk,UID:6b800d2a-1401-4e1b-939c-e2e9c27bd6cd,ResourceVersion:15629,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af21f7 0xc001af21f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af2260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af2280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.391: INFO: Pod "nginx-deployment-7b8c6f4498-l8kmn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-l8kmn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-l8kmn,UID:b979880d-1ee9-4a65-81d8-6850300c64f4,ResourceVersion:15641,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af2347 0xc001af2348}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af23b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af23d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.391: INFO: Pod "nginx-deployment-7b8c6f4498-llqw4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-llqw4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-llqw4,UID:4112a7c1-f98d-4b6f-a39a-224f8d6c553b,ResourceVersion:15636,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af24b7 0xc001af24b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af2530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af2550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.391: INFO: Pod "nginx-deployment-7b8c6f4498-p74nf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-p74nf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-p74nf,UID:09c07119-91fa-408b-8be2-46283ec8a5aa,ResourceVersion:15505,Generation:0,CreationTimestamp:2019-12-03 15:51:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.36/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af2627 0xc001af2628}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af2690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af26b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:100.64.1.36,StartTime:2019-12-03 15:51:18 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:51:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f908c4522a7302920545e9adf2c7d85e1fb702077525cda541ce5efb87529c55}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.391: INFO: Pod "nginx-deployment-7b8c6f4498-pqj2d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-pqj2d,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-pqj2d,UID:3217972d-c5fe-45cb-920d-557bdb40888d,ResourceVersion:15640,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af2780 0xc001af2781}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af27e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af2800}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.391: INFO: Pod "nginx-deployment-7b8c6f4498-qcfjb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qcfjb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-qcfjb,UID:d5d45a25-8b2a-4023-a01c-9ed0606c9e71,ResourceVersion:15653,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af28c7 0xc001af28c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af2930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af2950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.391: INFO: Pod "nginx-deployment-7b8c6f4498-qlhz9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qlhz9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-qlhz9,UID:a500be6b-07f1-4d50-addf-7790f64d6cdc,ResourceVersion:15637,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af2a17 0xc001af2a18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af2a80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af2aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.391: INFO: Pod "nginx-deployment-7b8c6f4498-qn7j9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-qn7j9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-qn7j9,UID:f9f86bb0-2f63-4b1d-aff6-2b45a2e14f0c,ResourceVersion:15516,Generation:0,CreationTimestamp:2019-12-03 15:51:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.197/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af2b97 0xc001af2b98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af2c00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af2c20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:100.64.0.197,StartTime:2019-12-03 15:51:18 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:51:20 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4ac116937a1b1e71978544ec77432c3681ccb1bef15a6a4909247d853be60986}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.392: INFO: Pod "nginx-deployment-7b8c6f4498-rmjhn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rmjhn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-rmjhn,UID:b72a4f41-e11a-495b-b7f8-05614a1975f6,ResourceVersion:15508,Generation:0,CreationTimestamp:2019-12-03 15:51:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.35/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af2d07 0xc001af2d08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af2d70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af2d90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:100.64.1.35,StartTime:2019-12-03 15:51:18 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:51:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4ad9e1a89953f40dcdedfecf493f81aab7cdeda4bf16e8f83e6d7e14f48aea07}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.392: INFO: Pod "nginx-deployment-7b8c6f4498-rpzth" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rpzth,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-rpzth,UID:8ccbbc39-b96f-4d80-a2c1-d4f54d241fbe,ResourceVersion:15519,Generation:0,CreationTimestamp:2019-12-03 15:51:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.194/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af2e70 0xc001af2e71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af2ed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af2ef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:100.64.0.194,StartTime:2019-12-03 15:51:18 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:51:20 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8c5858bc2627974e1ee68ba546ecb0ccfd15b46b31aa826cc23259ef0115b53c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.392: INFO: Pod "nginx-deployment-7b8c6f4498-sjn6s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sjn6s,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-sjn6s,UID:969756a6-d9ed-44a5-99dd-8d4703ea5c71,ResourceVersion:15639,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af2fc7 0xc001af2fc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af3030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af3050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.392: INFO: Pod "nginx-deployment-7b8c6f4498-tcnm8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tcnm8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-tcnm8,UID:50f449c9-33d2-42dd-8931-f1fb426fa299,ResourceVersion:15525,Generation:0,CreationTimestamp:2019-12-03 15:51:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.196/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af3127 0xc001af3128}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af3190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af31b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:100.64.0.196,StartTime:2019-12-03 15:51:18 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:51:20 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2d216c44b68f3e197ef087897e8a593fc9b9e592c2bea915384c509274d7f55e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.392: INFO: Pod "nginx-deployment-7b8c6f4498-tgl5q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tgl5q,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-tgl5q,UID:8f399560-7579-485b-9d17-099c2c23fd02,ResourceVersion:15647,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af3287 0xc001af3288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af32f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af3310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.392: INFO: Pod "nginx-deployment-7b8c6f4498-vmqmh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vmqmh,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-vmqmh,UID:6197a3b7-9214-432a-a825-51c04172e4ed,ResourceVersion:15652,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af33d7 0xc001af33d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af3450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af3470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.392: INFO: Pod "nginx-deployment-7b8c6f4498-w7vzf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-w7vzf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-w7vzf,UID:6ca244c2-4032-40e8-b223-9789e80680be,ResourceVersion:15646,Generation:0,CreationTimestamp:2019-12-03 15:51:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af3537 0xc001af3538}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af35a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af35c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 15:51:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  3 15:51:24.392: INFO: Pod "nginx-deployment-7b8c6f4498-xbmxs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xbmxs,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-577,SelfLink:/api/v1/namespaces/deployment-577/pods/nginx-deployment-7b8c6f4498-xbmxs,UID:70570240-53b3-450f-bfb8-c87c1b1a50c7,ResourceVersion:15511,Generation:0,CreationTimestamp:2019-12-03 15:51:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.37/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 b7db3305-b071-42e4-8240-0e2171805115 0xc001af3697 0xc001af3698}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d9hjj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d9hjj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d9hjj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-9-228.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001af3700} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001af3720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 15:51:18 +0000 UTC  }],Message:,Reason:,HostIP:10.250.9.228,PodIP:100.64.1.37,StartTime:2019-12-03 15:51:18 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-03 15:51:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://91ac663732658922a4066973f80b36436e656332ff36f37f05574031da11b410}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:51:24.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-577" for this suite.
Dec  3 15:51:30.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:34.228: INFO: namespace deployment-577 deletion completed in 9.745693199s
•SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:51:34.229: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-cdd09899-1fee-4457-b429-42cbc2be7e09
STEP: Creating a pod to test consume configMaps
Dec  3 15:51:35.050: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-577cc9c1-b4f8-434b-b821-38543ee61a47" in namespace "projected-7932" to be "success or failure"
Dec  3 15:51:35.139: INFO: Pod "pod-projected-configmaps-577cc9c1-b4f8-434b-b821-38543ee61a47": Phase="Pending", Reason="", readiness=false. Elapsed: 89.318365ms
Dec  3 15:51:37.229: INFO: Pod "pod-projected-configmaps-577cc9c1-b4f8-434b-b821-38543ee61a47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179006605s
STEP: Saw pod success
Dec  3 15:51:37.229: INFO: Pod "pod-projected-configmaps-577cc9c1-b4f8-434b-b821-38543ee61a47" satisfied condition "success or failure"
Dec  3 15:51:37.318: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-configmaps-577cc9c1-b4f8-434b-b821-38543ee61a47 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:51:37.508: INFO: Waiting for pod pod-projected-configmaps-577cc9c1-b4f8-434b-b821-38543ee61a47 to disappear
Dec  3 15:51:37.597: INFO: Pod pod-projected-configmaps-577cc9c1-b4f8-434b-b821-38543ee61a47 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:51:37.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7932" for this suite.
Dec  3 15:51:43.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:47.458: INFO: namespace projected-7932 deletion completed in 9.771012398s
•SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:51:47.458: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-d083b6fe-271f-4c57-bd1c-b2656e0e5957
STEP: Creating a pod to test consume configMaps
Dec  3 15:51:48.293: INFO: Waiting up to 5m0s for pod "pod-configmaps-bba6dbf4-c957-48c5-a9db-416027edf6c4" in namespace "configmap-8021" to be "success or failure"
Dec  3 15:51:48.383: INFO: Pod "pod-configmaps-bba6dbf4-c957-48c5-a9db-416027edf6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 89.36989ms
Dec  3 15:51:50.473: INFO: Pod "pod-configmaps-bba6dbf4-c957-48c5-a9db-416027edf6c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179293026s
STEP: Saw pod success
Dec  3 15:51:50.473: INFO: Pod "pod-configmaps-bba6dbf4-c957-48c5-a9db-416027edf6c4" satisfied condition "success or failure"
Dec  3 15:51:50.565: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-configmaps-bba6dbf4-c957-48c5-a9db-416027edf6c4 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:51:50.755: INFO: Waiting for pod pod-configmaps-bba6dbf4-c957-48c5-a9db-416027edf6c4 to disappear
Dec  3 15:51:50.844: INFO: Pod pod-configmaps-bba6dbf4-c957-48c5-a9db-416027edf6c4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:51:50.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8021" for this suite.
Dec  3 15:51:57.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:52:00.703: INFO: namespace configmap-8021 deletion completed in 9.76757995s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:52:00.703: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3648
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  3 15:52:01.435: INFO: Waiting up to 5m0s for pod "pod-8f284c8c-2ec9-46ba-afde-6679e9093448" in namespace "emptydir-3648" to be "success or failure"
Dec  3 15:52:01.525: INFO: Pod "pod-8f284c8c-2ec9-46ba-afde-6679e9093448": Phase="Pending", Reason="", readiness=false. Elapsed: 89.593172ms
Dec  3 15:52:03.615: INFO: Pod "pod-8f284c8c-2ec9-46ba-afde-6679e9093448": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17953251s
STEP: Saw pod success
Dec  3 15:52:03.615: INFO: Pod "pod-8f284c8c-2ec9-46ba-afde-6679e9093448" satisfied condition "success or failure"
Dec  3 15:52:03.705: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-8f284c8c-2ec9-46ba-afde-6679e9093448 container test-container: <nil>
STEP: delete the pod
Dec  3 15:52:03.895: INFO: Waiting for pod pod-8f284c8c-2ec9-46ba-afde-6679e9093448 to disappear
Dec  3 15:52:03.984: INFO: Pod pod-8f284c8c-2ec9-46ba-afde-6679e9093448 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:52:03.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3648" for this suite.
Dec  3 15:52:10.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:52:13.831: INFO: namespace emptydir-3648 deletion completed in 9.757080526s
•S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:52:13.831: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2223
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:52:14.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2223" for this suite.
Dec  3 15:52:20.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:52:24.405: INFO: namespace services-2223 deletion completed in 9.754099486s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:52:24.406: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:52:25.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b7f1482-a042-46b7-bd89-e8b5ae79a4ae" in namespace "downward-api-4261" to be "success or failure"
Dec  3 15:52:25.228: INFO: Pod "downwardapi-volume-7b7f1482-a042-46b7-bd89-e8b5ae79a4ae": Phase="Pending", Reason="", readiness=false. Elapsed: 89.632303ms
Dec  3 15:52:27.318: INFO: Pod "downwardapi-volume-7b7f1482-a042-46b7-bd89-e8b5ae79a4ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179472702s
STEP: Saw pod success
Dec  3 15:52:27.318: INFO: Pod "downwardapi-volume-7b7f1482-a042-46b7-bd89-e8b5ae79a4ae" satisfied condition "success or failure"
Dec  3 15:52:27.407: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-7b7f1482-a042-46b7-bd89-e8b5ae79a4ae container client-container: <nil>
STEP: delete the pod
Dec  3 15:52:27.597: INFO: Waiting for pod downwardapi-volume-7b7f1482-a042-46b7-bd89-e8b5ae79a4ae to disappear
Dec  3 15:52:27.686: INFO: Pod downwardapi-volume-7b7f1482-a042-46b7-bd89-e8b5ae79a4ae no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:52:27.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4261" for this suite.
Dec  3 15:52:34.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:52:37.528: INFO: namespace downward-api-4261 deletion completed in 9.751053494s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:52:37.528: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7813
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7813
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec  3 15:52:38.437: INFO: Found 1 stateful pods, waiting for 3
Dec  3 15:52:48.528: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:52:48.529: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:52:48.529: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  3 15:52:48.988: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec  3 15:52:49.358: INFO: Updating stateful set ss2
Dec  3 15:52:49.537: INFO: Waiting for Pod statefulset-7813/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Dec  3 15:52:59.994: INFO: Found 2 stateful pods, waiting for 3
Dec  3 15:53:10.086: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:53:10.086: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:53:10.086: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec  3 15:53:10.455: INFO: Updating stateful set ss2
Dec  3 15:53:10.635: INFO: Waiting for Pod statefulset-7813/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 15:53:21.006: INFO: Updating stateful set ss2
Dec  3 15:53:21.185: INFO: Waiting for StatefulSet statefulset-7813/ss2 to complete update
Dec  3 15:53:21.185: INFO: Waiting for Pod statefulset-7813/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  3 15:53:31.366: INFO: Waiting for StatefulSet statefulset-7813/ss2 to complete update
Dec  3 15:53:31.366: INFO: Waiting for Pod statefulset-7813/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  3 15:53:41.366: INFO: Deleting all statefulset in ns statefulset-7813
Dec  3 15:53:41.455: INFO: Scaling statefulset ss2 to 0
Dec  3 15:54:11.815: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:54:11.905: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:54:12.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7813" for this suite.
Dec  3 15:54:18.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:54:22.021: INFO: namespace statefulset-7813 deletion completed in 9.755116411s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:54:22.021: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2970
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:54:25.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2970" for this suite.
Dec  3 15:54:47.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:54:51.047: INFO: namespace replication-controller-2970 deletion completed in 25.750282234s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:54:51.047: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:55:51.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5519" for this suite.
Dec  3 15:56:14.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:56:17.743: INFO: namespace container-probe-5519 deletion completed in 25.746914896s
•SSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:56:17.743: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4086
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-97118862-382d-4bbb-afbb-c00ae08aa678
STEP: Creating secret with name s-test-opt-upd-e244990f-7e2a-4ecb-9d5b-72cfbbf0b993
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-97118862-382d-4bbb-afbb-c00ae08aa678
STEP: Updating secret s-test-opt-upd-e244990f-7e2a-4ecb-9d5b-72cfbbf0b993
STEP: Creating secret with name s-test-opt-create-f597cbcb-894e-4e8e-962a-bd246a11370a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:57:31.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4086" for this suite.
Dec  3 15:57:53.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:57:56.884: INFO: namespace projected-4086 deletion completed in 25.756514246s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:57:56.885: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1102
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  3 15:57:57.616: INFO: Waiting up to 5m0s for pod "downward-api-f5200a24-9951-4cee-8f50-fd64bfa9675e" in namespace "downward-api-1102" to be "success or failure"
Dec  3 15:57:57.705: INFO: Pod "downward-api-f5200a24-9951-4cee-8f50-fd64bfa9675e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.252856ms
Dec  3 15:57:59.795: INFO: Pod "downward-api-f5200a24-9951-4cee-8f50-fd64bfa9675e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179335396s
STEP: Saw pod success
Dec  3 15:57:59.796: INFO: Pod "downward-api-f5200a24-9951-4cee-8f50-fd64bfa9675e" satisfied condition "success or failure"
Dec  3 15:57:59.885: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downward-api-f5200a24-9951-4cee-8f50-fd64bfa9675e container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:58:00.079: INFO: Waiting for pod downward-api-f5200a24-9951-4cee-8f50-fd64bfa9675e to disappear
Dec  3 15:58:00.168: INFO: Pod downward-api-f5200a24-9951-4cee-8f50-fd64bfa9675e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:58:00.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1102" for this suite.
Dec  3 15:58:06.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:58:10.019: INFO: namespace downward-api-1102 deletion completed in 9.759793829s
•SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:58:10.019: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9772
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 15:58:10.841: INFO: (0) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 94.891892ms)
Dec  3 15:58:10.934: INFO: (1) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.109777ms)
Dec  3 15:58:11.026: INFO: (2) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.252121ms)
Dec  3 15:58:11.119: INFO: (3) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.704258ms)
Dec  3 15:58:11.211: INFO: (4) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.225272ms)
Dec  3 15:58:11.304: INFO: (5) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.491108ms)
Dec  3 15:58:11.396: INFO: (6) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.063071ms)
Dec  3 15:58:11.488: INFO: (7) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.237814ms)
Dec  3 15:58:11.580: INFO: (8) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.338919ms)
Dec  3 15:58:11.673: INFO: (9) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.536277ms)
Dec  3 15:58:11.765: INFO: (10) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.920025ms)
Dec  3 15:58:11.857: INFO: (11) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.084926ms)
Dec  3 15:58:11.950: INFO: (12) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.564421ms)
Dec  3 15:58:12.042: INFO: (13) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.426923ms)
Dec  3 15:58:12.135: INFO: (14) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.756967ms)
Dec  3 15:58:12.228: INFO: (15) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.529322ms)
Dec  3 15:58:12.321: INFO: (16) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 93.219472ms)
Dec  3 15:58:12.413: INFO: (17) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.209891ms)
Dec  3 15:58:12.506: INFO: (18) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.809943ms)
Dec  3 15:58:12.599: INFO: (19) /api/v1/nodes/ip-10-250-31-164.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 93.120788ms)
[AfterEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:58:12.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9772" for this suite.
Dec  3 15:58:18.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:58:22.443: INFO: namespace proxy-9772 deletion completed in 9.752547184s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:58:22.443: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-623
I1203 15:58:23.172243    5065 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-623, replica count: 1
I1203 15:58:24.272875    5065 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 15:58:25.273181    5065 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  3 15:58:25.467: INFO: Created: latency-svc-vznzv
Dec  3 15:58:25.470: INFO: Got endpoints: latency-svc-vznzv [96.422057ms]
Dec  3 15:58:25.564: INFO: Created: latency-svc-cq2bs
Dec  3 15:58:25.567: INFO: Got endpoints: latency-svc-cq2bs [97.67676ms]
Dec  3 15:58:25.567: INFO: Created: latency-svc-t6bxw
Dec  3 15:58:25.569: INFO: Got endpoints: latency-svc-t6bxw [99.174744ms]
Dec  3 15:58:25.650: INFO: Created: latency-svc-np5vm
Dec  3 15:58:25.655: INFO: Created: latency-svc-q7wml
Dec  3 15:58:25.655: INFO: Got endpoints: latency-svc-np5vm [185.376951ms]
Dec  3 15:58:25.659: INFO: Created: latency-svc-bxwjz
Dec  3 15:58:25.659: INFO: Got endpoints: latency-svc-q7wml [189.345971ms]
Dec  3 15:58:25.660: INFO: Got endpoints: latency-svc-bxwjz [190.35464ms]
Dec  3 15:58:25.663: INFO: Created: latency-svc-mzw22
Dec  3 15:58:25.664: INFO: Got endpoints: latency-svc-mzw22 [194.547923ms]
Dec  3 15:58:25.667: INFO: Created: latency-svc-kb2w4
Dec  3 15:58:25.671: INFO: Got endpoints: latency-svc-kb2w4 [201.312229ms]
Dec  3 15:58:25.675: INFO: Created: latency-svc-zsqns
Dec  3 15:58:25.676: INFO: Got endpoints: latency-svc-zsqns [206.561822ms]
Dec  3 15:58:25.679: INFO: Created: latency-svc-cj8tb
Dec  3 15:58:25.681: INFO: Got endpoints: latency-svc-cj8tb [210.887998ms]
Dec  3 15:58:25.684: INFO: Created: latency-svc-4pglx
Dec  3 15:58:25.685: INFO: Got endpoints: latency-svc-4pglx [215.116318ms]
Dec  3 15:58:25.692: INFO: Created: latency-svc-hrbhq
Dec  3 15:58:25.696: INFO: Got endpoints: latency-svc-hrbhq [225.637788ms]
Dec  3 15:58:25.696: INFO: Created: latency-svc-hm9pl
Dec  3 15:58:25.698: INFO: Got endpoints: latency-svc-hm9pl [227.615434ms]
Dec  3 15:58:25.703: INFO: Created: latency-svc-jkgm4
Dec  3 15:58:25.706: INFO: Created: latency-svc-69m2m
Dec  3 15:58:25.707: INFO: Got endpoints: latency-svc-jkgm4 [236.589639ms]
Dec  3 15:58:25.710: INFO: Created: latency-svc-qvk5m
Dec  3 15:58:25.711: INFO: Got endpoints: latency-svc-69m2m [240.597417ms]
Dec  3 15:58:25.714: INFO: Got endpoints: latency-svc-qvk5m [244.205208ms]
Dec  3 15:58:25.714: INFO: Created: latency-svc-fzt8k
Dec  3 15:58:25.716: INFO: Got endpoints: latency-svc-fzt8k [148.989904ms]
Dec  3 15:58:25.718: INFO: Created: latency-svc-wmw72
Dec  3 15:58:25.720: INFO: Got endpoints: latency-svc-wmw72 [150.653145ms]
Dec  3 15:58:25.752: INFO: Created: latency-svc-lmqh4
Dec  3 15:58:25.757: INFO: Created: latency-svc-vpp9x
Dec  3 15:58:25.757: INFO: Got endpoints: latency-svc-lmqh4 [97.71214ms]
Dec  3 15:58:25.758: INFO: Got endpoints: latency-svc-vpp9x [102.764421ms]
Dec  3 15:58:25.761: INFO: Created: latency-svc-p7dbs
Dec  3 15:58:25.763: INFO: Got endpoints: latency-svc-p7dbs [102.532441ms]
Dec  3 15:58:25.766: INFO: Created: latency-svc-5qscw
Dec  3 15:58:25.767: INFO: Got endpoints: latency-svc-5qscw [102.466803ms]
Dec  3 15:58:25.770: INFO: Created: latency-svc-vld89
Dec  3 15:58:25.772: INFO: Got endpoints: latency-svc-vld89 [100.486841ms]
Dec  3 15:58:25.780: INFO: Created: latency-svc-w4sgx
Dec  3 15:58:25.783: INFO: Got endpoints: latency-svc-w4sgx [106.035328ms]
Dec  3 15:58:25.785: INFO: Created: latency-svc-xlz4r
Dec  3 15:58:25.787: INFO: Got endpoints: latency-svc-xlz4r [105.931791ms]
Dec  3 15:58:25.840: INFO: Created: latency-svc-m9rrh
Dec  3 15:58:25.841: INFO: Got endpoints: latency-svc-m9rrh [156.217097ms]
Dec  3 15:58:25.844: INFO: Created: latency-svc-9r56z
Dec  3 15:58:25.849: INFO: Got endpoints: latency-svc-9r56z [128.782558ms]
Dec  3 15:58:25.849: INFO: Created: latency-svc-dlg5v
Dec  3 15:58:25.856: INFO: Created: latency-svc-9dzxc
Dec  3 15:58:25.856: INFO: Got endpoints: latency-svc-9dzxc [145.863654ms]
Dec  3 15:58:25.857: INFO: Got endpoints: latency-svc-dlg5v [149.94057ms]
Dec  3 15:58:25.857: INFO: Created: latency-svc-xs758
Dec  3 15:58:25.858: INFO: Got endpoints: latency-svc-xs758 [141.821753ms]
Dec  3 15:58:25.861: INFO: Created: latency-svc-29j66
Dec  3 15:58:25.865: INFO: Created: latency-svc-9v2wk
Dec  3 15:58:25.865: INFO: Got endpoints: latency-svc-29j66 [150.932659ms]
Dec  3 15:58:25.867: INFO: Got endpoints: latency-svc-9v2wk [169.093953ms]
Dec  3 15:58:25.869: INFO: Created: latency-svc-kgwjx
Dec  3 15:58:25.870: INFO: Got endpoints: latency-svc-kgwjx [174.680576ms]
Dec  3 15:58:25.874: INFO: Created: latency-svc-5dvxx
Dec  3 15:58:25.877: INFO: Got endpoints: latency-svc-5dvxx [120.127561ms]
Dec  3 15:58:25.878: INFO: Created: latency-svc-vm5pp
Dec  3 15:58:25.879: INFO: Got endpoints: latency-svc-vm5pp [120.893929ms]
Dec  3 15:58:25.882: INFO: Created: latency-svc-zr8rc
Dec  3 15:58:25.884: INFO: Got endpoints: latency-svc-zr8rc [120.824874ms]
Dec  3 15:58:25.886: INFO: Created: latency-svc-5kkjz
Dec  3 15:58:25.888: INFO: Got endpoints: latency-svc-5kkjz [120.605577ms]
Dec  3 15:58:25.890: INFO: Created: latency-svc-zsbkm
Dec  3 15:58:25.895: INFO: Created: latency-svc-v5d4x
Dec  3 15:58:25.903: INFO: Created: latency-svc-2q6w8
Dec  3 15:58:25.915: INFO: Got endpoints: latency-svc-zsbkm [143.564057ms]
Dec  3 15:58:25.935: INFO: Created: latency-svc-2j8b9
Dec  3 15:58:25.942: INFO: Created: latency-svc-q7sfv
Dec  3 15:58:25.950: INFO: Created: latency-svc-pspbf
Dec  3 15:58:25.954: INFO: Created: latency-svc-cv48z
Dec  3 15:58:25.958: INFO: Created: latency-svc-xlvrt
Dec  3 15:58:25.962: INFO: Created: latency-svc-7gnsx
Dec  3 15:58:25.966: INFO: Got endpoints: latency-svc-v5d4x [183.013183ms]
Dec  3 15:58:25.966: INFO: Created: latency-svc-78nfc
Dec  3 15:58:25.970: INFO: Created: latency-svc-m8m8j
Dec  3 15:58:25.974: INFO: Created: latency-svc-6r6wm
Dec  3 15:58:25.978: INFO: Created: latency-svc-kn26t
Dec  3 15:58:25.982: INFO: Created: latency-svc-724qg
Dec  3 15:58:25.986: INFO: Created: latency-svc-kk6xl
Dec  3 15:58:26.013: INFO: Created: latency-svc-jkqcl
Dec  3 15:58:26.015: INFO: Got endpoints: latency-svc-2q6w8 [228.549589ms]
Dec  3 15:58:26.060: INFO: Created: latency-svc-l6bbn
Dec  3 15:58:26.065: INFO: Got endpoints: latency-svc-2j8b9 [224.090417ms]
Dec  3 15:58:26.109: INFO: Created: latency-svc-4mqwx
Dec  3 15:58:26.116: INFO: Got endpoints: latency-svc-q7sfv [266.933809ms]
Dec  3 15:58:26.161: INFO: Created: latency-svc-4v66l
Dec  3 15:58:26.165: INFO: Got endpoints: latency-svc-pspbf [308.865934ms]
Dec  3 15:58:26.209: INFO: Created: latency-svc-kz5zb
Dec  3 15:58:26.218: INFO: Got endpoints: latency-svc-cv48z [361.341773ms]
Dec  3 15:58:26.259: INFO: Created: latency-svc-dl944
Dec  3 15:58:26.266: INFO: Got endpoints: latency-svc-xlvrt [407.38094ms]
Dec  3 15:58:26.315: INFO: Got endpoints: latency-svc-7gnsx [450.294868ms]
Dec  3 15:58:26.322: INFO: Created: latency-svc-zcnqt
Dec  3 15:58:26.363: INFO: Created: latency-svc-548z8
Dec  3 15:58:26.366: INFO: Got endpoints: latency-svc-78nfc [499.063806ms]
Dec  3 15:58:26.410: INFO: Created: latency-svc-pvjgk
Dec  3 15:58:26.415: INFO: Got endpoints: latency-svc-m8m8j [545.117737ms]
Dec  3 15:58:26.464: INFO: Created: latency-svc-4svkn
Dec  3 15:58:26.465: INFO: Got endpoints: latency-svc-6r6wm [588.096053ms]
Dec  3 15:58:26.509: INFO: Created: latency-svc-6m84w
Dec  3 15:58:26.516: INFO: Got endpoints: latency-svc-kn26t [636.72018ms]
Dec  3 15:58:26.559: INFO: Created: latency-svc-plj59
Dec  3 15:58:26.565: INFO: Got endpoints: latency-svc-724qg [677.720802ms]
Dec  3 15:58:26.610: INFO: Created: latency-svc-9vts6
Dec  3 15:58:26.616: INFO: Got endpoints: latency-svc-kk6xl [732.063621ms]
Dec  3 15:58:26.659: INFO: Created: latency-svc-2pkln
Dec  3 15:58:26.665: INFO: Got endpoints: latency-svc-jkqcl [749.954645ms]
Dec  3 15:58:26.709: INFO: Created: latency-svc-v2rzp
Dec  3 15:58:26.715: INFO: Got endpoints: latency-svc-l6bbn [749.759627ms]
Dec  3 15:58:26.760: INFO: Created: latency-svc-45579
Dec  3 15:58:26.765: INFO: Got endpoints: latency-svc-4mqwx [750.061331ms]
Dec  3 15:58:26.811: INFO: Created: latency-svc-fzscc
Dec  3 15:58:26.815: INFO: Got endpoints: latency-svc-4v66l [749.771479ms]
Dec  3 15:58:26.859: INFO: Created: latency-svc-wddz7
Dec  3 15:58:26.865: INFO: Got endpoints: latency-svc-kz5zb [749.787872ms]
Dec  3 15:58:26.910: INFO: Created: latency-svc-rqkns
Dec  3 15:58:26.916: INFO: Got endpoints: latency-svc-dl944 [750.055115ms]
Dec  3 15:58:26.960: INFO: Created: latency-svc-d448t
Dec  3 15:58:26.965: INFO: Got endpoints: latency-svc-zcnqt [747.408678ms]
Dec  3 15:58:27.009: INFO: Created: latency-svc-qwthr
Dec  3 15:58:27.015: INFO: Got endpoints: latency-svc-548z8 [749.655599ms]
Dec  3 15:58:27.059: INFO: Created: latency-svc-v95bg
Dec  3 15:58:27.066: INFO: Got endpoints: latency-svc-pvjgk [749.949916ms]
Dec  3 15:58:27.109: INFO: Created: latency-svc-zxmmg
Dec  3 15:58:27.116: INFO: Got endpoints: latency-svc-4svkn [749.635645ms]
Dec  3 15:58:27.160: INFO: Created: latency-svc-5lb6m
Dec  3 15:58:27.167: INFO: Got endpoints: latency-svc-6m84w [751.613598ms]
Dec  3 15:58:27.210: INFO: Created: latency-svc-f2v6b
Dec  3 15:58:27.215: INFO: Got endpoints: latency-svc-plj59 [750.016097ms]
Dec  3 15:58:27.261: INFO: Created: latency-svc-zkrjx
Dec  3 15:58:27.265: INFO: Got endpoints: latency-svc-9vts6 [749.583111ms]
Dec  3 15:58:27.309: INFO: Created: latency-svc-sllsh
Dec  3 15:58:27.315: INFO: Got endpoints: latency-svc-2pkln [749.877996ms]
Dec  3 15:58:27.361: INFO: Created: latency-svc-ff729
Dec  3 15:58:27.365: INFO: Got endpoints: latency-svc-v2rzp [749.588729ms]
Dec  3 15:58:27.410: INFO: Created: latency-svc-xj9ld
Dec  3 15:58:27.415: INFO: Got endpoints: latency-svc-45579 [750.010814ms]
Dec  3 15:58:27.459: INFO: Created: latency-svc-n8ftg
Dec  3 15:58:27.465: INFO: Got endpoints: latency-svc-fzscc [749.845672ms]
Dec  3 15:58:27.509: INFO: Created: latency-svc-22nmn
Dec  3 15:58:27.516: INFO: Got endpoints: latency-svc-wddz7 [750.016669ms]
Dec  3 15:58:27.560: INFO: Created: latency-svc-dktnf
Dec  3 15:58:27.569: INFO: Got endpoints: latency-svc-rqkns [753.170802ms]
Dec  3 15:58:27.609: INFO: Created: latency-svc-zlzp6
Dec  3 15:58:27.615: INFO: Got endpoints: latency-svc-d448t [749.995169ms]
Dec  3 15:58:27.664: INFO: Created: latency-svc-8jnf5
Dec  3 15:58:27.665: INFO: Got endpoints: latency-svc-qwthr [749.612915ms]
Dec  3 15:58:27.710: INFO: Created: latency-svc-67zch
Dec  3 15:58:27.715: INFO: Got endpoints: latency-svc-v95bg [749.770756ms]
Dec  3 15:58:27.759: INFO: Created: latency-svc-pvjdv
Dec  3 15:58:27.765: INFO: Got endpoints: latency-svc-zxmmg [749.883503ms]
Dec  3 15:58:27.809: INFO: Created: latency-svc-2h4ch
Dec  3 15:58:27.816: INFO: Got endpoints: latency-svc-5lb6m [750.133314ms]
Dec  3 15:58:27.860: INFO: Created: latency-svc-9bf69
Dec  3 15:58:27.865: INFO: Got endpoints: latency-svc-f2v6b [749.707034ms]
Dec  3 15:58:27.910: INFO: Created: latency-svc-lp9tc
Dec  3 15:58:27.915: INFO: Got endpoints: latency-svc-zkrjx [747.993522ms]
Dec  3 15:58:27.959: INFO: Created: latency-svc-lfhq5
Dec  3 15:58:27.966: INFO: Got endpoints: latency-svc-sllsh [750.074201ms]
Dec  3 15:58:28.008: INFO: Created: latency-svc-hw695
Dec  3 15:58:28.015: INFO: Got endpoints: latency-svc-ff729 [749.946288ms]
Dec  3 15:58:28.060: INFO: Created: latency-svc-97xsx
Dec  3 15:58:28.065: INFO: Got endpoints: latency-svc-xj9ld [749.860684ms]
Dec  3 15:58:28.108: INFO: Created: latency-svc-79snr
Dec  3 15:58:28.115: INFO: Got endpoints: latency-svc-n8ftg [749.854767ms]
Dec  3 15:58:28.159: INFO: Created: latency-svc-9lnhw
Dec  3 15:58:28.165: INFO: Got endpoints: latency-svc-22nmn [749.770045ms]
Dec  3 15:58:28.220: INFO: Got endpoints: latency-svc-dktnf [755.022076ms]
Dec  3 15:58:28.222: INFO: Created: latency-svc-7kzkm
Dec  3 15:58:28.258: INFO: Created: latency-svc-8f5rc
Dec  3 15:58:28.265: INFO: Got endpoints: latency-svc-zlzp6 [749.739449ms]
Dec  3 15:58:28.315: INFO: Created: latency-svc-75rm4
Dec  3 15:58:28.316: INFO: Got endpoints: latency-svc-8jnf5 [747.025555ms]
Dec  3 15:58:28.360: INFO: Created: latency-svc-c2r8p
Dec  3 15:58:28.365: INFO: Got endpoints: latency-svc-67zch [749.82258ms]
Dec  3 15:58:28.409: INFO: Created: latency-svc-kw7kk
Dec  3 15:58:28.416: INFO: Got endpoints: latency-svc-pvjdv [750.317882ms]
Dec  3 15:58:28.459: INFO: Created: latency-svc-tkd54
Dec  3 15:58:28.465: INFO: Got endpoints: latency-svc-2h4ch [750.00612ms]
Dec  3 15:58:28.509: INFO: Created: latency-svc-c55jp
Dec  3 15:58:28.516: INFO: Got endpoints: latency-svc-9bf69 [750.366166ms]
Dec  3 15:58:28.559: INFO: Created: latency-svc-wqwt5
Dec  3 15:58:28.566: INFO: Got endpoints: latency-svc-lp9tc [749.706492ms]
Dec  3 15:58:28.614: INFO: Created: latency-svc-bbnrp
Dec  3 15:58:28.615: INFO: Got endpoints: latency-svc-lfhq5 [749.950711ms]
Dec  3 15:58:28.659: INFO: Created: latency-svc-5hvd2
Dec  3 15:58:28.666: INFO: Got endpoints: latency-svc-hw695 [750.36284ms]
Dec  3 15:58:28.713: INFO: Created: latency-svc-h9pzb
Dec  3 15:58:28.715: INFO: Got endpoints: latency-svc-97xsx [749.622459ms]
Dec  3 15:58:28.759: INFO: Created: latency-svc-94g28
Dec  3 15:58:28.765: INFO: Got endpoints: latency-svc-79snr [749.923879ms]
Dec  3 15:58:28.809: INFO: Created: latency-svc-cbccq
Dec  3 15:58:28.817: INFO: Got endpoints: latency-svc-9lnhw [751.936223ms]
Dec  3 15:58:28.860: INFO: Created: latency-svc-hztdm
Dec  3 15:58:28.865: INFO: Got endpoints: latency-svc-7kzkm [749.957007ms]
Dec  3 15:58:28.912: INFO: Created: latency-svc-w8h8d
Dec  3 15:58:28.915: INFO: Got endpoints: latency-svc-8f5rc [749.992389ms]
Dec  3 15:58:28.959: INFO: Created: latency-svc-zcdlf
Dec  3 15:58:28.967: INFO: Got endpoints: latency-svc-75rm4 [746.145278ms]
Dec  3 15:58:29.010: INFO: Created: latency-svc-hkxxh
Dec  3 15:58:29.016: INFO: Got endpoints: latency-svc-c2r8p [750.152303ms]
Dec  3 15:58:29.061: INFO: Created: latency-svc-qgdx9
Dec  3 15:58:29.066: INFO: Got endpoints: latency-svc-kw7kk [750.159161ms]
Dec  3 15:58:29.120: INFO: Created: latency-svc-p55hx
Dec  3 15:58:29.120: INFO: Got endpoints: latency-svc-tkd54 [754.834395ms]
Dec  3 15:58:29.160: INFO: Created: latency-svc-5wsdx
Dec  3 15:58:29.165: INFO: Got endpoints: latency-svc-c55jp [749.757403ms]
Dec  3 15:58:29.214: INFO: Created: latency-svc-v4nb8
Dec  3 15:58:29.215: INFO: Got endpoints: latency-svc-wqwt5 [750.132678ms]
Dec  3 15:58:29.260: INFO: Created: latency-svc-726tc
Dec  3 15:58:29.265: INFO: Got endpoints: latency-svc-bbnrp [749.369617ms]
Dec  3 15:58:29.309: INFO: Created: latency-svc-4xfq5
Dec  3 15:58:29.316: INFO: Got endpoints: latency-svc-5hvd2 [750.399516ms]
Dec  3 15:58:29.359: INFO: Created: latency-svc-c7hx4
Dec  3 15:58:29.366: INFO: Got endpoints: latency-svc-h9pzb [750.327447ms]
Dec  3 15:58:29.410: INFO: Created: latency-svc-t4tf6
Dec  3 15:58:29.416: INFO: Got endpoints: latency-svc-94g28 [750.051433ms]
Dec  3 15:58:29.461: INFO: Created: latency-svc-9m7bc
Dec  3 15:58:29.466: INFO: Got endpoints: latency-svc-cbccq [750.204353ms]
Dec  3 15:58:29.510: INFO: Created: latency-svc-twb4q
Dec  3 15:58:29.516: INFO: Got endpoints: latency-svc-hztdm [750.672031ms]
Dec  3 15:58:29.559: INFO: Created: latency-svc-shgp5
Dec  3 15:58:29.565: INFO: Got endpoints: latency-svc-w8h8d [747.988952ms]
Dec  3 15:58:29.610: INFO: Created: latency-svc-47b59
Dec  3 15:58:29.615: INFO: Got endpoints: latency-svc-zcdlf [749.935553ms]
Dec  3 15:58:29.660: INFO: Created: latency-svc-94wb6
Dec  3 15:58:29.665: INFO: Got endpoints: latency-svc-hkxxh [749.944484ms]
Dec  3 15:58:29.709: INFO: Created: latency-svc-tzbjj
Dec  3 15:58:29.715: INFO: Got endpoints: latency-svc-qgdx9 [748.578007ms]
Dec  3 15:58:29.759: INFO: Created: latency-svc-vjn9b
Dec  3 15:58:29.766: INFO: Got endpoints: latency-svc-p55hx [750.142115ms]
Dec  3 15:58:29.810: INFO: Created: latency-svc-p6zs6
Dec  3 15:58:29.816: INFO: Got endpoints: latency-svc-5wsdx [749.400041ms]
Dec  3 15:58:29.863: INFO: Created: latency-svc-dr8sr
Dec  3 15:58:29.865: INFO: Got endpoints: latency-svc-v4nb8 [744.934444ms]
Dec  3 15:58:29.910: INFO: Created: latency-svc-xmh2n
Dec  3 15:58:29.915: INFO: Got endpoints: latency-svc-726tc [749.927567ms]
Dec  3 15:58:29.963: INFO: Created: latency-svc-rmw29
Dec  3 15:58:29.965: INFO: Got endpoints: latency-svc-4xfq5 [749.861852ms]
Dec  3 15:58:30.010: INFO: Created: latency-svc-x5hr8
Dec  3 15:58:30.016: INFO: Got endpoints: latency-svc-c7hx4 [750.37691ms]
Dec  3 15:58:30.059: INFO: Created: latency-svc-6ddsh
Dec  3 15:58:30.066: INFO: Got endpoints: latency-svc-t4tf6 [749.922451ms]
Dec  3 15:58:30.111: INFO: Created: latency-svc-qsx2h
Dec  3 15:58:30.115: INFO: Got endpoints: latency-svc-9m7bc [749.543419ms]
Dec  3 15:58:30.160: INFO: Created: latency-svc-z64ms
Dec  3 15:58:30.165: INFO: Got endpoints: latency-svc-twb4q [749.515674ms]
Dec  3 15:58:30.209: INFO: Created: latency-svc-c4ssm
Dec  3 15:58:30.215: INFO: Got endpoints: latency-svc-shgp5 [749.614083ms]
Dec  3 15:58:30.260: INFO: Created: latency-svc-6mfgl
Dec  3 15:58:30.266: INFO: Got endpoints: latency-svc-47b59 [749.373116ms]
Dec  3 15:58:30.310: INFO: Created: latency-svc-55m6d
Dec  3 15:58:30.316: INFO: Got endpoints: latency-svc-94wb6 [749.972393ms]
Dec  3 15:58:30.360: INFO: Created: latency-svc-prqrf
Dec  3 15:58:30.365: INFO: Got endpoints: latency-svc-tzbjj [749.994623ms]
Dec  3 15:58:30.410: INFO: Created: latency-svc-kqxl5
Dec  3 15:58:30.415: INFO: Got endpoints: latency-svc-vjn9b [749.886927ms]
Dec  3 15:58:30.459: INFO: Created: latency-svc-h65ll
Dec  3 15:58:30.465: INFO: Got endpoints: latency-svc-p6zs6 [749.808207ms]
Dec  3 15:58:30.509: INFO: Created: latency-svc-s2s7m
Dec  3 15:58:30.516: INFO: Got endpoints: latency-svc-dr8sr [749.541872ms]
Dec  3 15:58:30.559: INFO: Created: latency-svc-qpmmn
Dec  3 15:58:30.565: INFO: Got endpoints: latency-svc-xmh2n [749.719555ms]
Dec  3 15:58:30.609: INFO: Created: latency-svc-8c9dz
Dec  3 15:58:30.616: INFO: Got endpoints: latency-svc-rmw29 [750.4394ms]
Dec  3 15:58:30.660: INFO: Created: latency-svc-t4hw2
Dec  3 15:58:30.666: INFO: Got endpoints: latency-svc-x5hr8 [750.083896ms]
Dec  3 15:58:30.709: INFO: Created: latency-svc-w8gph
Dec  3 15:58:30.715: INFO: Got endpoints: latency-svc-6ddsh [749.948434ms]
Dec  3 15:58:30.760: INFO: Created: latency-svc-qfhzx
Dec  3 15:58:30.779: INFO: Got endpoints: latency-svc-qsx2h [762.625691ms]
Dec  3 15:58:30.809: INFO: Created: latency-svc-kthcc
Dec  3 15:58:30.815: INFO: Got endpoints: latency-svc-z64ms [749.336914ms]
Dec  3 15:58:30.866: INFO: Got endpoints: latency-svc-c4ssm [750.289954ms]
Dec  3 15:58:30.872: INFO: Created: latency-svc-sfdqj
Dec  3 15:58:30.910: INFO: Created: latency-svc-z7jsn
Dec  3 15:58:30.916: INFO: Got endpoints: latency-svc-6mfgl [750.961862ms]
Dec  3 15:58:30.959: INFO: Created: latency-svc-v49pj
Dec  3 15:58:30.965: INFO: Got endpoints: latency-svc-55m6d [749.896445ms]
Dec  3 15:58:31.010: INFO: Created: latency-svc-mt8zt
Dec  3 15:58:31.015: INFO: Got endpoints: latency-svc-prqrf [749.653737ms]
Dec  3 15:58:31.060: INFO: Created: latency-svc-dxgq5
Dec  3 15:58:31.066: INFO: Got endpoints: latency-svc-kqxl5 [749.941528ms]
Dec  3 15:58:31.112: INFO: Created: latency-svc-cwlhp
Dec  3 15:58:31.116: INFO: Got endpoints: latency-svc-h65ll [750.105597ms]
Dec  3 15:58:31.160: INFO: Created: latency-svc-dswms
Dec  3 15:58:31.165: INFO: Got endpoints: latency-svc-s2s7m [750.079304ms]
Dec  3 15:58:31.213: INFO: Created: latency-svc-nm5k6
Dec  3 15:58:31.215: INFO: Got endpoints: latency-svc-qpmmn [750.034936ms]
Dec  3 15:58:31.260: INFO: Created: latency-svc-6792c
Dec  3 15:58:31.265: INFO: Got endpoints: latency-svc-8c9dz [749.786331ms]
Dec  3 15:58:31.310: INFO: Created: latency-svc-jnk2x
Dec  3 15:58:31.317: INFO: Got endpoints: latency-svc-t4hw2 [751.679396ms]
Dec  3 15:58:31.360: INFO: Created: latency-svc-s5hp8
Dec  3 15:58:31.365: INFO: Got endpoints: latency-svc-w8gph [749.465903ms]
Dec  3 15:58:31.411: INFO: Created: latency-svc-2j2dv
Dec  3 15:58:31.415: INFO: Got endpoints: latency-svc-qfhzx [749.652976ms]
Dec  3 15:58:31.460: INFO: Created: latency-svc-9f52l
Dec  3 15:58:31.465: INFO: Got endpoints: latency-svc-kthcc [749.913684ms]
Dec  3 15:58:31.509: INFO: Created: latency-svc-ltxbk
Dec  3 15:58:31.515: INFO: Got endpoints: latency-svc-sfdqj [736.587601ms]
Dec  3 15:58:31.559: INFO: Created: latency-svc-g9ktz
Dec  3 15:58:31.565: INFO: Got endpoints: latency-svc-z7jsn [749.772408ms]
Dec  3 15:58:31.609: INFO: Created: latency-svc-cpq2w
Dec  3 15:58:31.615: INFO: Got endpoints: latency-svc-v49pj [749.708892ms]
Dec  3 15:58:31.659: INFO: Created: latency-svc-8sc7c
Dec  3 15:58:31.665: INFO: Got endpoints: latency-svc-mt8zt [748.799127ms]
Dec  3 15:58:31.709: INFO: Created: latency-svc-l5qx5
Dec  3 15:58:31.715: INFO: Got endpoints: latency-svc-dxgq5 [749.946239ms]
Dec  3 15:58:31.759: INFO: Created: latency-svc-bvttf
Dec  3 15:58:31.765: INFO: Got endpoints: latency-svc-cwlhp [749.919576ms]
Dec  3 15:58:31.809: INFO: Created: latency-svc-dl2hn
Dec  3 15:58:31.815: INFO: Got endpoints: latency-svc-dswms [749.823753ms]
Dec  3 15:58:31.859: INFO: Created: latency-svc-mmvdn
Dec  3 15:58:31.865: INFO: Got endpoints: latency-svc-nm5k6 [749.632235ms]
Dec  3 15:58:31.909: INFO: Created: latency-svc-j9wt4
Dec  3 15:58:31.915: INFO: Got endpoints: latency-svc-6792c [749.796407ms]
Dec  3 15:58:31.960: INFO: Created: latency-svc-w5b8v
Dec  3 15:58:31.966: INFO: Got endpoints: latency-svc-jnk2x [750.131383ms]
Dec  3 15:58:32.009: INFO: Created: latency-svc-zslf9
Dec  3 15:58:32.016: INFO: Got endpoints: latency-svc-s5hp8 [750.023579ms]
Dec  3 15:58:32.059: INFO: Created: latency-svc-7jmmw
Dec  3 15:58:32.065: INFO: Got endpoints: latency-svc-2j2dv [748.053768ms]
Dec  3 15:58:32.109: INFO: Created: latency-svc-5ppw4
Dec  3 15:58:32.117: INFO: Got endpoints: latency-svc-9f52l [751.139434ms]
Dec  3 15:58:32.159: INFO: Created: latency-svc-nc4m4
Dec  3 15:58:32.165: INFO: Got endpoints: latency-svc-ltxbk [749.884306ms]
Dec  3 15:58:32.211: INFO: Created: latency-svc-bhzkk
Dec  3 15:58:32.215: INFO: Got endpoints: latency-svc-g9ktz [749.918007ms]
Dec  3 15:58:32.259: INFO: Created: latency-svc-fw67w
Dec  3 15:58:32.265: INFO: Got endpoints: latency-svc-cpq2w [749.965042ms]
Dec  3 15:58:32.309: INFO: Created: latency-svc-nsjp9
Dec  3 15:58:32.316: INFO: Got endpoints: latency-svc-8sc7c [750.418928ms]
Dec  3 15:58:32.361: INFO: Created: latency-svc-gtx9v
Dec  3 15:58:32.366: INFO: Got endpoints: latency-svc-l5qx5 [750.704173ms]
Dec  3 15:58:32.409: INFO: Created: latency-svc-2jq8j
Dec  3 15:58:32.415: INFO: Got endpoints: latency-svc-bvttf [749.966351ms]
Dec  3 15:58:32.463: INFO: Created: latency-svc-9rfvv
Dec  3 15:58:32.465: INFO: Got endpoints: latency-svc-dl2hn [749.905514ms]
Dec  3 15:58:32.509: INFO: Created: latency-svc-4pnzp
Dec  3 15:58:32.516: INFO: Got endpoints: latency-svc-mmvdn [750.13335ms]
Dec  3 15:58:32.559: INFO: Created: latency-svc-zfrjq
Dec  3 15:58:32.566: INFO: Got endpoints: latency-svc-j9wt4 [750.01558ms]
Dec  3 15:58:32.610: INFO: Created: latency-svc-xc8vj
Dec  3 15:58:32.615: INFO: Got endpoints: latency-svc-w5b8v [749.907314ms]
Dec  3 15:58:32.659: INFO: Created: latency-svc-4zhtd
Dec  3 15:58:32.665: INFO: Got endpoints: latency-svc-zslf9 [749.806744ms]
Dec  3 15:58:32.708: INFO: Created: latency-svc-cncsp
Dec  3 15:58:32.715: INFO: Got endpoints: latency-svc-7jmmw [749.665355ms]
Dec  3 15:58:32.759: INFO: Created: latency-svc-5hw44
Dec  3 15:58:32.765: INFO: Got endpoints: latency-svc-5ppw4 [749.569137ms]
Dec  3 15:58:32.809: INFO: Created: latency-svc-8rxml
Dec  3 15:58:32.815: INFO: Got endpoints: latency-svc-nc4m4 [749.99703ms]
Dec  3 15:58:32.858: INFO: Created: latency-svc-7bp5c
Dec  3 15:58:32.865: INFO: Got endpoints: latency-svc-bhzkk [748.732675ms]
Dec  3 15:58:32.909: INFO: Created: latency-svc-86skz
Dec  3 15:58:32.916: INFO: Got endpoints: latency-svc-fw67w [750.377576ms]
Dec  3 15:58:32.959: INFO: Created: latency-svc-tkgp6
Dec  3 15:58:32.966: INFO: Got endpoints: latency-svc-nsjp9 [750.129718ms]
Dec  3 15:58:33.010: INFO: Created: latency-svc-g5hj7
Dec  3 15:58:33.015: INFO: Got endpoints: latency-svc-gtx9v [749.776569ms]
Dec  3 15:58:33.059: INFO: Created: latency-svc-b8xv5
Dec  3 15:58:33.065: INFO: Got endpoints: latency-svc-2jq8j [749.500725ms]
Dec  3 15:58:33.108: INFO: Created: latency-svc-t4pgz
Dec  3 15:58:33.115: INFO: Got endpoints: latency-svc-9rfvv [749.08305ms]
Dec  3 15:58:33.159: INFO: Created: latency-svc-v7qjh
Dec  3 15:58:33.165: INFO: Got endpoints: latency-svc-4pnzp [749.954106ms]
Dec  3 15:58:33.209: INFO: Created: latency-svc-7rqw2
Dec  3 15:58:33.215: INFO: Got endpoints: latency-svc-zfrjq [750.012222ms]
Dec  3 15:58:33.259: INFO: Created: latency-svc-hm8vb
Dec  3 15:58:33.266: INFO: Got endpoints: latency-svc-xc8vj [749.962615ms]
Dec  3 15:58:33.309: INFO: Created: latency-svc-8th4w
Dec  3 15:58:33.315: INFO: Got endpoints: latency-svc-4zhtd [749.823774ms]
Dec  3 15:58:33.360: INFO: Created: latency-svc-2wwxx
Dec  3 15:58:33.365: INFO: Got endpoints: latency-svc-cncsp [749.881296ms]
Dec  3 15:58:33.409: INFO: Created: latency-svc-gnc6z
Dec  3 15:58:33.415: INFO: Got endpoints: latency-svc-5hw44 [750.009034ms]
Dec  3 15:58:33.466: INFO: Got endpoints: latency-svc-8rxml [750.304676ms]
Dec  3 15:58:33.516: INFO: Got endpoints: latency-svc-7bp5c [750.171494ms]
Dec  3 15:58:33.566: INFO: Got endpoints: latency-svc-86skz [750.107158ms]
Dec  3 15:58:33.616: INFO: Got endpoints: latency-svc-tkgp6 [750.295587ms]
Dec  3 15:58:33.666: INFO: Got endpoints: latency-svc-g5hj7 [750.354759ms]
Dec  3 15:58:33.716: INFO: Got endpoints: latency-svc-b8xv5 [749.814202ms]
Dec  3 15:58:33.766: INFO: Got endpoints: latency-svc-t4pgz [750.128825ms]
Dec  3 15:58:33.816: INFO: Got endpoints: latency-svc-v7qjh [750.403254ms]
Dec  3 15:58:33.866: INFO: Got endpoints: latency-svc-7rqw2 [749.990336ms]
Dec  3 15:58:33.916: INFO: Got endpoints: latency-svc-hm8vb [750.099997ms]
Dec  3 15:58:33.966: INFO: Got endpoints: latency-svc-8th4w [750.397243ms]
Dec  3 15:58:34.016: INFO: Got endpoints: latency-svc-2wwxx [749.908535ms]
Dec  3 15:58:34.065: INFO: Got endpoints: latency-svc-gnc6z [750.009742ms]
Dec  3 15:58:34.066: INFO: Latencies: [97.67676ms 97.71214ms 99.174744ms 100.486841ms 102.466803ms 102.532441ms 102.764421ms 105.931791ms 106.035328ms 120.127561ms 120.605577ms 120.824874ms 120.893929ms 128.782558ms 141.821753ms 143.564057ms 145.863654ms 148.989904ms 149.94057ms 150.653145ms 150.932659ms 156.217097ms 169.093953ms 174.680576ms 183.013183ms 185.376951ms 189.345971ms 190.35464ms 194.547923ms 201.312229ms 206.561822ms 210.887998ms 215.116318ms 224.090417ms 225.637788ms 227.615434ms 228.549589ms 236.589639ms 240.597417ms 244.205208ms 266.933809ms 308.865934ms 361.341773ms 407.38094ms 450.294868ms 499.063806ms 545.117737ms 588.096053ms 636.72018ms 677.720802ms 732.063621ms 736.587601ms 744.934444ms 746.145278ms 747.025555ms 747.408678ms 747.988952ms 747.993522ms 748.053768ms 748.578007ms 748.732675ms 748.799127ms 749.08305ms 749.336914ms 749.369617ms 749.373116ms 749.400041ms 749.465903ms 749.500725ms 749.515674ms 749.541872ms 749.543419ms 749.569137ms 749.583111ms 749.588729ms 749.612915ms 749.614083ms 749.622459ms 749.632235ms 749.635645ms 749.652976ms 749.653737ms 749.655599ms 749.665355ms 749.706492ms 749.707034ms 749.708892ms 749.719555ms 749.739449ms 749.757403ms 749.759627ms 749.770045ms 749.770756ms 749.771479ms 749.772408ms 749.776569ms 749.786331ms 749.787872ms 749.796407ms 749.806744ms 749.808207ms 749.814202ms 749.82258ms 749.823753ms 749.823774ms 749.845672ms 749.854767ms 749.860684ms 749.861852ms 749.877996ms 749.881296ms 749.883503ms 749.884306ms 749.886927ms 749.896445ms 749.905514ms 749.907314ms 749.908535ms 749.913684ms 749.918007ms 749.919576ms 749.922451ms 749.923879ms 749.927567ms 749.935553ms 749.941528ms 749.944484ms 749.946239ms 749.946288ms 749.948434ms 749.949916ms 749.950711ms 749.954106ms 749.954645ms 749.957007ms 749.962615ms 749.965042ms 749.966351ms 749.972393ms 749.990336ms 749.992389ms 749.994623ms 749.995169ms 749.99703ms 750.00612ms 750.009034ms 750.009742ms 750.010814ms 750.012222ms 750.01558ms 750.016097ms 750.016669ms 750.023579ms 750.034936ms 750.051433ms 750.055115ms 750.061331ms 750.074201ms 750.079304ms 750.083896ms 750.099997ms 750.105597ms 750.107158ms 750.128825ms 750.129718ms 750.131383ms 750.132678ms 750.133314ms 750.13335ms 750.142115ms 750.152303ms 750.159161ms 750.171494ms 750.204353ms 750.289954ms 750.295587ms 750.304676ms 750.317882ms 750.327447ms 750.354759ms 750.36284ms 750.366166ms 750.37691ms 750.377576ms 750.397243ms 750.399516ms 750.403254ms 750.418928ms 750.4394ms 750.672031ms 750.704173ms 750.961862ms 751.139434ms 751.613598ms 751.679396ms 751.936223ms 753.170802ms 754.834395ms 755.022076ms 762.625691ms]
Dec  3 15:58:34.066: INFO: 50 %ile: 749.808207ms
Dec  3 15:58:34.066: INFO: 90 %ile: 750.36284ms
Dec  3 15:58:34.066: INFO: 99 %ile: 755.022076ms
Dec  3 15:58:34.066: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:58:34.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-623" for this suite.
Dec  3 15:59:04.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:59:07.921: INFO: namespace svc-latency-623 deletion completed in 33.765155781s
•SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:59:07.921: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5474
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  3 15:59:08.651: INFO: Waiting up to 5m0s for pod "pod-98d470c2-cbfb-4b75-9cc1-a0230a36d7d7" in namespace "emptydir-5474" to be "success or failure"
Dec  3 15:59:08.741: INFO: Pod "pod-98d470c2-cbfb-4b75-9cc1-a0230a36d7d7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.304457ms
Dec  3 15:59:10.830: INFO: Pod "pod-98d470c2-cbfb-4b75-9cc1-a0230a36d7d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179241576s
STEP: Saw pod success
Dec  3 15:59:10.831: INFO: Pod "pod-98d470c2-cbfb-4b75-9cc1-a0230a36d7d7" satisfied condition "success or failure"
Dec  3 15:59:10.920: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-98d470c2-cbfb-4b75-9cc1-a0230a36d7d7 container test-container: <nil>
STEP: delete the pod
Dec  3 15:59:11.111: INFO: Waiting for pod pod-98d470c2-cbfb-4b75-9cc1-a0230a36d7d7 to disappear
Dec  3 15:59:11.200: INFO: Pod pod-98d470c2-cbfb-4b75-9cc1-a0230a36d7d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:59:11.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5474" for this suite.
Dec  3 15:59:17.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:59:21.052: INFO: namespace emptydir-5474 deletion completed in 9.760370058s
•SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:59:21.052: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4552
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec  3 15:59:24.731: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-4552 pod-service-account-c7c4052c-6e6b-4558-b559-abcfe55c2006 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec  3 15:59:26.540: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-4552 pod-service-account-c7c4052c-6e6b-4558-b559-abcfe55c2006 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec  3 15:59:27.830: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-4552 pod-service-account-c7c4052c-6e6b-4558-b559-abcfe55c2006 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:59:29.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4552" for this suite.
Dec  3 15:59:35.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:59:39.020: INFO: namespace svcaccounts-4552 deletion completed in 9.756474446s
•
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:59:39.020: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5831
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec  3 15:59:42.199: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec pod-sharedvolume-4e5cede8-d992-42d7-a796-55ca39f18b18 -c busybox-main-container --namespace=emptydir-5831 -- cat /usr/share/volumeshare/shareddata.txt'
Dec  3 15:59:43.488: INFO: stderr: ""
Dec  3 15:59:43.488: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:59:43.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5831" for this suite.
Dec  3 15:59:49.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:59:53.337: INFO: namespace emptydir-5831 deletion completed in 9.758882408s
•SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 15:59:53.337: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Dec  3 15:59:54.067: INFO: Waiting up to 5m0s for pod "var-expansion-0cc97719-ff81-4b82-ac31-b62604437918" in namespace "var-expansion-8381" to be "success or failure"
Dec  3 15:59:54.158: INFO: Pod "var-expansion-0cc97719-ff81-4b82-ac31-b62604437918": Phase="Pending", Reason="", readiness=false. Elapsed: 90.141328ms
Dec  3 15:59:56.248: INFO: Pod "var-expansion-0cc97719-ff81-4b82-ac31-b62604437918": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180861881s
STEP: Saw pod success
Dec  3 15:59:56.248: INFO: Pod "var-expansion-0cc97719-ff81-4b82-ac31-b62604437918" satisfied condition "success or failure"
Dec  3 15:59:56.338: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod var-expansion-0cc97719-ff81-4b82-ac31-b62604437918 container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:59:56.529: INFO: Waiting for pod var-expansion-0cc97719-ff81-4b82-ac31-b62604437918 to disappear
Dec  3 15:59:56.618: INFO: Pod var-expansion-0cc97719-ff81-4b82-ac31-b62604437918 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 15:59:56.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8381" for this suite.
Dec  3 16:00:02.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:06.603: INFO: namespace var-expansion-8381 deletion completed in 9.89424429s
•SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:00:06.605: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5230
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-4f2b2b76-a090-44bd-ad75-bae4bc1485b9
STEP: Creating a pod to test consume configMaps
Dec  3 16:00:07.446: INFO: Waiting up to 5m0s for pod "pod-configmaps-edd9d383-c816-4bed-94c8-e98c191e6f22" in namespace "configmap-5230" to be "success or failure"
Dec  3 16:00:07.536: INFO: Pod "pod-configmaps-edd9d383-c816-4bed-94c8-e98c191e6f22": Phase="Pending", Reason="", readiness=false. Elapsed: 89.530042ms
Dec  3 16:00:09.626: INFO: Pod "pod-configmaps-edd9d383-c816-4bed-94c8-e98c191e6f22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179630704s
STEP: Saw pod success
Dec  3 16:00:09.626: INFO: Pod "pod-configmaps-edd9d383-c816-4bed-94c8-e98c191e6f22" satisfied condition "success or failure"
Dec  3 16:00:09.716: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-configmaps-edd9d383-c816-4bed-94c8-e98c191e6f22 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:00:09.904: INFO: Waiting for pod pod-configmaps-edd9d383-c816-4bed-94c8-e98c191e6f22 to disappear
Dec  3 16:00:10.002: INFO: Pod pod-configmaps-edd9d383-c816-4bed-94c8-e98c191e6f22 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:00:10.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5230" for this suite.
Dec  3 16:00:16.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:19.850: INFO: namespace configmap-5230 deletion completed in 9.757703658s
•SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:00:19.851: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2051
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:00:23.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2051" for this suite.
Dec  3 16:00:29.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:33.152: INFO: namespace emptydir-wrapper-2051 deletion completed in 9.758483321s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:00:33.153: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  3 16:00:33.884: INFO: Waiting up to 5m0s for pod "pod-885b39ab-ea04-4b39-a44d-948c175b1827" in namespace "emptydir-4820" to be "success or failure"
Dec  3 16:00:33.974: INFO: Pod "pod-885b39ab-ea04-4b39-a44d-948c175b1827": Phase="Pending", Reason="", readiness=false. Elapsed: 89.274951ms
Dec  3 16:00:36.064: INFO: Pod "pod-885b39ab-ea04-4b39-a44d-948c175b1827": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179453126s
STEP: Saw pod success
Dec  3 16:00:36.064: INFO: Pod "pod-885b39ab-ea04-4b39-a44d-948c175b1827" satisfied condition "success or failure"
Dec  3 16:00:36.154: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-885b39ab-ea04-4b39-a44d-948c175b1827 container test-container: <nil>
STEP: delete the pod
Dec  3 16:00:36.344: INFO: Waiting for pod pod-885b39ab-ea04-4b39-a44d-948c175b1827 to disappear
Dec  3 16:00:36.433: INFO: Pod pod-885b39ab-ea04-4b39-a44d-948c175b1827 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:00:36.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4820" for this suite.
Dec  3 16:00:42.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:46.296: INFO: namespace emptydir-4820 deletion completed in 9.772133004s
•SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:00:46.296: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-e90cfc2d-517d-4a5d-b12d-867a51c68deb
STEP: Creating a pod to test consume secrets
Dec  3 16:00:47.119: INFO: Waiting up to 5m0s for pod "pod-secrets-c8adc48b-14e2-4bf5-9bd5-efcd9e840cb1" in namespace "secrets-6264" to be "success or failure"
Dec  3 16:00:47.208: INFO: Pod "pod-secrets-c8adc48b-14e2-4bf5-9bd5-efcd9e840cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 89.571938ms
Dec  3 16:00:49.299: INFO: Pod "pod-secrets-c8adc48b-14e2-4bf5-9bd5-efcd9e840cb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180027486s
STEP: Saw pod success
Dec  3 16:00:49.299: INFO: Pod "pod-secrets-c8adc48b-14e2-4bf5-9bd5-efcd9e840cb1" satisfied condition "success or failure"
Dec  3 16:00:49.388: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-secrets-c8adc48b-14e2-4bf5-9bd5-efcd9e840cb1 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:00:49.577: INFO: Waiting for pod pod-secrets-c8adc48b-14e2-4bf5-9bd5-efcd9e840cb1 to disappear
Dec  3 16:00:49.666: INFO: Pod pod-secrets-c8adc48b-14e2-4bf5-9bd5-efcd9e840cb1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:00:49.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6264" for this suite.
Dec  3 16:00:56.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:59.529: INFO: namespace secrets-6264 deletion completed in 9.772760224s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:00:59.530: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4602
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-4602
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4602
Dec  3 16:01:00.438: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Dec  3 16:01:10.529: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec  3 16:01:10.619: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:01:11.887: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:01:11.887: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:01:11.887: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:01:11.977: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  3 16:01:22.069: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:01:22.069: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:01:22.431: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999955s
Dec  3 16:01:23.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.908995656s
Dec  3 16:01:24.612: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.817945754s
Dec  3 16:01:25.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.727609419s
Dec  3 16:01:26.793: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.637333837s
Dec  3 16:01:27.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.546815782s
Dec  3 16:01:28.974: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.456170838s
Dec  3 16:01:30.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.365850986s
Dec  3 16:01:31.155: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.275487279s
Dec  3 16:01:32.246: INFO: Verifying statefulset ss doesn't scale past 3 for another 184.836579ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4602
Dec  3 16:01:33.336: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:01:34.593: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 16:01:34.593: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 16:01:34.593: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 16:01:34.593: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:01:35.866: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  3 16:01:35.866: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 16:01:35.866: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 16:01:35.866: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:01:37.131: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  3 16:01:37.131: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 16:01:37.131: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 16:01:37.221: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 16:01:37.221: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 16:01:37.221: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec  3 16:01:37.311: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:01:38.564: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:01:38.564: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:01:38.564: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:01:38.564: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:01:39.865: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:01:39.865: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:01:39.865: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:01:39.865: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:01:41.118: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:01:41.118: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:01:41.118: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:01:41.118: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:01:41.208: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec  3 16:01:51.390: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:01:51.390: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:01:51.390: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:01:51.660: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Dec  3 16:01:51.660: INFO: ss-0  ip-10-250-31-164.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  }]
Dec  3 16:01:51.660: INFO: ss-1  ip-10-250-9-228.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  }]
Dec  3 16:01:51.660: INFO: ss-2  ip-10-250-31-164.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  }]
Dec  3 16:01:51.660: INFO: 
Dec  3 16:01:51.660: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  3 16:01:52.751: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Dec  3 16:01:52.751: INFO: ss-0  ip-10-250-31-164.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  }]
Dec  3 16:01:52.751: INFO: ss-1  ip-10-250-9-228.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  }]
Dec  3 16:01:52.751: INFO: ss-2  ip-10-250-31-164.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  }]
Dec  3 16:01:52.751: INFO: 
Dec  3 16:01:52.751: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  3 16:01:53.841: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Dec  3 16:01:53.841: INFO: ss-0  ip-10-250-31-164.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  }]
Dec  3 16:01:53.841: INFO: ss-1  ip-10-250-9-228.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  }]
Dec  3 16:01:53.841: INFO: 
Dec  3 16:01:53.841: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  3 16:01:54.932: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Dec  3 16:01:54.932: INFO: ss-0  ip-10-250-31-164.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  }]
Dec  3 16:01:54.932: INFO: ss-1  ip-10-250-9-228.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  }]
Dec  3 16:01:54.932: INFO: 
Dec  3 16:01:54.932: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  3 16:01:56.022: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Dec  3 16:01:56.022: INFO: ss-0  ip-10-250-31-164.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  }]
Dec  3 16:01:56.022: INFO: ss-1  ip-10-250-9-228.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  }]
Dec  3 16:01:56.022: INFO: 
Dec  3 16:01:56.022: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  3 16:01:57.113: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Dec  3 16:01:57.113: INFO: ss-0  ip-10-250-31-164.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  }]
Dec  3 16:01:57.113: INFO: ss-1  ip-10-250-9-228.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:22 +0000 UTC  }]
Dec  3 16:01:57.113: INFO: 
Dec  3 16:01:57.113: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  3 16:01:58.203: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Dec  3 16:01:58.203: INFO: ss-0  ip-10-250-31-164.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  }]
Dec  3 16:01:58.203: INFO: 
Dec  3 16:01:58.203: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  3 16:01:59.294: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Dec  3 16:01:59.294: INFO: ss-0  ip-10-250-31-164.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  }]
Dec  3 16:01:59.294: INFO: 
Dec  3 16:01:59.294: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  3 16:02:00.384: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Dec  3 16:02:00.384: INFO: ss-0  ip-10-250-31-164.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  }]
Dec  3 16:02:00.384: INFO: 
Dec  3 16:02:00.384: INFO: StatefulSet ss has not reached scale 0, at 1
Dec  3 16:02:01.474: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Dec  3 16:02:01.474: INFO: ss-0  ip-10-250-31-164.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:01:00 +0000 UTC  }]
Dec  3 16:02:01.474: INFO: 
Dec  3 16:02:01.475: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4602
Dec  3 16:02:02.564: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:02:03.352: INFO: rc: 1
Dec  3 16:02:03.352: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002c5cff0 exit status 1 <nil> <nil> true [0xc0000113c8 0xc000011548 0xc0000117a8] [0xc0000113c8 0xc000011548 0xc0000117a8] [0xc0000114c8 0xc000011758] [0xba6c10 0xba6c10] 0xc0023d89c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:02:13.353: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:02:13.866: INFO: rc: 1
Dec  3 16:02:13.867: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002488060 exit status 1 <nil> <nil> true [0xc00032e620 0xc00032e6b8 0xc00032e738] [0xc00032e620 0xc00032e6b8 0xc00032e738] [0xc00032e6a0 0xc00032e708] [0xba6c10 0xba6c10] 0xc0020e1800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:02:23.867: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:02:24.382: INFO: rc: 1
Dec  3 16:02:24.382: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002c5d650 exit status 1 <nil> <nil> true [0xc0000118b0 0xc000011a48 0xc000011af0] [0xc0000118b0 0xc000011a48 0xc000011af0] [0xc0000119b8 0xc000011ad0] [0xba6c10 0xba6c10] 0xc0023d8de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:02:34.382: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:02:34.893: INFO: rc: 1
Dec  3 16:02:34.893: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001be9410 exit status 1 <nil> <nil> true [0xc0020d8330 0xc0020d8350 0xc0020d8368] [0xc0020d8330 0xc0020d8350 0xc0020d8368] [0xc0020d8340 0xc0020d8360] [0xba6c10 0xba6c10] 0xc002ab0780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:02:44.893: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:02:45.420: INFO: rc: 1
Dec  3 16:02:45.420: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002488630 exit status 1 <nil> <nil> true [0xc00032e788 0xc00032e7c8 0xc00032e868] [0xc00032e788 0xc00032e7c8 0xc00032e868] [0xc00032e7b8 0xc00032e858] [0xba6c10 0xba6c10] 0xc0020e1b00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:02:55.420: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:02:55.935: INFO: rc: 1
Dec  3 16:02:55.935: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002488c30 exit status 1 <nil> <nil> true [0xc00032e888 0xc00032e8f8 0xc00032e990] [0xc00032e888 0xc00032e8f8 0xc00032e990] [0xc00032e8b8 0xc00032e980] [0xba6c10 0xba6c10] 0xc0020e1e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:03:05.936: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:03:06.448: INFO: rc: 1
Dec  3 16:03:06.448: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001be9aa0 exit status 1 <nil> <nil> true [0xc0020d8370 0xc0020d8388 0xc0020d83a0] [0xc0020d8370 0xc0020d8388 0xc0020d83a0] [0xc0020d8380 0xc0020d8398] [0xba6c10 0xba6c10] 0xc002ab0ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:03:16.449: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:03:16.963: INFO: rc: 1
Dec  3 16:03:16.963: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0006c7710 exit status 1 <nil> <nil> true [0xc00017a4f0 0xc00017a9c8 0xc00017ad18] [0xc00017a4f0 0xc00017a9c8 0xc00017ad18] [0xc00017a830 0xc00017abf0] [0xba6c10 0xba6c10] 0xc00248d440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:03:26.964: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:03:27.477: INFO: rc: 1
Dec  3 16:03:27.477: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001940600 exit status 1 <nil> <nil> true [0xc0006ac650 0xc0006ac6c0 0xc0006ac768] [0xc0006ac650 0xc0006ac6c0 0xc0006ac768] [0xc0006ac698 0xc0006ac720] [0xba6c10 0xba6c10] 0xc00213a240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:03:37.477: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:03:38.079: INFO: rc: 1
Dec  3 16:03:38.079: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002080630 exit status 1 <nil> <nil> true [0xc000610098 0xc000610a80 0xc000610b70] [0xc000610098 0xc000610a80 0xc000610b70] [0xc000610a70 0xc000610b50] [0xba6c10 0xba6c10] 0xc0020e02a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:03:48.079: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:03:48.596: INFO: rc: 1
Dec  3 16:03:48.596: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002080db0 exit status 1 <nil> <nil> true [0xc000610c18 0xc000610ca0 0xc000610cf8] [0xc000610c18 0xc000610ca0 0xc000610cf8] [0xc000610c60 0xc000610cc0] [0xba6c10 0xba6c10] 0xc0020e05a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:03:58.596: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:03:59.109: INFO: rc: 1
Dec  3 16:03:59.109: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0006c7dd0 exit status 1 <nil> <nil> true [0xc00017ae08 0xc00017afd8 0xc00017b0f8] [0xc00017ae08 0xc00017afd8 0xc00017b0f8] [0xc00017af50 0xc00017b090] [0xba6c10 0xba6c10] 0xc002ba4360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:04:09.109: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:04:09.618: INFO: rc: 1
Dec  3 16:04:09.618: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020843c0 exit status 1 <nil> <nil> true [0xc00017b1d0 0xc00017b330 0xc00017b430] [0xc00017b1d0 0xc00017b330 0xc00017b430] [0xc00017b210 0xc00017b3f8] [0xba6c10 0xba6c10] 0xc002ba48a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:04:19.619: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:04:20.143: INFO: rc: 1
Dec  3 16:04:20.143: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001fe2600 exit status 1 <nil> <nil> true [0xc00032e068 0xc00032e110 0xc00032e190] [0xc00032e068 0xc00032e110 0xc00032e190] [0xc00032e0e8 0xc00032e138] [0xba6c10 0xba6c10] 0xc0023d8240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:04:30.144: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:04:30.655: INFO: rc: 1
Dec  3 16:04:30.655: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001fe2c00 exit status 1 <nil> <nil> true [0xc00032e210 0xc00032e280 0xc00032e328] [0xc00032e210 0xc00032e280 0xc00032e328] [0xc00032e260 0xc00032e2d0] [0xba6c10 0xba6c10] 0xc0023d8540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:04:40.656: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:04:41.170: INFO: rc: 1
Dec  3 16:04:41.170: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002084990 exit status 1 <nil> <nil> true [0xc00017b448 0xc00017b6c0 0xc00017b978] [0xc00017b448 0xc00017b6c0 0xc00017b978] [0xc00017b4d0 0xc00017b940] [0xba6c10 0xba6c10] 0xc002ba4ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:04:51.170: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:04:51.687: INFO: rc: 1
Dec  3 16:04:51.687: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002081410 exit status 1 <nil> <nil> true [0xc000610d20 0xc000610f98 0xc000610fc8] [0xc000610d20 0xc000610f98 0xc000610fc8] [0xc000610f20 0xc000610fb8] [0xba6c10 0xba6c10] 0xc0020e08a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:05:01.688: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:05:02.209: INFO: rc: 1
Dec  3 16:05:02.209: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002084fc0 exit status 1 <nil> <nil> true [0xc00017b9d0 0xc00017bbd0 0xc00017bf10] [0xc00017b9d0 0xc00017bbd0 0xc00017bf10] [0xc00017bab8 0xc00017be68] [0xba6c10 0xba6c10] 0xc002ba4f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:05:12.209: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:05:12.723: INFO: rc: 1
Dec  3 16:05:12.723: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0006c7740 exit status 1 <nil> <nil> true [0xc00032e080 0xc00032e130 0xc00032e210] [0xc00032e080 0xc00032e130 0xc00032e210] [0xc00032e110 0xc00032e190] [0xba6c10 0xba6c10] 0xc00248d440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:05:22.723: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:05:23.237: INFO: rc: 1
Dec  3 16:05:23.237: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020845d0 exit status 1 <nil> <nil> true [0xc00017a4f0 0xc00017a9c8 0xc00017ad18] [0xc00017a4f0 0xc00017a9c8 0xc00017ad18] [0xc00017a830 0xc00017abf0] [0xba6c10 0xba6c10] 0xc002ba4300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:05:33.238: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:05:33.767: INFO: rc: 1
Dec  3 16:05:33.767: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002084bd0 exit status 1 <nil> <nil> true [0xc00017ae08 0xc00017afd8 0xc00017b0f8] [0xc00017ae08 0xc00017afd8 0xc00017b0f8] [0xc00017af50 0xc00017b090] [0xba6c10 0xba6c10] 0xc002ba4840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:05:43.767: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:05:44.306: INFO: rc: 1
Dec  3 16:05:44.306: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0006c7d40 exit status 1 <nil> <nil> true [0xc00032e250 0xc00032e2b0 0xc00032e380] [0xc00032e250 0xc00032e2b0 0xc00032e380] [0xc00032e280 0xc00032e328] [0xba6c10 0xba6c10] 0xc0023d82a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:05:54.306: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:05:54.855: INFO: rc: 1
Dec  3 16:05:54.855: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001fe2660 exit status 1 <nil> <nil> true [0xc0006ac630 0xc0006ac698 0xc0006ac720] [0xc0006ac630 0xc0006ac698 0xc0006ac720] [0xc0006ac680 0xc0006ac6e8] [0xba6c10 0xba6c10] 0xc00213a240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:06:04.856: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:06:05.385: INFO: rc: 1
Dec  3 16:06:05.385: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001fe2c90 exit status 1 <nil> <nil> true [0xc0006ac768 0xc0006ac7d0 0xc0006ac848] [0xc0006ac768 0xc0006ac7d0 0xc0006ac848] [0xc0006ac7c0 0xc0006ac808] [0xba6c10 0xba6c10] 0xc00213a540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:06:15.385: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:06:15.914: INFO: rc: 1
Dec  3 16:06:15.914: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001fe32c0 exit status 1 <nil> <nil> true [0xc0006ac8a0 0xc0006ac900 0xc0006ac930] [0xc0006ac8a0 0xc0006ac900 0xc0006ac930] [0xc0006ac8f0 0xc0006ac910] [0xba6c10 0xba6c10] 0xc00213a840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:06:25.914: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:06:26.441: INFO: rc: 1
Dec  3 16:06:26.441: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001fe3aa0 exit status 1 <nil> <nil> true [0xc0006ac968 0xc0006ac9f0 0xc0006aca70] [0xc0006ac968 0xc0006ac9f0 0xc0006aca70] [0xc0006ac9b0 0xc0006aca60] [0xba6c10 0xba6c10] 0xc00213ab40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:06:36.441: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:06:36.952: INFO: rc: 1
Dec  3 16:06:36.952: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020800f0 exit status 1 <nil> <nil> true [0xc0006aca80 0xc0006acaf0 0xc0006acb88] [0xc0006aca80 0xc0006acaf0 0xc0006acb88] [0xc0006acac0 0xc0006acb50] [0xba6c10 0xba6c10] 0xc00213aea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:06:46.953: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:06:47.498: INFO: rc: 1
Dec  3 16:06:47.499: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002c383f0 exit status 1 <nil> <nil> true [0xc00032e390 0xc00032e400 0xc00032e460] [0xc00032e390 0xc00032e400 0xc00032e460] [0xc00032e3b0 0xc00032e450] [0xba6c10 0xba6c10] 0xc0023d85a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:06:57.499: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:06:58.046: INFO: rc: 1
Dec  3 16:06:58.046: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002c389c0 exit status 1 <nil> <nil> true [0xc00032e480 0xc00032e4f0 0xc00032e538] [0xc00032e480 0xc00032e4f0 0xc00032e538] [0xc00032e4d8 0xc00032e530] [0xba6c10 0xba6c10] 0xc0023d8900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec  3 16:07:08.047: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4602 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:07:08.581: INFO: rc: 1
Dec  3 16:07:08.581: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Dec  3 16:07:08.581: INFO: Scaling statefulset ss to 0
Dec  3 16:07:08.851: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  3 16:07:08.941: INFO: Deleting all statefulset in ns statefulset-4602
Dec  3 16:07:09.031: INFO: Scaling statefulset ss to 0
Dec  3 16:07:09.300: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:07:09.390: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:07:09.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4602" for this suite.
Dec  3 16:07:16.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:07:19.503: INFO: namespace statefulset-4602 deletion completed in 9.752159828s

• [SLOW TEST:379.974 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:07:19.504: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:07:20.505: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ce6a70a0-210e-4937-9c7f-aae7893a0247", Controller:(*bool)(0xc0027eaa9a), BlockOwnerDeletion:(*bool)(0xc0027eaa9b)}}
Dec  3 16:07:20.595: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"55b34476-2c91-4a07-a5ab-71325c47d9d1", Controller:(*bool)(0xc001abc0c6), BlockOwnerDeletion:(*bool)(0xc001abc0c7)}}
Dec  3 16:07:20.686: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"81da2236-6035-4c48-ba50-1eeb4fa12151", Controller:(*bool)(0xc0029b8426), BlockOwnerDeletion:(*bool)(0xc0029b8427)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:07:25.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-561" for this suite.
Dec  3 16:07:32.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:07:35.709: INFO: namespace gc-561 deletion completed in 9.752231603s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:07:35.748: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2472
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2472
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2472
Dec  3 16:07:36.744: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Dec  3 16:07:46.834: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec  3 16:07:46.925: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:07:53.252: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:07:53.252: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:07:53.252: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:07:53.342: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  3 16:08:03.434: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:08:03.434: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:08:03.794: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999448s
Dec  3 16:08:04.884: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.909908097s
Dec  3 16:08:05.974: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.819683715s
Dec  3 16:08:07.064: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.72927849s
Dec  3 16:08:08.155: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.639267916s
Dec  3 16:08:09.248: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.548953186s
Dec  3 16:08:10.455: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.338876711s
Dec  3 16:08:11.545: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.24839944s
Dec  3 16:08:12.636: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.158216183s
Dec  3 16:08:13.726: INFO: Verifying statefulset ss doesn't scale past 1 for another 67.846579ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2472
Dec  3 16:08:14.816: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:08:16.198: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 16:08:16.198: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 16:08:16.198: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 16:08:16.287: INFO: Found 1 stateful pods, waiting for 3
Dec  3 16:08:26.379: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 16:08:26.379: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 16:08:26.379: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec  3 16:08:26.558: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:08:27.850: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:08:27.850: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:08:27.850: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:08:27.850: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:08:29.126: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:08:29.126: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:08:29.126: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:08:29.126: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  3 16:08:30.432: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  3 16:08:30.432: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  3 16:08:30.432: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  3 16:08:30.432: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:08:30.521: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec  3 16:08:40.702: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:08:40.702: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:08:40.702: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:08:40.972: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999942s
Dec  3 16:08:42.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.90975879s
Dec  3 16:08:43.153: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.819128206s
Dec  3 16:08:44.244: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.728472141s
Dec  3 16:08:45.334: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.637604936s
Dec  3 16:08:46.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.547325253s
Dec  3 16:08:47.515: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.4567959s
Dec  3 16:08:48.605: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.36666745s
Dec  3 16:08:49.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.237624717s
Dec  3 16:08:50.825: INFO: Verifying statefulset ss doesn't scale past 3 for another 147.374015ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2472
Dec  3 16:08:51.916: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:08:53.228: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 16:08:53.228: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 16:08:53.228: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 16:08:53.228: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:08:54.498: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  3 16:08:54.498: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  3 16:08:54.499: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  3 16:08:54.499: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:08:55.414: INFO: rc: 1
Dec  3 16:08:55.414: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc003526180 exit status 1 <nil> <nil> true [0xc00250d590 0xc00250d5a8 0xc00250d5e0] [0xc00250d590 0xc00250d5a8 0xc00250d5e0] [0xc00250d5a0 0xc00250d5c8] [0xba6c10 0xba6c10] 0xc002bff740 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Dec  3 16:09:05.414: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:09:05.944: INFO: rc: 1
Dec  3 16:09:05.944: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e8ecc0 exit status 1 <nil> <nil> true [0xc003266ac8 0xc003266ae0 0xc003266af8] [0xc003266ac8 0xc003266ae0 0xc003266af8] [0xc003266ad8 0xc003266af0] [0xba6c10 0xba6c10] 0xc001d59800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:09:15.945: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:09:16.486: INFO: rc: 1
Dec  3 16:09:16.486: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019405d0 exit status 1 <nil> <nil> true [0xc000610098 0xc000610a80 0xc000610b70] [0xc000610098 0xc000610a80 0xc000610b70] [0xc000610a70 0xc000610b50] [0xba6c10 0xba6c10] 0xc0023d8240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:09:26.486: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:09:27.636: INFO: rc: 1
Dec  3 16:09:27.636: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002c5c720 exit status 1 <nil> <nil> true [0xc0006ac630 0xc0006ac698 0xc0006ac720] [0xc0006ac630 0xc0006ac698 0xc0006ac720] [0xc0006ac680 0xc0006ac6e8] [0xba6c10 0xba6c10] 0xc00248d440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:09:37.637: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:09:38.178: INFO: rc: 1
Dec  3 16:09:38.178: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001940bd0 exit status 1 <nil> <nil> true [0xc000610c18 0xc000610ca0 0xc000610cf8] [0xc000610c18 0xc000610ca0 0xc000610cf8] [0xc000610c60 0xc000610cc0] [0xba6c10 0xba6c10] 0xc0023d8540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:09:48.179: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:09:48.795: INFO: rc: 1
Dec  3 16:09:48.795: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001be87e0 exit status 1 <nil> <nil> true [0xc00017a670 0xc00017aa88 0xc00017ae08] [0xc00017a670 0xc00017aa88 0xc00017ae08] [0xc00017a9c8 0xc00017ad18] [0xba6c10 0xba6c10] 0xc0020f4240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:09:58.795: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:09:59.442: INFO: rc: 1
Dec  3 16:09:59.442: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002c5cd20 exit status 1 <nil> <nil> true [0xc0006ac768 0xc0006ac7d0 0xc0006ac848] [0xc0006ac768 0xc0006ac7d0 0xc0006ac848] [0xc0006ac7c0 0xc0006ac808] [0xba6c10 0xba6c10] 0xc0027869c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:10:09.442: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:10:09.987: INFO: rc: 1
Dec  3 16:10:09.987: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0024885d0 exit status 1 <nil> <nil> true [0xc003266000 0xc003266018 0xc003266030] [0xc003266000 0xc003266018 0xc003266030] [0xc003266010 0xc003266028] [0xba6c10 0xba6c10] 0xc002c102a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:10:19.987: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:10:20.593: INFO: rc: 1
Dec  3 16:10:20.593: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002c5d320 exit status 1 <nil> <nil> true [0xc0006ac8a0 0xc0006ac900 0xc0006ac930] [0xc0006ac8a0 0xc0006ac900 0xc0006ac930] [0xc0006ac8f0 0xc0006ac910] [0xba6c10 0xba6c10] 0xc002787500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:10:30.593: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:10:31.172: INFO: rc: 1
Dec  3 16:10:31.172: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002c5d980 exit status 1 <nil> <nil> true [0xc0006ac968 0xc0006ac9f0 0xc0006aca70] [0xc0006ac968 0xc0006ac9f0 0xc0006aca70] [0xc0006ac9b0 0xc0006aca60] [0xba6c10 0xba6c10] 0xc002787ce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:10:41.173: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:10:41.721: INFO: rc: 1
Dec  3 16:10:41.746: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001941230 exit status 1 <nil> <nil> true [0xc000610d20 0xc000610f98 0xc000610fc8] [0xc000610d20 0xc000610f98 0xc000610fc8] [0xc000610f20 0xc000610fb8] [0xba6c10 0xba6c10] 0xc0023d8840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:10:51.746: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:10:52.318: INFO: rc: 1
Dec  3 16:10:52.318: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001941860 exit status 1 <nil> <nil> true [0xc000610fe8 0xc000611060 0xc000611170] [0xc000610fe8 0xc000611060 0xc000611170] [0xc000611028 0xc0006110f8] [0xba6c10 0xba6c10] 0xc0023d8c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:11:02.318: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:11:02.933: INFO: rc: 1
Dec  3 16:11:02.933: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002488ba0 exit status 1 <nil> <nil> true [0xc003266038 0xc003266050 0xc003266068] [0xc003266038 0xc003266050 0xc003266068] [0xc003266048 0xc003266060] [0xba6c10 0xba6c10] 0xc002c105a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:11:12.933: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:11:13.489: INFO: rc: 1
Dec  3 16:11:13.490: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001be8780 exit status 1 <nil> <nil> true [0xc00017a670 0xc00017aa88 0xc00017ae08] [0xc00017a670 0xc00017aa88 0xc00017ae08] [0xc00017a9c8 0xc00017ad18] [0xba6c10 0xba6c10] 0xc002786c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:11:23.490: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:11:24.028: INFO: rc: 1
Dec  3 16:11:24.028: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001be8db0 exit status 1 <nil> <nil> true [0xc00017ae98 0xc00017b040 0xc00017b1d0] [0xc00017ae98 0xc00017b040 0xc00017b1d0] [0xc00017afd8 0xc00017b0f8] [0xba6c10 0xba6c10] 0xc002787680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:11:34.028: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:11:34.548: INFO: rc: 1
Dec  3 16:11:34.548: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001be93b0 exit status 1 <nil> <nil> true [0xc00017b1f0 0xc00017b398 0xc00017b448] [0xc00017b1f0 0xc00017b398 0xc00017b448] [0xc00017b330 0xc00017b430] [0xba6c10 0xba6c10] 0xc001761140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:11:44.548: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:11:45.082: INFO: rc: 1
Dec  3 16:11:45.082: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001be99e0 exit status 1 <nil> <nil> true [0xc00017b478 0xc00017b930 0xc00017b9d0] [0xc00017b478 0xc00017b930 0xc00017b9d0] [0xc00017b6c0 0xc00017b978] [0xba6c10 0xba6c10] 0xc00248d440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:11:55.082: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:11:55.621: INFO: rc: 1
Dec  3 16:11:55.621: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019401e0 exit status 1 <nil> <nil> true [0xc00017ba58 0xc00017bdd8 0xc00017bf20] [0xc00017ba58 0xc00017bdd8 0xc00017bf20] [0xc00017bbd0 0xc00017bf10] [0xba6c10 0xba6c10] 0xc0020f42a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:12:05.621: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:12:06.163: INFO: rc: 1
Dec  3 16:12:06.163: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002c5c780 exit status 1 <nil> <nil> true [0xc0006ac630 0xc0006ac698 0xc0006ac720] [0xc0006ac630 0xc0006ac698 0xc0006ac720] [0xc0006ac680 0xc0006ac6e8] [0xba6c10 0xba6c10] 0xc00216c960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:12:16.164: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:12:16.684: INFO: rc: 1
Dec  3 16:12:16.684: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001940840 exit status 1 <nil> <nil> true [0xc00017bf38 0xc00017bfb8 0xc003266008] [0xc00017bf38 0xc00017bfb8 0xc003266008] [0xc00017bfa8 0xc003266000] [0xba6c10 0xba6c10] 0xc0020f4600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:12:26.685: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:12:27.214: INFO: rc: 1
Dec  3 16:12:27.214: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001940e70 exit status 1 <nil> <nil> true [0xc003266010 0xc003266028 0xc003266040] [0xc003266010 0xc003266028 0xc003266040] [0xc003266020 0xc003266038] [0xba6c10 0xba6c10] 0xc0020f4960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:12:37.215: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:12:37.736: INFO: rc: 1
Dec  3 16:12:37.736: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002488690 exit status 1 <nil> <nil> true [0xc000610098 0xc000610a80 0xc000610b70] [0xc000610098 0xc000610a80 0xc000610b70] [0xc000610a70 0xc000610b50] [0xba6c10 0xba6c10] 0xc002c102a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:12:47.736: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:12:48.255: INFO: rc: 1
Dec  3 16:12:48.255: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002488cf0 exit status 1 <nil> <nil> true [0xc000610c18 0xc000610ca0 0xc000610cf8] [0xc000610c18 0xc000610ca0 0xc000610cf8] [0xc000610c60 0xc000610cc0] [0xba6c10 0xba6c10] 0xc002c105a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:12:58.256: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:12:58.788: INFO: rc: 1
Dec  3 16:12:58.788: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0024892c0 exit status 1 <nil> <nil> true [0xc000610d20 0xc000610f98 0xc000610fc8] [0xc000610d20 0xc000610f98 0xc000610fc8] [0xc000610f20 0xc000610fb8] [0xba6c10 0xba6c10] 0xc002c10960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:13:08.789: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:13:09.312: INFO: rc: 1
Dec  3 16:13:09.312: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002c5cdb0 exit status 1 <nil> <nil> true [0xc0006ac768 0xc0006ac7d0 0xc0006ac848] [0xc0006ac768 0xc0006ac7d0 0xc0006ac848] [0xc0006ac7c0 0xc0006ac808] [0xba6c10 0xba6c10] 0xc00216d560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:13:19.312: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:13:19.836: INFO: rc: 1
Dec  3 16:13:19.836: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001be87b0 exit status 1 <nil> <nil> true [0xc00017a670 0xc00017aa88 0xc00017ae08] [0xc00017a670 0xc00017aa88 0xc00017ae08] [0xc00017a9c8 0xc00017ad18] [0xba6c10 0xba6c10] 0xc00248d440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:13:29.837: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:13:30.351: INFO: rc: 1
Dec  3 16:13:30.351: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001be8de0 exit status 1 <nil> <nil> true [0xc00017ae98 0xc00017b040 0xc00017b1d0] [0xc00017ae98 0xc00017b040 0xc00017b1d0] [0xc00017afd8 0xc00017b0f8] [0xba6c10 0xba6c10] 0xc002786840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:13:40.351: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:13:40.947: INFO: rc: 1
Dec  3 16:13:40.947: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019400f0 exit status 1 <nil> <nil> true [0xc000610098 0xc000610a80 0xc000610b70] [0xc000610098 0xc000610a80 0xc000610b70] [0xc000610a70 0xc000610b50] [0xba6c10 0xba6c10] 0xc002c10060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:13:50.947: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:13:51.542: INFO: rc: 1
Dec  3 16:13:51.542: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002488600 exit status 1 <nil> <nil> true [0xc003266000 0xc003266018 0xc003266030] [0xc003266000 0xc003266018 0xc003266030] [0xc003266010 0xc003266028] [0xba6c10 0xba6c10] 0xc0020f4240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:14:01.543: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-2472 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  3 16:14:02.204: INFO: rc: 1
Dec  3 16:14:02.204: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Dec  3 16:14:02.204: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec  3 16:14:02.475: INFO: Deleting all statefulset in ns statefulset-2472
Dec  3 16:14:02.564: INFO: Scaling statefulset ss to 0
Dec  3 16:14:02.835: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:14:02.924: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:14:03.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2472" for this suite.
Dec  3 16:14:09.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:14:13.046: INFO: namespace statefulset-2472 deletion completed in 9.756567731s

• [SLOW TEST:397.299 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:14:13.047: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6341
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:14:13.686: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:14:14.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6341" for this suite.
Dec  3 16:14:20.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:14:24.213: INFO: namespace custom-resource-definition-6341 deletion completed in 9.793102295s
•SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:14:24.214: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5096
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5096
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 16:14:24.856: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 16:14:48.473: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.1.53:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5096 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:14:48.473: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:14:49.350: INFO: Found all expected endpoints: [netserver-0]
Dec  3 16:14:49.439: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.0.242:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5096 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:14:49.439: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:14:50.268: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:14:50.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5096" for this suite.
Dec  3 16:15:12.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:15:16.133: INFO: namespace pod-network-test-5096 deletion completed in 25.774480891s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:15:16.134: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Dec  3 16:15:19.226: INFO: Pod pod-hostip-6916d80b-a348-4d09-a425-c9ec8705c2b5 has hostIP: 10.250.31.164
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:15:19.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3750" for this suite.
Dec  3 16:15:41.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:15:45.076: INFO: namespace pods-3750 deletion completed in 25.759430981s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:15:45.077: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5032
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  3 16:15:45.808: INFO: Waiting up to 5m0s for pod "pod-c5aa1a53-1817-4254-ab20-1dbb4598a067" in namespace "emptydir-5032" to be "success or failure"
Dec  3 16:15:45.898: INFO: Pod "pod-c5aa1a53-1817-4254-ab20-1dbb4598a067": Phase="Pending", Reason="", readiness=false. Elapsed: 90.703264ms
Dec  3 16:15:47.994: INFO: Pod "pod-c5aa1a53-1817-4254-ab20-1dbb4598a067": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.186848717s
STEP: Saw pod success
Dec  3 16:15:47.994: INFO: Pod "pod-c5aa1a53-1817-4254-ab20-1dbb4598a067" satisfied condition "success or failure"
Dec  3 16:15:48.084: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-c5aa1a53-1817-4254-ab20-1dbb4598a067 container test-container: <nil>
STEP: delete the pod
Dec  3 16:15:48.276: INFO: Waiting for pod pod-c5aa1a53-1817-4254-ab20-1dbb4598a067 to disappear
Dec  3 16:15:48.365: INFO: Pod pod-c5aa1a53-1817-4254-ab20-1dbb4598a067 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:15:48.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5032" for this suite.
Dec  3 16:15:54.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:15:58.220: INFO: namespace emptydir-5032 deletion completed in 9.764445202s
•SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:15:58.220: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec  3 16:15:58.859: INFO: namespace kubectl-6023
Dec  3 16:15:58.859: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-6023'
Dec  3 16:16:00.052: INFO: stderr: ""
Dec  3 16:16:00.052: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  3 16:16:01.142: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:16:01.143: INFO: Found 0 / 1
Dec  3 16:16:02.142: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:16:02.142: INFO: Found 1 / 1
Dec  3 16:16:02.142: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  3 16:16:02.232: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:16:02.232: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  3 16:16:02.232: INFO: wait on redis-master startup in kubectl-6023 
Dec  3 16:16:02.232: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-gb7xz redis-master --namespace=kubectl-6023'
Dec  3 16:16:02.847: INFO: stderr: ""
Dec  3 16:16:02.847: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 Dec 16:16:00.809 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Dec 16:16:00.809 # Server started, Redis version 3.2.12\n1:M 03 Dec 16:16:00.809 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Dec 16:16:00.809 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec  3 16:16:02.848: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6023'
Dec  3 16:16:03.450: INFO: stderr: ""
Dec  3 16:16:03.450: INFO: stdout: "service/rm2 exposed\n"
Dec  3 16:16:03.539: INFO: Service rm2 in namespace kubectl-6023 found.
STEP: exposing service
Dec  3 16:16:05.718: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6023'
Dec  3 16:16:06.312: INFO: stderr: ""
Dec  3 16:16:06.312: INFO: stdout: "service/rm3 exposed\n"
Dec  3 16:16:06.402: INFO: Service rm3 in namespace kubectl-6023 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:16:08.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6023" for this suite.
Dec  3 16:16:30.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:16:34.431: INFO: namespace kubectl-6023 deletion completed in 25.75924775s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:16:34.432: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7754
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  3 16:16:35.168: INFO: Waiting up to 5m0s for pod "downward-api-fa47f56e-8f55-400c-9a57-d7372487c079" in namespace "downward-api-7754" to be "success or failure"
Dec  3 16:16:35.259: INFO: Pod "downward-api-fa47f56e-8f55-400c-9a57-d7372487c079": Phase="Pending", Reason="", readiness=false. Elapsed: 90.324396ms
Dec  3 16:16:37.349: INFO: Pod "downward-api-fa47f56e-8f55-400c-9a57-d7372487c079": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180108008s
STEP: Saw pod success
Dec  3 16:16:37.349: INFO: Pod "downward-api-fa47f56e-8f55-400c-9a57-d7372487c079" satisfied condition "success or failure"
Dec  3 16:16:37.438: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downward-api-fa47f56e-8f55-400c-9a57-d7372487c079 container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:16:37.626: INFO: Waiting for pod downward-api-fa47f56e-8f55-400c-9a57-d7372487c079 to disappear
Dec  3 16:16:37.715: INFO: Pod downward-api-fa47f56e-8f55-400c-9a57-d7372487c079 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:16:37.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7754" for this suite.
Dec  3 16:16:44.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:16:47.584: INFO: namespace downward-api-7754 deletion completed in 9.778122218s
•S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:16:47.584: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-7bb86ec0-e8a3-421b-9198-898ff5cee6c5 in namespace container-probe-8011
Dec  3 16:16:50.501: INFO: Started pod test-webserver-7bb86ec0-e8a3-421b-9198-898ff5cee6c5 in namespace container-probe-8011
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 16:16:50.591: INFO: Initial restart count of pod test-webserver-7bb86ec0-e8a3-421b-9198-898ff5cee6c5 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:20:51.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8011" for this suite.
Dec  3 16:20:57.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:21:00.905: INFO: namespace container-probe-8011 deletion completed in 9.763615741s
•SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:21:00.906: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2296
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Dec  3 16:21:01.637: INFO: Waiting up to 5m0s for pod "var-expansion-37750f0b-12a2-44ed-95c4-762c118f7d65" in namespace "var-expansion-2296" to be "success or failure"
Dec  3 16:21:01.726: INFO: Pod "var-expansion-37750f0b-12a2-44ed-95c4-762c118f7d65": Phase="Pending", Reason="", readiness=false. Elapsed: 89.172491ms
Dec  3 16:21:03.816: INFO: Pod "var-expansion-37750f0b-12a2-44ed-95c4-762c118f7d65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179676044s
STEP: Saw pod success
Dec  3 16:21:03.817: INFO: Pod "var-expansion-37750f0b-12a2-44ed-95c4-762c118f7d65" satisfied condition "success or failure"
Dec  3 16:21:03.906: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod var-expansion-37750f0b-12a2-44ed-95c4-762c118f7d65 container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:21:04.098: INFO: Waiting for pod var-expansion-37750f0b-12a2-44ed-95c4-762c118f7d65 to disappear
Dec  3 16:21:04.187: INFO: Pod var-expansion-37750f0b-12a2-44ed-95c4-762c118f7d65 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:21:04.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2296" for this suite.
Dec  3 16:21:10.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:21:14.037: INFO: namespace var-expansion-2296 deletion completed in 9.75719298s
•SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:21:14.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9530
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-7107b620-ec56-42f0-a3b3-92189aeda90e
STEP: Creating a pod to test consume secrets
Dec  3 16:21:14.861: INFO: Waiting up to 5m0s for pod "pod-secrets-49f5882b-3c90-4be7-8c0a-c30396c7362b" in namespace "secrets-9530" to be "success or failure"
Dec  3 16:21:14.950: INFO: Pod "pod-secrets-49f5882b-3c90-4be7-8c0a-c30396c7362b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.385169ms
Dec  3 16:21:17.040: INFO: Pod "pod-secrets-49f5882b-3c90-4be7-8c0a-c30396c7362b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179444093s
STEP: Saw pod success
Dec  3 16:21:17.040: INFO: Pod "pod-secrets-49f5882b-3c90-4be7-8c0a-c30396c7362b" satisfied condition "success or failure"
Dec  3 16:21:17.130: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-secrets-49f5882b-3c90-4be7-8c0a-c30396c7362b container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:21:17.320: INFO: Waiting for pod pod-secrets-49f5882b-3c90-4be7-8c0a-c30396c7362b to disappear
Dec  3 16:21:17.409: INFO: Pod pod-secrets-49f5882b-3c90-4be7-8c0a-c30396c7362b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:21:17.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9530" for this suite.
Dec  3 16:21:23.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:21:27.260: INFO: namespace secrets-9530 deletion completed in 9.760775678s
•
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:21:27.261: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5104
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5104.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5104.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5104.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5104.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5104.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5104.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 16:21:31.101: INFO: DNS probes using dns-5104/dns-test-c69d5daa-7637-4977-80b2-402559a2352f succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:21:31.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5104" for this suite.
Dec  3 16:21:37.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:21:41.049: INFO: namespace dns-5104 deletion completed in 9.763767676s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:21:41.050: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6168
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:21:55.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6168" for this suite.
Dec  3 16:22:01.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:22:05.153: INFO: namespace watch-6168 deletion completed in 9.7550611s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:22:05.154: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8462
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-c1f4fcea-0b8c-4d21-8890-d2abc3f232da
STEP: Creating a pod to test consume secrets
Dec  3 16:22:05.976: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a328915d-a291-4fe0-af71-af37293d510d" in namespace "projected-8462" to be "success or failure"
Dec  3 16:22:06.066: INFO: Pod "pod-projected-secrets-a328915d-a291-4fe0-af71-af37293d510d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.777973ms
Dec  3 16:22:08.156: INFO: Pod "pod-projected-secrets-a328915d-a291-4fe0-af71-af37293d510d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179724312s
STEP: Saw pod success
Dec  3 16:22:08.156: INFO: Pod "pod-projected-secrets-a328915d-a291-4fe0-af71-af37293d510d" satisfied condition "success or failure"
Dec  3 16:22:08.245: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-secrets-a328915d-a291-4fe0-af71-af37293d510d container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:22:08.434: INFO: Waiting for pod pod-projected-secrets-a328915d-a291-4fe0-af71-af37293d510d to disappear
Dec  3 16:22:08.523: INFO: Pod pod-projected-secrets-a328915d-a291-4fe0-af71-af37293d510d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:22:08.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8462" for this suite.
Dec  3 16:22:14.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:22:18.366: INFO: namespace projected-8462 deletion completed in 9.752534987s
•SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:22:18.366: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3329
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Dec  3 16:22:59.644: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1203 16:22:59.643989    5065 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:22:59.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3329" for this suite.
Dec  3 16:23:06.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:23:09.498: INFO: namespace gc-3329 deletion completed in 9.764164317s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:23:09.500: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-150432df-b894-460c-9870-be3d6c1bcd46
STEP: Creating a pod to test consume secrets
Dec  3 16:23:10.320: INFO: Waiting up to 5m0s for pod "pod-secrets-cc6d3546-524b-4e02-aaa0-a5d6b21f2b9e" in namespace "secrets-3803" to be "success or failure"
Dec  3 16:23:10.409: INFO: Pod "pod-secrets-cc6d3546-524b-4e02-aaa0-a5d6b21f2b9e": Phase="Pending", Reason="", readiness=false. Elapsed: 89.196218ms
Dec  3 16:23:12.499: INFO: Pod "pod-secrets-cc6d3546-524b-4e02-aaa0-a5d6b21f2b9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179265892s
STEP: Saw pod success
Dec  3 16:23:12.499: INFO: Pod "pod-secrets-cc6d3546-524b-4e02-aaa0-a5d6b21f2b9e" satisfied condition "success or failure"
Dec  3 16:23:12.589: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-secrets-cc6d3546-524b-4e02-aaa0-a5d6b21f2b9e container secret-env-test: <nil>
STEP: delete the pod
Dec  3 16:23:12.779: INFO: Waiting for pod pod-secrets-cc6d3546-524b-4e02-aaa0-a5d6b21f2b9e to disappear
Dec  3 16:23:12.868: INFO: Pod pod-secrets-cc6d3546-524b-4e02-aaa0-a5d6b21f2b9e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:23:12.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3803" for this suite.
Dec  3 16:23:19.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:23:22.715: INFO: namespace secrets-3803 deletion completed in 9.757298731s
•S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:23:22.716: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2036
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-67479bbc-619e-479b-a118-b2cbebb97965
STEP: Creating a pod to test consume configMaps
Dec  3 16:23:23.535: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-32c85ff0-3049-4eea-87eb-b94bc85d3bb8" in namespace "projected-2036" to be "success or failure"
Dec  3 16:23:23.625: INFO: Pod "pod-projected-configmaps-32c85ff0-3049-4eea-87eb-b94bc85d3bb8": Phase="Pending", Reason="", readiness=false. Elapsed: 89.375058ms
Dec  3 16:23:25.715: INFO: Pod "pod-projected-configmaps-32c85ff0-3049-4eea-87eb-b94bc85d3bb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179519411s
STEP: Saw pod success
Dec  3 16:23:25.715: INFO: Pod "pod-projected-configmaps-32c85ff0-3049-4eea-87eb-b94bc85d3bb8" satisfied condition "success or failure"
Dec  3 16:23:25.805: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-configmaps-32c85ff0-3049-4eea-87eb-b94bc85d3bb8 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:23:25.997: INFO: Waiting for pod pod-projected-configmaps-32c85ff0-3049-4eea-87eb-b94bc85d3bb8 to disappear
Dec  3 16:23:26.086: INFO: Pod pod-projected-configmaps-32c85ff0-3049-4eea-87eb-b94bc85d3bb8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:23:26.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2036" for this suite.
Dec  3 16:23:32.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:23:35.927: INFO: namespace projected-2036 deletion completed in 9.750801408s
•
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:23:35.928: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:23:36.658: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12006665-8d90-40d6-b6fc-c2391f951f98" in namespace "downward-api-2930" to be "success or failure"
Dec  3 16:23:36.748: INFO: Pod "downwardapi-volume-12006665-8d90-40d6-b6fc-c2391f951f98": Phase="Pending", Reason="", readiness=false. Elapsed: 89.66274ms
Dec  3 16:23:38.838: INFO: Pod "downwardapi-volume-12006665-8d90-40d6-b6fc-c2391f951f98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179808937s
STEP: Saw pod success
Dec  3 16:23:38.838: INFO: Pod "downwardapi-volume-12006665-8d90-40d6-b6fc-c2391f951f98" satisfied condition "success or failure"
Dec  3 16:23:38.928: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-12006665-8d90-40d6-b6fc-c2391f951f98 container client-container: <nil>
STEP: delete the pod
Dec  3 16:23:39.117: INFO: Waiting for pod downwardapi-volume-12006665-8d90-40d6-b6fc-c2391f951f98 to disappear
Dec  3 16:23:39.207: INFO: Pod downwardapi-volume-12006665-8d90-40d6-b6fc-c2391f951f98 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:23:39.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2930" for this suite.
Dec  3 16:23:45.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:23:49.066: INFO: namespace downward-api-2930 deletion completed in 9.765989071s
•S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:23:49.066: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-c6aa977b-b45a-4bcc-a8e8-7c5456a5943f
Dec  3 16:23:49.886: INFO: Pod name my-hostname-basic-c6aa977b-b45a-4bcc-a8e8-7c5456a5943f: Found 1 pods out of 1
Dec  3 16:23:49.886: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-c6aa977b-b45a-4bcc-a8e8-7c5456a5943f" are running
Dec  3 16:23:52.066: INFO: Pod "my-hostname-basic-c6aa977b-b45a-4bcc-a8e8-7c5456a5943f-9njkt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:23:49 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:23:49 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-c6aa977b-b45a-4bcc-a8e8-7c5456a5943f]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:23:49 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-c6aa977b-b45a-4bcc-a8e8-7c5456a5943f]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:23:49 +0000 UTC Reason: Message:}])
Dec  3 16:23:52.066: INFO: Trying to dial the pod
Dec  3 16:23:57.424: INFO: Controller my-hostname-basic-c6aa977b-b45a-4bcc-a8e8-7c5456a5943f: Got expected result from replica 1 [my-hostname-basic-c6aa977b-b45a-4bcc-a8e8-7c5456a5943f-9njkt]: "my-hostname-basic-c6aa977b-b45a-4bcc-a8e8-7c5456a5943f-9njkt", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:23:57.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2483" for this suite.
Dec  3 16:24:03.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:24:07.269: INFO: namespace replication-controller-2483 deletion completed in 9.754426529s
•SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:24:07.269: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-652
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 16:24:07.909: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-652'
Dec  3 16:24:08.886: INFO: stderr: ""
Dec  3 16:24:08.886: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Dec  3 16:24:08.975: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-652'
Dec  3 16:24:13.161: INFO: stderr: ""
Dec  3 16:24:13.161: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:24:13.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-652" for this suite.
Dec  3 16:24:19.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:24:23.005: INFO: namespace kubectl-652 deletion completed in 9.75393466s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:24:23.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:24:26.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8005" for this suite.
Dec  3 16:25:04.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:25:07.958: INFO: namespace kubelet-test-8005 deletion completed in 41.764362635s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:25:07.959: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-70
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:25:08.600: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-70'
Dec  3 16:25:09.644: INFO: stderr: ""
Dec  3 16:25:09.644: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec  3 16:25:09.644: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-70'
Dec  3 16:25:10.656: INFO: stderr: ""
Dec  3 16:25:10.656: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  3 16:25:11.747: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:25:11.747: INFO: Found 1 / 1
Dec  3 16:25:11.747: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  3 16:25:11.837: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:25:11.837: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  3 16:25:11.837: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe pod redis-master-tjcqp --namespace=kubectl-70'
Dec  3 16:25:17.525: INFO: stderr: ""
Dec  3 16:25:17.525: INFO: stdout: "Name:           redis-master-tjcqp\nNamespace:      kubectl-70\nPriority:       0\nNode:           ip-10-250-31-164.ec2.internal/10.250.31.164\nStart Time:     Tue, 03 Dec 2019 16:25:09 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 100.64.0.15/32\n                kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             100.64.0.15\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://e4e504966317fc965d3ce1a9d8561ee2be0967692bbbc0e97c94df7b81e02b76\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 03 Dec 2019 16:25:10 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mcct7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-mcct7:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-mcct7\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                    Message\n  ----    ------     ----  ----                                    -------\n  Normal  Scheduled  8s    default-scheduler                       Successfully assigned kubectl-70/redis-master-tjcqp to ip-10-250-31-164.ec2.internal\n  Normal  Pulled     7s    kubelet, ip-10-250-31-164.ec2.internal  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    7s    kubelet, ip-10-250-31-164.ec2.internal  Created container redis-master\n  Normal  Started    7s    kubelet, ip-10-250-31-164.ec2.internal  Started container redis-master\n"
Dec  3 16:25:17.525: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe rc redis-master --namespace=kubectl-70'
Dec  3 16:25:18.311: INFO: stderr: ""
Dec  3 16:25:18.311: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-70\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  9s    replication-controller  Created pod: redis-master-tjcqp\n"
Dec  3 16:25:18.311: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe service redis-master --namespace=kubectl-70'
Dec  3 16:25:19.039: INFO: stderr: ""
Dec  3 16:25:19.039: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-70\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.108.228.51\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.64.0.15:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  3 16:25:19.130: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe node ip-10-250-31-164.ec2.internal'
Dec  3 16:25:20.063: INFO: stderr: ""
Dec  3 16:25:20.063: INFO: stdout: "Name:               ip-10-250-31-164.ec2.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-250-31-164.ec2.internal\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/role=node\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/pool=worker-1\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.31.164/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.64.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 03 Dec 2019 14:30:13 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  FrequentDockerRestart         False   Tue, 03 Dec 2019 16:24:55 +0000   Tue, 03 Dec 2019 14:30:48 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Tue, 03 Dec 2019 16:24:55 +0000   Tue, 03 Dec 2019 14:30:48 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  KernelDeadlock                False   Tue, 03 Dec 2019 16:24:55 +0000   Tue, 03 Dec 2019 14:30:48 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Tue, 03 Dec 2019 16:24:55 +0000   Tue, 03 Dec 2019 14:30:48 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  CorruptDockerOverlay2         False   Tue, 03 Dec 2019 16:24:55 +0000   Tue, 03 Dec 2019 14:30:48 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentUnregisterNetDevice   False   Tue, 03 Dec 2019 16:24:55 +0000   Tue, 03 Dec 2019 14:30:48 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  FrequentKubeletRestart        False   Tue, 03 Dec 2019 16:24:55 +0000   Tue, 03 Dec 2019 14:30:48 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  NetworkUnavailable            False   Tue, 03 Dec 2019 14:30:43 +0000   Tue, 03 Dec 2019 14:30:43 +0000   CalicoIsUp                      Calico is running on this node\n  MemoryPressure                False   Tue, 03 Dec 2019 16:25:10 +0000   Tue, 03 Dec 2019 14:30:13 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Tue, 03 Dec 2019 16:25:10 +0000   Tue, 03 Dec 2019 14:30:13 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Tue, 03 Dec 2019 16:25:10 +0000   Tue, 03 Dec 2019 14:30:13 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Tue, 03 Dec 2019 16:25:10 +0000   Tue, 03 Dec 2019 14:30:43 +0000   KubeletReady                    kubelet is posting ready status\nAddresses:\n  InternalIP:   10.250.31.164\n  Hostname:     ip-10-250-31-164.ec2.internal\n  InternalDNS:  ip-10-250-31-164.ec2.internal\nCapacity:\n attachable-volumes-aws-ebs:  25\n cpu:                         2\n ephemeral-storage:           28056816Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      7865420Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  25\n cpu:                         1920m\n ephemeral-storage:           27293670584\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      6577738746\n pods:                        110\nSystem Info:\n Machine ID:                 ec264d9118f0cd30342724bd80f714b2\n System UUID:                ec264d91-18f0-cd30-3427-24bd80f714b2\n Boot ID:                    cabd59e4-f73e-459b-9c3d-9a79c011b7c4\n Kernel Version:             4.19.56-coreos-r1\n OS Image:                   Container Linux by CoreOS 2135.6.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.15.6\n Kube-Proxy Version:         v1.15.6\nPodCIDR:                     100.64.0.0/24\nProviderID:                  aws:///us-east-1c/i-07f3ef5c8c079f9ea\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                    ------------  ----------  ---------------  -------------  ---\n  kube-system                blackbox-exporter-c87bdd467-vscd2       5m (0%)       10m (0%)    5Mi (0%)         35Mi (0%)      116m\n  kube-system                calico-node-w5f9r                       100m (5%)     500m (26%)  100Mi (1%)       700Mi (11%)    115m\n  kube-system                calico-typha-deploy-5547c4cdc6-zvk82    0 (0%)        0 (0%)      0 (0%)           0 (0%)         112m\n  kube-system                kube-proxy-8xnnr                        20m (1%)      0 (0%)      64Mi (1%)        0 (0%)         115m\n  kube-system                node-exporter-68n8d                     5m (0%)       25m (1%)    10Mi (0%)        100Mi (1%)     115m\n  kube-system                node-problem-detector-hnrtk             20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)     115m\n  kubectl-70                 redis-master-tjcqp                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         11s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         150m (7%)   735m (38%)\n  memory                      199Mi (3%)  935Mi (14%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:                       <none>\n"
Dec  3 16:25:20.064: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe namespace kubectl-70'
Dec  3 16:25:20.803: INFO: stderr: ""
Dec  3 16:25:20.803: INFO: stdout: "Name:         kubectl-70\nLabels:       e2e-framework=kubectl\n              e2e-run=2dd116ee-353c-4a76-b427-d334ebb975bf\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:25:20.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-70" for this suite.
Dec  3 16:25:43.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:25:46.650: INFO: namespace kubectl-70 deletion completed in 25.755806364s
•SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:25:46.650: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1435
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Dec  3 16:25:48.237: INFO: created pod pod-service-account-defaultsa
Dec  3 16:25:48.237: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  3 16:25:48.327: INFO: created pod pod-service-account-mountsa
Dec  3 16:25:48.327: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  3 16:25:48.417: INFO: created pod pod-service-account-nomountsa
Dec  3 16:25:48.417: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  3 16:25:48.507: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  3 16:25:48.507: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  3 16:25:48.601: INFO: created pod pod-service-account-mountsa-mountspec
Dec  3 16:25:48.601: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  3 16:25:48.692: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  3 16:25:48.692: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  3 16:25:48.781: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  3 16:25:48.782: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  3 16:25:48.877: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  3 16:25:48.877: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  3 16:25:48.968: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  3 16:25:48.968: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:25:48.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1435" for this suite.
Dec  3 16:25:55.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:25:58.815: INFO: namespace svcaccounts-1435 deletion completed in 9.75738528s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:25:58.816: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:25:59.549: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d726abea-4718-4057-8d10-805193dd556f" in namespace "downward-api-2413" to be "success or failure"
Dec  3 16:25:59.638: INFO: Pod "downwardapi-volume-d726abea-4718-4057-8d10-805193dd556f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.371602ms
Dec  3 16:26:01.729: INFO: Pod "downwardapi-volume-d726abea-4718-4057-8d10-805193dd556f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179450761s
STEP: Saw pod success
Dec  3 16:26:01.729: INFO: Pod "downwardapi-volume-d726abea-4718-4057-8d10-805193dd556f" satisfied condition "success or failure"
Dec  3 16:26:01.823: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-d726abea-4718-4057-8d10-805193dd556f container client-container: <nil>
STEP: delete the pod
Dec  3 16:26:02.030: INFO: Waiting for pod downwardapi-volume-d726abea-4718-4057-8d10-805193dd556f to disappear
Dec  3 16:26:02.129: INFO: Pod downwardapi-volume-d726abea-4718-4057-8d10-805193dd556f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:26:02.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2413" for this suite.
Dec  3 16:26:08.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:26:11.979: INFO: namespace downward-api-2413 deletion completed in 9.759424503s
•S
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:26:11.979: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8175
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:26:12.710: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1df2708-27b1-49a9-9e03-2c5958553f08" in namespace "projected-8175" to be "success or failure"
Dec  3 16:26:12.800: INFO: Pod "downwardapi-volume-f1df2708-27b1-49a9-9e03-2c5958553f08": Phase="Pending", Reason="", readiness=false. Elapsed: 89.505495ms
Dec  3 16:26:14.889: INFO: Pod "downwardapi-volume-f1df2708-27b1-49a9-9e03-2c5958553f08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17934896s
STEP: Saw pod success
Dec  3 16:26:14.889: INFO: Pod "downwardapi-volume-f1df2708-27b1-49a9-9e03-2c5958553f08" satisfied condition "success or failure"
Dec  3 16:26:14.979: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-f1df2708-27b1-49a9-9e03-2c5958553f08 container client-container: <nil>
STEP: delete the pod
Dec  3 16:26:15.172: INFO: Waiting for pod downwardapi-volume-f1df2708-27b1-49a9-9e03-2c5958553f08 to disappear
Dec  3 16:26:15.261: INFO: Pod downwardapi-volume-f1df2708-27b1-49a9-9e03-2c5958553f08 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:26:15.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8175" for this suite.
Dec  3 16:26:21.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:26:25.125: INFO: namespace projected-8175 deletion completed in 9.773741316s
•SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:26:25.126: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2626
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-5af5c534-0638-4bc4-80b7-f05a3075a730
STEP: Creating a pod to test consume configMaps
Dec  3 16:26:25.946: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d9636119-3d87-484b-9278-1cac402db143" in namespace "projected-2626" to be "success or failure"
Dec  3 16:26:26.036: INFO: Pod "pod-projected-configmaps-d9636119-3d87-484b-9278-1cac402db143": Phase="Pending", Reason="", readiness=false. Elapsed: 89.66446ms
Dec  3 16:26:28.126: INFO: Pod "pod-projected-configmaps-d9636119-3d87-484b-9278-1cac402db143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17972832s
STEP: Saw pod success
Dec  3 16:26:28.126: INFO: Pod "pod-projected-configmaps-d9636119-3d87-484b-9278-1cac402db143" satisfied condition "success or failure"
Dec  3 16:26:28.215: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-configmaps-d9636119-3d87-484b-9278-1cac402db143 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:26:28.406: INFO: Waiting for pod pod-projected-configmaps-d9636119-3d87-484b-9278-1cac402db143 to disappear
Dec  3 16:26:28.495: INFO: Pod pod-projected-configmaps-d9636119-3d87-484b-9278-1cac402db143 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:26:28.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2626" for this suite.
Dec  3 16:26:34.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:26:38.358: INFO: namespace projected-2626 deletion completed in 9.771444325s
•SSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:26:38.358: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:26:38.996: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec  3 16:26:39.535: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:26:39.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-730" for this suite.
Dec  3 16:26:45.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:26:49.466: INFO: namespace replication-controller-730 deletion completed in 9.750524726s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:26:49.466: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-825
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-825
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-825 to expose endpoints map[]
Dec  3 16:26:50.288: INFO: successfully validated that service endpoint-test2 in namespace services-825 exposes endpoints map[] (89.228004ms elapsed)
STEP: Creating pod pod1 in namespace services-825
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-825 to expose endpoints map[pod1:[80]]
Dec  3 16:26:52.921: INFO: successfully validated that service endpoint-test2 in namespace services-825 exposes endpoints map[pod1:[80]] (2.540445906s elapsed)
STEP: Creating pod pod2 in namespace services-825
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-825 to expose endpoints map[pod1:[80] pod2:[80]]
Dec  3 16:26:54.548: INFO: successfully validated that service endpoint-test2 in namespace services-825 exposes endpoints map[pod1:[80] pod2:[80]] (1.536547855s elapsed)
STEP: Deleting pod pod1 in namespace services-825
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-825 to expose endpoints map[pod2:[80]]
Dec  3 16:26:54.817: INFO: successfully validated that service endpoint-test2 in namespace services-825 exposes endpoints map[pod2:[80]] (178.933111ms elapsed)
STEP: Deleting pod pod2 in namespace services-825
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-825 to expose endpoints map[]
Dec  3 16:26:54.998: INFO: successfully validated that service endpoint-test2 in namespace services-825 exposes endpoints map[] (89.34179ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:26:55.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-825" for this suite.
Dec  3 16:27:17.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:27:20.944: INFO: namespace services-825 deletion completed in 25.760229733s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:27:20.944: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6670
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 16:27:21.618: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 16:27:39.147: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.0.30:8080/dial?request=hostName&protocol=udp&host=100.64.0.29&port=8081&tries=1'] Namespace:pod-network-test-6670 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:27:39.147: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:27:40.022: INFO: Waiting for endpoints: map[]
Dec  3 16:27:40.111: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.0.30:8080/dial?request=hostName&protocol=udp&host=100.64.1.60&port=8081&tries=1'] Namespace:pod-network-test-6670 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:27:40.111: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:27:40.979: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:27:40.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6670" for this suite.
Dec  3 16:28:03.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:28:06.828: INFO: namespace pod-network-test-6670 deletion completed in 25.759455803s
•SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:28:06.829: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9001
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Dec  3 16:28:07.562: INFO: Waiting up to 5m0s for pod "pod-f7af35dd-0609-4f97-ae45-004f394f87e8" in namespace "emptydir-9001" to be "success or failure"
Dec  3 16:28:07.651: INFO: Pod "pod-f7af35dd-0609-4f97-ae45-004f394f87e8": Phase="Pending", Reason="", readiness=false. Elapsed: 89.434284ms
Dec  3 16:28:09.741: INFO: Pod "pod-f7af35dd-0609-4f97-ae45-004f394f87e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179513855s
STEP: Saw pod success
Dec  3 16:28:09.742: INFO: Pod "pod-f7af35dd-0609-4f97-ae45-004f394f87e8" satisfied condition "success or failure"
Dec  3 16:28:09.831: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-f7af35dd-0609-4f97-ae45-004f394f87e8 container test-container: <nil>
STEP: delete the pod
Dec  3 16:28:10.021: INFO: Waiting for pod pod-f7af35dd-0609-4f97-ae45-004f394f87e8 to disappear
Dec  3 16:28:10.110: INFO: Pod pod-f7af35dd-0609-4f97-ae45-004f394f87e8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:28:10.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9001" for this suite.
Dec  3 16:28:16.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:28:19.956: INFO: namespace emptydir-9001 deletion completed in 9.755467309s
•SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:28:19.957: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:28:20.595: INFO: Creating deployment "test-recreate-deployment"
Dec  3 16:28:20.685: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  3 16:28:20.864: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  3 16:28:20.954: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987300, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987300, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987300, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987300, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:28:23.044: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  3 16:28:23.223: INFO: Updating deployment test-recreate-deployment
Dec  3 16:28:23.223: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec  3 16:28:23.402: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3582,SelfLink:/apis/apps/v1/namespaces/deployment-3582/deployments/test-recreate-deployment,UID:364ffb93-33cc-48e8-96ce-f2e1b3bbca84,ResourceVersion:23261,Generation:2,CreationTimestamp:2019-12-03 16:28:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-12-03 16:28:23 +0000 UTC 2019-12-03 16:28:23 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-03 16:28:23 +0000 UTC 2019-12-03 16:28:20 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec  3 16:28:23.493: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-3582,SelfLink:/apis/apps/v1/namespaces/deployment-3582/replicasets/test-recreate-deployment-5c8c9cc69d,UID:34cb0148-0858-486a-a4a1-25a6739c283d,ResourceVersion:23260,Generation:1,CreationTimestamp:2019-12-03 16:28:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 364ffb93-33cc-48e8-96ce-f2e1b3bbca84 0xc001693917 0xc001693918}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 16:28:23.493: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  3 16:28:23.493: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-3582,SelfLink:/apis/apps/v1/namespaces/deployment-3582/replicasets/test-recreate-deployment-6df85df6b9,UID:03cbbf80-f6f9-4a82-b734-c858e482a61a,ResourceVersion:23253,Generation:2,CreationTimestamp:2019-12-03 16:28:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 364ffb93-33cc-48e8-96ce-f2e1b3bbca84 0xc0016939e7 0xc0016939e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  3 16:28:23.583: INFO: Pod "test-recreate-deployment-5c8c9cc69d-9jz7p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-9jz7p,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-3582,SelfLink:/api/v1/namespaces/deployment-3582/pods/test-recreate-deployment-5c8c9cc69d-9jz7p,UID:6a2b9a3e-5adb-4f88-9b2b-7ab83c8f424e,ResourceVersion:23262,Generation:0,CreationTimestamp:2019-12-03 16:28:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 34cb0148-0858-486a-a4a1-25a6739c283d 0xc0015b02c7 0xc0015b02c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wcg9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wcg9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wcg9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-31-164.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0015b0330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0015b0350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:28:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:28:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 16:28:23 +0000 UTC  }],Message:,Reason:,HostIP:10.250.31.164,PodIP:,StartTime:2019-12-03 16:28:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:28:23.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3582" for this suite.
Dec  3 16:28:29.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:28:33.426: INFO: namespace deployment-3582 deletion completed in 9.75253083s
•S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:28:33.426: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1572
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 16:28:34.069: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-1572'
Dec  3 16:28:34.589: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 16:28:34.589: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Dec  3 16:28:36.768: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-1572'
Dec  3 16:28:37.344: INFO: stderr: ""
Dec  3 16:28:37.344: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:28:37.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1572" for this suite.
Dec  3 16:28:59.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:29:03.207: INFO: namespace kubectl-1572 deletion completed in 25.773475615s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:29:03.208: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  3 16:29:04.566: INFO: Number of nodes with available pods: 0
Dec  3 16:29:04.566: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 16:29:05.746: INFO: Number of nodes with available pods: 2
Dec  3 16:29:05.746: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec  3 16:29:06.195: INFO: Number of nodes with available pods: 1
Dec  3 16:29:06.195: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 16:29:07.377: INFO: Number of nodes with available pods: 1
Dec  3 16:29:07.377: INFO: Node ip-10-250-31-164.ec2.internal is running more than one daemon pod
Dec  3 16:29:08.378: INFO: Number of nodes with available pods: 2
Dec  3 16:29:08.378: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9582, will wait for the garbage collector to delete the pods
Dec  3 16:29:08.838: INFO: Deleting DaemonSet.extensions daemon-set took: 91.637506ms
Dec  3 16:29:09.039: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.303302ms
Dec  3 16:29:23.428: INFO: Number of nodes with available pods: 0
Dec  3 16:29:23.428: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 16:29:23.518: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9582/daemonsets","resourceVersion":"23479"},"items":null}

Dec  3 16:29:23.607: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9582/pods","resourceVersion":"23479"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:29:23.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9582" for this suite.
Dec  3 16:29:30.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:29:33.726: INFO: namespace daemonsets-9582 deletion completed in 9.759552196s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:29:33.727: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  3 16:29:34.458: INFO: Waiting up to 5m0s for pod "pod-5b454569-b062-414f-833d-8f3d01bd754d" in namespace "emptydir-7967" to be "success or failure"
Dec  3 16:29:34.547: INFO: Pod "pod-5b454569-b062-414f-833d-8f3d01bd754d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.305204ms
Dec  3 16:29:36.637: INFO: Pod "pod-5b454569-b062-414f-833d-8f3d01bd754d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179495247s
STEP: Saw pod success
Dec  3 16:29:36.637: INFO: Pod "pod-5b454569-b062-414f-833d-8f3d01bd754d" satisfied condition "success or failure"
Dec  3 16:29:36.728: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-5b454569-b062-414f-833d-8f3d01bd754d container test-container: <nil>
STEP: delete the pod
Dec  3 16:29:36.919: INFO: Waiting for pod pod-5b454569-b062-414f-833d-8f3d01bd754d to disappear
Dec  3 16:29:37.008: INFO: Pod pod-5b454569-b062-414f-833d-8f3d01bd754d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:29:37.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7967" for this suite.
Dec  3 16:29:43.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:29:46.854: INFO: namespace emptydir-7967 deletion completed in 9.755372156s
•SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:29:46.854: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  3 16:29:47.584: INFO: Waiting up to 5m0s for pod "pod-0af7d697-84b4-4e35-8028-0587c13aa73d" in namespace "emptydir-4999" to be "success or failure"
Dec  3 16:29:47.674: INFO: Pod "pod-0af7d697-84b4-4e35-8028-0587c13aa73d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.354186ms
Dec  3 16:29:49.764: INFO: Pod "pod-0af7d697-84b4-4e35-8028-0587c13aa73d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179287589s
STEP: Saw pod success
Dec  3 16:29:49.764: INFO: Pod "pod-0af7d697-84b4-4e35-8028-0587c13aa73d" satisfied condition "success or failure"
Dec  3 16:29:49.853: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-0af7d697-84b4-4e35-8028-0587c13aa73d container test-container: <nil>
STEP: delete the pod
Dec  3 16:29:50.044: INFO: Waiting for pod pod-0af7d697-84b4-4e35-8028-0587c13aa73d to disappear
Dec  3 16:29:50.133: INFO: Pod pod-0af7d697-84b4-4e35-8028-0587c13aa73d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:29:50.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4999" for this suite.
Dec  3 16:29:56.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:29:59.985: INFO: namespace emptydir-4999 deletion completed in 9.761254914s
•SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:29:59.985: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1202
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:30:24.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1202" for this suite.
Dec  3 16:30:30.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:30:34.497: INFO: namespace container-runtime-1202 deletion completed in 9.804250119s
•S
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:30:34.497: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-261
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:30:35.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-261" for this suite.
Dec  3 16:30:57.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:31:01.162: INFO: namespace pods-261 deletion completed in 25.755993771s
•SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:31:01.162: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec  3 16:31:24.162: INFO: Container started at 2019-12-03 16:31:02 +0000 UTC, pod became ready at 2019-12-03 16:31:22 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:31:24.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1398" for this suite.
Dec  3 16:31:46.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:31:50.014: INFO: namespace container-probe-1398 deletion completed in 25.761215951s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:31:50.014: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4804
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec  3 16:31:51.014: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4804,SelfLink:/api/v1/namespaces/watch-4804/configmaps/e2e-watch-test-watch-closed,UID:568d4bd2-fdbe-4589-8f48-c46d5b69559e,ResourceVersion:23906,Generation:0,CreationTimestamp:2019-12-03 16:31:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 16:31:51.014: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4804,SelfLink:/api/v1/namespaces/watch-4804/configmaps/e2e-watch-test-watch-closed,UID:568d4bd2-fdbe-4589-8f48-c46d5b69559e,ResourceVersion:23907,Generation:0,CreationTimestamp:2019-12-03 16:31:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec  3 16:31:51.373: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4804,SelfLink:/api/v1/namespaces/watch-4804/configmaps/e2e-watch-test-watch-closed,UID:568d4bd2-fdbe-4589-8f48-c46d5b69559e,ResourceVersion:23909,Generation:0,CreationTimestamp:2019-12-03 16:31:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 16:31:51.374: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4804,SelfLink:/api/v1/namespaces/watch-4804/configmaps/e2e-watch-test-watch-closed,UID:568d4bd2-fdbe-4589-8f48-c46d5b69559e,ResourceVersion:23910,Generation:0,CreationTimestamp:2019-12-03 16:31:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:31:51.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4804" for this suite.
Dec  3 16:31:57.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:32:01.224: INFO: namespace watch-4804 deletion completed in 9.759812431s
•SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:32:01.224: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:32:01.956: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5a7b67a-e357-4bd7-a8bb-b1ebc46685b2" in namespace "projected-428" to be "success or failure"
Dec  3 16:32:02.046: INFO: Pod "downwardapi-volume-d5a7b67a-e357-4bd7-a8bb-b1ebc46685b2": Phase="Pending", Reason="", readiness=false. Elapsed: 90.611915ms
Dec  3 16:32:04.137: INFO: Pod "downwardapi-volume-d5a7b67a-e357-4bd7-a8bb-b1ebc46685b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.18077808s
STEP: Saw pod success
Dec  3 16:32:04.137: INFO: Pod "downwardapi-volume-d5a7b67a-e357-4bd7-a8bb-b1ebc46685b2" satisfied condition "success or failure"
Dec  3 16:32:04.226: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-d5a7b67a-e357-4bd7-a8bb-b1ebc46685b2 container client-container: <nil>
STEP: delete the pod
Dec  3 16:32:04.418: INFO: Waiting for pod downwardapi-volume-d5a7b67a-e357-4bd7-a8bb-b1ebc46685b2 to disappear
Dec  3 16:32:04.508: INFO: Pod downwardapi-volume-d5a7b67a-e357-4bd7-a8bb-b1ebc46685b2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:32:04.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-428" for this suite.
Dec  3 16:32:10.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:32:14.352: INFO: namespace projected-428 deletion completed in 9.753277401s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:32:14.352: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Dec  3 16:32:14.991: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec  3 16:32:14.991: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-166'
Dec  3 16:32:15.966: INFO: stderr: ""
Dec  3 16:32:15.966: INFO: stdout: "service/redis-slave created\n"
Dec  3 16:32:15.966: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec  3 16:32:15.966: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-166'
Dec  3 16:32:16.957: INFO: stderr: ""
Dec  3 16:32:16.957: INFO: stdout: "service/redis-master created\n"
Dec  3 16:32:16.957: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  3 16:32:16.957: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-166'
Dec  3 16:32:17.581: INFO: stderr: ""
Dec  3 16:32:17.581: INFO: stdout: "service/frontend created\n"
Dec  3 16:32:17.582: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec  3 16:32:17.582: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-166'
Dec  3 16:32:18.222: INFO: stderr: ""
Dec  3 16:32:18.222: INFO: stdout: "deployment.apps/frontend created\n"
Dec  3 16:32:18.222: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  3 16:32:18.222: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-166'
Dec  3 16:32:18.836: INFO: stderr: ""
Dec  3 16:32:18.836: INFO: stdout: "deployment.apps/redis-master created\n"
Dec  3 16:32:18.837: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec  3 16:32:18.837: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-166'
Dec  3 16:32:19.836: INFO: stderr: ""
Dec  3 16:32:19.836: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec  3 16:32:19.836: INFO: Waiting for all frontend pods to be Running.
Dec  3 16:32:34.937: INFO: Waiting for frontend to serve content.
Dec  3 16:32:40.085: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Dec  3 16:32:45.264: INFO: Trying to add a new entry to the guestbook.
Dec  3 16:32:45.444: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec  3 16:32:45.625: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-166'
Dec  3 16:32:46.141: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:32:46.141: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:32:46.141: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-166'
Dec  3 16:32:46.688: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:32:46.688: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:32:46.688: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-166'
Dec  3 16:32:47.234: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:32:47.234: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:32:47.234: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-166'
Dec  3 16:32:47.769: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:32:47.769: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:32:47.769: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-166'
Dec  3 16:32:48.303: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:32:48.303: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 16:32:48.303: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-166'
Dec  3 16:32:48.832: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:32:48.832: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:32:48.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-166" for this suite.
Dec  3 16:33:27.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:33:30.685: INFO: namespace kubectl-166 deletion completed in 41.76269394s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:33:30.685: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-b0d53b89-cbd6-4e4a-96b0-c5dae16d0c06
STEP: Creating a pod to test consume secrets
Dec  3 16:33:31.505: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-87cb4d88-1227-4549-a4d3-ea21e42f7425" in namespace "projected-3745" to be "success or failure"
Dec  3 16:33:31.595: INFO: Pod "pod-projected-secrets-87cb4d88-1227-4549-a4d3-ea21e42f7425": Phase="Pending", Reason="", readiness=false. Elapsed: 89.682078ms
Dec  3 16:33:33.685: INFO: Pod "pod-projected-secrets-87cb4d88-1227-4549-a4d3-ea21e42f7425": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179927109s
STEP: Saw pod success
Dec  3 16:33:33.685: INFO: Pod "pod-projected-secrets-87cb4d88-1227-4549-a4d3-ea21e42f7425" satisfied condition "success or failure"
Dec  3 16:33:33.775: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod pod-projected-secrets-87cb4d88-1227-4549-a4d3-ea21e42f7425 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:33:33.965: INFO: Waiting for pod pod-projected-secrets-87cb4d88-1227-4549-a4d3-ea21e42f7425 to disappear
Dec  3 16:33:34.054: INFO: Pod pod-projected-secrets-87cb4d88-1227-4549-a4d3-ea21e42f7425 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:33:34.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3745" for this suite.
Dec  3 16:33:40.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:33:43.914: INFO: namespace projected-3745 deletion completed in 9.769089227s
•SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:33:43.914: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2181
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec  3 16:33:44.643: INFO: Waiting up to 5m0s for pod "downward-api-9d6d2866-0605-4107-9556-5e109599db79" in namespace "downward-api-2181" to be "success or failure"
Dec  3 16:33:44.733: INFO: Pod "downward-api-9d6d2866-0605-4107-9556-5e109599db79": Phase="Pending", Reason="", readiness=false. Elapsed: 89.36366ms
Dec  3 16:33:46.823: INFO: Pod "downward-api-9d6d2866-0605-4107-9556-5e109599db79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179338275s
STEP: Saw pod success
Dec  3 16:33:46.823: INFO: Pod "downward-api-9d6d2866-0605-4107-9556-5e109599db79" satisfied condition "success or failure"
Dec  3 16:33:46.912: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downward-api-9d6d2866-0605-4107-9556-5e109599db79 container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:33:47.102: INFO: Waiting for pod downward-api-9d6d2866-0605-4107-9556-5e109599db79 to disappear
Dec  3 16:33:47.191: INFO: Pod downward-api-9d6d2866-0605-4107-9556-5e109599db79 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:33:47.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2181" for this suite.
Dec  3 16:33:53.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:33:57.035: INFO: namespace downward-api-2181 deletion completed in 9.753361129s
•SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:33:57.036: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec  3 16:33:58.030: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1980,SelfLink:/api/v1/namespaces/watch-1980/configmaps/e2e-watch-test-configmap-a,UID:f8e25f1d-bd08-4e14-bf83-0df864515462,ResourceVersion:24366,Generation:0,CreationTimestamp:2019-12-03 16:33:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 16:33:58.030: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1980,SelfLink:/api/v1/namespaces/watch-1980/configmaps/e2e-watch-test-configmap-a,UID:f8e25f1d-bd08-4e14-bf83-0df864515462,ResourceVersion:24366,Generation:0,CreationTimestamp:2019-12-03 16:33:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec  3 16:34:08.211: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1980,SelfLink:/api/v1/namespaces/watch-1980/configmaps/e2e-watch-test-configmap-a,UID:f8e25f1d-bd08-4e14-bf83-0df864515462,ResourceVersion:24388,Generation:0,CreationTimestamp:2019-12-03 16:33:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  3 16:34:08.211: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1980,SelfLink:/api/v1/namespaces/watch-1980/configmaps/e2e-watch-test-configmap-a,UID:f8e25f1d-bd08-4e14-bf83-0df864515462,ResourceVersion:24388,Generation:0,CreationTimestamp:2019-12-03 16:33:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec  3 16:34:18.392: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1980,SelfLink:/api/v1/namespaces/watch-1980/configmaps/e2e-watch-test-configmap-a,UID:f8e25f1d-bd08-4e14-bf83-0df864515462,ResourceVersion:24411,Generation:0,CreationTimestamp:2019-12-03 16:33:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 16:34:18.392: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1980,SelfLink:/api/v1/namespaces/watch-1980/configmaps/e2e-watch-test-configmap-a,UID:f8e25f1d-bd08-4e14-bf83-0df864515462,ResourceVersion:24411,Generation:0,CreationTimestamp:2019-12-03 16:33:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec  3 16:34:28.485: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1980,SelfLink:/api/v1/namespaces/watch-1980/configmaps/e2e-watch-test-configmap-a,UID:f8e25f1d-bd08-4e14-bf83-0df864515462,ResourceVersion:24433,Generation:0,CreationTimestamp:2019-12-03 16:33:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 16:34:28.485: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1980,SelfLink:/api/v1/namespaces/watch-1980/configmaps/e2e-watch-test-configmap-a,UID:f8e25f1d-bd08-4e14-bf83-0df864515462,ResourceVersion:24433,Generation:0,CreationTimestamp:2019-12-03 16:33:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec  3 16:34:38.577: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1980,SelfLink:/api/v1/namespaces/watch-1980/configmaps/e2e-watch-test-configmap-b,UID:0998a1c0-22c7-4207-bb97-c3624e392e45,ResourceVersion:24455,Generation:0,CreationTimestamp:2019-12-03 16:34:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 16:34:38.577: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1980,SelfLink:/api/v1/namespaces/watch-1980/configmaps/e2e-watch-test-configmap-b,UID:0998a1c0-22c7-4207-bb97-c3624e392e45,ResourceVersion:24455,Generation:0,CreationTimestamp:2019-12-03 16:34:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec  3 16:34:48.669: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1980,SelfLink:/api/v1/namespaces/watch-1980/configmaps/e2e-watch-test-configmap-b,UID:0998a1c0-22c7-4207-bb97-c3624e392e45,ResourceVersion:24477,Generation:0,CreationTimestamp:2019-12-03 16:34:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 16:34:48.669: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1980,SelfLink:/api/v1/namespaces/watch-1980/configmaps/e2e-watch-test-configmap-b,UID:0998a1c0-22c7-4207-bb97-c3624e392e45,ResourceVersion:24477,Generation:0,CreationTimestamp:2019-12-03 16:34:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:34:58.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1980" for this suite.
Dec  3 16:35:05.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:35:08.517: INFO: namespace watch-1980 deletion completed in 9.756421756s
•
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:35:08.517: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8946.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8946.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 16:35:12.429: INFO: DNS probes using dns-8946/dns-test-6455851f-6691-4272-8c49-9504f35403c9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:35:12.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8946" for this suite.
Dec  3 16:35:18.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:35:22.370: INFO: namespace dns-8946 deletion completed in 9.757597921s
•SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:35:22.370: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5463
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  3 16:35:27.917: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 16:35:28.007: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 16:35:30.007: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 16:35:30.097: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 16:35:32.007: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 16:35:32.097: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 16:35:34.007: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 16:35:34.097: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:35:34.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5463" for this suite.
Dec  3 16:35:56.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:35:59.943: INFO: namespace container-lifecycle-hook-5463 deletion completed in 25.756098903s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:35:59.943: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:36:00.674: INFO: Waiting up to 5m0s for pod "downwardapi-volume-25625b5a-6723-4e04-bb4f-66436021fb12" in namespace "downward-api-3307" to be "success or failure"
Dec  3 16:36:00.763: INFO: Pod "downwardapi-volume-25625b5a-6723-4e04-bb4f-66436021fb12": Phase="Pending", Reason="", readiness=false. Elapsed: 89.563811ms
Dec  3 16:36:02.853: INFO: Pod "downwardapi-volume-25625b5a-6723-4e04-bb4f-66436021fb12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179630962s
STEP: Saw pod success
Dec  3 16:36:02.853: INFO: Pod "downwardapi-volume-25625b5a-6723-4e04-bb4f-66436021fb12" satisfied condition "success or failure"
Dec  3 16:36:02.943: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-25625b5a-6723-4e04-bb4f-66436021fb12 container client-container: <nil>
STEP: delete the pod
Dec  3 16:36:03.133: INFO: Waiting for pod downwardapi-volume-25625b5a-6723-4e04-bb4f-66436021fb12 to disappear
Dec  3 16:36:03.223: INFO: Pod downwardapi-volume-25625b5a-6723-4e04-bb4f-66436021fb12 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:36:03.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3307" for this suite.
Dec  3 16:36:09.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:13.067: INFO: namespace downward-api-3307 deletion completed in 9.753491279s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:36:13.068: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  3 16:36:13.706: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7499'
Dec  3 16:36:14.746: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 16:36:14.746: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec  3 16:36:14.926: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-rst4p]
Dec  3 16:36:14.926: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-rst4p" in namespace "kubectl-7499" to be "running and ready"
Dec  3 16:36:15.015: INFO: Pod "e2e-test-nginx-rc-rst4p": Phase="Pending", Reason="", readiness=false. Elapsed: 89.417214ms
Dec  3 16:36:17.105: INFO: Pod "e2e-test-nginx-rc-rst4p": Phase="Running", Reason="", readiness=true. Elapsed: 2.179304398s
Dec  3 16:36:17.105: INFO: Pod "e2e-test-nginx-rc-rst4p" satisfied condition "running and ready"
Dec  3 16:36:17.105: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-rst4p]
Dec  3 16:36:17.105: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs rc/e2e-test-nginx-rc --namespace=kubectl-7499'
Dec  3 16:36:17.738: INFO: stderr: ""
Dec  3 16:36:17.738: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Dec  3 16:36:17.738: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm2em-9re.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-7499'
Dec  3 16:36:18.290: INFO: stderr: ""
Dec  3 16:36:18.290: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:36:18.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7499" for this suite.
Dec  3 16:36:24.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:28.141: INFO: namespace kubectl-7499 deletion completed in 9.760444069s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:36:28.141: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:36:28.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2770" for this suite.
Dec  3 16:36:35.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:38.803: INFO: namespace kubelet-test-2770 deletion completed in 9.752746767s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec  3 16:36:38.804: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:36:39.533: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c94f9b73-1c91-4dac-a593-e51b3ba5c868" in namespace "downward-api-3189" to be "success or failure"
Dec  3 16:36:39.622: INFO: Pod "downwardapi-volume-c94f9b73-1c91-4dac-a593-e51b3ba5c868": Phase="Pending", Reason="", readiness=false. Elapsed: 89.652327ms
Dec  3 16:36:41.713: INFO: Pod "downwardapi-volume-c94f9b73-1c91-4dac-a593-e51b3ba5c868": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180568139s
STEP: Saw pod success
Dec  3 16:36:41.713: INFO: Pod "downwardapi-volume-c94f9b73-1c91-4dac-a593-e51b3ba5c868" satisfied condition "success or failure"
Dec  3 16:36:41.804: INFO: Trying to get logs from node ip-10-250-31-164.ec2.internal pod downwardapi-volume-c94f9b73-1c91-4dac-a593-e51b3ba5c868 container client-container: <nil>
STEP: delete the pod
Dec  3 16:36:41.994: INFO: Waiting for pod downwardapi-volume-c94f9b73-1c91-4dac-a593-e51b3ba5c868 to disappear
Dec  3 16:36:42.083: INFO: Pod downwardapi-volume-c94f9b73-1c91-4dac-a593-e51b3ba5c868 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec  3 16:36:42.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3189" for this suite.
Dec  3 16:36:48.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:51.933: INFO: namespace downward-api-3189 deletion completed in 9.759384392s
•SSSSSSSSDec  3 16:36:51.933: INFO: Running AfterSuite actions on all nodes
Dec  3 16:36:51.933: INFO: Running AfterSuite actions on node 1
Dec  3 16:36:51.933: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 6973.069 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Flaked | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h56m50.762418746s
Test Suite Passed
