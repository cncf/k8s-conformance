Conformance test: not doing test setup.
I0722 19:58:23.744393    5669 e2e.go:129] Starting e2e run "6e115f74-6950-4f9c-8f26-a6c038b297c3" on Ginkgo node 1
{"msg":"Test Suite starting","total":339,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1626983902 - Will randomize all specs
Will run 339 of 5771 specs

Jul 22 19:58:23.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 19:58:23.886: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 22 19:58:23.942: INFO: Waiting up to 10m0s for all pods (need at least 1) in namespace 'kube-system' to be running and ready
Jul 22 19:58:24.022: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 22 19:58:24.022: INFO: expected 12 pod replicas in namespace 'kube-system', 12 are Running and Ready.
Jul 22 19:58:24.022: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 22 19:58:24.042: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'apiserver-proxy' (0 seconds elapsed)
Jul 22 19:58:24.042: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jul 22 19:58:24.042: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-driver-node' (0 seconds elapsed)
Jul 22 19:58:24.042: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 22 19:58:24.042: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Jul 22 19:58:24.042: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Jul 22 19:58:24.042: INFO: e2e test version: v1.21.2
Jul 22 19:58:24.051: INFO: kube-apiserver version: v1.21.2
Jul 22 19:58:24.051: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 19:58:24.063: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 19:58:24.064: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
W0722 19:58:24.129995    5669 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jul 22 19:58:24.130: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jul 22 19:58:24.160: INFO: PSP annotation exists on dry run pod: "extensions.gardener.cloud.provider-gcp.csi-driver-node"; assuming PodSecurityPolicy is enabled
W0722 19:58:24.171263    5669 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
W0722 19:58:24.182834    5669 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jul 22 19:58:24.199: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-244
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 19:58:24.353: INFO: Creating ReplicaSet my-hostname-basic-cb415fb6-cdee-433b-a4ae-b6960a58dabd
Jul 22 19:58:24.375: INFO: Pod name my-hostname-basic-cb415fb6-cdee-433b-a4ae-b6960a58dabd: Found 0 pods out of 1
Jul 22 19:58:29.387: INFO: Pod name my-hostname-basic-cb415fb6-cdee-433b-a4ae-b6960a58dabd: Found 1 pods out of 1
Jul 22 19:58:29.387: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-cb415fb6-cdee-433b-a4ae-b6960a58dabd" is running
Jul 22 19:58:31.410: INFO: Pod "my-hostname-basic-cb415fb6-cdee-433b-a4ae-b6960a58dabd-bm4kc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 19:58:24 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 19:58:24 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-cb415fb6-cdee-433b-a4ae-b6960a58dabd]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 19:58:24 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-cb415fb6-cdee-433b-a4ae-b6960a58dabd]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 19:58:24 +0000 UTC Reason: Message:}])
Jul 22 19:58:31.411: INFO: Trying to dial the pod
Jul 22 19:58:36.503: INFO: Controller my-hostname-basic-cb415fb6-cdee-433b-a4ae-b6960a58dabd: Got expected result from replica 1 [my-hostname-basic-cb415fb6-cdee-433b-a4ae-b6960a58dabd-bm4kc]: "my-hostname-basic-cb415fb6-cdee-433b-a4ae-b6960a58dabd-bm4kc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 19:58:36.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-244" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":1,"skipped":14,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 19:58:36.536: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-178
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jul 22 19:58:36.738: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 create -f -'
Jul 22 19:58:37.060: INFO: stderr: ""
Jul 22 19:58:37.061: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 19:58:37.061: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 19:58:37.159: INFO: stderr: ""
Jul 22 19:58:37.159: INFO: stdout: "update-demo-nautilus-8dg4q update-demo-nautilus-t48v5 "
Jul 22 19:58:37.159: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 get pods update-demo-nautilus-8dg4q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 19:58:37.259: INFO: stderr: ""
Jul 22 19:58:37.259: INFO: stdout: ""
Jul 22 19:58:37.259: INFO: update-demo-nautilus-8dg4q is created but not running
Jul 22 19:58:42.259: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 19:58:42.361: INFO: stderr: ""
Jul 22 19:58:42.362: INFO: stdout: "update-demo-nautilus-8dg4q update-demo-nautilus-t48v5 "
Jul 22 19:58:42.362: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 get pods update-demo-nautilus-8dg4q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 19:58:42.456: INFO: stderr: ""
Jul 22 19:58:42.456: INFO: stdout: ""
Jul 22 19:58:42.456: INFO: update-demo-nautilus-8dg4q is created but not running
Jul 22 19:58:47.456: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 19:58:47.557: INFO: stderr: ""
Jul 22 19:58:47.557: INFO: stdout: "update-demo-nautilus-8dg4q update-demo-nautilus-t48v5 "
Jul 22 19:58:47.557: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 get pods update-demo-nautilus-8dg4q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 19:58:47.649: INFO: stderr: ""
Jul 22 19:58:47.649: INFO: stdout: "true"
Jul 22 19:58:47.649: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 get pods update-demo-nautilus-8dg4q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 19:58:47.741: INFO: stderr: ""
Jul 22 19:58:47.741: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 22 19:58:47.741: INFO: validating pod update-demo-nautilus-8dg4q
Jul 22 19:58:47.769: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 19:58:47.769: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 19:58:47.769: INFO: update-demo-nautilus-8dg4q is verified up and running
Jul 22 19:58:47.769: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 get pods update-demo-nautilus-t48v5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 19:58:47.860: INFO: stderr: ""
Jul 22 19:58:47.860: INFO: stdout: "true"
Jul 22 19:58:47.860: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 get pods update-demo-nautilus-t48v5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 19:58:47.955: INFO: stderr: ""
Jul 22 19:58:47.955: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 22 19:58:47.955: INFO: validating pod update-demo-nautilus-t48v5
Jul 22 19:58:48.067: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 19:58:48.067: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 19:58:48.067: INFO: update-demo-nautilus-t48v5 is verified up and running
STEP: using delete to clean up resources
Jul 22 19:58:48.068: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 delete --grace-period=0 --force -f -'
Jul 22 19:58:48.171: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 19:58:48.171: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 22 19:58:48.171: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 get rc,svc -l name=update-demo --no-headers'
Jul 22 19:58:48.276: INFO: stderr: "No resources found in kubectl-178 namespace.\n"
Jul 22 19:58:48.276: INFO: stdout: ""
Jul 22 19:58:48.276: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-178 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 19:58:48.373: INFO: stderr: ""
Jul 22 19:58:48.374: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 19:58:48.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-178" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":339,"completed":2,"skipped":40,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 19:58:48.407: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 22 19:58:48.642: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9199  b689d24a-487c-49e0-bb38-d39e6971ac43 4451 0 2021-07-22 19:58:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-22 19:58:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 19:58:48.642: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9199  b689d24a-487c-49e0-bb38-d39e6971ac43 4452 0 2021-07-22 19:58:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-22 19:58:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 22 19:58:48.688: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9199  b689d24a-487c-49e0-bb38-d39e6971ac43 4453 0 2021-07-22 19:58:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-22 19:58:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 19:58:48.688: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9199  b689d24a-487c-49e0-bb38-d39e6971ac43 4454 0 2021-07-22 19:58:48 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-22 19:58:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 19:58:48.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9199" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":339,"completed":3,"skipped":40,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 19:58:48.712: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7722
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 19:58:48.918: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98994cca-2412-44da-ab7d-95b2c3d7ecdc" in namespace "projected-7722" to be "Succeeded or Failed"
Jul 22 19:58:48.929: INFO: Pod "downwardapi-volume-98994cca-2412-44da-ab7d-95b2c3d7ecdc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.456354ms
Jul 22 19:58:50.942: INFO: Pod "downwardapi-volume-98994cca-2412-44da-ab7d-95b2c3d7ecdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023996649s
STEP: Saw pod success
Jul 22 19:58:50.942: INFO: Pod "downwardapi-volume-98994cca-2412-44da-ab7d-95b2c3d7ecdc" satisfied condition "Succeeded or Failed"
Jul 22 19:58:50.953: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-98994cca-2412-44da-ab7d-95b2c3d7ecdc container client-container: <nil>
STEP: delete the pod
Jul 22 19:58:51.027: INFO: Waiting for pod downwardapi-volume-98994cca-2412-44da-ab7d-95b2c3d7ecdc to disappear
Jul 22 19:58:51.038: INFO: Pod downwardapi-volume-98994cca-2412-44da-ab7d-95b2c3d7ecdc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 19:58:51.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7722" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":4,"skipped":41,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 19:58:51.070: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 22 19:58:53.325: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 19:58:53.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-635" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":339,"completed":5,"skipped":90,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 19:58:53.393: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul 22 19:58:53.684: INFO: Waiting up to 5m0s for pod "downward-api-c668b0dd-0d35-41e5-906f-ba6ff3353bf7" in namespace "downward-api-8783" to be "Succeeded or Failed"
Jul 22 19:58:53.695: INFO: Pod "downward-api-c668b0dd-0d35-41e5-906f-ba6ff3353bf7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.428862ms
Jul 22 19:58:55.707: INFO: Pod "downward-api-c668b0dd-0d35-41e5-906f-ba6ff3353bf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023295332s
STEP: Saw pod success
Jul 22 19:58:55.707: INFO: Pod "downward-api-c668b0dd-0d35-41e5-906f-ba6ff3353bf7" satisfied condition "Succeeded or Failed"
Jul 22 19:58:55.718: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downward-api-c668b0dd-0d35-41e5-906f-ba6ff3353bf7 container dapi-container: <nil>
STEP: delete the pod
Jul 22 19:58:55.761: INFO: Waiting for pod downward-api-c668b0dd-0d35-41e5-906f-ba6ff3353bf7 to disappear
Jul 22 19:58:55.773: INFO: Pod downward-api-c668b0dd-0d35-41e5-906f-ba6ff3353bf7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 19:58:55.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8783" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":339,"completed":6,"skipped":100,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 19:58:55.806: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6604
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 19:59:04.056: INFO: Deleting pod "var-expansion-23358795-a6e0-4bf0-a509-f780f6d44298" in namespace "var-expansion-6604"
Jul 22 19:59:04.069: INFO: Wait up to 5m0s for pod "var-expansion-23358795-a6e0-4bf0-a509-f780f6d44298" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 19:59:16.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6604" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":339,"completed":7,"skipped":112,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 19:59:16.125: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7212
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 19:59:32.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7212" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":339,"completed":8,"skipped":120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 19:59:32.789: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-939
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 19:59:46.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-939" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":339,"completed":9,"skipped":151,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 19:59:46.271: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5848
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jul 22 19:59:46.498: INFO: The status of Pod annotationupdate7f5deb13-a6fa-4375-91aa-2c68d7089d84 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 19:59:48.510: INFO: The status of Pod annotationupdate7f5deb13-a6fa-4375-91aa-2c68d7089d84 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 19:59:50.510: INFO: The status of Pod annotationupdate7f5deb13-a6fa-4375-91aa-2c68d7089d84 is Running (Ready = true)
Jul 22 19:59:51.087: INFO: Successfully updated pod "annotationupdate7f5deb13-a6fa-4375-91aa-2c68d7089d84"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 19:59:53.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5848" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":10,"skipped":151,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 19:59:53.161: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1163
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jul 22 19:59:53.349: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jul 22 20:00:10.394: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:00:15.792: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:00:34.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1163" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":339,"completed":11,"skipped":153,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:00:34.185: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-3225
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jul 22 20:00:34.431: INFO: running pods: 0 < 1
Jul 22 20:00:36.443: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:00:38.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3225" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":339,"completed":12,"skipped":165,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:00:38.569: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7668
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:00:38.761: INFO: Creating deployment "webserver-deployment"
Jul 22 20:00:38.773: INFO: Waiting for observed generation 1
Jul 22 20:00:40.796: INFO: Waiting for all required pods to come up
Jul 22 20:00:40.816: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 22 20:00:50.840: INFO: Waiting for deployment "webserver-deployment" to complete
Jul 22 20:00:50.863: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul 22 20:00:50.888: INFO: Updating deployment webserver-deployment
Jul 22 20:00:50.888: INFO: Waiting for observed generation 2
Jul 22 20:00:52.912: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 22 20:00:52.923: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 22 20:00:52.934: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 22 20:00:52.967: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 22 20:00:52.967: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 22 20:00:52.979: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 22 20:00:53.001: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul 22 20:00:53.001: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul 22 20:00:53.026: INFO: Updating deployment webserver-deployment
Jul 22 20:00:53.026: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul 22 20:00:53.048: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 22 20:00:55.070: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 22 20:00:55.092: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7668  c38fee16-488d-48ad-8294-e05b0f7deecf 5629 3 2021-07-22 20:00:38 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-22 20:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d0f868 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-22 20:00:53 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-07-22 20:00:53 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jul 22 20:00:55.104: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-7668  9fdd35fa-8821-4cc4-bd2d-f497811f28b6 5624 3 2021-07-22 20:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c38fee16-488d-48ad-8294-e05b0f7deecf 0xc004d0fd17 0xc004d0fd18}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c38fee16-488d-48ad-8294-e05b0f7deecf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d0fda8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:00:55.104: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul 22 20:00:55.104: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-7668  020ec1e4-bbc8-42dd-87a2-0cc15b371573 5625 3 2021-07-22 20:00:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c38fee16-488d-48ad-8294-e05b0f7deecf 0xc004d0fe17 0xc004d0fe18}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:00:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c38fee16-488d-48ad-8294-e05b0f7deecf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d0ff08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:00:55.135: INFO: Pod "webserver-deployment-795d758f88-68gld" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-68gld webserver-deployment-795d758f88- deployment-7668  24cdc2c6-d6c3-457c-b50d-7d9fa91fd76b 5626 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d60060 0xc004d60061}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f4v4n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f4v4n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.135: INFO: Pod "webserver-deployment-795d758f88-6r789" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6r789 webserver-deployment-795d758f88- deployment-7668  f0650f9f-73ab-4b4e-b9cc-036bfb52d5c4 5640 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d60340 0xc004d60341}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kthnt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kthnt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.135: INFO: Pod "webserver-deployment-795d758f88-8b9cd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8b9cd webserver-deployment-795d758f88- deployment-7668  8d98d7e3-1ed3-49f0-accd-865d80c786db 5603 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d60590 0xc004d60591}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bz7ps,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bz7ps,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.136: INFO: Pod "webserver-deployment-795d758f88-gn558" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gn558 webserver-deployment-795d758f88- deployment-7668  61a53104-37c4-4a95-bdb5-a4eee0099394 5605 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d60820 0xc004d60821}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b7q5x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b7q5x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.136: INFO: Pod "webserver-deployment-795d758f88-jdr4v" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jdr4v webserver-deployment-795d758f88- deployment-7668  d1a56ec2-de8d-4094-8151-161ccc2eef0b 5579 0 2021-07-22 20:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.22/32 cni.projectcalico.org/podIPs:100.96.1.22/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d60b00 0xc004d60b01}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7v6bh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7v6bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.136: INFO: Pod "webserver-deployment-795d758f88-n2jss" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-n2jss webserver-deployment-795d758f88- deployment-7668  0f635fa3-73a0-4c79-91ea-8886bf3e5748 5556 0 2021-07-22 20:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.0.21/32 cni.projectcalico.org/podIPs:100.96.0.21/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d60ec0 0xc004d60ec1}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-22 20:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rl4pm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rl4pm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2021-07-22 20:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.136: INFO: Pod "webserver-deployment-795d758f88-qhbjz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qhbjz webserver-deployment-795d758f88- deployment-7668  5e32a28b-ac27-41d3-9c8e-49c842631613 5559 0 2021-07-22 20:00:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.21/32 cni.projectcalico.org/podIPs:100.96.1.21/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d61260 0xc004d61261}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-22 20:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xd8gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xd8gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.136: INFO: Pod "webserver-deployment-795d758f88-qxz6h" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qxz6h webserver-deployment-795d758f88- deployment-7668  9a136202-b45a-4da4-8f46-c42879cd135a 5634 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d61520 0xc004d61521}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bwvwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bwvwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.136: INFO: Pod "webserver-deployment-795d758f88-rfg5h" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rfg5h webserver-deployment-795d758f88- deployment-7668  6124642b-d238-4d4f-bb16-8d8580155883 5599 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d61750 0xc004d61751}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbvqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbvqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.136: INFO: Pod "webserver-deployment-795d758f88-sv72k" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-sv72k webserver-deployment-795d758f88- deployment-7668  1dd4d7ba-7128-446e-8d59-0ad6e8aa08ad 5601 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d619e0 0xc004d619e1}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dvgtr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dvgtr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.137: INFO: Pod "webserver-deployment-795d758f88-t4l5n" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-t4l5n webserver-deployment-795d758f88- deployment-7668  dce314e1-1903-4a2f-a9d6-b345a0f84db9 5593 0 2021-07-22 20:00:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.1.23/32 cni.projectcalico.org/podIPs:100.96.1.23/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d61d30 0xc004d61d31}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p2csd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p2csd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.137: INFO: Pod "webserver-deployment-795d758f88-vzjhf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vzjhf webserver-deployment-795d758f88- deployment-7668  e522b9b9-d0ac-4c32-9086-2874cb14df59 5627 0 2021-07-22 20:00:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:100.96.0.20/32 cni.projectcalico.org/podIPs:100.96.0.20/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d61fd0 0xc004d61fd1}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:00:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nwl46,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nwl46,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.20,StartTime:2021-07-22 20:00:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.137: INFO: Pod "webserver-deployment-795d758f88-zvxzd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zvxzd webserver-deployment-795d758f88- deployment-7668  6b18e478-6e5e-48b8-8882-499db2b0c4c5 5581 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9fdd35fa-8821-4cc4-bd2d-f497811f28b6 0xc004d82300 0xc004d82301}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fdd35fa-8821-4cc4-bd2d-f497811f28b6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rhngt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rhngt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.137: INFO: Pod "webserver-deployment-847dcfb7fb-2l7wf" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2l7wf webserver-deployment-847dcfb7fb- deployment-7668  3042b466-69ca-4980-8fca-65cdb173a649 5472 0 2021-07-22 20:00:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.18/32 cni.projectcalico.org/podIPs:100.96.0.18/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004d82580 0xc004d82581}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:00:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h76mj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h76mj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.18,StartTime:2021-07-22 20:00:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://fb5e5e1ec576f8edf6b4a8c5a17c7e8337fa345a20462272581580270ac94bcb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.137: INFO: Pod "webserver-deployment-847dcfb7fb-7g6tk" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-7g6tk webserver-deployment-847dcfb7fb- deployment-7668  9a38a319-e58e-47b6-a563-89a75a8b266b 5481 0 2021-07-22 20:00:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.16/32 cni.projectcalico.org/podIPs:100.96.0.16/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004d82900 0xc004d82901}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:00:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dtfk2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dtfk2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.16,StartTime:2021-07-22 20:00:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://792af01f73e667dd6f47b6bbf8facb068ba21d4aef900afbc1468feff7496438,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.137: INFO: Pod "webserver-deployment-847dcfb7fb-8d5sx" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8d5sx webserver-deployment-847dcfb7fb- deployment-7668  a2b1cb77-68d3-4e3a-bbbc-b985a939ca2b 5616 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004d82b70 0xc004d82b71}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8w9k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8w9k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.137: INFO: Pod "webserver-deployment-847dcfb7fb-8phbm" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8phbm webserver-deployment-847dcfb7fb- deployment-7668  28c71ccc-e26a-4c2a-bcfe-85306ec84580 5615 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004d82ec0 0xc004d82ec1}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pggm5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pggm5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.138: INFO: Pod "webserver-deployment-847dcfb7fb-9dsgb" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-9dsgb webserver-deployment-847dcfb7fb- deployment-7668  a2b4296c-ea5f-4330-bb95-c927a3fda3fd 5465 0 2021-07-22 20:00:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.17/32 cni.projectcalico.org/podIPs:100.96.1.17/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004d83260 0xc004d83261}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:00:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-68p2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-68p2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.17,StartTime:2021-07-22 20:00:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:00:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://8845151efd7bb3ebc1cec370182eb7aa27b7ab1eac660e499a288f180a3c4cd8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.138: INFO: Pod "webserver-deployment-847dcfb7fb-b9w2h" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-b9w2h webserver-deployment-847dcfb7fb- deployment-7668  413d7f21-c888-424c-a83f-b30f0097b811 5639 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004d83580 0xc004d83581}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7kf68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7kf68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.138: INFO: Pod "webserver-deployment-847dcfb7fb-fkxrp" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-fkxrp webserver-deployment-847dcfb7fb- deployment-7668  bbc884a8-59f7-4431-a842-5f0a4e585cd6 5462 0 2021-07-22 20:00:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.16/32 cni.projectcalico.org/podIPs:100.96.1.16/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004d838b0 0xc004d838b1}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:00:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f584d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f584d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.16,StartTime:2021-07-22 20:00:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:00:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://f5693b585d5f3685c2b6aa21710f2790383deec5ec6570631456539e3bb29b21,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.138: INFO: Pod "webserver-deployment-847dcfb7fb-fxlc8" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-fxlc8 webserver-deployment-847dcfb7fb- deployment-7668  87507c07-6c36-4ac8-9029-0dfffe9d1ef1 5628 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004d83b30 0xc004d83b31}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4v85,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4v85,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.138: INFO: Pod "webserver-deployment-847dcfb7fb-g8gcq" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-g8gcq webserver-deployment-847dcfb7fb- deployment-7668  d575331a-f5e3-4b88-8a0a-8f0203f3f01b 5475 0 2021-07-22 20:00:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.19/32 cni.projectcalico.org/podIPs:100.96.0.19/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004d83e90 0xc004d83e91}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:00:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bt6z4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bt6z4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.19,StartTime:2021-07-22 20:00:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://0240b7587f54a3df7b55c8182bf1ab550aa84d4e0defa9f0be8345f2b1d31760,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.138: INFO: Pod "webserver-deployment-847dcfb7fb-mln4c" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mln4c webserver-deployment-847dcfb7fb- deployment-7668  77372201-65c8-469b-a73c-2b8971a3b2fa 5494 0 2021-07-22 20:00:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.19/32 cni.projectcalico.org/podIPs:100.96.1.19/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004db4160 0xc004db4161}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:00:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:00:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhjwt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhjwt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.19,StartTime:2021-07-22 20:00:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://2f127b58d53bd203dd25373c7e3918b182bd3db23808bf30826c5d40f6bfaddc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.138: INFO: Pod "webserver-deployment-847dcfb7fb-nrznb" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-nrznb webserver-deployment-847dcfb7fb- deployment-7668  1aaf3582-f1ee-46e8-984d-b5ab31873a88 5635 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004db4430 0xc004db4431}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9x5z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9x5z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.139: INFO: Pod "webserver-deployment-847dcfb7fb-p66bt" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-p66bt webserver-deployment-847dcfb7fb- deployment-7668  d778a661-3d5a-40fd-8439-85a7f9396361 5478 0 2021-07-22 20:00:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.0.17/32 cni.projectcalico.org/podIPs:100.96.0.17/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004db4670 0xc004db4671}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:00:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.0.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2dv28,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2dv28,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.0.17,StartTime:2021-07-22 20:00:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:00:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://3d10ef589fc89ae0a1e1025b763c46f4fa2099a7726a444b3f4822809e4c8504,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.0.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.139: INFO: Pod "webserver-deployment-847dcfb7fb-qccs7" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-qccs7 webserver-deployment-847dcfb7fb- deployment-7668  d76048df-3fcd-4d02-b117-1dfb6938a1d1 5641 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004db4890 0xc004db4891}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r84qm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r84qm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.139: INFO: Pod "webserver-deployment-847dcfb7fb-qpv7v" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-qpv7v webserver-deployment-847dcfb7fb- deployment-7668  3db126a2-7be8-4bea-8692-66443a79b125 5631 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004db4b20 0xc004db4b21}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qmf7r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qmf7r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.139: INFO: Pod "webserver-deployment-847dcfb7fb-r5tn8" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-r5tn8 webserver-deployment-847dcfb7fb- deployment-7668  46fa1f76-b685-43c0-9c89-bd10e62184f1 5600 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004db4d40 0xc004db4d41}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dj9cx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dj9cx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.139: INFO: Pod "webserver-deployment-847dcfb7fb-rllkf" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-rllkf webserver-deployment-847dcfb7fb- deployment-7668  1d325cd2-c9ae-4889-85de-d506e6034faa 5602 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004db5010 0xc004db5011}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8qtgt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8qtgt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.140: INFO: Pod "webserver-deployment-847dcfb7fb-tp672" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tp672 webserver-deployment-847dcfb7fb- deployment-7668  0fb1c261-37c4-431f-9674-79fff330efa6 5587 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004db5230 0xc004db5231}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4jm6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4jm6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.140: INFO: Pod "webserver-deployment-847dcfb7fb-tsm9j" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tsm9j webserver-deployment-847dcfb7fb- deployment-7668  7fb3e776-fa62-46a2-a77c-dcb3d5a67972 5637 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004db54c0 0xc004db54c1}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9lplc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9lplc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.140: INFO: Pod "webserver-deployment-847dcfb7fb-vhxc7" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-vhxc7 webserver-deployment-847dcfb7fb- deployment-7668  3be090bb-f77a-4a3a-8610-4c4a8072463a 5617 0 2021-07-22 20:00:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004db5760 0xc004db5761}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:00:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9kqpv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9kqpv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2021-07-22 20:00:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:00:55.140: INFO: Pod "webserver-deployment-847dcfb7fb-xmvs6" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-xmvs6 webserver-deployment-847dcfb7fb- deployment-7668  73912f21-4c8b-416a-b737-92573f1eeecf 5468 0 2021-07-22 20:00:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.15/32 cni.projectcalico.org/podIPs:100.96.1.15/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 020ec1e4-bbc8-42dd-87a2-0cc15b371573 0xc004db5a30 0xc004db5a31}] []  [{kube-controller-manager Update v1 2021-07-22 20:00:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"020ec1e4-bbc8-42dd-87a2-0cc15b371573\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:00:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:00:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dlwdq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dlwdq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:00:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.15,StartTime:2021-07-22 20:00:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:00:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://27491b92d8a980da2856b82e1949ce87c4725911e6330c356c64c89f4f1bdf89,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:00:55.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7668" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":339,"completed":13,"skipped":192,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:00:55.170: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1915
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:01:09.411: INFO: Deleting pod "var-expansion-39201d96-b435-4915-8dc4-7fd3f0c7147e" in namespace "var-expansion-1915"
Jul 22 20:01:09.423: INFO: Wait up to 5m0s for pod "var-expansion-39201d96-b435-4915-8dc4-7fd3f0c7147e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:01:17.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1915" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":339,"completed":14,"skipped":237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:01:17.478: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5390
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:01:17.670: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jul 22 20:01:23.071: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 --namespace=crd-publish-openapi-5390 create -f -'
Jul 22 20:01:23.756: INFO: stderr: ""
Jul 22 20:01:23.756: INFO: stdout: "e2e-test-crd-publish-openapi-4367-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 22 20:01:23.756: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 --namespace=crd-publish-openapi-5390 delete e2e-test-crd-publish-openapi-4367-crds test-foo'
Jul 22 20:01:23.876: INFO: stderr: ""
Jul 22 20:01:23.876: INFO: stdout: "e2e-test-crd-publish-openapi-4367-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul 22 20:01:23.877: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 --namespace=crd-publish-openapi-5390 apply -f -'
Jul 22 20:01:24.380: INFO: stderr: ""
Jul 22 20:01:24.380: INFO: stdout: "e2e-test-crd-publish-openapi-4367-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 22 20:01:24.381: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 --namespace=crd-publish-openapi-5390 delete e2e-test-crd-publish-openapi-4367-crds test-foo'
Jul 22 20:01:24.503: INFO: stderr: ""
Jul 22 20:01:24.503: INFO: stdout: "e2e-test-crd-publish-openapi-4367-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jul 22 20:01:24.503: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 --namespace=crd-publish-openapi-5390 create -f -'
Jul 22 20:01:24.878: INFO: rc: 1
Jul 22 20:01:24.878: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 --namespace=crd-publish-openapi-5390 apply -f -'
Jul 22 20:01:25.211: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jul 22 20:01:25.212: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 --namespace=crd-publish-openapi-5390 create -f -'
Jul 22 20:01:25.582: INFO: rc: 1
Jul 22 20:01:25.582: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 --namespace=crd-publish-openapi-5390 apply -f -'
Jul 22 20:01:25.844: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jul 22 20:01:25.844: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 explain e2e-test-crd-publish-openapi-4367-crds'
Jul 22 20:01:26.202: INFO: stderr: ""
Jul 22 20:01:26.202: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4367-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jul 22 20:01:26.202: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 explain e2e-test-crd-publish-openapi-4367-crds.metadata'
Jul 22 20:01:26.455: INFO: stderr: ""
Jul 22 20:01:26.455: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4367-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul 22 20:01:26.455: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 explain e2e-test-crd-publish-openapi-4367-crds.spec'
Jul 22 20:01:26.814: INFO: stderr: ""
Jul 22 20:01:26.814: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4367-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul 22 20:01:26.815: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 explain e2e-test-crd-publish-openapi-4367-crds.spec.bars'
Jul 22 20:01:27.214: INFO: stderr: ""
Jul 22 20:01:27.214: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4367-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jul 22 20:01:27.214: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5390 explain e2e-test-crd-publish-openapi-4367-crds.spec.bars2'
Jul 22 20:01:27.584: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:01:32.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5390" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":339,"completed":15,"skipped":309,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:01:32.535: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-f1f54eaf-3f9b-481c-a365-b44fb0d018bf
STEP: Creating a pod to test consume configMaps
Jul 22 20:01:32.762: INFO: Waiting up to 5m0s for pod "pod-configmaps-15e3f7e9-118e-45bf-91c1-719c15ff8f12" in namespace "configmap-2614" to be "Succeeded or Failed"
Jul 22 20:01:32.778: INFO: Pod "pod-configmaps-15e3f7e9-118e-45bf-91c1-719c15ff8f12": Phase="Pending", Reason="", readiness=false. Elapsed: 15.845369ms
Jul 22 20:01:34.790: INFO: Pod "pod-configmaps-15e3f7e9-118e-45bf-91c1-719c15ff8f12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027678695s
STEP: Saw pod success
Jul 22 20:01:34.790: INFO: Pod "pod-configmaps-15e3f7e9-118e-45bf-91c1-719c15ff8f12" satisfied condition "Succeeded or Failed"
Jul 22 20:01:34.801: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-configmaps-15e3f7e9-118e-45bf-91c1-719c15ff8f12 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:01:34.874: INFO: Waiting for pod pod-configmaps-15e3f7e9-118e-45bf-91c1-719c15ff8f12 to disappear
Jul 22 20:01:34.885: INFO: Pod pod-configmaps-15e3f7e9-118e-45bf-91c1-719c15ff8f12 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:01:34.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2614" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":16,"skipped":316,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:01:34.920: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7414
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 22 20:01:35.176: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7414  845dbb52-f2ec-4d76-b1a1-2b745b9e48ce 6083 0 2021-07-22 20:01:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-22 20:01:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 20:01:35.177: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7414  845dbb52-f2ec-4d76-b1a1-2b745b9e48ce 6084 0 2021-07-22 20:01:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-22 20:01:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 20:01:35.177: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7414  845dbb52-f2ec-4d76-b1a1-2b745b9e48ce 6085 0 2021-07-22 20:01:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-22 20:01:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 22 20:01:45.258: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7414  845dbb52-f2ec-4d76-b1a1-2b745b9e48ce 6155 0 2021-07-22 20:01:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-22 20:01:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 20:01:45.259: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7414  845dbb52-f2ec-4d76-b1a1-2b745b9e48ce 6156 0 2021-07-22 20:01:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-22 20:01:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 20:01:45.259: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7414  845dbb52-f2ec-4d76-b1a1-2b745b9e48ce 6157 0 2021-07-22 20:01:35 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-22 20:01:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:01:45.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7414" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":339,"completed":17,"skipped":322,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:01:45.291: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 22 20:01:46.101: INFO: Pod name wrapped-volume-race-d39c7071-6002-49d5-bad3-bb3c34f65686: Found 0 pods out of 5
Jul 22 20:01:51.141: INFO: Pod name wrapped-volume-race-d39c7071-6002-49d5-bad3-bb3c34f65686: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d39c7071-6002-49d5-bad3-bb3c34f65686 in namespace emptydir-wrapper-1803, will wait for the garbage collector to delete the pods
Jul 22 20:01:53.297: INFO: Deleting ReplicationController wrapped-volume-race-d39c7071-6002-49d5-bad3-bb3c34f65686 took: 14.41309ms
Jul 22 20:01:53.398: INFO: Terminating ReplicationController wrapped-volume-race-d39c7071-6002-49d5-bad3-bb3c34f65686 pods took: 101.200533ms
STEP: Creating RC which spawns configmap-volume pods
Jul 22 20:02:08.039: INFO: Pod name wrapped-volume-race-553a191e-c055-4b66-b110-a0d36df4f014: Found 0 pods out of 5
Jul 22 20:02:13.112: INFO: Pod name wrapped-volume-race-553a191e-c055-4b66-b110-a0d36df4f014: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-553a191e-c055-4b66-b110-a0d36df4f014 in namespace emptydir-wrapper-1803, will wait for the garbage collector to delete the pods
Jul 22 20:02:13.391: INFO: Deleting ReplicationController wrapped-volume-race-553a191e-c055-4b66-b110-a0d36df4f014 took: 42.313798ms
Jul 22 20:02:13.592: INFO: Terminating ReplicationController wrapped-volume-race-553a191e-c055-4b66-b110-a0d36df4f014 pods took: 200.469308ms
STEP: Creating RC which spawns configmap-volume pods
Jul 22 20:02:27.933: INFO: Pod name wrapped-volume-race-258267df-849d-4659-88a1-3854ca118a2a: Found 0 pods out of 5
Jul 22 20:02:32.966: INFO: Pod name wrapped-volume-race-258267df-849d-4659-88a1-3854ca118a2a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-258267df-849d-4659-88a1-3854ca118a2a in namespace emptydir-wrapper-1803, will wait for the garbage collector to delete the pods
Jul 22 20:02:35.120: INFO: Deleting ReplicationController wrapped-volume-race-258267df-849d-4659-88a1-3854ca118a2a took: 13.201184ms
Jul 22 20:02:35.220: INFO: Terminating ReplicationController wrapped-volume-race-258267df-849d-4659-88a1-3854ca118a2a pods took: 100.475428ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:02:48.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1803" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":339,"completed":18,"skipped":325,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:02:48.548: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1911
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Jul 22 20:02:48.748: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jul 22 20:02:48.748: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1911 create -f -'
Jul 22 20:02:49.241: INFO: stderr: ""
Jul 22 20:02:49.241: INFO: stdout: "service/agnhost-replica created\n"
Jul 22 20:02:49.241: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jul 22 20:02:49.241: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1911 create -f -'
Jul 22 20:02:49.599: INFO: stderr: ""
Jul 22 20:02:49.599: INFO: stdout: "service/agnhost-primary created\n"
Jul 22 20:02:49.599: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 22 20:02:49.599: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1911 create -f -'
Jul 22 20:02:49.965: INFO: stderr: ""
Jul 22 20:02:49.965: INFO: stdout: "service/frontend created\n"
Jul 22 20:02:49.965: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul 22 20:02:49.965: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1911 create -f -'
Jul 22 20:02:50.404: INFO: stderr: ""
Jul 22 20:02:50.404: INFO: stdout: "deployment.apps/frontend created\n"
Jul 22 20:02:50.404: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 22 20:02:50.404: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1911 create -f -'
Jul 22 20:02:50.828: INFO: stderr: ""
Jul 22 20:02:50.828: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jul 22 20:02:50.828: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 22 20:02:50.828: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1911 create -f -'
Jul 22 20:02:51.303: INFO: stderr: ""
Jul 22 20:02:51.303: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jul 22 20:02:51.303: INFO: Waiting for all frontend pods to be Running.
Jul 22 20:03:01.356: INFO: Waiting for frontend to serve content.
Jul 22 20:03:01.428: INFO: Trying to add a new entry to the guestbook.
Jul 22 20:03:01.448: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul 22 20:03:01.472: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1911 delete --grace-period=0 --force -f -'
Jul 22 20:03:01.583: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 20:03:01.583: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 20:03:01.583: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1911 delete --grace-period=0 --force -f -'
Jul 22 20:03:01.717: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 20:03:01.717: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 20:03:01.717: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1911 delete --grace-period=0 --force -f -'
Jul 22 20:03:01.864: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 20:03:01.864: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 20:03:01.865: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1911 delete --grace-period=0 --force -f -'
Jul 22 20:03:01.979: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 20:03:01.979: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 20:03:01.980: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1911 delete --grace-period=0 --force -f -'
Jul 22 20:03:02.088: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 20:03:02.088: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 20:03:02.089: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1911 delete --grace-period=0 --force -f -'
Jul 22 20:03:02.201: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 20:03:02.201: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:03:02.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1911" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":339,"completed":19,"skipped":353,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:03:02.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2286
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-d29fb7bc-d51a-4719-9861-13b39295e178
STEP: Creating the pod
Jul 22 20:03:02.474: INFO: The status of Pod pod-projected-configmaps-778e298f-6b57-4df7-8045-4ee9e23ae3b5 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:03:04.486: INFO: The status of Pod pod-projected-configmaps-778e298f-6b57-4df7-8045-4ee9e23ae3b5 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:03:06.486: INFO: The status of Pod pod-projected-configmaps-778e298f-6b57-4df7-8045-4ee9e23ae3b5 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-d29fb7bc-d51a-4719-9861-13b39295e178
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:04:17.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2286" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":20,"skipped":367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:04:17.782: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2839
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Jul 22 20:04:17.978: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2839 create -f -'
Jul 22 20:04:18.259: INFO: stderr: ""
Jul 22 20:04:18.259: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jul 22 20:04:18.259: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2839 diff -f -'
Jul 22 20:04:18.624: INFO: rc: 1
Jul 22 20:04:18.624: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2839 delete -f -'
Jul 22 20:04:18.755: INFO: stderr: ""
Jul 22 20:04:18.755: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:04:18.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2839" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":339,"completed":21,"skipped":414,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:04:18.791: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 22 20:04:18.999: INFO: Waiting up to 5m0s for pod "pod-44f5fd54-f1ae-4097-afea-524a5760e243" in namespace "emptydir-4442" to be "Succeeded or Failed"
Jul 22 20:04:19.010: INFO: Pod "pod-44f5fd54-f1ae-4097-afea-524a5760e243": Phase="Pending", Reason="", readiness=false. Elapsed: 10.52265ms
Jul 22 20:04:21.022: INFO: Pod "pod-44f5fd54-f1ae-4097-afea-524a5760e243": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022792059s
STEP: Saw pod success
Jul 22 20:04:21.022: INFO: Pod "pod-44f5fd54-f1ae-4097-afea-524a5760e243" satisfied condition "Succeeded or Failed"
Jul 22 20:04:21.033: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-44f5fd54-f1ae-4097-afea-524a5760e243 container test-container: <nil>
STEP: delete the pod
Jul 22 20:04:21.070: INFO: Waiting for pod pod-44f5fd54-f1ae-4097-afea-524a5760e243 to disappear
Jul 22 20:04:21.081: INFO: Pod pod-44f5fd54-f1ae-4097-afea-524a5760e243 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:04:21.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4442" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":22,"skipped":435,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:04:21.114: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5070
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Jul 22 20:04:21.873: INFO: created pod pod-service-account-defaultsa
Jul 22 20:04:21.873: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 22 20:04:21.891: INFO: created pod pod-service-account-mountsa
Jul 22 20:04:21.891: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 22 20:04:21.908: INFO: created pod pod-service-account-nomountsa
Jul 22 20:04:21.908: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 22 20:04:21.925: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 22 20:04:21.925: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 22 20:04:21.942: INFO: created pod pod-service-account-mountsa-mountspec
Jul 22 20:04:21.942: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 22 20:04:21.993: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 22 20:04:21.993: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 22 20:04:22.008: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 22 20:04:22.008: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 22 20:04:22.024: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 22 20:04:22.024: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 22 20:04:22.040: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 22 20:04:22.040: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:04:22.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5070" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":339,"completed":23,"skipped":448,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:04:22.105: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1001
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-1001
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 20:04:22.290: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 22 20:04:22.367: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:04:24.379: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:04:26.380: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:04:28.382: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:04:30.379: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:04:32.379: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:04:34.379: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:04:36.380: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:04:38.382: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:04:40.380: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:04:42.383: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 22 20:04:42.407: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 22 20:04:44.474: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul 22 20:04:44.474: INFO: Breadth first check of 100.96.0.50 on host 10.250.0.3...
Jul 22 20:04:44.485: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.48:9080/dial?request=hostname&protocol=http&host=100.96.0.50&port=8080&tries=1'] Namespace:pod-network-test-1001 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:04:44.485: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:04:44.696: INFO: Waiting for responses: map[]
Jul 22 20:04:44.696: INFO: reached 100.96.0.50 after 0/1 tries
Jul 22 20:04:44.696: INFO: Breadth first check of 100.96.1.47 on host 10.250.0.2...
Jul 22 20:04:44.708: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.48:9080/dial?request=hostname&protocol=http&host=100.96.1.47&port=8080&tries=1'] Namespace:pod-network-test-1001 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:04:44.708: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:04:44.975: INFO: Waiting for responses: map[]
Jul 22 20:04:44.975: INFO: reached 100.96.1.47 after 0/1 tries
Jul 22 20:04:44.975: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:04:44.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1001" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":339,"completed":24,"skipped":448,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:04:45.008: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-4fcd
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 20:04:45.276: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-4fcd" in namespace "subpath-412" to be "Succeeded or Failed"
Jul 22 20:04:45.287: INFO: Pod "pod-subpath-test-secret-4fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.136332ms
Jul 22 20:04:47.299: INFO: Pod "pod-subpath-test-secret-4fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022891859s
Jul 22 20:04:49.311: INFO: Pod "pod-subpath-test-secret-4fcd": Phase="Running", Reason="", readiness=true. Elapsed: 4.034423304s
Jul 22 20:04:51.323: INFO: Pod "pod-subpath-test-secret-4fcd": Phase="Running", Reason="", readiness=true. Elapsed: 6.046864843s
Jul 22 20:04:53.336: INFO: Pod "pod-subpath-test-secret-4fcd": Phase="Running", Reason="", readiness=true. Elapsed: 8.059539142s
Jul 22 20:04:55.351: INFO: Pod "pod-subpath-test-secret-4fcd": Phase="Running", Reason="", readiness=true. Elapsed: 10.074712757s
Jul 22 20:04:57.363: INFO: Pod "pod-subpath-test-secret-4fcd": Phase="Running", Reason="", readiness=true. Elapsed: 12.086910521s
Jul 22 20:04:59.375: INFO: Pod "pod-subpath-test-secret-4fcd": Phase="Running", Reason="", readiness=true. Elapsed: 14.098780828s
Jul 22 20:05:01.388: INFO: Pod "pod-subpath-test-secret-4fcd": Phase="Running", Reason="", readiness=true. Elapsed: 16.111281935s
Jul 22 20:05:03.400: INFO: Pod "pod-subpath-test-secret-4fcd": Phase="Running", Reason="", readiness=true. Elapsed: 18.123760799s
Jul 22 20:05:05.413: INFO: Pod "pod-subpath-test-secret-4fcd": Phase="Running", Reason="", readiness=true. Elapsed: 20.136451729s
Jul 22 20:05:07.425: INFO: Pod "pod-subpath-test-secret-4fcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.149208852s
STEP: Saw pod success
Jul 22 20:05:07.426: INFO: Pod "pod-subpath-test-secret-4fcd" satisfied condition "Succeeded or Failed"
Jul 22 20:05:07.436: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-subpath-test-secret-4fcd container test-container-subpath-secret-4fcd: <nil>
STEP: delete the pod
Jul 22 20:05:07.476: INFO: Waiting for pod pod-subpath-test-secret-4fcd to disappear
Jul 22 20:05:07.487: INFO: Pod pod-subpath-test-secret-4fcd no longer exists
STEP: Deleting pod pod-subpath-test-secret-4fcd
Jul 22 20:05:07.487: INFO: Deleting pod "pod-subpath-test-secret-4fcd" in namespace "subpath-412"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:05:07.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-412" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":339,"completed":25,"skipped":529,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:05:07.530: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4095
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-c27a2119-1219-46fa-b08c-f323cba8e97d
STEP: Creating a pod to test consume configMaps
Jul 22 20:05:07.752: INFO: Waiting up to 5m0s for pod "pod-configmaps-080b6d16-a4c7-41b5-b16a-62a2244658fb" in namespace "configmap-4095" to be "Succeeded or Failed"
Jul 22 20:05:07.763: INFO: Pod "pod-configmaps-080b6d16-a4c7-41b5-b16a-62a2244658fb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.41301ms
Jul 22 20:05:09.776: INFO: Pod "pod-configmaps-080b6d16-a4c7-41b5-b16a-62a2244658fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023413674s
STEP: Saw pod success
Jul 22 20:05:09.776: INFO: Pod "pod-configmaps-080b6d16-a4c7-41b5-b16a-62a2244658fb" satisfied condition "Succeeded or Failed"
Jul 22 20:05:09.787: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-configmaps-080b6d16-a4c7-41b5-b16a-62a2244658fb container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 20:05:09.822: INFO: Waiting for pod pod-configmaps-080b6d16-a4c7-41b5-b16a-62a2244658fb to disappear
Jul 22 20:05:09.833: INFO: Pod pod-configmaps-080b6d16-a4c7-41b5-b16a-62a2244658fb no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:05:09.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4095" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":26,"skipped":537,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:05:09.865: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8519
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-0494ecfe-56e5-431d-82cd-53560eb37f1e
STEP: Creating a pod to test consume secrets
Jul 22 20:05:10.092: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6047b8df-5766-49d9-9818-2840b021fe6e" in namespace "projected-8519" to be "Succeeded or Failed"
Jul 22 20:05:10.103: INFO: Pod "pod-projected-secrets-6047b8df-5766-49d9-9818-2840b021fe6e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.80854ms
Jul 22 20:05:12.116: INFO: Pod "pod-projected-secrets-6047b8df-5766-49d9-9818-2840b021fe6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024839122s
Jul 22 20:05:14.129: INFO: Pod "pod-projected-secrets-6047b8df-5766-49d9-9818-2840b021fe6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037285506s
STEP: Saw pod success
Jul 22 20:05:14.129: INFO: Pod "pod-projected-secrets-6047b8df-5766-49d9-9818-2840b021fe6e" satisfied condition "Succeeded or Failed"
Jul 22 20:05:14.140: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-secrets-6047b8df-5766-49d9-9818-2840b021fe6e container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:05:14.178: INFO: Waiting for pod pod-projected-secrets-6047b8df-5766-49d9-9818-2840b021fe6e to disappear
Jul 22 20:05:14.188: INFO: Pod pod-projected-secrets-6047b8df-5766-49d9-9818-2840b021fe6e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:05:14.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8519" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":27,"skipped":553,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:05:14.221: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6274
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6274
STEP: creating replication controller externalsvc in namespace services-6274
I0722 20:05:14.462079    5669 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6274, replica count: 2
I0722 20:05:17.513577    5669 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jul 22 20:05:17.556: INFO: Creating new exec pod
Jul 22 20:05:21.598: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6274 exec execpoddm8l6 -- /bin/sh -x -c nslookup nodeport-service.services-6274.svc.cluster.local'
Jul 22 20:05:22.096: INFO: stderr: "+ nslookup nodeport-service.services-6274.svc.cluster.local\n"
Jul 22 20:05:22.096: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nnodeport-service.services-6274.svc.cluster.local\tcanonical name = externalsvc.services-6274.svc.cluster.local.\nName:\texternalsvc.services-6274.svc.cluster.local\nAddress: 100.64.2.191\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6274, will wait for the garbage collector to delete the pods
Jul 22 20:05:22.175: INFO: Deleting ReplicationController externalsvc took: 15.975596ms
Jul 22 20:05:22.276: INFO: Terminating ReplicationController externalsvc pods took: 100.613335ms
Jul 22 20:05:36.997: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:05:37.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6274" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":339,"completed":28,"skipped":558,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:05:37.043: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4349
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4349
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4349
I0722 20:05:37.287253    5669 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4349, replica count: 2
Jul 22 20:05:40.338: INFO: Creating new exec pod
I0722 20:05:40.338290    5669 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:05:43.404: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4349 exec execpodsnfm8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 22 20:05:43.802: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 22 20:05:43.802: INFO: stdout: ""
Jul 22 20:05:44.803: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4349 exec execpodsnfm8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 22 20:05:45.430: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 22 20:05:45.430: INFO: stdout: ""
Jul 22 20:05:45.803: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4349 exec execpodsnfm8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 22 20:05:46.182: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 22 20:05:46.182: INFO: stdout: ""
Jul 22 20:05:46.803: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4349 exec execpodsnfm8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 22 20:05:47.185: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 22 20:05:47.185: INFO: stdout: ""
Jul 22 20:05:47.803: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4349 exec execpodsnfm8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 22 20:05:48.416: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 22 20:05:48.416: INFO: stdout: "externalname-service-2m8cj"
Jul 22 20:05:48.416: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4349 exec execpodsnfm8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.71.135.133 80'
Jul 22 20:05:48.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.71.135.133 80\nConnection to 100.71.135.133 80 port [tcp/http] succeeded!\n"
Jul 22 20:05:48.851: INFO: stdout: "externalname-service-2m8cj"
Jul 22 20:05:48.851: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4349 exec execpodsnfm8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.3 31257'
Jul 22 20:05:49.272: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.3 31257\nConnection to 10.250.0.3 31257 port [tcp/*] succeeded!\n"
Jul 22 20:05:49.272: INFO: stdout: "externalname-service-spf4r"
Jul 22 20:05:49.272: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4349 exec execpodsnfm8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.2 31257'
Jul 22 20:05:49.663: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.2 31257\nConnection to 10.250.0.2 31257 port [tcp/*] succeeded!\n"
Jul 22 20:05:49.663: INFO: stdout: "externalname-service-spf4r"
Jul 22 20:05:49.663: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:05:49.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4349" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":339,"completed":29,"skipped":579,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:05:49.721: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jul 22 20:05:49.912: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 create -f -'
Jul 22 20:05:50.331: INFO: stderr: ""
Jul 22 20:05:50.331: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 20:05:50.331: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:05:50.445: INFO: stderr: ""
Jul 22 20:05:50.445: INFO: stdout: "update-demo-nautilus-rgg4m update-demo-nautilus-x88tf "
Jul 22 20:05:50.445: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-rgg4m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:05:50.555: INFO: stderr: ""
Jul 22 20:05:50.555: INFO: stdout: ""
Jul 22 20:05:50.555: INFO: update-demo-nautilus-rgg4m is created but not running
Jul 22 20:05:55.556: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:05:55.678: INFO: stderr: ""
Jul 22 20:05:55.678: INFO: stdout: "update-demo-nautilus-rgg4m update-demo-nautilus-x88tf "
Jul 22 20:05:55.678: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-rgg4m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:05:55.775: INFO: stderr: ""
Jul 22 20:05:55.776: INFO: stdout: ""
Jul 22 20:05:55.776: INFO: update-demo-nautilus-rgg4m is created but not running
Jul 22 20:06:00.777: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:06:00.877: INFO: stderr: ""
Jul 22 20:06:00.877: INFO: stdout: "update-demo-nautilus-rgg4m update-demo-nautilus-x88tf "
Jul 22 20:06:00.877: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-rgg4m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:06:00.977: INFO: stderr: ""
Jul 22 20:06:00.977: INFO: stdout: "true"
Jul 22 20:06:00.977: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-rgg4m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 20:06:01.072: INFO: stderr: ""
Jul 22 20:06:01.072: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 22 20:06:01.072: INFO: validating pod update-demo-nautilus-rgg4m
Jul 22 20:06:01.143: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 20:06:01.143: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 20:06:01.143: INFO: update-demo-nautilus-rgg4m is verified up and running
Jul 22 20:06:01.143: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-x88tf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:06:01.263: INFO: stderr: ""
Jul 22 20:06:01.263: INFO: stdout: "true"
Jul 22 20:06:01.263: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-x88tf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 20:06:01.385: INFO: stderr: ""
Jul 22 20:06:01.385: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 22 20:06:01.386: INFO: validating pod update-demo-nautilus-x88tf
Jul 22 20:06:01.411: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 20:06:01.411: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 20:06:01.411: INFO: update-demo-nautilus-x88tf is verified up and running
STEP: scaling down the replication controller
Jul 22 20:06:01.414: INFO: scanned /root for discovery docs: <nil>
Jul 22 20:06:01.414: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jul 22 20:06:02.580: INFO: stderr: ""
Jul 22 20:06:02.580: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 20:06:02.580: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:06:02.682: INFO: stderr: ""
Jul 22 20:06:02.682: INFO: stdout: "update-demo-nautilus-rgg4m update-demo-nautilus-x88tf "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 22 20:06:07.683: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:06:07.784: INFO: stderr: ""
Jul 22 20:06:07.784: INFO: stdout: "update-demo-nautilus-rgg4m update-demo-nautilus-x88tf "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 22 20:06:12.785: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:06:12.886: INFO: stderr: ""
Jul 22 20:06:12.886: INFO: stdout: "update-demo-nautilus-x88tf "
Jul 22 20:06:12.886: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-x88tf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:06:12.979: INFO: stderr: ""
Jul 22 20:06:12.979: INFO: stdout: "true"
Jul 22 20:06:12.979: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-x88tf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 20:06:13.077: INFO: stderr: ""
Jul 22 20:06:13.077: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 22 20:06:13.077: INFO: validating pod update-demo-nautilus-x88tf
Jul 22 20:06:13.133: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 20:06:13.133: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 20:06:13.133: INFO: update-demo-nautilus-x88tf is verified up and running
STEP: scaling up the replication controller
Jul 22 20:06:13.135: INFO: scanned /root for discovery docs: <nil>
Jul 22 20:06:13.135: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jul 22 20:06:14.305: INFO: stderr: ""
Jul 22 20:06:14.305: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 20:06:14.305: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:06:14.441: INFO: stderr: ""
Jul 22 20:06:14.441: INFO: stdout: "update-demo-nautilus-vxmpk update-demo-nautilus-x88tf "
Jul 22 20:06:14.441: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-vxmpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:06:14.557: INFO: stderr: ""
Jul 22 20:06:14.557: INFO: stdout: ""
Jul 22 20:06:14.557: INFO: update-demo-nautilus-vxmpk is created but not running
Jul 22 20:06:19.558: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 22 20:06:19.676: INFO: stderr: ""
Jul 22 20:06:19.676: INFO: stdout: "update-demo-nautilus-vxmpk update-demo-nautilus-x88tf "
Jul 22 20:06:19.676: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-vxmpk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:06:19.781: INFO: stderr: ""
Jul 22 20:06:19.781: INFO: stdout: "true"
Jul 22 20:06:19.781: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-vxmpk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 20:06:19.885: INFO: stderr: ""
Jul 22 20:06:19.885: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 22 20:06:19.885: INFO: validating pod update-demo-nautilus-vxmpk
Jul 22 20:06:19.953: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 20:06:19.953: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 20:06:19.953: INFO: update-demo-nautilus-vxmpk is verified up and running
Jul 22 20:06:19.953: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-x88tf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 22 20:06:20.069: INFO: stderr: ""
Jul 22 20:06:20.069: INFO: stdout: "true"
Jul 22 20:06:20.069: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods update-demo-nautilus-x88tf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 22 20:06:20.172: INFO: stderr: ""
Jul 22 20:06:20.172: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 22 20:06:20.172: INFO: validating pod update-demo-nautilus-x88tf
Jul 22 20:06:20.186: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 20:06:20.187: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 20:06:20.187: INFO: update-demo-nautilus-x88tf is verified up and running
STEP: using delete to clean up resources
Jul 22 20:06:20.187: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 delete --grace-period=0 --force -f -'
Jul 22 20:06:20.314: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 20:06:20.314: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 22 20:06:20.314: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get rc,svc -l name=update-demo --no-headers'
Jul 22 20:06:20.430: INFO: stderr: "No resources found in kubectl-7112 namespace.\n"
Jul 22 20:06:20.431: INFO: stdout: ""
Jul 22 20:06:20.431: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7112 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 20:06:20.561: INFO: stderr: ""
Jul 22 20:06:20.561: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:06:20.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7112" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":339,"completed":30,"skipped":590,"failed":0}

------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:06:20.594: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-4585
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0722 20:06:20.791755    5669 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:08:00.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4585" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":339,"completed":31,"skipped":590,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:08:00.896: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8334
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:08:01.097: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 22 20:08:06.579: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8334 --namespace=crd-publish-openapi-8334 create -f -'
Jul 22 20:08:07.209: INFO: stderr: ""
Jul 22 20:08:07.209: INFO: stdout: "e2e-test-crd-publish-openapi-8164-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 22 20:08:07.209: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8334 --namespace=crd-publish-openapi-8334 delete e2e-test-crd-publish-openapi-8164-crds test-cr'
Jul 22 20:08:07.316: INFO: stderr: ""
Jul 22 20:08:07.316: INFO: stdout: "e2e-test-crd-publish-openapi-8164-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul 22 20:08:07.316: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8334 --namespace=crd-publish-openapi-8334 apply -f -'
Jul 22 20:08:07.682: INFO: stderr: ""
Jul 22 20:08:07.682: INFO: stdout: "e2e-test-crd-publish-openapi-8164-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 22 20:08:07.682: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8334 --namespace=crd-publish-openapi-8334 delete e2e-test-crd-publish-openapi-8164-crds test-cr'
Jul 22 20:08:07.792: INFO: stderr: ""
Jul 22 20:08:07.792: INFO: stdout: "e2e-test-crd-publish-openapi-8164-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 22 20:08:07.792: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-8334 explain e2e-test-crd-publish-openapi-8164-crds'
Jul 22 20:08:08.026: INFO: stderr: ""
Jul 22 20:08:08.026: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8164-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:08:12.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8334" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":339,"completed":32,"skipped":602,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:08:12.815: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7556
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7556.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7556.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7556.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7556.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7556.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7556.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 20:08:25.258: INFO: DNS probes using dns-7556/dns-test-7ba4909a-34fc-4ce3-bfac-c348ba610cab succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:08:25.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7556" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":339,"completed":33,"skipped":609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:08:25.323: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7895
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jul 22 20:08:25.521: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:08:50.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7895" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":339,"completed":34,"skipped":713,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:08:50.246: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-9992
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-9992
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9992
STEP: Deleting pre-stop pod
Jul 22 20:09:01.574: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:09:01.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9992" for this suite.
•{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":339,"completed":35,"skipped":726,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:09:01.622: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-232
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 22 20:09:01.903: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-232  a274e4dd-a69c-4def-a00b-20ad32664592 9763 0 2021-07-22 20:09:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-07-22 20:09:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 20:09:01.903: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-232  a274e4dd-a69c-4def-a00b-20ad32664592 9764 0 2021-07-22 20:09:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-07-22 20:09:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:09:01.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-232" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":339,"completed":36,"skipped":730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:09:01.928: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2949
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jul 22 20:09:02.121: INFO: namespace kubectl-2949
Jul 22 20:09:02.121: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2949 create -f -'
Jul 22 20:09:02.485: INFO: stderr: ""
Jul 22 20:09:02.485: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 22 20:09:03.497: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 20:09:03.497: INFO: Found 0 / 1
Jul 22 20:09:04.496: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 20:09:04.496: INFO: Found 1 / 1
Jul 22 20:09:04.496: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 22 20:09:04.506: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 20:09:04.506: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 22 20:09:04.506: INFO: wait on agnhost-primary startup in kubectl-2949 
Jul 22 20:09:04.506: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2949 logs agnhost-primary-9mc62 agnhost-primary'
Jul 22 20:09:04.679: INFO: stderr: ""
Jul 22 20:09:04.679: INFO: stdout: "Paused\n"
STEP: exposing RC
Jul 22 20:09:04.679: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2949 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jul 22 20:09:04.820: INFO: stderr: ""
Jul 22 20:09:04.820: INFO: stdout: "service/rm2 exposed\n"
Jul 22 20:09:04.831: INFO: Service rm2 in namespace kubectl-2949 found.
STEP: exposing service
Jul 22 20:09:06.890: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2949 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jul 22 20:09:07.001: INFO: stderr: ""
Jul 22 20:09:07.001: INFO: stdout: "service/rm3 exposed\n"
Jul 22 20:09:07.012: INFO: Service rm3 in namespace kubectl-2949 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:09:09.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2949" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":339,"completed":37,"skipped":774,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:09:09.069: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8124
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul 22 20:09:09.284: INFO: Waiting up to 5m0s for pod "downward-api-d61417cc-3179-4c36-a00d-8afd418e8d12" in namespace "downward-api-8124" to be "Succeeded or Failed"
Jul 22 20:09:09.295: INFO: Pod "downward-api-d61417cc-3179-4c36-a00d-8afd418e8d12": Phase="Pending", Reason="", readiness=false. Elapsed: 11.784146ms
Jul 22 20:09:11.308: INFO: Pod "downward-api-d61417cc-3179-4c36-a00d-8afd418e8d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024359398s
STEP: Saw pod success
Jul 22 20:09:11.308: INFO: Pod "downward-api-d61417cc-3179-4c36-a00d-8afd418e8d12" satisfied condition "Succeeded or Failed"
Jul 22 20:09:11.319: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downward-api-d61417cc-3179-4c36-a00d-8afd418e8d12 container dapi-container: <nil>
STEP: delete the pod
Jul 22 20:09:11.355: INFO: Waiting for pod downward-api-d61417cc-3179-4c36-a00d-8afd418e8d12 to disappear
Jul 22 20:09:11.366: INFO: Pod downward-api-d61417cc-3179-4c36-a00d-8afd418e8d12 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:09:11.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8124" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":339,"completed":38,"skipped":819,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:09:11.402: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:09:12.145: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:09:15.218: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:09:15.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3945" for this suite.
STEP: Destroying namespace "webhook-3945-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":339,"completed":39,"skipped":831,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:09:15.508: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4600
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-4600
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 20:09:15.695: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 22 20:09:15.771: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:09:17.783: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:09:19.783: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:09:21.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:09:23.783: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:09:25.783: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:09:27.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:09:29.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:09:31.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:09:33.783: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:09:35.783: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 22 20:09:35.804: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 22 20:09:39.945: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul 22 20:09:39.945: INFO: Going to poll 100.96.0.52 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Jul 22 20:09:39.955: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.52:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4600 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:09:39.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:09:40.324: INFO: Found all 1 expected endpoints: [netserver-0]
Jul 22 20:09:40.324: INFO: Going to poll 100.96.1.68 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Jul 22 20:09:40.335: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.68:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4600 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:09:40.335: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:09:40.648: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:09:40.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4600" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":40,"skipped":886,"failed":0}
S
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:09:40.688: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1179
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:09:40.910: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-dd93bacf-cd58-47e8-977f-f73453baa6eb" in namespace "security-context-test-1179" to be "Succeeded or Failed"
Jul 22 20:09:40.921: INFO: Pod "busybox-privileged-false-dd93bacf-cd58-47e8-977f-f73453baa6eb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.555255ms
Jul 22 20:09:42.934: INFO: Pod "busybox-privileged-false-dd93bacf-cd58-47e8-977f-f73453baa6eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023579697s
Jul 22 20:09:44.947: INFO: Pod "busybox-privileged-false-dd93bacf-cd58-47e8-977f-f73453baa6eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036470968s
Jul 22 20:09:44.947: INFO: Pod "busybox-privileged-false-dd93bacf-cd58-47e8-977f-f73453baa6eb" satisfied condition "Succeeded or Failed"
Jul 22 20:09:45.003: INFO: Got logs for pod "busybox-privileged-false-dd93bacf-cd58-47e8-977f-f73453baa6eb": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:09:45.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1179" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":41,"skipped":887,"failed":0}
SS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:09:45.035: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3004
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jul 22 20:09:45.244: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:09:51.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3004" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":339,"completed":42,"skipped":889,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:09:51.767: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6563
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-14d05e47-51d8-49e1-b6f5-fb639d4f4b3a
STEP: Creating a pod to test consume secrets
Jul 22 20:09:51.990: INFO: Waiting up to 5m0s for pod "pod-secrets-81ab38d7-7686-4697-b298-e5b0173b5754" in namespace "secrets-6563" to be "Succeeded or Failed"
Jul 22 20:09:52.001: INFO: Pod "pod-secrets-81ab38d7-7686-4697-b298-e5b0173b5754": Phase="Pending", Reason="", readiness=false. Elapsed: 10.637302ms
Jul 22 20:09:54.013: INFO: Pod "pod-secrets-81ab38d7-7686-4697-b298-e5b0173b5754": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022746193s
STEP: Saw pod success
Jul 22 20:09:54.013: INFO: Pod "pod-secrets-81ab38d7-7686-4697-b298-e5b0173b5754" satisfied condition "Succeeded or Failed"
Jul 22 20:09:54.024: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-secrets-81ab38d7-7686-4697-b298-e5b0173b5754 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:09:54.066: INFO: Waiting for pod pod-secrets-81ab38d7-7686-4697-b298-e5b0173b5754 to disappear
Jul 22 20:09:54.076: INFO: Pod pod-secrets-81ab38d7-7686-4697-b298-e5b0173b5754 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:09:54.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6563" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":43,"skipped":898,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:09:54.108: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Jul 22 20:09:54.310: INFO: created test-event-1
Jul 22 20:09:54.323: INFO: created test-event-2
Jul 22 20:09:54.334: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jul 22 20:09:54.345: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jul 22 20:09:54.369: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:09:54.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5743" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":339,"completed":44,"skipped":957,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:09:54.404: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-7223
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0722 20:09:54.600477    5669 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:15:00.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7223" for this suite.

• [SLOW TEST:306.310 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":339,"completed":45,"skipped":965,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:15:00.714: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7524
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jul 22 20:15:41.016: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0722 20:15:41.015965    5669 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0722 20:15:41.015997    5669 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0722 20:15:41.016004    5669 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 22 20:15:41.016: INFO: Deleting pod "simpletest.rc-btgd5" in namespace "gc-7524"
Jul 22 20:15:41.030: INFO: Deleting pod "simpletest.rc-cvh57" in namespace "gc-7524"
Jul 22 20:15:41.046: INFO: Deleting pod "simpletest.rc-gtjhp" in namespace "gc-7524"
Jul 22 20:15:41.062: INFO: Deleting pod "simpletest.rc-j57qs" in namespace "gc-7524"
Jul 22 20:15:41.077: INFO: Deleting pod "simpletest.rc-krd4h" in namespace "gc-7524"
Jul 22 20:15:41.094: INFO: Deleting pod "simpletest.rc-kw7lq" in namespace "gc-7524"
Jul 22 20:15:41.110: INFO: Deleting pod "simpletest.rc-l8hmd" in namespace "gc-7524"
Jul 22 20:15:41.125: INFO: Deleting pod "simpletest.rc-rqmh9" in namespace "gc-7524"
Jul 22 20:15:41.141: INFO: Deleting pod "simpletest.rc-vk77j" in namespace "gc-7524"
Jul 22 20:15:41.157: INFO: Deleting pod "simpletest.rc-xhm97" in namespace "gc-7524"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:15:41.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7524" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":339,"completed":46,"skipped":976,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:15:41.210: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:15:41.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1148" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":47,"skipped":978,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:15:41.605: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2841
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:15:41.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b48b14c2-6b3e-497e-a61c-ded9af7ae280" in namespace "downward-api-2841" to be "Succeeded or Failed"
Jul 22 20:15:41.836: INFO: Pod "downwardapi-volume-b48b14c2-6b3e-497e-a61c-ded9af7ae280": Phase="Pending", Reason="", readiness=false. Elapsed: 10.712077ms
Jul 22 20:15:43.849: INFO: Pod "downwardapi-volume-b48b14c2-6b3e-497e-a61c-ded9af7ae280": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02381706s
Jul 22 20:15:45.860: INFO: Pod "downwardapi-volume-b48b14c2-6b3e-497e-a61c-ded9af7ae280": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035152368s
STEP: Saw pod success
Jul 22 20:15:45.860: INFO: Pod "downwardapi-volume-b48b14c2-6b3e-497e-a61c-ded9af7ae280" satisfied condition "Succeeded or Failed"
Jul 22 20:15:45.872: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-b48b14c2-6b3e-497e-a61c-ded9af7ae280 container client-container: <nil>
STEP: delete the pod
Jul 22 20:15:45.947: INFO: Waiting for pod downwardapi-volume-b48b14c2-6b3e-497e-a61c-ded9af7ae280 to disappear
Jul 22 20:15:45.958: INFO: Pod downwardapi-volume-b48b14c2-6b3e-497e-a61c-ded9af7ae280 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:15:45.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2841" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":48,"skipped":1011,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:15:45.992: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7535
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jul 22 20:15:48.237: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7535 PodName:var-expansion-a38d7daf-902f-4036-b8a2-0d4722fadbc7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:15:48.237: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: test for file in mounted path
Jul 22 20:15:48.555: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7535 PodName:var-expansion-a38d7daf-902f-4036-b8a2-0d4722fadbc7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:15:48.555: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: updating the annotation value
Jul 22 20:15:49.353: INFO: Successfully updated pod "var-expansion-a38d7daf-902f-4036-b8a2-0d4722fadbc7"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jul 22 20:15:49.363: INFO: Deleting pod "var-expansion-a38d7daf-902f-4036-b8a2-0d4722fadbc7" in namespace "var-expansion-7535"
Jul 22 20:15:49.376: INFO: Wait up to 5m0s for pod "var-expansion-a38d7daf-902f-4036-b8a2-0d4722fadbc7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:16:27.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7535" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":339,"completed":49,"skipped":1027,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:16:27.430: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4093
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 22 20:16:27.661: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 22 20:17:27.768: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Jul 22 20:17:27.836: INFO: Created pod: pod0-sched-preemption-low-priority
Jul 22 20:17:27.871: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:17:40.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4093" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":339,"completed":50,"skipped":1041,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:17:40.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8878
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-a61638b7-09d4-486d-bab9-64481dc1e793
STEP: Creating secret with name s-test-opt-upd-fa98ae75-6591-4858-bb80-f0cb54d5d4c7
STEP: Creating the pod
Jul 22 20:17:40.384: INFO: The status of Pod pod-secrets-6542cfc7-d810-439e-83fc-9351257dd186 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:17:42.396: INFO: The status of Pod pod-secrets-6542cfc7-d810-439e-83fc-9351257dd186 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:17:44.396: INFO: The status of Pod pod-secrets-6542cfc7-d810-439e-83fc-9351257dd186 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-a61638b7-09d4-486d-bab9-64481dc1e793
STEP: Updating secret s-test-opt-upd-fa98ae75-6591-4858-bb80-f0cb54d5d4c7
STEP: Creating secret with name s-test-opt-create-9ac8a7ec-53ea-4cde-9b6e-f1f00f51c8d9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:18:51.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8878" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":51,"skipped":1045,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:18:51.410: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:19:02.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4481" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":339,"completed":52,"skipped":1088,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:19:02.731: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6765
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0722 20:19:13.049717    5669 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0722 20:19:13.049923    5669 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0722 20:19:13.049986    5669 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 22 20:19:13.050: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:19:13.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6765" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":339,"completed":53,"skipped":1090,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:19:13.074: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5164
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 22 20:19:17.340: INFO: &Pod{ObjectMeta:{send-events-3d1c1c5e-f4f2-4c0a-b233-5786daaf39e2  events-5164  2c3f5550-28eb-41f6-8149-7365471bc987 13894 0 2021-07-22 20:19:13 +0000 UTC <nil> <nil> map[name:foo time:268335119] map[cni.projectcalico.org/podIP:100.96.1.87/32 cni.projectcalico.org/podIPs:100.96.1.87/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-07-22 20:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:19:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:19:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzv47,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzv47,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:19:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:19:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:19:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:19:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.87,StartTime:2021-07-22 20:19:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:19:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://a82c8158cdc225e17cca318207982793f251d73bfc0cf810ca1cac3d016baa6b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jul 22 20:19:19.352: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 22 20:19:21.365: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:19:21.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5164" for this suite.
•{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":339,"completed":54,"skipped":1115,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:19:21.412: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2800
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:19:21.600: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 22 20:19:21.622: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 22 20:19:26.636: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 22 20:19:26.636: INFO: Creating deployment "test-rolling-update-deployment"
Jul 22 20:19:26.649: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 22 20:19:26.685: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Jul 22 20:19:28.709: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 22 20:19:28.720: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 22 20:19:28.753: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2800  a5c8544a-a5d1-4bbd-8c34-f4c17ff0e44b 14011 1 2021-07-22 20:19:26 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-07-22 20:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:19:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006cd8488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-22 20:19:26 +0000 UTC,LastTransitionTime:2021-07-22 20:19:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-07-22 20:19:28 +0000 UTC,LastTransitionTime:2021-07-22 20:19:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 22 20:19:28.767: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-2800  2fc0c94c-bbe6-4292-8487-4e009e66969c 14003 1 2021-07-22 20:19:26 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a5c8544a-a5d1-4bbd-8c34-f4c17ff0e44b 0xc003216ee7 0xc003216ee8}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:19:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5c8544a-a5d1-4bbd-8c34-f4c17ff0e44b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003216fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:19:28.767: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 22 20:19:28.767: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2800  fd25fcf5-1009-4429-9645-bf25148fb24f 14010 2 2021-07-22 20:19:21 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a5c8544a-a5d1-4bbd-8c34-f4c17ff0e44b 0xc003216cd7 0xc003216cd8}] []  [{e2e.test Update apps/v1 2021-07-22 20:19:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:19:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a5c8544a-a5d1-4bbd-8c34-f4c17ff0e44b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003216dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:19:28.779: INFO: Pod "test-rolling-update-deployment-585b757574-ngpm7" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-ngpm7 test-rolling-update-deployment-585b757574- deployment-2800  c56c59e0-3420-45bd-987c-25fc1298d16b 14002 0 2021-07-22 20:19:26 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[cni.projectcalico.org/podIP:100.96.1.89/32 cni.projectcalico.org/podIPs:100.96.1.89/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 2fc0c94c-bbe6-4292-8487-4e009e66969c 0xc006cd8a87 0xc006cd8a88}] []  [{kube-controller-manager Update v1 2021-07-22 20:19:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fc0c94c-bbe6-4292-8487-4e009e66969c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:19:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:19:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2lhj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2lhj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:19:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:19:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:19:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.89,StartTime:2021-07-22 20:19:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:19:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://ee1bca72a25659d8c0a0c75642dd15a16785adb394429bb804c67cf33ae770ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.89,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:19:28.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2800" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":55,"skipped":1127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:19:28.811: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:19:29.000: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5735 version'
Jul 22 20:19:29.117: INFO: stderr: ""
Jul 22 20:19:29.117: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.2\", GitCommit:\"092fbfbf53427de67cac1e9fa54aaa09a28371d7\", GitTreeState:\"clean\", BuildDate:\"2021-06-16T12:59:11Z\", GoVersion:\"go1.16.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.2\", GitCommit:\"092fbfbf53427de67cac1e9fa54aaa09a28371d7\", GitTreeState:\"clean\", BuildDate:\"2021-06-16T12:53:14Z\", GoVersion:\"go1.16.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:19:29.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5735" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":339,"completed":56,"skipped":1163,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:19:29.143: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-9505
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:19:29.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9505" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":339,"completed":57,"skipped":1184,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:19:29.431: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2191
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:19:29.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2191" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":339,"completed":58,"skipped":1197,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:19:29.701: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-748
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-748
STEP: creating replication controller nodeport-test in namespace services-748
I0722 20:19:30.006996    5669 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-748, replica count: 2
I0722 20:19:33.058649    5669 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:19:33.058: INFO: Creating new exec pod
Jul 22 20:19:36.128: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-748 exec execpod8jjcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul 22 20:19:36.961: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 22 20:19:36.961: INFO: stdout: "nodeport-test-nkdv7"
Jul 22 20:19:36.961: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-748 exec execpod8jjcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.66.32.199 80'
Jul 22 20:19:42.336: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.66.32.199 80\nConnection to 100.66.32.199 80 port [tcp/http] succeeded!\n"
Jul 22 20:19:42.336: INFO: stdout: "nodeport-test-s5rw7"
Jul 22 20:19:42.336: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-748 exec execpod8jjcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.3 30288'
Jul 22 20:19:42.780: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.3 30288\nConnection to 10.250.0.3 30288 port [tcp/*] succeeded!\n"
Jul 22 20:19:42.781: INFO: stdout: ""
Jul 22 20:19:43.781: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-748 exec execpod8jjcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.3 30288'
Jul 22 20:19:44.189: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.3 30288\nConnection to 10.250.0.3 30288 port [tcp/*] succeeded!\n"
Jul 22 20:19:44.189: INFO: stdout: ""
Jul 22 20:19:44.782: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-748 exec execpod8jjcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.3 30288'
Jul 22 20:19:45.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.3 30288\nConnection to 10.250.0.3 30288 port [tcp/*] succeeded!\n"
Jul 22 20:19:45.154: INFO: stdout: ""
Jul 22 20:19:45.781: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-748 exec execpod8jjcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.3 30288'
Jul 22 20:19:46.103: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.3 30288\nConnection to 10.250.0.3 30288 port [tcp/*] succeeded!\n"
Jul 22 20:19:46.103: INFO: stdout: "nodeport-test-s5rw7"
Jul 22 20:19:46.103: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-748 exec execpod8jjcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.2 30288'
Jul 22 20:19:46.480: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.2 30288\nConnection to 10.250.0.2 30288 port [tcp/*] succeeded!\n"
Jul 22 20:19:46.480: INFO: stdout: ""
Jul 22 20:19:47.480: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-748 exec execpod8jjcx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.2 30288'
Jul 22 20:19:47.857: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.2 30288\nConnection to 10.250.0.2 30288 port [tcp/*] succeeded!\n"
Jul 22 20:19:47.857: INFO: stdout: "nodeport-test-s5rw7"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:19:47.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-748" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":339,"completed":59,"skipped":1228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:19:47.907: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-1934
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Jul 22 20:19:48.134: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
STEP: mirroring deletion of a custom Endpoint
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:19:50.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-1934" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":339,"completed":60,"skipped":1250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:19:50.222: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jul 22 20:19:50.407: INFO: PodSpec: initContainers in spec.initContainers
Jul 22 20:20:32.964: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-dc636c72-ee36-472c-b449-77b8c340b4fe", GenerateName:"", Namespace:"init-container-642", SelfLink:"", UID:"9e9623ca-bb0e-47fa-97f6-418b5f7a589e", ResourceVersion:"14552", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63762581990, loc:(*time.Location)(0x9dde5a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"407377378"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.92/32", "cni.projectcalico.org/podIPs":"100.96.1.92/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002a3be90), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002a3bea8)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002a3bed8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002a3bef0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002a3bf08), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002a3bf20)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-wslgk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003497300), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wslgk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wslgk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.4.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar{v1.EnvVar{Name:"KUBERNETES_SERVICE_HOST", Value:"api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com", ValueFrom:(*v1.EnvVarSource)(nil)}}, Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-wslgk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005f5ade8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0011f85b0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005f5ae70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005f5ae90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005f5ae98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005f5ae9c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002dcced0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762581990, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762581990, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762581990, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762581990, loc:(*time.Location)(0x9dde5a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.2", PodIP:"100.96.1.92", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.96.1.92"}}, StartTime:(*v1.Time)(0xc002a3bf50), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0011f8690)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0011f8700)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"docker-pullable://k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"docker://37519f9251df8d55a5e22ecb63005a84204fe0a81e3456a76e4e2e93f05c3e6f", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003497380), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003497360), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.4.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc005f5af1f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:20:32.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-642" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":339,"completed":61,"skipped":1285,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:20:32.997: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1548
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jul 22 20:20:33.190: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3693 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Jul 22 20:20:33.429: INFO: stderr: ""
Jul 22 20:20:33.429: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jul 22 20:20:38.479: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3693 get pod e2e-test-httpd-pod -o json'
Jul 22 20:20:38.602: INFO: stderr: ""
Jul 22 20:20:38.602: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.93/32\",\n            \"cni.projectcalico.org/podIPs\": \"100.96.1.93/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-07-22T20:20:33Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3693\",\n        \"resourceVersion\": \"14577\",\n        \"uid\": \"2d91ad10-26f7-4253-a8ef-1c8384c4e36f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"env\": [\n                    {\n                        \"name\": \"KUBERNETES_SERVICE_HOST\",\n                        \"value\": \"api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com\"\n                    }\n                ],\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-d6fbt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-d6fbt\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-22T20:20:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-22T20:20:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-22T20:20:35Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-22T20:20:33Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://505f916169ebe91b7a8518660c7cb7e4db829a7dc77166c3e773a84c7808f0c7\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-07-22T20:20:34Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.2\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.93\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.96.1.93\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-07-22T20:20:33Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 22 20:20:38.602: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3693 replace -f -'
Jul 22 20:20:38.919: INFO: stderr: ""
Jul 22 20:20:38.919: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1552
Jul 22 20:20:38.930: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3693 delete pods e2e-test-httpd-pod'
Jul 22 20:20:41.412: INFO: stderr: ""
Jul 22 20:20:41.412: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:20:41.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3693" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":339,"completed":62,"skipped":1295,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:20:41.445: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-6166
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jul 22 20:20:41.661: INFO: Waiting up to 5m0s for pod "security-context-798f8508-6155-4a54-9a95-9b9ddcc01dcc" in namespace "security-context-6166" to be "Succeeded or Failed"
Jul 22 20:20:41.672: INFO: Pod "security-context-798f8508-6155-4a54-9a95-9b9ddcc01dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.076625ms
Jul 22 20:20:43.685: INFO: Pod "security-context-798f8508-6155-4a54-9a95-9b9ddcc01dcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023755604s
STEP: Saw pod success
Jul 22 20:20:43.685: INFO: Pod "security-context-798f8508-6155-4a54-9a95-9b9ddcc01dcc" satisfied condition "Succeeded or Failed"
Jul 22 20:20:43.696: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod security-context-798f8508-6155-4a54-9a95-9b9ddcc01dcc container test-container: <nil>
STEP: delete the pod
Jul 22 20:20:43.753: INFO: Waiting for pod security-context-798f8508-6155-4a54-9a95-9b9ddcc01dcc to disappear
Jul 22 20:20:43.764: INFO: Pod security-context-798f8508-6155-4a54-9a95-9b9ddcc01dcc no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:20:43.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6166" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":63,"skipped":1304,"failed":0}

------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:20:43.796: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2854
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:20:44.035: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b73c2e82-fed1-414a-990d-0c0136fbf191" in namespace "security-context-test-2854" to be "Succeeded or Failed"
Jul 22 20:20:44.046: INFO: Pod "busybox-user-65534-b73c2e82-fed1-414a-990d-0c0136fbf191": Phase="Pending", Reason="", readiness=false. Elapsed: 11.357775ms
Jul 22 20:20:46.059: INFO: Pod "busybox-user-65534-b73c2e82-fed1-414a-990d-0c0136fbf191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024273855s
Jul 22 20:20:46.059: INFO: Pod "busybox-user-65534-b73c2e82-fed1-414a-990d-0c0136fbf191" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:20:46.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2854" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":64,"skipped":1304,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:20:46.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6978
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:20:46.286: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 22 20:20:47.366: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:20:47.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6978" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":339,"completed":65,"skipped":1340,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:20:47.412: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:20:47.628: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ac4e26e-65ba-4fe2-b010-431121e16633" in namespace "projected-5109" to be "Succeeded or Failed"
Jul 22 20:20:47.641: INFO: Pod "downwardapi-volume-9ac4e26e-65ba-4fe2-b010-431121e16633": Phase="Pending", Reason="", readiness=false. Elapsed: 13.18655ms
Jul 22 20:20:49.654: INFO: Pod "downwardapi-volume-9ac4e26e-65ba-4fe2-b010-431121e16633": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025428437s
Jul 22 20:20:51.667: INFO: Pod "downwardapi-volume-9ac4e26e-65ba-4fe2-b010-431121e16633": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038415095s
STEP: Saw pod success
Jul 22 20:20:51.667: INFO: Pod "downwardapi-volume-9ac4e26e-65ba-4fe2-b010-431121e16633" satisfied condition "Succeeded or Failed"
Jul 22 20:20:51.678: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-9ac4e26e-65ba-4fe2-b010-431121e16633 container client-container: <nil>
STEP: delete the pod
Jul 22 20:20:51.752: INFO: Waiting for pod downwardapi-volume-9ac4e26e-65ba-4fe2-b010-431121e16633 to disappear
Jul 22 20:20:51.763: INFO: Pod downwardapi-volume-9ac4e26e-65ba-4fe2-b010-431121e16633 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:20:51.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5109" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":66,"skipped":1386,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:20:51.795: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-6175
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:20:52.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6175" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":339,"completed":67,"skipped":1401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:20:52.085: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Jul 22 20:20:52.272: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2729 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:20:52.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2729" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":339,"completed":68,"skipped":1423,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:20:52.380: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8134
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:20:52.614: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96b57a27-bc47-43e2-8473-cccf475c69e5" in namespace "projected-8134" to be "Succeeded or Failed"
Jul 22 20:20:52.625: INFO: Pod "downwardapi-volume-96b57a27-bc47-43e2-8473-cccf475c69e5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.329861ms
Jul 22 20:20:54.637: INFO: Pod "downwardapi-volume-96b57a27-bc47-43e2-8473-cccf475c69e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022274793s
Jul 22 20:20:56.649: INFO: Pod "downwardapi-volume-96b57a27-bc47-43e2-8473-cccf475c69e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034639365s
STEP: Saw pod success
Jul 22 20:20:56.649: INFO: Pod "downwardapi-volume-96b57a27-bc47-43e2-8473-cccf475c69e5" satisfied condition "Succeeded or Failed"
Jul 22 20:20:56.660: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-96b57a27-bc47-43e2-8473-cccf475c69e5 container client-container: <nil>
STEP: delete the pod
Jul 22 20:20:56.695: INFO: Waiting for pod downwardapi-volume-96b57a27-bc47-43e2-8473-cccf475c69e5 to disappear
Jul 22 20:20:56.705: INFO: Pod downwardapi-volume-96b57a27-bc47-43e2-8473-cccf475c69e5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:20:56.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8134" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":69,"skipped":1444,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:20:56.739: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:20:56.995: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 22 20:20:57.031: INFO: Number of nodes with available pods: 0
Jul 22 20:20:57.031: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 20:20:58.064: INFO: Number of nodes with available pods: 0
Jul 22 20:20:58.064: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 20:20:59.065: INFO: Number of nodes with available pods: 1
Jul 22 20:20:59.065: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 is running more than one daemon pod
Jul 22 20:21:00.063: INFO: Number of nodes with available pods: 2
Jul 22 20:21:00.063: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 22 20:21:00.184: INFO: Wrong image for pod: daemon-set-qcrw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 22 20:21:01.208: INFO: Wrong image for pod: daemon-set-qcrw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 22 20:21:02.208: INFO: Wrong image for pod: daemon-set-qcrw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 22 20:21:03.208: INFO: Wrong image for pod: daemon-set-qcrw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 22 20:21:04.208: INFO: Wrong image for pod: daemon-set-qcrw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 22 20:21:05.208: INFO: Wrong image for pod: daemon-set-qcrw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 22 20:21:06.209: INFO: Wrong image for pod: daemon-set-qcrw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 22 20:21:07.209: INFO: Wrong image for pod: daemon-set-qcrw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 22 20:21:08.208: INFO: Wrong image for pod: daemon-set-qcrw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 22 20:21:08.208: INFO: Pod daemon-set-qsngq is not available
Jul 22 20:21:09.208: INFO: Wrong image for pod: daemon-set-qcrw5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 22 20:21:09.208: INFO: Pod daemon-set-qsngq is not available
Jul 22 20:21:12.207: INFO: Pod daemon-set-w84mz is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 22 20:21:12.284: INFO: Number of nodes with available pods: 1
Jul 22 20:21:12.284: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 is running more than one daemon pod
Jul 22 20:21:13.316: INFO: Number of nodes with available pods: 1
Jul 22 20:21:13.316: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 is running more than one daemon pod
Jul 22 20:21:14.318: INFO: Number of nodes with available pods: 2
Jul 22 20:21:14.318: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7006, will wait for the garbage collector to delete the pods
Jul 22 20:21:14.449: INFO: Deleting DaemonSet.extensions daemon-set took: 13.698581ms
Jul 22 20:21:14.549: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.548953ms
Jul 22 20:21:27.861: INFO: Number of nodes with available pods: 0
Jul 22 20:21:27.861: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 20:21:27.872: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15125"},"items":null}

Jul 22 20:21:27.883: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15125"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:21:27.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7006" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":339,"completed":70,"skipped":1452,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:21:27.951: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4863
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:21:28.146: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 22 20:21:33.122: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4863 --namespace=crd-publish-openapi-4863 create -f -'
Jul 22 20:21:33.827: INFO: stderr: ""
Jul 22 20:21:33.827: INFO: stdout: "e2e-test-crd-publish-openapi-7728-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 22 20:21:33.827: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4863 --namespace=crd-publish-openapi-4863 delete e2e-test-crd-publish-openapi-7728-crds test-cr'
Jul 22 20:21:33.971: INFO: stderr: ""
Jul 22 20:21:33.971: INFO: stdout: "e2e-test-crd-publish-openapi-7728-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul 22 20:21:33.971: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4863 --namespace=crd-publish-openapi-4863 apply -f -'
Jul 22 20:21:34.437: INFO: stderr: ""
Jul 22 20:21:34.437: INFO: stdout: "e2e-test-crd-publish-openapi-7728-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 22 20:21:34.437: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4863 --namespace=crd-publish-openapi-4863 delete e2e-test-crd-publish-openapi-7728-crds test-cr'
Jul 22 20:21:34.625: INFO: stderr: ""
Jul 22 20:21:34.625: INFO: stdout: "e2e-test-crd-publish-openapi-7728-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jul 22 20:21:34.625: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4863 explain e2e-test-crd-publish-openapi-7728-crds'
Jul 22 20:21:35.088: INFO: stderr: ""
Jul 22 20:21:35.088: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7728-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:21:39.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4863" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":339,"completed":71,"skipped":1455,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:21:39.820: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8354
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 22 20:21:40.031: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 22 20:21:45.083: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:21:45.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8354" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":339,"completed":72,"skipped":1467,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:21:45.204: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:21:45.486: INFO: The status of Pod server-envvars-1764d685-2552-4872-9552-da2f8917e4a0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:21:47.498: INFO: The status of Pod server-envvars-1764d685-2552-4872-9552-da2f8917e4a0 is Running (Ready = true)
Jul 22 20:21:47.545: INFO: Waiting up to 5m0s for pod "client-envvars-a92fc314-e34f-4fd1-8877-27b8dffc4e25" in namespace "pods-9366" to be "Succeeded or Failed"
Jul 22 20:21:47.556: INFO: Pod "client-envvars-a92fc314-e34f-4fd1-8877-27b8dffc4e25": Phase="Pending", Reason="", readiness=false. Elapsed: 10.824579ms
Jul 22 20:21:49.568: INFO: Pod "client-envvars-a92fc314-e34f-4fd1-8877-27b8dffc4e25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023036672s
STEP: Saw pod success
Jul 22 20:21:49.568: INFO: Pod "client-envvars-a92fc314-e34f-4fd1-8877-27b8dffc4e25" satisfied condition "Succeeded or Failed"
Jul 22 20:21:49.579: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod client-envvars-a92fc314-e34f-4fd1-8877-27b8dffc4e25 container env3cont: <nil>
STEP: delete the pod
Jul 22 20:21:49.614: INFO: Waiting for pod client-envvars-a92fc314-e34f-4fd1-8877-27b8dffc4e25 to disappear
Jul 22 20:21:49.625: INFO: Pod client-envvars-a92fc314-e34f-4fd1-8877-27b8dffc4e25 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:21:49.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9366" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":339,"completed":73,"skipped":1471,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:21:49.658: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-3652
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:21:49.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3652" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":339,"completed":74,"skipped":1514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:21:49.894: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3719
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-13d18df6-75ab-42d0-b5dd-11e9912e5762 in namespace container-probe-3719
Jul 22 20:21:54.124: INFO: Started pod busybox-13d18df6-75ab-42d0-b5dd-11e9912e5762 in namespace container-probe-3719
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 20:21:54.134: INFO: Initial restart count of pod busybox-13d18df6-75ab-42d0-b5dd-11e9912e5762 is 0
Jul 22 20:22:42.452: INFO: Restart count of pod container-probe-3719/busybox-13d18df6-75ab-42d0-b5dd-11e9912e5762 is now 1 (48.317867235s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:22:42.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3719" for this suite.
•{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":75,"skipped":1542,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:22:42.505: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jul 22 20:22:42.750: INFO: observed Pod pod-test in namespace pods-9355 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jul 22 20:22:42.750: INFO: observed Pod pod-test in namespace pods-9355 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:42 +0000 UTC  }]
Jul 22 20:22:42.759: INFO: observed Pod pod-test in namespace pods-9355 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:42 +0000 UTC  }]
Jul 22 20:22:43.855: INFO: observed Pod pod-test in namespace pods-9355 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:42 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:42 +0000 UTC  }]
Jul 22 20:22:44.772: INFO: Found Pod pod-test in namespace pods-9355 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:44 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 20:22:42 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jul 22 20:22:44.800: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jul 22 20:22:44.863: INFO: observed event type ADDED
Jul 22 20:22:44.863: INFO: observed event type MODIFIED
Jul 22 20:22:44.863: INFO: observed event type MODIFIED
Jul 22 20:22:44.863: INFO: observed event type MODIFIED
Jul 22 20:22:44.863: INFO: observed event type MODIFIED
Jul 22 20:22:44.863: INFO: observed event type MODIFIED
Jul 22 20:22:44.864: INFO: observed event type MODIFIED
Jul 22 20:22:44.864: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:22:44.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9355" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":339,"completed":76,"skipped":1546,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:22:44.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5053
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-6f69ed12-bc43-4620-aded-1fb33713c62a
STEP: Creating a pod to test consume configMaps
Jul 22 20:22:45.113: INFO: Waiting up to 5m0s for pod "pod-configmaps-07476459-9ae0-4559-945a-2f1243c1d14b" in namespace "configmap-5053" to be "Succeeded or Failed"
Jul 22 20:22:45.123: INFO: Pod "pod-configmaps-07476459-9ae0-4559-945a-2f1243c1d14b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.551927ms
Jul 22 20:22:47.136: INFO: Pod "pod-configmaps-07476459-9ae0-4559-945a-2f1243c1d14b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023177623s
STEP: Saw pod success
Jul 22 20:22:47.136: INFO: Pod "pod-configmaps-07476459-9ae0-4559-945a-2f1243c1d14b" satisfied condition "Succeeded or Failed"
Jul 22 20:22:47.147: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-configmaps-07476459-9ae0-4559-945a-2f1243c1d14b container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:22:47.185: INFO: Waiting for pod pod-configmaps-07476459-9ae0-4559-945a-2f1243c1d14b to disappear
Jul 22 20:22:47.195: INFO: Pod pod-configmaps-07476459-9ae0-4559-945a-2f1243c1d14b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:22:47.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5053" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":77,"skipped":1548,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:22:47.227: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:22:48.014: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:22:51.065: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:22:51.077: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:22:54.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8685" for this suite.
STEP: Destroying namespace "webhook-8685-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":339,"completed":78,"skipped":1553,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:22:54.817: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-7732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:22:55.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7732" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":339,"completed":79,"skipped":1572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:22:55.100: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1660
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:22:55.288: INFO: Creating deployment "test-recreate-deployment"
Jul 22 20:22:55.300: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 22 20:22:55.322: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 22 20:22:55.333: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582175, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582175, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582175, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582175, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6cb8b65c46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:22:57.345: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582175, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582175, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582175, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582175, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6cb8b65c46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:22:59.346: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 22 20:22:59.370: INFO: Updating deployment test-recreate-deployment
Jul 22 20:22:59.370: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 22 20:22:59.510: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1660  4d8509f4-f567-49e7-a749-cf152ac96ed7 15942 2 2021-07-22 20:22:55 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-22 20:22:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:22:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004438998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-22 20:22:59 +0000 UTC,LastTransitionTime:2021-07-22 20:22:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-07-22 20:22:59 +0000 UTC,LastTransitionTime:2021-07-22 20:22:55 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul 22 20:22:59.521: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-1660  10b72147-436a-49c7-a2ba-e483aa86c9e0 15941 1 2021-07-22 20:22:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 4d8509f4-f567-49e7-a749-cf152ac96ed7 0xc0044390c0 0xc0044390c1}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:22:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d8509f4-f567-49e7-a749-cf152ac96ed7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004439198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:22:59.521: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 22 20:22:59.521: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-1660  1f48da52-3d6e-40bc-be38-4d747c012447 15933 2 2021-07-22 20:22:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 4d8509f4-f567-49e7-a749-cf152ac96ed7 0xc004438e77 0xc004438e78}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:22:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d8509f4-f567-49e7-a749-cf152ac96ed7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004438f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:22:59.532: INFO: Pod "test-recreate-deployment-85d47dcb4-97drk" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-97drk test-recreate-deployment-85d47dcb4- deployment-1660  aaa320b9-10ba-4cd3-9f97-61db12874100 15940 0 2021-07-22 20:22:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 10b72147-436a-49c7-a2ba-e483aa86c9e0 0xc004439830 0xc004439831}] []  [{kube-controller-manager Update v1 2021-07-22 20:22:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10b72147-436a-49c7-a2ba-e483aa86c9e0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:22:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kw7n2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kw7n2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:22:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:22:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:22:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:22:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:22:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:22:59.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1660" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":80,"skipped":1595,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:22:59.565: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2780
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9296
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3502
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:23:13.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2780" for this suite.
STEP: Destroying namespace "nsdeletetest-9296" for this suite.
Jul 22 20:23:13.427: INFO: Namespace nsdeletetest-9296 was already deleted
STEP: Destroying namespace "nsdeletetest-3502" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":339,"completed":81,"skipped":1609,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:23:13.481: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4567
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:23:13.984: INFO: The status of Pod pod-secrets-f8b2f54e-2b38-40e3-ba78-5ebb70a4c137 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:23:15.997: INFO: The status of Pod pod-secrets-f8b2f54e-2b38-40e3-ba78-5ebb70a4c137 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:23:16.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4567" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":339,"completed":82,"skipped":1621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:23:16.089: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:23:16.751: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 22 20:23:18.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582196, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582196, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582196, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582196, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:23:21.844: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jul 22 20:23:21.925: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:23:22.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1583" for this suite.
STEP: Destroying namespace "webhook-1583-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":339,"completed":83,"skipped":1666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:23:22.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 22 20:23:22.324: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 20:23:22.349: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 20:23:22.360: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 before test
Jul 22 20:23:22.384: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6d48bc7cdd-cctqm from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul 22 20:23:22.384: INFO: apiserver-proxy-666sf from kube-system started at 2021-07-22 19:51:18 +0000 UTC (2 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container proxy ready: true, restart count 0
Jul 22 20:23:22.384: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 20:23:22.384: INFO: calico-kube-controllers-6f857b9885-2ktk5 from kube-system started at 2021-07-22 19:51:18 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 22 20:23:22.384: INFO: calico-node-kdx2r from kube-system started at 2021-07-22 19:52:34 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 20:23:22.384: INFO: calico-node-vertical-autoscaler-785b5f968-2667d from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:23:22.384: INFO: calico-typha-deploy-b7996d4cf-mqt22 from kube-system started at 2021-07-22 19:51:26 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container calico-typha ready: true, restart count 0
Jul 22 20:23:22.384: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-tj7hs from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:23:22.384: INFO: calico-typha-vertical-autoscaler-5c9655cddd-9mb9p from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:23:22.384: INFO: coredns-5d966c6cdc-bb8pm from kube-system started at 2021-07-22 19:51:46 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container coredns ready: true, restart count 0
Jul 22 20:23:22.384: INFO: coredns-5d966c6cdc-sbsmp from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container coredns ready: true, restart count 0
Jul 22 20:23:22.384: INFO: csi-driver-node-x8gk6 from kube-system started at 2021-07-22 19:51:18 +0000 UTC (3 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container csi-driver ready: true, restart count 0
Jul 22 20:23:22.384: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul 22 20:23:22.384: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jul 22 20:23:22.384: INFO: kube-proxy-ntfxw from kube-system started at 2021-07-22 19:54:58 +0000 UTC (2 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 20:23:22.384: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:23:22.384: INFO: metrics-server-7774b47fd5-95ldg from kube-system started at 2021-07-22 19:51:47 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container metrics-server ready: true, restart count 0
Jul 22 20:23:22.384: INFO: node-exporter-slf8j from kube-system started at 2021-07-22 19:51:18 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 20:23:22.384: INFO: node-problem-detector-82tqx from kube-system started at 2021-07-22 20:19:08 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 20:23:22.384: INFO: vpn-shoot-7ff9f78fff-c8rk2 from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul 22 20:23:22.384: INFO: dashboard-metrics-scraper-6dd4bbbc68-kdk6t from kubernetes-dashboard started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 22 20:23:22.384: INFO: kubernetes-dashboard-565499686d-dfd8j from kubernetes-dashboard started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.384: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
Jul 22 20:23:22.384: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 before test
Jul 22 20:23:22.400: INFO: addons-nginx-ingress-controller-56b97f886d-t8r78 from kube-system started at 2021-07-22 19:54:57 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.400: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 22 20:23:22.400: INFO: apiserver-proxy-65bs6 from kube-system started at 2021-07-22 19:51:37 +0000 UTC (2 container statuses recorded)
Jul 22 20:23:22.400: INFO: 	Container proxy ready: true, restart count 0
Jul 22 20:23:22.400: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 20:23:22.400: INFO: blackbox-exporter-859b5d9c8c-694bg from kube-system started at 2021-07-22 19:58:57 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.400: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul 22 20:23:22.400: INFO: calico-node-hxcn8 from kube-system started at 2021-07-22 19:51:37 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.400: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 20:23:22.400: INFO: csi-driver-node-mmj64 from kube-system started at 2021-07-22 19:51:37 +0000 UTC (3 container statuses recorded)
Jul 22 20:23:22.400: INFO: 	Container csi-driver ready: true, restart count 0
Jul 22 20:23:22.400: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul 22 20:23:22.400: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jul 22 20:23:22.400: INFO: kube-proxy-fjzj9 from kube-system started at 2021-07-22 19:55:05 +0000 UTC (2 container statuses recorded)
Jul 22 20:23:22.400: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 20:23:22.400: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:23:22.400: INFO: node-exporter-4gh9r from kube-system started at 2021-07-22 19:51:37 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.400: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 20:23:22.400: INFO: node-problem-detector-mttv7 from kube-system started at 2021-07-22 20:19:06 +0000 UTC (1 container statuses recorded)
Jul 22 20:23:22.400: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
STEP: verifying the node has the label node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
Jul 22 20:23:22.496: INFO: Pod addons-nginx-ingress-controller-56b97f886d-t8r78 requesting resource cpu=100m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
Jul 22 20:23:22.496: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-6d48bc7cdd-cctqm requesting resource cpu=0m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod apiserver-proxy-65bs6 requesting resource cpu=40m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
Jul 22 20:23:22.496: INFO: Pod apiserver-proxy-666sf requesting resource cpu=40m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod blackbox-exporter-859b5d9c8c-694bg requesting resource cpu=11m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
Jul 22 20:23:22.496: INFO: Pod calico-kube-controllers-6f857b9885-2ktk5 requesting resource cpu=10m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod calico-node-hxcn8 requesting resource cpu=250m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
Jul 22 20:23:22.496: INFO: Pod calico-node-kdx2r requesting resource cpu=250m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod calico-node-vertical-autoscaler-785b5f968-2667d requesting resource cpu=10m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod calico-typha-deploy-b7996d4cf-mqt22 requesting resource cpu=200m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod calico-typha-horizontal-autoscaler-5b58bb446c-tj7hs requesting resource cpu=10m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod calico-typha-vertical-autoscaler-5c9655cddd-9mb9p requesting resource cpu=10m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod coredns-5d966c6cdc-bb8pm requesting resource cpu=50m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod coredns-5d966c6cdc-sbsmp requesting resource cpu=50m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod csi-driver-node-mmj64 requesting resource cpu=40m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
Jul 22 20:23:22.496: INFO: Pod csi-driver-node-x8gk6 requesting resource cpu=40m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod kube-proxy-fjzj9 requesting resource cpu=34m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
Jul 22 20:23:22.496: INFO: Pod kube-proxy-ntfxw requesting resource cpu=34m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod metrics-server-7774b47fd5-95ldg requesting resource cpu=50m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod node-exporter-4gh9r requesting resource cpu=50m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
Jul 22 20:23:22.496: INFO: Pod node-exporter-slf8j requesting resource cpu=50m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod node-problem-detector-82tqx requesting resource cpu=11m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod node-problem-detector-mttv7 requesting resource cpu=11m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
Jul 22 20:23:22.496: INFO: Pod vpn-shoot-7ff9f78fff-c8rk2 requesting resource cpu=100m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod dashboard-metrics-scraper-6dd4bbbc68-kdk6t requesting resource cpu=0m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.496: INFO: Pod kubernetes-dashboard-565499686d-dfd8j requesting resource cpu=50m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
STEP: Starting Pods to consume most of the cluster CPU.
Jul 22 20:23:22.496: INFO: Creating a pod which consumes cpu=668m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
Jul 22 20:23:22.518: INFO: Creating a pod which consumes cpu=968m on Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-279ec804-29cc-4a12-ba90-e5d75d7eac72.1694369a7dfac16c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8361/filler-pod-279ec804-29cc-4a12-ba90-e5d75d7eac72 to shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-279ec804-29cc-4a12-ba90-e5d75d7eac72.1694369abbcc9c9d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-279ec804-29cc-4a12-ba90-e5d75d7eac72.1694369abd9a6f1f], Reason = [Created], Message = [Created container filler-pod-279ec804-29cc-4a12-ba90-e5d75d7eac72]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-279ec804-29cc-4a12-ba90-e5d75d7eac72.1694369ac3dfe0cc], Reason = [Started], Message = [Started container filler-pod-279ec804-29cc-4a12-ba90-e5d75d7eac72]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c0a777e7-1314-45ce-a9fd-43b7d9caa134.1694369a7cd91d20], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8361/filler-pod-c0a777e7-1314-45ce-a9fd-43b7d9caa134 to shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c0a777e7-1314-45ce-a9fd-43b7d9caa134.1694369ac5fb7d04], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c0a777e7-1314-45ce-a9fd-43b7d9caa134.1694369aca851adf], Reason = [Created], Message = [Created container filler-pod-c0a777e7-1314-45ce-a9fd-43b7d9caa134]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c0a777e7-1314-45ce-a9fd-43b7d9caa134.1694369ad323e6b0], Reason = [Started], Message = [Started container filler-pod-c0a777e7-1314-45ce-a9fd-43b7d9caa134]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1694369b706bacbc], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:23:27.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8361" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":339,"completed":84,"skipped":1704,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:23:27.732: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:23:27.920: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7374 create -f -'
Jul 22 20:23:28.275: INFO: stderr: ""
Jul 22 20:23:28.275: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jul 22 20:23:28.275: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7374 create -f -'
Jul 22 20:23:28.577: INFO: stderr: ""
Jul 22 20:23:28.577: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 22 20:23:29.590: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 20:23:29.590: INFO: Found 0 / 1
Jul 22 20:23:30.588: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 20:23:30.588: INFO: Found 1 / 1
Jul 22 20:23:30.588: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 22 20:23:30.599: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 20:23:30.599: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 22 20:23:30.599: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7374 describe pod agnhost-primary-z2dfl'
Jul 22 20:23:30.753: INFO: stderr: ""
Jul 22 20:23:30.753: INFO: stdout: "Name:         agnhost-primary-z2dfl\nNamespace:    kubectl-7374\nPriority:     0\nNode:         shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9/10.250.0.2\nStart Time:   Thu, 22 Jul 2021 20:23:28 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 100.96.1.114/32\n              cni.projectcalico.org/podIPs: 100.96.1.114/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           100.96.1.114\nIPs:\n  IP:           100.96.1.114\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://e55c475a5f59a94c519f323cf9fd50361f17d29f490925d35bf7dc5dc9a7d9b1\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 22 Jul 2021 20:23:29 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:\n      KUBERNETES_SERVICE_HOST:  api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fxpxc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-fxpxc:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-7374/agnhost-primary-z2dfl to shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jul 22 20:23:30.754: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7374 describe rc agnhost-primary'
Jul 22 20:23:30.908: INFO: stderr: ""
Jul 22 20:23:30.908: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7374\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-z2dfl\n"
Jul 22 20:23:30.908: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7374 describe service agnhost-primary'
Jul 22 20:23:31.074: INFO: stderr: ""
Jul 22 20:23:31.074: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7374\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.70.173.3\nIPs:               100.70.173.3\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.96.1.114:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 22 20:23:31.094: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7374 describe node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5'
Jul 22 20:23:31.302: INFO: stderr: ""
Jul 22 20:23:31.302: INFO: stdout: "Name:               shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=n1-standard-2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=europe-west1\n                    failure-domain.beta.kubernetes.io/zone=europe-west1-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=n1-standard-2\n                    node.kubernetes.io/role=node\n                    topology.gke.io/zone=europe-west1-b\n                    topology.kubernetes.io/region=europe-west1\n                    topology.kubernetes.io/zone=europe-west1-b\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/pool=worker-1\n                    worker.gardener.cloud/system-components=true\nAnnotations:        checksum/cloud-config-data: 627f8aa36f46fa8fdca92b85f16efe9b8a7fdb67ac9e1b399a4f55cf7714def1\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"pd.csi.storage.gke.io\":\"projects/sap-gcp-k8s-canary-custom/zones/europe-west1-b/instances/shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.machine.sapcloud.io/last-applied-anno-labels-taints:\n                      {\"metadata\":{\"creationTimestamp\":null,\"labels\":{\"node.kubernetes.io/role\":\"node\",\"worker.garden.sapcloud.io/group\":\"worker-1\",\"worker.gard...\n                    projectcalico.org/IPv4Address: 10.250.0.3/32\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.96.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 22 Jul 2021 19:51:17 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 22 Jul 2021 20:23:27 +0000\nConditions:\n  Type                          Status    LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------    -----------------                 ------------------                ------                          -------\n  FrequentContainerdRestart     Unknown   Thu, 22 Jul 2021 20:19:12 +0000   Thu, 22 Jul 2021 20:19:12 +0000   NoFrequentContainerdRestart     error watching journald: failed to stat the log path \"/var/log/journal\": stat /v\n  KernelDeadlock                False     Thu, 22 Jul 2021 20:19:12 +0000   Thu, 22 Jul 2021 20:19:10 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False     Thu, 22 Jul 2021 20:19:12 +0000   Thu, 22 Jul 2021 20:19:10 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  FrequentUnregisterNetDevice   Unknown   Thu, 22 Jul 2021 20:19:12 +0000   Thu, 22 Jul 2021 20:19:11 +0000   NoFrequentUnregisterNetDevice   error watching journald: failed to stat the log path \"/var/log/journal\": stat /v\n  FrequentKubeletRestart        Unknown   Thu, 22 Jul 2021 20:19:12 +0000   Thu, 22 Jul 2021 20:19:11 +0000   NoFrequentKubeletRestart        error watching journald: failed to stat the log path \"/var/log/journal\": stat /v\n  FrequentDockerRestart         Unknown   Thu, 22 Jul 2021 20:19:12 +0000   Thu, 22 Jul 2021 20:19:12 +0000   NoFrequentDockerRestart         error watching journald: failed to stat the log path \"/var/log/journal\": stat /v\n  NetworkUnavailable            False     Thu, 22 Jul 2021 19:52:49 +0000   Thu, 22 Jul 2021 19:52:49 +0000   CalicoIsUp                      Calico is running on this node\n  MemoryPressure                False     Thu, 22 Jul 2021 20:23:30 +0000   Thu, 22 Jul 2021 19:51:17 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False     Thu, 22 Jul 2021 20:23:30 +0000   Thu, 22 Jul 2021 19:51:17 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False     Thu, 22 Jul 2021 20:23:30 +0000   Thu, 22 Jul 2021 19:51:17 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True      Thu, 22 Jul 2021 20:23:30 +0000   Thu, 22 Jul 2021 19:51:38 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.250.0.3\n  Hostname:    shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5\nCapacity:\n  cpu:                    2\n  ephemeral-storage:      31423468Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 7624208Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nAllocatable:\n  cpu:                    1920m\n  ephemeral-storage:      30568749647\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 6473232Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  3\nSystem Info:\n  Machine ID:                 43e807d3de64fc93fcc18d90e6f6f7b5\n  System UUID:                43e807d3-de64-fc93-fcc1-8d90e6f6f7b5\n  Boot ID:                    8fed9a12-91aa-4d07-8aaa-56690d41c916\n  Kernel Version:             5.3.18-24.67-default\n  OS Image:                   SUSE Linux Enterprise Server 15 SP2\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.15\n  Kubelet Version:            v1.21.2\n  Kube-Proxy Version:         v1.21.2\nPodCIDR:                      100.96.0.0/24\nPodCIDRs:                     100.96.0.0/24\nProviderID:                   gce://sap-gcp-k8s-canary-custom/europe-west1-b/shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5\nNon-terminated Pods:          (19 in total)\n  Namespace                   Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits   Age\n  ---------                   ----                                                               ------------  ----------  ---------------  -------------   ---\n  kube-system                 addons-nginx-ingress-nginx-ingress-k8s-backend-6d48bc7cdd-cctqm    0 (0%)        0 (0%)      0 (0%)           0 (0%)          33m\n  kube-system                 apiserver-proxy-666sf                                              40m (2%)      400m (20%)  40Mi (0%)        500Mi (7%)      32m\n  kube-system                 calico-kube-controllers-6f857b9885-2ktk5                           10m (0%)      50m (2%)    50Mi (0%)        100Mi (1%)      33m\n  kube-system                 calico-node-kdx2r                                                  250m (13%)    800m (41%)  100Mi (1%)       700Mi (11%)     30m\n  kube-system                 calico-node-vertical-autoscaler-785b5f968-2667d                    10m (0%)      10m (0%)    50Mi (0%)        50Mi (0%)       33m\n  kube-system                 calico-typha-deploy-b7996d4cf-mqt22                                200m (10%)    500m (26%)  100Mi (1%)       700Mi (11%)     33m\n  kube-system                 calico-typha-horizontal-autoscaler-5b58bb446c-tj7hs                10m (0%)      10m (0%)    50Mi (0%)        50Mi (0%)       33m\n  kube-system                 calico-typha-vertical-autoscaler-5c9655cddd-9mb9p                  10m (0%)      10m (0%)    50Mi (0%)        50Mi (0%)       33m\n  kube-system                 coredns-5d966c6cdc-bb8pm                                           50m (2%)      250m (13%)  15Mi (0%)        500Mi (7%)      33m\n  kube-system                 coredns-5d966c6cdc-sbsmp                                           50m (2%)      250m (13%)  15Mi (0%)        500Mi (7%)      33m\n  kube-system                 csi-driver-node-x8gk6                                              40m (2%)      110m (5%)   114Mi (1%)       180Mi (2%)      32m\n  kube-system                 kube-proxy-ntfxw                                                   34m (1%)      92m (4%)    47753748 (0%)    145014992 (2%)  28m\n  kube-system                 metrics-server-7774b47fd5-95ldg                                    50m (2%)      500m (26%)  150Mi (2%)       1Gi (16%)       33m\n  kube-system                 node-exporter-slf8j                                                50m (2%)      50m (2%)    50Mi (0%)        50Mi (0%)       32m\n  kube-system                 node-problem-detector-82tqx                                        11m (0%)      44m (2%)    23574998 (0%)    94299992 (1%)   4m24s\n  kube-system                 vpn-shoot-7ff9f78fff-c8rk2                                         100m (5%)     400m (20%)  100Mi (1%)       400Mi (6%)      33m\n  kubernetes-dashboard        dashboard-metrics-scraper-6dd4bbbc68-kdk6t                         0 (0%)        0 (0%)      0 (0%)           0 (0%)          33m\n  kubernetes-dashboard        kubernetes-dashboard-565499686d-dfd8j                              50m (2%)      200m (10%)  50Mi (0%)        200Mi (3%)      33m\n  sched-pred-8361             filler-pod-c0a777e7-1314-45ce-a9fd-43b7d9caa134                    668m (34%)    668m (34%)  0 (0%)           0 (0%)          9s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests          Limits\n  --------               --------          ------\n  cpu                    1633m (85%)       4344m (226%)\n  memory                 1050698730 (15%)  5486389288 (82%)\n  ephemeral-storage      0 (0%)            0 (0%)\n  hugepages-1Gi          0 (0%)            0 (0%)\n  hugepages-2Mi          0 (0%)            0 (0%)\n  scheduling.k8s.io/foo  0                 0\nEvents:\n  Type    Reason                         Age                From             Message\n  ----    ------                         ----               ----             -------\n  Normal  NodeHasSufficientMemory        32m (x2 over 32m)  kubelet          Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure          32m (x2 over 32m)  kubelet          Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID           32m (x2 over 32m)  kubelet          Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced        32m                kubelet          Updated Node Allocatable limit across pods\n  Normal  Starting                       32m                kubelet          Starting kubelet.\n  Normal  Starting                       32m                kube-proxy       Starting kube-proxy.\n  Normal  NodeReady                      31m                kubelet          Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 status is now: NodeReady\n  Normal  NoFrequentDockerRestart        30m                systemd-monitor  Node condition FrequentDockerRestart is now: Unknown, reason: NoFrequentDockerRestart\n  Normal  NoFrequentKubeletRestart       30m                systemd-monitor  Node condition FrequentKubeletRestart is now: Unknown, reason: NoFrequentKubeletRestart\n  Normal  NoFrequentUnregisterNetDevice  30m                kernel-monitor   Node condition FrequentUnregisterNetDevice is now: Unknown, reason: NoFrequentUnregisterNetDevice\n  Normal  NoFrequentContainerdRestart    30m                systemd-monitor  Node condition FrequentContainerdRestart is now: Unknown, reason: NoFrequentContainerdRestart\n  Normal  Starting                       28m                kube-proxy       Starting kube-proxy.\n  Normal  NoFrequentUnregisterNetDevice  4m20s              kernel-monitor   Node condition FrequentUnregisterNetDevice is now: Unknown, reason: NoFrequentUnregisterNetDevice\n  Normal  NoFrequentKubeletRestart       4m20s              systemd-monitor  Node condition FrequentKubeletRestart is now: Unknown, reason: NoFrequentKubeletRestart\n  Normal  NoFrequentDockerRestart        4m19s              systemd-monitor  Node condition FrequentDockerRestart is now: Unknown, reason: NoFrequentDockerRestart\n  Normal  NoFrequentContainerdRestart    4m19s              systemd-monitor  Node condition FrequentContainerdRestart is now: Unknown, reason: NoFrequentContainerdRestart\n"
Jul 22 20:23:31.302: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7374 describe namespace kubectl-7374'
Jul 22 20:23:31.450: INFO: stderr: ""
Jul 22 20:23:31.450: INFO: stdout: "Name:         kubectl-7374\nLabels:       e2e-framework=kubectl\n              e2e-run=6e115f74-6950-4f9c-8f26-a6c038b297c3\n              kubernetes.io/metadata.name=kubectl-7374\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:23:31.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7374" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":339,"completed":85,"skipped":1716,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:23:31.484: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-9965
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 22 20:23:31.767: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 22 20:23:31.787: INFO: starting watch
STEP: patching
STEP: updating
Jul 22 20:23:31.833: INFO: waiting for watch events with expected annotations
Jul 22 20:23:31.833: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:23:31.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9965" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":339,"completed":86,"skipped":1738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:23:31.966: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:23:32.793: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 22 20:23:34.826: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582212, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582212, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582212, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582212, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:23:37.882: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:23:37.894: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-299-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:23:41.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-952" for this suite.
STEP: Destroying namespace "webhook-952-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":339,"completed":87,"skipped":1766,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:23:41.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3672
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 22 20:23:41.934: INFO: Waiting up to 5m0s for pod "pod-1febe007-1904-4c17-b2f9-5f459c069a85" in namespace "emptydir-3672" to be "Succeeded or Failed"
Jul 22 20:23:41.981: INFO: Pod "pod-1febe007-1904-4c17-b2f9-5f459c069a85": Phase="Pending", Reason="", readiness=false. Elapsed: 47.21948ms
Jul 22 20:23:43.994: INFO: Pod "pod-1febe007-1904-4c17-b2f9-5f459c069a85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.060016445s
STEP: Saw pod success
Jul 22 20:23:43.994: INFO: Pod "pod-1febe007-1904-4c17-b2f9-5f459c069a85" satisfied condition "Succeeded or Failed"
Jul 22 20:23:44.005: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-1febe007-1904-4c17-b2f9-5f459c069a85 container test-container: <nil>
STEP: delete the pod
Jul 22 20:23:44.043: INFO: Waiting for pod pod-1febe007-1904-4c17-b2f9-5f459c069a85 to disappear
Jul 22 20:23:44.085: INFO: Pod pod-1febe007-1904-4c17-b2f9-5f459c069a85 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:23:44.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3672" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":88,"skipped":1772,"failed":0}
SS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:23:44.185: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostport
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-3294
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jul 22 20:23:44.885: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:23:46.898: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:23:48.897: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.250.0.3 on the node which pod1 resides and expect scheduled
Jul 22 20:23:48.927: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:23:50.939: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.250.0.3 but use UDP protocol on the node which pod2 resides
Jul 22 20:23:50.970: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:23:52.983: INFO: The status of Pod pod3 is Running (Ready = true)
Jul 22 20:23:53.013: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:23:55.026: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jul 22 20:23:55.036: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.250.0.3 http://127.0.0.1:54323/hostname] Namespace:hostport-3294 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:23:55.036: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.3, port: 54323
Jul 22 20:23:55.293: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.250.0.3:54323/hostname] Namespace:hostport-3294 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:23:55.293: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.250.0.3, port: 54323 UDP
Jul 22 20:23:55.591: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.250.0.3 54323] Namespace:hostport-3294 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:23:55.591: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:00.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-3294" for this suite.
•{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":339,"completed":89,"skipped":1774,"failed":0}
S
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:00.934: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-817ec39a-9f60-490a-96a7-89f3fdc394fb
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:01.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9606" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":339,"completed":90,"skipped":1775,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:01.198: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 22 20:24:01.404: INFO: Waiting up to 5m0s for pod "pod-4ec5dba1-8ab5-4406-b030-1993793bd97b" in namespace "emptydir-5818" to be "Succeeded or Failed"
Jul 22 20:24:01.414: INFO: Pod "pod-4ec5dba1-8ab5-4406-b030-1993793bd97b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.510401ms
Jul 22 20:24:03.426: INFO: Pod "pod-4ec5dba1-8ab5-4406-b030-1993793bd97b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022570452s
STEP: Saw pod success
Jul 22 20:24:03.426: INFO: Pod "pod-4ec5dba1-8ab5-4406-b030-1993793bd97b" satisfied condition "Succeeded or Failed"
Jul 22 20:24:03.437: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-4ec5dba1-8ab5-4406-b030-1993793bd97b container test-container: <nil>
STEP: delete the pod
Jul 22 20:24:03.477: INFO: Waiting for pod pod-4ec5dba1-8ab5-4406-b030-1993793bd97b to disappear
Jul 22 20:24:03.488: INFO: Pod pod-4ec5dba1-8ab5-4406-b030-1993793bd97b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:03.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5818" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":91,"skipped":1784,"failed":0}
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:03.528: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-4994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:05.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4994" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":92,"skipped":1787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:05.826: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9392
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:24:06.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-445165e8-bf34-4d7b-a4ec-9abf6bce9e2e" in namespace "projected-9392" to be "Succeeded or Failed"
Jul 22 20:24:06.045: INFO: Pod "downwardapi-volume-445165e8-bf34-4d7b-a4ec-9abf6bce9e2e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.754705ms
Jul 22 20:24:08.058: INFO: Pod "downwardapi-volume-445165e8-bf34-4d7b-a4ec-9abf6bce9e2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023595988s
STEP: Saw pod success
Jul 22 20:24:08.058: INFO: Pod "downwardapi-volume-445165e8-bf34-4d7b-a4ec-9abf6bce9e2e" satisfied condition "Succeeded or Failed"
Jul 22 20:24:08.069: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-445165e8-bf34-4d7b-a4ec-9abf6bce9e2e container client-container: <nil>
STEP: delete the pod
Jul 22 20:24:08.107: INFO: Waiting for pod downwardapi-volume-445165e8-bf34-4d7b-a4ec-9abf6bce9e2e to disappear
Jul 22 20:24:08.117: INFO: Pod downwardapi-volume-445165e8-bf34-4d7b-a4ec-9abf6bce9e2e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:08.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9392" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":93,"skipped":1823,"failed":0}

------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:08.150: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5687
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:08.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5687" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":339,"completed":94,"skipped":1823,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:08.401: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-4070
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:10.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4070" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":95,"skipped":1854,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:10.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2977
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:24:11.091: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1fb83c11-94b1-4099-be22-445fbeabd28c" in namespace "security-context-test-2977" to be "Succeeded or Failed"
Jul 22 20:24:11.103: INFO: Pod "busybox-readonly-false-1fb83c11-94b1-4099-be22-445fbeabd28c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.658101ms
Jul 22 20:24:13.114: INFO: Pod "busybox-readonly-false-1fb83c11-94b1-4099-be22-445fbeabd28c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022666101s
Jul 22 20:24:13.114: INFO: Pod "busybox-readonly-false-1fb83c11-94b1-4099-be22-445fbeabd28c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:13.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2977" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":339,"completed":96,"skipped":1865,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:13.146: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:24:13.376: INFO: Got root ca configmap in namespace "svcaccounts-433"
Jul 22 20:24:13.390: INFO: Deleted root ca configmap in namespace "svcaccounts-433"
STEP: waiting for a new root ca configmap created
Jul 22 20:24:13.985: INFO: Recreated root ca configmap in namespace "svcaccounts-433"
Jul 22 20:24:14.085: INFO: Updated root ca configmap in namespace "svcaccounts-433"
STEP: waiting for the root ca configmap reconciled
Jul 22 20:24:14.595: INFO: Reconciled root ca configmap in namespace "svcaccounts-433"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:14.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-433" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":339,"completed":97,"skipped":1893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:14.685: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9758
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-afa772e2-f15a-44fc-a7a2-a8b8909b6ffd
STEP: Creating a pod to test consume configMaps
Jul 22 20:24:14.993: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1d790c4d-8477-4877-bdff-04e98c3c05a4" in namespace "projected-9758" to be "Succeeded or Failed"
Jul 22 20:24:15.007: INFO: Pod "pod-projected-configmaps-1d790c4d-8477-4877-bdff-04e98c3c05a4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.29644ms
Jul 22 20:24:17.019: INFO: Pod "pod-projected-configmaps-1d790c4d-8477-4877-bdff-04e98c3c05a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02630228s
Jul 22 20:24:19.031: INFO: Pod "pod-projected-configmaps-1d790c4d-8477-4877-bdff-04e98c3c05a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037881118s
STEP: Saw pod success
Jul 22 20:24:19.031: INFO: Pod "pod-projected-configmaps-1d790c4d-8477-4877-bdff-04e98c3c05a4" satisfied condition "Succeeded or Failed"
Jul 22 20:24:19.042: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-configmaps-1d790c4d-8477-4877-bdff-04e98c3c05a4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 20:24:19.076: INFO: Waiting for pod pod-projected-configmaps-1d790c4d-8477-4877-bdff-04e98c3c05a4 to disappear
Jul 22 20:24:19.087: INFO: Pod pod-projected-configmaps-1d790c4d-8477-4877-bdff-04e98c3c05a4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:19.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9758" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":98,"skipped":1916,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:19.119: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:24:19.907: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:24:22.961: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jul 22 20:24:27.078: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=webhook-8644 attach --namespace=webhook-8644 to-be-attached-pod -i -c=container1'
Jul 22 20:24:27.349: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:27.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8644" for this suite.
STEP: Destroying namespace "webhook-8644-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":339,"completed":99,"skipped":1922,"failed":0}
SS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:27.467: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Jul 22 20:24:27.704: INFO: Found Service test-service-74r6m in namespace services-4690 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jul 22 20:24:27.704: INFO: Service test-service-74r6m created
STEP: Getting /status
Jul 22 20:24:27.715: INFO: Service test-service-74r6m has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jul 22 20:24:27.737: INFO: observed Service test-service-74r6m in namespace services-4690 with annotations: map[] & LoadBalancer: {[]}
Jul 22 20:24:27.737: INFO: Found Service test-service-74r6m in namespace services-4690 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jul 22 20:24:27.737: INFO: Service test-service-74r6m has service status patched
STEP: updating the ServiceStatus
Jul 22 20:24:27.759: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jul 22 20:24:27.768: INFO: Observed Service test-service-74r6m in namespace services-4690 with annotations: map[] & Conditions: {[]}
Jul 22 20:24:27.768: INFO: Observed event: &Service{ObjectMeta:{test-service-74r6m  services-4690  e20351e0-f0ce-4757-9735-a956ec0b9242 17096 0 2021-07-22 20:24:27 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2021-07-22 20:24:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}},"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}}}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.71.106.186,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,TopologyKeys:[],IPFamilyPolicy:*SingleStack,ClusterIPs:[100.71.106.186],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:nil,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jul 22 20:24:27.769: INFO: Found Service test-service-74r6m in namespace services-4690 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 22 20:24:27.769: INFO: Service test-service-74r6m has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jul 22 20:24:27.795: INFO: observed Service test-service-74r6m in namespace services-4690 with labels: map[test-service-static:true]
Jul 22 20:24:27.795: INFO: observed Service test-service-74r6m in namespace services-4690 with labels: map[test-service-static:true]
Jul 22 20:24:27.795: INFO: observed Service test-service-74r6m in namespace services-4690 with labels: map[test-service-static:true]
Jul 22 20:24:27.795: INFO: Found Service test-service-74r6m in namespace services-4690 with labels: map[test-service:patched test-service-static:true]
Jul 22 20:24:27.795: INFO: Service test-service-74r6m patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jul 22 20:24:27.822: INFO: Observed event: ADDED
Jul 22 20:24:27.822: INFO: Observed event: MODIFIED
Jul 22 20:24:27.822: INFO: Observed event: MODIFIED
Jul 22 20:24:27.822: INFO: Observed event: MODIFIED
Jul 22 20:24:27.822: INFO: Found Service test-service-74r6m in namespace services-4690 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jul 22 20:24:27.822: INFO: Service test-service-74r6m deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:27.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4690" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":339,"completed":100,"skipped":1924,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:27.847: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4626
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul 22 20:24:28.058: INFO: Waiting up to 5m0s for pod "downward-api-6505be00-e894-432e-a9ba-3b49422517cf" in namespace "downward-api-4626" to be "Succeeded or Failed"
Jul 22 20:24:28.068: INFO: Pod "downward-api-6505be00-e894-432e-a9ba-3b49422517cf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.655307ms
Jul 22 20:24:30.080: INFO: Pod "downward-api-6505be00-e894-432e-a9ba-3b49422517cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022295288s
STEP: Saw pod success
Jul 22 20:24:30.080: INFO: Pod "downward-api-6505be00-e894-432e-a9ba-3b49422517cf" satisfied condition "Succeeded or Failed"
Jul 22 20:24:30.091: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downward-api-6505be00-e894-432e-a9ba-3b49422517cf container dapi-container: <nil>
STEP: delete the pod
Jul 22 20:24:30.127: INFO: Waiting for pod downward-api-6505be00-e894-432e-a9ba-3b49422517cf to disappear
Jul 22 20:24:30.137: INFO: Pod downward-api-6505be00-e894-432e-a9ba-3b49422517cf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:30.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4626" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":339,"completed":101,"skipped":1930,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:30.170: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5851
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:24:30.658: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:24:33.724: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:24:33.735: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:36.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5851" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":339,"completed":102,"skipped":1949,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:37.116: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:24:37.435: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul 22 20:24:37.502: INFO: The status of Pod pod-logs-websocket-74e286c3-9637-4e4c-9b3c-95f82cb585c9 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:24:39.513: INFO: The status of Pod pod-logs-websocket-74e286c3-9637-4e4c-9b3c-95f82cb585c9 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:39.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5478" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":339,"completed":103,"skipped":1962,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:39.601: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jul 22 20:24:39.848: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:24:41.860: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:24:43.860: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jul 22 20:24:43.906: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:24:45.918: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 22 20:24:45.929: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-522 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:24:45.929: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:24:51.244: INFO: Exec stderr: ""
Jul 22 20:24:51.244: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-522 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:24:51.244: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:24:51.552: INFO: Exec stderr: ""
Jul 22 20:24:51.552: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-522 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:24:51.552: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:24:51.847: INFO: Exec stderr: ""
Jul 22 20:24:51.847: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-522 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:24:51.847: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:24:52.163: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 22 20:24:52.163: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-522 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:24:52.163: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:24:52.583: INFO: Exec stderr: ""
Jul 22 20:24:52.583: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-522 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:24:52.583: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:24:52.876: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 22 20:24:52.876: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-522 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:24:52.876: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:24:53.171: INFO: Exec stderr: ""
Jul 22 20:24:53.171: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-522 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:24:53.171: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:24:53.468: INFO: Exec stderr: ""
Jul 22 20:24:53.468: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-522 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:24:53.468: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:24:53.750: INFO: Exec stderr: ""
Jul 22 20:24:53.750: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-522 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:24:53.750: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:24:54.036: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:24:54.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-522" for this suite.
•{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":104,"skipped":2006,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:24:54.069: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6025
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6025.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6025.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6025.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6025.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6025.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6025.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 20:24:58.409: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:24:58.423: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:24:58.436: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:24:58.483: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:24:58.523: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:24:58.537: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:24:58.550: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:24:58.563: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:24:58.591: INFO: Lookups using dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local]

Jul 22 20:25:03.606: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:03.619: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:03.633: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:03.678: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:03.718: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:03.732: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:03.745: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:03.758: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:03.791: INFO: Lookups using dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local]

Jul 22 20:25:08.605: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:08.650: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:08.664: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:08.677: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:08.750: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:08.764: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:08.777: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:08.791: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:08.819: INFO: Lookups using dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local]

Jul 22 20:25:13.606: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:13.620: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:13.666: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:13.680: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:13.726: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:13.740: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:13.790: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:13.807: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:13.835: INFO: Lookups using dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local]

Jul 22 20:25:18.605: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:18.620: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:18.633: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:18.678: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:18.722: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:18.735: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:18.749: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:18.762: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:18.790: INFO: Lookups using dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local]

Jul 22 20:25:23.606: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:23.651: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:23.665: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:23.677: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:23.750: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:23.763: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:23.780: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:23.796: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local from pod dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40: the server could not find the requested resource (get pods dns-test-42f05337-0774-4b09-a797-cbf607267a40)
Jul 22 20:25:23.825: INFO: Lookups using dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6025.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6025.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6025.svc.cluster.local jessie_udp@dns-test-service-2.dns-6025.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6025.svc.cluster.local]

Jul 22 20:25:28.791: INFO: DNS probes using dns-6025/dns-test-42f05337-0774-4b09-a797-cbf607267a40 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:25:28.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6025" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":339,"completed":105,"skipped":2016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:25:28.855: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:25:29.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-961" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":339,"completed":106,"skipped":2062,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:25:29.110: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:26:29.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-721" for this suite.
•{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":339,"completed":107,"skipped":2072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:26:29.365: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:26:29.598: INFO: The status of Pod test-webserver-2b018f72-311c-4d03-8e45-55c3fea1e1b2 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:26:31.611: INFO: The status of Pod test-webserver-2b018f72-311c-4d03-8e45-55c3fea1e1b2 is Running (Ready = false)
Jul 22 20:26:33.610: INFO: The status of Pod test-webserver-2b018f72-311c-4d03-8e45-55c3fea1e1b2 is Running (Ready = false)
Jul 22 20:26:35.611: INFO: The status of Pod test-webserver-2b018f72-311c-4d03-8e45-55c3fea1e1b2 is Running (Ready = false)
Jul 22 20:26:37.610: INFO: The status of Pod test-webserver-2b018f72-311c-4d03-8e45-55c3fea1e1b2 is Running (Ready = false)
Jul 22 20:26:39.611: INFO: The status of Pod test-webserver-2b018f72-311c-4d03-8e45-55c3fea1e1b2 is Running (Ready = false)
Jul 22 20:26:41.612: INFO: The status of Pod test-webserver-2b018f72-311c-4d03-8e45-55c3fea1e1b2 is Running (Ready = false)
Jul 22 20:26:43.611: INFO: The status of Pod test-webserver-2b018f72-311c-4d03-8e45-55c3fea1e1b2 is Running (Ready = false)
Jul 22 20:26:45.611: INFO: The status of Pod test-webserver-2b018f72-311c-4d03-8e45-55c3fea1e1b2 is Running (Ready = false)
Jul 22 20:26:47.612: INFO: The status of Pod test-webserver-2b018f72-311c-4d03-8e45-55c3fea1e1b2 is Running (Ready = false)
Jul 22 20:26:49.613: INFO: The status of Pod test-webserver-2b018f72-311c-4d03-8e45-55c3fea1e1b2 is Running (Ready = false)
Jul 22 20:26:51.610: INFO: The status of Pod test-webserver-2b018f72-311c-4d03-8e45-55c3fea1e1b2 is Running (Ready = true)
Jul 22 20:26:51.621: INFO: Container started at 2021-07-22 20:26:30 +0000 UTC, pod became ready at 2021-07-22 20:26:49 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:26:51.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9678" for this suite.
•{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":339,"completed":108,"skipped":2095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:26:51.653: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5481
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Jul 22 20:26:51.862: INFO: Waiting up to 5m0s for pod "client-containers-9fc40041-2a4d-40c9-b374-86797283b4a2" in namespace "containers-5481" to be "Succeeded or Failed"
Jul 22 20:26:51.874: INFO: Pod "client-containers-9fc40041-2a4d-40c9-b374-86797283b4a2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.441576ms
Jul 22 20:26:53.894: INFO: Pod "client-containers-9fc40041-2a4d-40c9-b374-86797283b4a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032581664s
STEP: Saw pod success
Jul 22 20:26:53.894: INFO: Pod "client-containers-9fc40041-2a4d-40c9-b374-86797283b4a2" satisfied condition "Succeeded or Failed"
Jul 22 20:26:53.905: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod client-containers-9fc40041-2a4d-40c9-b374-86797283b4a2 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:26:53.984: INFO: Waiting for pod client-containers-9fc40041-2a4d-40c9-b374-86797283b4a2 to disappear
Jul 22 20:26:53.994: INFO: Pod client-containers-9fc40041-2a4d-40c9-b374-86797283b4a2 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:26:53.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5481" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":339,"completed":109,"skipped":2121,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:26:54.026: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3000
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:26:54.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3000" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":339,"completed":110,"skipped":2130,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:26:54.267: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1659
STEP: creating service affinity-clusterip-transition in namespace services-1659
STEP: creating replication controller affinity-clusterip-transition in namespace services-1659
I0722 20:26:54.478743    5669 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-1659, replica count: 3
I0722 20:26:57.531391    5669 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:26:57.553: INFO: Creating new exec pod
Jul 22 20:27:02.593: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1659 exec execpod-affinity9khr9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jul 22 20:27:02.997: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jul 22 20:27:02.997: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 20:27:02.997: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1659 exec execpod-affinity9khr9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.71.0.242 80'
Jul 22 20:27:03.338: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.71.0.242 80\nConnection to 100.71.0.242 80 port [tcp/http] succeeded!\n"
Jul 22 20:27:03.338: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 20:27:03.362: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1659 exec execpod-affinity9khr9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.71.0.242:80/ ; done'
Jul 22 20:27:03.843: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n"
Jul 22 20:27:03.844: INFO: stdout: "\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht"
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:03.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:33.844: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1659 exec execpod-affinity9khr9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.71.0.242:80/ ; done'
Jul 22 20:27:34.304: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n"
Jul 22 20:27:34.304: INFO: stdout: "\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dk72b\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-dk72b\naffinity-clusterip-transition-dk72b\naffinity-clusterip-transition-dk72b\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dk72b\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht"
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-dk72b
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-dk72b
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-dk72b
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-dk72b
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-dk72b
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.304: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.330: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1659 exec execpod-affinity9khr9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.71.0.242:80/ ; done'
Jul 22 20:27:34.844: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n"
Jul 22 20:27:34.844: INFO: stdout: "\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dk72b\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-dk72b\naffinity-clusterip-transition-dlbht\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-dlbht"
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-dk72b
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-dk72b
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:27:34.844: INFO: Received response from host: affinity-clusterip-transition-dlbht
Jul 22 20:28:04.845: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-1659 exec execpod-affinity9khr9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.71.0.242:80/ ; done'
Jul 22 20:28:05.393: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.0.242:80/\n"
Jul 22 20:28:05.393: INFO: stdout: "\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m\naffinity-clusterip-transition-8vx8m"
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Received response from host: affinity-clusterip-transition-8vx8m
Jul 22 20:28:05.393: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1659, will wait for the garbage collector to delete the pods
Jul 22 20:28:05.485: INFO: Deleting ReplicationController affinity-clusterip-transition took: 13.03911ms
Jul 22 20:28:05.585: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.742932ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:28:17.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1659" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":111,"skipped":2142,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:28:17.939: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5602
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5602
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 20:28:18.127: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 22 20:28:18.204: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:28:20.219: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:28:22.217: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:28:24.216: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:28:26.218: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:28:28.227: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:28:30.217: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:28:32.217: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:28:34.216: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:28:36.216: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:28:38.219: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 22 20:28:38.242: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 22 20:28:42.342: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul 22 20:28:42.342: INFO: Going to poll 100.96.0.68 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jul 22 20:28:42.353: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.0.68 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5602 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:28:42.353: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:28:43.622: INFO: Found all 1 expected endpoints: [netserver-0]
Jul 22 20:28:43.622: INFO: Going to poll 100.96.1.136 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jul 22 20:28:43.633: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.136 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5602 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:28:43.633: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:28:44.918: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:28:44.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5602" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":112,"skipped":2149,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:28:44.952: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5687
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-kggt
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 20:28:45.199: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kggt" in namespace "subpath-5687" to be "Succeeded or Failed"
Jul 22 20:28:45.210: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Pending", Reason="", readiness=false. Elapsed: 11.02389ms
Jul 22 20:28:47.222: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Running", Reason="", readiness=true. Elapsed: 2.022851919s
Jul 22 20:28:49.234: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Running", Reason="", readiness=true. Elapsed: 4.034593776s
Jul 22 20:28:51.247: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Running", Reason="", readiness=true. Elapsed: 6.047125452s
Jul 22 20:28:53.258: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Running", Reason="", readiness=true. Elapsed: 8.058290749s
Jul 22 20:28:55.270: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Running", Reason="", readiness=true. Elapsed: 10.070252579s
Jul 22 20:28:57.283: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Running", Reason="", readiness=true. Elapsed: 12.083533105s
Jul 22 20:28:59.295: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Running", Reason="", readiness=true. Elapsed: 14.095409387s
Jul 22 20:29:01.307: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Running", Reason="", readiness=true. Elapsed: 16.107375643s
Jul 22 20:29:03.318: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Running", Reason="", readiness=true. Elapsed: 18.119084229s
Jul 22 20:29:05.382: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Running", Reason="", readiness=true. Elapsed: 20.18267987s
Jul 22 20:29:07.394: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Running", Reason="", readiness=true. Elapsed: 22.194742771s
Jul 22 20:29:09.407: INFO: Pod "pod-subpath-test-configmap-kggt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.207591375s
STEP: Saw pod success
Jul 22 20:29:09.407: INFO: Pod "pod-subpath-test-configmap-kggt" satisfied condition "Succeeded or Failed"
Jul 22 20:29:09.418: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-subpath-test-configmap-kggt container test-container-subpath-configmap-kggt: <nil>
STEP: delete the pod
Jul 22 20:29:09.453: INFO: Waiting for pod pod-subpath-test-configmap-kggt to disappear
Jul 22 20:29:09.464: INFO: Pod pod-subpath-test-configmap-kggt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kggt
Jul 22 20:29:09.464: INFO: Deleting pod "pod-subpath-test-configmap-kggt" in namespace "subpath-5687"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:29:09.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5687" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":339,"completed":113,"skipped":2152,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:29:09.506: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4064
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:29:09.723: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d561dee-f0e3-45e2-ad55-e7d138f23a6b" in namespace "downward-api-4064" to be "Succeeded or Failed"
Jul 22 20:29:09.734: INFO: Pod "downwardapi-volume-3d561dee-f0e3-45e2-ad55-e7d138f23a6b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.065389ms
Jul 22 20:29:11.747: INFO: Pod "downwardapi-volume-3d561dee-f0e3-45e2-ad55-e7d138f23a6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023141726s
STEP: Saw pod success
Jul 22 20:29:11.747: INFO: Pod "downwardapi-volume-3d561dee-f0e3-45e2-ad55-e7d138f23a6b" satisfied condition "Succeeded or Failed"
Jul 22 20:29:11.758: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-3d561dee-f0e3-45e2-ad55-e7d138f23a6b container client-container: <nil>
STEP: delete the pod
Jul 22 20:29:11.836: INFO: Waiting for pod downwardapi-volume-3d561dee-f0e3-45e2-ad55-e7d138f23a6b to disappear
Jul 22 20:29:11.847: INFO: Pod downwardapi-volume-3d561dee-f0e3-45e2-ad55-e7d138f23a6b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:29:11.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4064" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":114,"skipped":2155,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:29:11.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-8414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 22 20:29:12.175: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 22 20:29:12.197: INFO: starting watch
STEP: patching
STEP: updating
Jul 22 20:29:12.242: INFO: waiting for watch events with expected annotations
Jul 22 20:29:12.242: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:29:12.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8414" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":339,"completed":115,"skipped":2164,"failed":0}
S
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:29:12.326: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:29:12.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5824" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":116,"skipped":2165,"failed":0}
SSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:29:12.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-2391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jul 22 20:29:33.308: INFO: EndpointSlice for Service endpointslice-2391/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:29:43.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2391" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":339,"completed":117,"skipped":2172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:29:43.366: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3851
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jul 22 20:29:43.592: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:29:45.604: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:29:47.604: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jul 22 20:29:47.645: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:29:49.657: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:29:51.658: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 22 20:29:51.744: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 20:29:51.755: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 20:29:53.756: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 20:29:53.774: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 20:29:55.756: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 20:29:55.767: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 20:29:57.756: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 20:29:57.768: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:29:57.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3851" for this suite.
•{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":339,"completed":118,"skipped":2256,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:29:57.801: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7061
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jul 22 20:30:02.037: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7061 PodName:pod-sharedvolume-b97ba576-a9d2-4a95-82c3-44bfbe3f707d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:30:02.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:30:02.342: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:30:02.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7061" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":339,"completed":119,"skipped":2259,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:30:02.377: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-150
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:30:02.587: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02c5508b-2937-476f-8e58-bd376a9fb417" in namespace "projected-150" to be "Succeeded or Failed"
Jul 22 20:30:02.598: INFO: Pod "downwardapi-volume-02c5508b-2937-476f-8e58-bd376a9fb417": Phase="Pending", Reason="", readiness=false. Elapsed: 10.867389ms
Jul 22 20:30:04.610: INFO: Pod "downwardapi-volume-02c5508b-2937-476f-8e58-bd376a9fb417": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023386548s
Jul 22 20:30:06.621: INFO: Pod "downwardapi-volume-02c5508b-2937-476f-8e58-bd376a9fb417": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034662701s
STEP: Saw pod success
Jul 22 20:30:06.621: INFO: Pod "downwardapi-volume-02c5508b-2937-476f-8e58-bd376a9fb417" satisfied condition "Succeeded or Failed"
Jul 22 20:30:06.632: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-02c5508b-2937-476f-8e58-bd376a9fb417 container client-container: <nil>
STEP: delete the pod
Jul 22 20:30:06.710: INFO: Waiting for pod downwardapi-volume-02c5508b-2937-476f-8e58-bd376a9fb417 to disappear
Jul 22 20:30:06.721: INFO: Pod downwardapi-volume-02c5508b-2937-476f-8e58-bd376a9fb417 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:30:06.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-150" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":120,"skipped":2300,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:30:06.754: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2379
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul 22 20:30:06.974: INFO: The status of Pod pod-update-b1d648b5-7661-4c07-8216-9f664cb694b0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:30:08.987: INFO: The status of Pod pod-update-b1d648b5-7661-4c07-8216-9f664cb694b0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:30:10.986: INFO: The status of Pod pod-update-b1d648b5-7661-4c07-8216-9f664cb694b0 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 22 20:30:11.534: INFO: Successfully updated pod "pod-update-b1d648b5-7661-4c07-8216-9f664cb694b0"
STEP: verifying the updated pod is in kubernetes
Jul 22 20:30:11.556: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:30:11.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2379" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":339,"completed":121,"skipped":2322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:30:11.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1123
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jul 22 20:30:11.793: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:30:17.888: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:30:35.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1123" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":339,"completed":122,"skipped":2345,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:30:35.828: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8079
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:30:36.650: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 22 20:30:38.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582636, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582636, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582636, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582636, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:30:41.718: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:30:42.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8079" for this suite.
STEP: Destroying namespace "webhook-8079-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":339,"completed":123,"skipped":2360,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:30:42.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 22 20:30:42.372: INFO: Waiting up to 5m0s for pod "pod-14c73afe-79fb-4ce0-b1c3-54bade5044dd" in namespace "emptydir-6151" to be "Succeeded or Failed"
Jul 22 20:30:42.383: INFO: Pod "pod-14c73afe-79fb-4ce0-b1c3-54bade5044dd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.590176ms
Jul 22 20:30:44.395: INFO: Pod "pod-14c73afe-79fb-4ce0-b1c3-54bade5044dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023266339s
STEP: Saw pod success
Jul 22 20:30:44.395: INFO: Pod "pod-14c73afe-79fb-4ce0-b1c3-54bade5044dd" satisfied condition "Succeeded or Failed"
Jul 22 20:30:44.408: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-14c73afe-79fb-4ce0-b1c3-54bade5044dd container test-container: <nil>
STEP: delete the pod
Jul 22 20:30:44.445: INFO: Waiting for pod pod-14c73afe-79fb-4ce0-b1c3-54bade5044dd to disappear
Jul 22 20:30:44.455: INFO: Pod pod-14c73afe-79fb-4ce0-b1c3-54bade5044dd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:30:44.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6151" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":124,"skipped":2369,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:30:44.487: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 22 20:30:44.697: INFO: Waiting up to 5m0s for pod "pod-972d0076-8b5e-468c-8a8e-35880760cd84" in namespace "emptydir-7158" to be "Succeeded or Failed"
Jul 22 20:30:44.709: INFO: Pod "pod-972d0076-8b5e-468c-8a8e-35880760cd84": Phase="Pending", Reason="", readiness=false. Elapsed: 11.596781ms
Jul 22 20:30:46.721: INFO: Pod "pod-972d0076-8b5e-468c-8a8e-35880760cd84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023709638s
STEP: Saw pod success
Jul 22 20:30:46.721: INFO: Pod "pod-972d0076-8b5e-468c-8a8e-35880760cd84" satisfied condition "Succeeded or Failed"
Jul 22 20:30:46.732: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-972d0076-8b5e-468c-8a8e-35880760cd84 container test-container: <nil>
STEP: delete the pod
Jul 22 20:30:46.770: INFO: Waiting for pod pod-972d0076-8b5e-468c-8a8e-35880760cd84 to disappear
Jul 22 20:30:46.780: INFO: Pod pod-972d0076-8b5e-468c-8a8e-35880760cd84 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:30:46.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7158" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":125,"skipped":2369,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:30:46.812: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-77
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jul 22 20:30:47.035: INFO: The status of Pod annotationupdatea7d2f7d0-facd-437c-bf35-134861c96231 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:30:49.048: INFO: The status of Pod annotationupdatea7d2f7d0-facd-437c-bf35-134861c96231 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:30:51.047: INFO: The status of Pod annotationupdatea7d2f7d0-facd-437c-bf35-134861c96231 is Running (Ready = true)
Jul 22 20:30:51.606: INFO: Successfully updated pod "annotationupdatea7d2f7d0-facd-437c-bf35-134861c96231"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:30:53.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-77" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":126,"skipped":2372,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:30:53.681: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
W0722 20:30:53.880797    5669 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: getting
STEP: listing
STEP: watching
Jul 22 20:30:53.914: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 22 20:30:53.935: INFO: starting watch
STEP: patching
STEP: updating
Jul 22 20:30:53.987: INFO: waiting for watch events with expected annotations
Jul 22 20:30:53.987: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:30:54.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-596" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":339,"completed":127,"skipped":2396,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:30:54.120: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3099
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 22 20:31:00.403: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0722 20:31:00.403074    5669 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0722 20:31:00.403128    5669 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0722 20:31:00.403135    5669 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:31:00.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3099" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":339,"completed":128,"skipped":2409,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:31:00.428: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5197
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:31:00.638: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 22 20:31:05.687: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 22 20:31:05.687: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 22 20:31:07.701: INFO: Creating deployment "test-rollover-deployment"
Jul 22 20:31:07.727: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 22 20:31:09.751: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 22 20:31:09.774: INFO: Ensure that both replica sets have 1 created replica
Jul 22 20:31:09.797: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 22 20:31:09.822: INFO: Updating deployment test-rollover-deployment
Jul 22 20:31:09.822: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 22 20:31:11.846: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 22 20:31:11.868: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 22 20:31:11.892: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 20:31:11.892: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582669, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:31:13.977: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 20:31:13.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:31:15.914: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 20:31:15.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:31:17.914: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 20:31:17.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:31:19.914: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 20:31:19.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:31:21.917: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 20:31:21.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762582667, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:31:23.990: INFO: 
Jul 22 20:31:23.990: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 22 20:31:24.023: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5197  548cecdf-b267-457f-981f-3beef13464c0 20488 2 2021-07-22 20:31:07 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-22 20:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:31:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a05ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-22 20:31:07 +0000 UTC,LastTransitionTime:2021-07-22 20:31:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-07-22 20:31:22 +0000 UTC,LastTransitionTime:2021-07-22 20:31:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 22 20:31:24.035: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-5197  11e2f1e0-24db-4ac3-96d7-d409e71a8a7d 20481 2 2021-07-22 20:31:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 548cecdf-b267-457f-981f-3beef13464c0 0xc005ac2240 0xc005ac2241}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:31:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548cecdf-b267-457f-981f-3beef13464c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac22d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:31:24.035: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 22 20:31:24.035: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5197  66594454-8f9d-470b-94fb-765a53ae7610 20487 2 2021-07-22 20:31:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 548cecdf-b267-457f-981f-3beef13464c0 0xc005a05fa7 0xc005a05fa8}] []  [{e2e.test Update apps/v1 2021-07-22 20:31:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:31:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548cecdf-b267-457f-981f-3beef13464c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005ac2098 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:31:24.035: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-5197  032fcc60-f50e-4420-9150-e3d3fcde0096 20408 2 2021-07-22 20:31:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 548cecdf-b267-457f-981f-3beef13464c0 0xc005ac2107 0xc005ac2108}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"548cecdf-b267-457f-981f-3beef13464c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005ac21c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:31:24.047: INFO: Pod "test-rollover-deployment-98c5f4599-zt5fq" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-zt5fq test-rollover-deployment-98c5f4599- deployment-5197  dec9bf0b-c457-42f1-b3ae-4d79c01a963d 20425 0 2021-07-22 20:31:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[cni.projectcalico.org/podIP:100.96.1.159/32 cni.projectcalico.org/podIPs:100.96.1.159/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 11e2f1e0-24db-4ac3-96d7-d409e71a8a7d 0xc005ac2900 0xc005ac2901}] []  [{kube-controller-manager Update v1 2021-07-22 20:31:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11e2f1e0-24db-4ac3-96d7-d409e71a8a7d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:31:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:31:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6kk5c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6kk5c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:31:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:31:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.159,StartTime:2021-07-22 20:31:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:31:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://6acf9cd35e53a8bbbbbe5c5262ecdafcd23fff57c11aa4b810a01ee9066ba6a2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:31:24.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5197" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":339,"completed":129,"skipped":2418,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:31:24.079: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6652
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:31:24.276: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:31:26.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6652" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":339,"completed":130,"skipped":2421,"failed":0}
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:31:26.188: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:31:26.599: INFO: The status of Pod busybox-readonly-fsdd785cf9-7545-4a88-a729-4a0c9e93ee0a is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:31:28.609: INFO: The status of Pod busybox-readonly-fsdd785cf9-7545-4a88-a729-4a0c9e93ee0a is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:31:28.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8059" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":131,"skipped":2428,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:31:28.780: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7934
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:31:29.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7934" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":339,"completed":132,"skipped":2429,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:31:29.292: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6342
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 22 20:31:29.688: INFO: Waiting up to 5m0s for pod "pod-ae95e590-0713-4c35-a8d9-575814a51608" in namespace "emptydir-6342" to be "Succeeded or Failed"
Jul 22 20:31:29.699: INFO: Pod "pod-ae95e590-0713-4c35-a8d9-575814a51608": Phase="Pending", Reason="", readiness=false. Elapsed: 10.419624ms
Jul 22 20:31:31.711: INFO: Pod "pod-ae95e590-0713-4c35-a8d9-575814a51608": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022010792s
Jul 22 20:31:33.723: INFO: Pod "pod-ae95e590-0713-4c35-a8d9-575814a51608": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034400093s
STEP: Saw pod success
Jul 22 20:31:33.723: INFO: Pod "pod-ae95e590-0713-4c35-a8d9-575814a51608" satisfied condition "Succeeded or Failed"
Jul 22 20:31:33.734: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-ae95e590-0713-4c35-a8d9-575814a51608 container test-container: <nil>
STEP: delete the pod
Jul 22 20:31:33.771: INFO: Waiting for pod pod-ae95e590-0713-4c35-a8d9-575814a51608 to disappear
Jul 22 20:31:33.782: INFO: Pod pod-ae95e590-0713-4c35-a8d9-575814a51608 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:31:33.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6342" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":133,"skipped":2436,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:31:33.814: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3751
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:31:34.024: INFO: Creating pod...
Jul 22 20:31:34.079: INFO: Pod Quantity: 1 Status: Pending
Jul 22 20:31:35.090: INFO: Pod Quantity: 1 Status: Pending
Jul 22 20:31:36.091: INFO: Pod Status: Running
Jul 22 20:31:36.092: INFO: Creating service...
Jul 22 20:31:36.109: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/pods/agnhost/proxy/some/path/with/DELETE
Jul 22 20:31:36.221: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 22 20:31:36.221: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/pods/agnhost/proxy/some/path/with/GET
Jul 22 20:31:36.235: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul 22 20:31:36.235: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/pods/agnhost/proxy/some/path/with/HEAD
Jul 22 20:31:36.249: INFO: http.Client request:HEAD | StatusCode:200
Jul 22 20:31:36.249: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/pods/agnhost/proxy/some/path/with/OPTIONS
Jul 22 20:31:36.294: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 22 20:31:36.294: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/pods/agnhost/proxy/some/path/with/PATCH
Jul 22 20:31:36.314: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 22 20:31:36.314: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/pods/agnhost/proxy/some/path/with/POST
Jul 22 20:31:36.327: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 22 20:31:36.327: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/pods/agnhost/proxy/some/path/with/PUT
Jul 22 20:31:36.345: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul 22 20:31:36.345: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/services/test-service/proxy/some/path/with/DELETE
Jul 22 20:31:36.359: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 22 20:31:36.359: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/services/test-service/proxy/some/path/with/GET
Jul 22 20:31:36.374: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul 22 20:31:36.374: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/services/test-service/proxy/some/path/with/HEAD
Jul 22 20:31:36.388: INFO: http.Client request:HEAD | StatusCode:200
Jul 22 20:31:36.388: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/services/test-service/proxy/some/path/with/OPTIONS
Jul 22 20:31:36.402: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 22 20:31:36.402: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/services/test-service/proxy/some/path/with/PATCH
Jul 22 20:31:36.416: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 22 20:31:36.416: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/services/test-service/proxy/some/path/with/POST
Jul 22 20:31:36.430: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 22 20:31:36.430: INFO: Starting http.Client for https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com/api/v1/namespaces/proxy-3751/services/test-service/proxy/some/path/with/PUT
Jul 22 20:31:36.444: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:31:36.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3751" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":339,"completed":134,"skipped":2448,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:31:36.477: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6132
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Jul 22 20:31:36.685: INFO: Waiting up to 5m0s for pod "var-expansion-e07ff14d-6651-4ff3-b34a-74e0424f957c" in namespace "var-expansion-6132" to be "Succeeded or Failed"
Jul 22 20:31:36.695: INFO: Pod "var-expansion-e07ff14d-6651-4ff3-b34a-74e0424f957c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.467763ms
Jul 22 20:31:38.707: INFO: Pod "var-expansion-e07ff14d-6651-4ff3-b34a-74e0424f957c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022566711s
Jul 22 20:31:40.719: INFO: Pod "var-expansion-e07ff14d-6651-4ff3-b34a-74e0424f957c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03450478s
STEP: Saw pod success
Jul 22 20:31:40.719: INFO: Pod "var-expansion-e07ff14d-6651-4ff3-b34a-74e0424f957c" satisfied condition "Succeeded or Failed"
Jul 22 20:31:40.731: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod var-expansion-e07ff14d-6651-4ff3-b34a-74e0424f957c container dapi-container: <nil>
STEP: delete the pod
Jul 22 20:31:40.811: INFO: Waiting for pod var-expansion-e07ff14d-6651-4ff3-b34a-74e0424f957c to disappear
Jul 22 20:31:40.822: INFO: Pod var-expansion-e07ff14d-6651-4ff3-b34a-74e0424f957c no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:31:40.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6132" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":339,"completed":135,"skipped":2454,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:31:40.854: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2250
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-8ba3a7a4-4af3-484a-8616-db5290b35670
STEP: Creating configMap with name cm-test-opt-upd-b28370df-16fe-49ad-88dd-a0616b582497
STEP: Creating the pod
Jul 22 20:31:41.112: INFO: The status of Pod pod-configmaps-cdfa3906-221d-4458-8721-f06466adafc1 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:31:43.126: INFO: The status of Pod pod-configmaps-cdfa3906-221d-4458-8721-f06466adafc1 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:31:45.123: INFO: The status of Pod pod-configmaps-cdfa3906-221d-4458-8721-f06466adafc1 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-8ba3a7a4-4af3-484a-8616-db5290b35670
STEP: Updating configmap cm-test-opt-upd-b28370df-16fe-49ad-88dd-a0616b582497
STEP: Creating configMap with name cm-test-opt-create-5edf2555-5d91-4670-bdd9-a7fd11b34056
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:33:02.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2250" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":136,"skipped":2467,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:33:02.215: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-7787
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jul 22 20:33:02.490: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jul 22 20:33:02.556: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:33:02.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7787" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":339,"completed":137,"skipped":2494,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:33:02.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4061
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-eb0e5694-c4f3-48de-882c-169ee1b17d70
STEP: Creating a pod to test consume secrets
Jul 22 20:33:02.870: INFO: Waiting up to 5m0s for pod "pod-secrets-dd96791d-2cea-4b76-af85-594aff336269" in namespace "secrets-4061" to be "Succeeded or Failed"
Jul 22 20:33:02.883: INFO: Pod "pod-secrets-dd96791d-2cea-4b76-af85-594aff336269": Phase="Pending", Reason="", readiness=false. Elapsed: 13.541067ms
Jul 22 20:33:04.896: INFO: Pod "pod-secrets-dd96791d-2cea-4b76-af85-594aff336269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025977565s
STEP: Saw pod success
Jul 22 20:33:04.896: INFO: Pod "pod-secrets-dd96791d-2cea-4b76-af85-594aff336269" satisfied condition "Succeeded or Failed"
Jul 22 20:33:04.907: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-secrets-dd96791d-2cea-4b76-af85-594aff336269 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:33:04.950: INFO: Waiting for pod pod-secrets-dd96791d-2cea-4b76-af85-594aff336269 to disappear
Jul 22 20:33:04.961: INFO: Pod pod-secrets-dd96791d-2cea-4b76-af85-594aff336269 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:33:04.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4061" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":138,"skipped":2502,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:33:04.993: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:33:05.200: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef99f148-58ab-4153-aa79-a074127dd4fe" in namespace "downward-api-9472" to be "Succeeded or Failed"
Jul 22 20:33:05.210: INFO: Pod "downwardapi-volume-ef99f148-58ab-4153-aa79-a074127dd4fe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.67038ms
Jul 22 20:33:07.222: INFO: Pod "downwardapi-volume-ef99f148-58ab-4153-aa79-a074127dd4fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022044873s
STEP: Saw pod success
Jul 22 20:33:07.222: INFO: Pod "downwardapi-volume-ef99f148-58ab-4153-aa79-a074127dd4fe" satisfied condition "Succeeded or Failed"
Jul 22 20:33:07.233: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-ef99f148-58ab-4153-aa79-a074127dd4fe container client-container: <nil>
STEP: delete the pod
Jul 22 20:33:07.294: INFO: Waiting for pod downwardapi-volume-ef99f148-58ab-4153-aa79-a074127dd4fe to disappear
Jul 22 20:33:07.304: INFO: Pod downwardapi-volume-ef99f148-58ab-4153-aa79-a074127dd4fe no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:33:07.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9472" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":139,"skipped":2505,"failed":0}
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:33:07.336: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jul 22 20:33:07.598: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:33:09.610: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jul 22 20:33:09.650: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:33:11.663: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jul 22 20:33:11.688: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:33:11.700: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:33:13.701: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:33:13.712: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 20:33:15.701: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 20:33:15.712: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:33:15.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2925" for this suite.
•{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":339,"completed":140,"skipped":2509,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:33:15.766: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7890
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1386
STEP: creating an pod
Jul 22 20:33:15.958: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7890 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul 22 20:33:16.355: INFO: stderr: ""
Jul 22 20:33:16.355: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Jul 22 20:33:16.355: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul 22 20:33:16.355: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7890" to be "running and ready, or succeeded"
Jul 22 20:33:16.367: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.351913ms
Jul 22 20:33:18.380: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.024043263s
Jul 22 20:33:18.380: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul 22 20:33:18.380: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jul 22 20:33:18.380: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7890 logs logs-generator logs-generator'
Jul 22 20:33:18.506: INFO: stderr: ""
Jul 22 20:33:18.506: INFO: stdout: "I0722 20:33:17.600511       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/4sx9 440\nI0722 20:33:17.796321       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/vtsd 236\nI0722 20:33:17.996791       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/79b 446\nI0722 20:33:18.197141       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/mc4 298\nI0722 20:33:18.396486       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/bq6j 497\n"
STEP: limiting log lines
Jul 22 20:33:18.506: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7890 logs logs-generator logs-generator --tail=1'
Jul 22 20:33:18.681: INFO: stderr: ""
Jul 22 20:33:18.681: INFO: stdout: "I0722 20:33:18.596774       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/5d9d 560\n"
Jul 22 20:33:18.681: INFO: got output "I0722 20:33:18.596774       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/5d9d 560\n"
STEP: limiting log bytes
Jul 22 20:33:18.681: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7890 logs logs-generator logs-generator --limit-bytes=1'
Jul 22 20:33:18.799: INFO: stderr: ""
Jul 22 20:33:18.799: INFO: stdout: "I"
Jul 22 20:33:18.799: INFO: got output "I"
STEP: exposing timestamps
Jul 22 20:33:18.800: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7890 logs logs-generator logs-generator --tail=1 --timestamps'
Jul 22 20:33:18.935: INFO: stderr: ""
Jul 22 20:33:18.935: INFO: stdout: "2021-07-22T20:33:18.797292697Z I0722 20:33:18.797126       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/7gg 458\n"
Jul 22 20:33:18.935: INFO: got output "2021-07-22T20:33:18.797292697Z I0722 20:33:18.797126       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/7gg 458\n"
STEP: restricting to a time range
Jul 22 20:33:21.437: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7890 logs logs-generator logs-generator --since=1s'
Jul 22 20:33:21.579: INFO: stderr: ""
Jul 22 20:33:21.579: INFO: stdout: "I0722 20:33:20.596222       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/t4r 553\nI0722 20:33:20.796569       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/75p 543\nI0722 20:33:20.997435       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/jhzk 389\nI0722 20:33:21.196825       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/8cwv 285\nI0722 20:33:21.396981       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/7c7 292\n"
Jul 22 20:33:21.579: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7890 logs logs-generator logs-generator --since=24h'
Jul 22 20:33:21.706: INFO: stderr: ""
Jul 22 20:33:21.706: INFO: stdout: "I0722 20:33:17.600511       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/4sx9 440\nI0722 20:33:17.796321       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/vtsd 236\nI0722 20:33:17.996791       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/79b 446\nI0722 20:33:18.197141       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/mc4 298\nI0722 20:33:18.396486       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/bq6j 497\nI0722 20:33:18.596774       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/5d9d 560\nI0722 20:33:18.797126       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/7gg 458\nI0722 20:33:18.996513       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/6wtr 222\nI0722 20:33:19.196922       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/xnx7 466\nI0722 20:33:19.396208       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/28x 270\nI0722 20:33:19.596590       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/2d6 491\nI0722 20:33:19.796982       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/5jb 213\nI0722 20:33:19.996268       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/qh2 516\nI0722 20:33:20.196597       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/xht 567\nI0722 20:33:20.396944       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/rqr2 528\nI0722 20:33:20.596222       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/t4r 553\nI0722 20:33:20.796569       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/75p 543\nI0722 20:33:20.997435       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/jhzk 389\nI0722 20:33:21.196825       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/8cwv 285\nI0722 20:33:21.396981       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/7c7 292\nI0722 20:33:21.596198       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/bqqw 585\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1391
Jul 22 20:33:21.706: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-7890 delete pod logs-generator'
Jul 22 20:33:26.934: INFO: stderr: ""
Jul 22 20:33:26.934: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:33:26.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7890" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":339,"completed":141,"skipped":2513,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:33:26.968: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3488
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-c0996bc4-27be-4a43-af54-6b13781eb769
STEP: Creating a pod to test consume secrets
Jul 22 20:33:27.193: INFO: Waiting up to 5m0s for pod "pod-secrets-81116b9b-1bdb-45f8-8be1-66f8121cb377" in namespace "secrets-3488" to be "Succeeded or Failed"
Jul 22 20:33:27.203: INFO: Pod "pod-secrets-81116b9b-1bdb-45f8-8be1-66f8121cb377": Phase="Pending", Reason="", readiness=false. Elapsed: 10.697733ms
Jul 22 20:33:29.215: INFO: Pod "pod-secrets-81116b9b-1bdb-45f8-8be1-66f8121cb377": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022185438s
STEP: Saw pod success
Jul 22 20:33:29.215: INFO: Pod "pod-secrets-81116b9b-1bdb-45f8-8be1-66f8121cb377" satisfied condition "Succeeded or Failed"
Jul 22 20:33:29.226: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-secrets-81116b9b-1bdb-45f8-8be1-66f8121cb377 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:33:29.262: INFO: Waiting for pod pod-secrets-81116b9b-1bdb-45f8-8be1-66f8121cb377 to disappear
Jul 22 20:33:29.273: INFO: Pod pod-secrets-81116b9b-1bdb-45f8-8be1-66f8121cb377 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:33:29.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3488" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":142,"skipped":2520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:33:29.305: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-3429
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:33:29.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3429
I0722 20:33:29.507892    5669 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3429, replica count: 1
I0722 20:33:30.559683    5669 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 20:33:31.560275    5669 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:33:31.679: INFO: Created: latency-svc-g9jtc
Jul 22 20:33:31.684: INFO: Got endpoints: latency-svc-g9jtc [23.131911ms]
Jul 22 20:33:31.701: INFO: Created: latency-svc-rrpld
Jul 22 20:33:31.706: INFO: Created: latency-svc-lzz8l
Jul 22 20:33:31.706: INFO: Got endpoints: latency-svc-rrpld [21.885881ms]
Jul 22 20:33:31.710: INFO: Got endpoints: latency-svc-lzz8l [26.33262ms]
Jul 22 20:33:31.711: INFO: Created: latency-svc-5t5zc
Jul 22 20:33:31.717: INFO: Got endpoints: latency-svc-5t5zc [33.07272ms]
Jul 22 20:33:31.718: INFO: Created: latency-svc-zk9cq
Jul 22 20:33:31.726: INFO: Got endpoints: latency-svc-zk9cq [41.430777ms]
Jul 22 20:33:31.728: INFO: Created: latency-svc-m5h5f
Jul 22 20:33:31.780: INFO: Got endpoints: latency-svc-m5h5f [96.312989ms]
Jul 22 20:33:31.781: INFO: Created: latency-svc-7tslq
Jul 22 20:33:31.786: INFO: Got endpoints: latency-svc-7tslq [101.793331ms]
Jul 22 20:33:31.789: INFO: Created: latency-svc-6dncf
Jul 22 20:33:31.793: INFO: Got endpoints: latency-svc-6dncf [108.469622ms]
Jul 22 20:33:31.793: INFO: Created: latency-svc-4fs5s
Jul 22 20:33:31.797: INFO: Got endpoints: latency-svc-4fs5s [112.802205ms]
Jul 22 20:33:31.797: INFO: Created: latency-svc-9lrv2
Jul 22 20:33:31.800: INFO: Got endpoints: latency-svc-9lrv2 [115.994119ms]
Jul 22 20:33:31.805: INFO: Created: latency-svc-4l8zl
Jul 22 20:33:31.876: INFO: Got endpoints: latency-svc-4l8zl [191.724924ms]
Jul 22 20:33:31.878: INFO: Created: latency-svc-569xd
Jul 22 20:33:31.882: INFO: Got endpoints: latency-svc-569xd [198.091407ms]
Jul 22 20:33:31.883: INFO: Created: latency-svc-5hjvg
Jul 22 20:33:31.890: INFO: Got endpoints: latency-svc-5hjvg [205.528538ms]
Jul 22 20:33:31.890: INFO: Created: latency-svc-mv7f4
Jul 22 20:33:31.895: INFO: Got endpoints: latency-svc-mv7f4 [211.077408ms]
Jul 22 20:33:31.896: INFO: Created: latency-svc-f9bcn
Jul 22 20:33:31.900: INFO: Got endpoints: latency-svc-f9bcn [216.140279ms]
Jul 22 20:33:31.901: INFO: Created: latency-svc-d8rzx
Jul 22 20:33:31.905: INFO: Got endpoints: latency-svc-d8rzx [221.023466ms]
Jul 22 20:33:31.906: INFO: Created: latency-svc-z6rbs
Jul 22 20:33:31.975: INFO: Got endpoints: latency-svc-z6rbs [269.242386ms]
Jul 22 20:33:31.977: INFO: Created: latency-svc-d5nw7
Jul 22 20:33:31.983: INFO: Created: latency-svc-fk65d
Jul 22 20:33:31.983: INFO: Got endpoints: latency-svc-d5nw7 [272.545156ms]
Jul 22 20:33:31.988: INFO: Created: latency-svc-m6rqx
Jul 22 20:33:31.988: INFO: Got endpoints: latency-svc-fk65d [271.071784ms]
Jul 22 20:33:31.994: INFO: Got endpoints: latency-svc-m6rqx [268.195635ms]
Jul 22 20:33:31.996: INFO: Created: latency-svc-nx8ql
Jul 22 20:33:32.077: INFO: Created: latency-svc-6cvq4
Jul 22 20:33:32.078: INFO: Got endpoints: latency-svc-nx8ql [297.105705ms]
Jul 22 20:33:32.081: INFO: Got endpoints: latency-svc-6cvq4 [294.828717ms]
Jul 22 20:33:32.086: INFO: Created: latency-svc-qpkjz
Jul 22 20:33:32.091: INFO: Got endpoints: latency-svc-qpkjz [297.843216ms]
Jul 22 20:33:32.091: INFO: Created: latency-svc-9nvhx
Jul 22 20:33:32.093: INFO: Got endpoints: latency-svc-9nvhx [296.319178ms]
Jul 22 20:33:32.099: INFO: Created: latency-svc-th6v5
Jul 22 20:33:32.103: INFO: Got endpoints: latency-svc-th6v5 [303.103018ms]
Jul 22 20:33:32.104: INFO: Created: latency-svc-mfpnb
Jul 22 20:33:32.177: INFO: Got endpoints: latency-svc-mfpnb [301.294549ms]
Jul 22 20:33:32.187: INFO: Created: latency-svc-6dfsl
Jul 22 20:33:32.191: INFO: Got endpoints: latency-svc-6dfsl [309.017308ms]
Jul 22 20:33:32.192: INFO: Created: latency-svc-lnw8d
Jul 22 20:33:32.200: INFO: Got endpoints: latency-svc-lnw8d [310.544141ms]
Jul 22 20:33:32.202: INFO: Created: latency-svc-w42c4
Jul 22 20:33:32.203: INFO: Got endpoints: latency-svc-w42c4 [307.848359ms]
Jul 22 20:33:32.214: INFO: Created: latency-svc-4qmr2
Jul 22 20:33:32.276: INFO: Got endpoints: latency-svc-4qmr2 [370.636514ms]
Jul 22 20:33:32.276: INFO: Created: latency-svc-jcl8g
Jul 22 20:33:32.281: INFO: Created: latency-svc-f8z2t
Jul 22 20:33:32.281: INFO: Got endpoints: latency-svc-jcl8g [381.139078ms]
Jul 22 20:33:32.288: INFO: Got endpoints: latency-svc-f8z2t [312.459669ms]
Jul 22 20:33:32.288: INFO: Created: latency-svc-ftzbh
Jul 22 20:33:32.292: INFO: Got endpoints: latency-svc-ftzbh [308.531189ms]
Jul 22 20:33:32.295: INFO: Created: latency-svc-5qcmh
Jul 22 20:33:32.304: INFO: Created: latency-svc-8nqzf
Jul 22 20:33:32.310: INFO: Created: latency-svc-5prn4
Jul 22 20:33:32.310: INFO: Got endpoints: latency-svc-5qcmh [321.271188ms]
Jul 22 20:33:32.310: INFO: Got endpoints: latency-svc-8nqzf [316.420066ms]
Jul 22 20:33:32.375: INFO: Got endpoints: latency-svc-5prn4 [297.508522ms]
Jul 22 20:33:32.381: INFO: Created: latency-svc-z4rfp
Jul 22 20:33:32.388: INFO: Got endpoints: latency-svc-z4rfp [306.663014ms]
Jul 22 20:33:32.388: INFO: Created: latency-svc-zgzf4
Jul 22 20:33:32.391: INFO: Got endpoints: latency-svc-zgzf4 [299.891266ms]
Jul 22 20:33:32.393: INFO: Created: latency-svc-l6tsk
Jul 22 20:33:32.396: INFO: Got endpoints: latency-svc-l6tsk [302.437056ms]
Jul 22 20:33:32.399: INFO: Created: latency-svc-npnvw
Jul 22 20:33:32.408: INFO: Created: latency-svc-jqm5p
Jul 22 20:33:32.408: INFO: Got endpoints: latency-svc-npnvw [304.728869ms]
Jul 22 20:33:32.476: INFO: Got endpoints: latency-svc-jqm5p [298.224005ms]
Jul 22 20:33:32.477: INFO: Created: latency-svc-xxktt
Jul 22 20:33:32.482: INFO: Got endpoints: latency-svc-xxktt [290.653591ms]
Jul 22 20:33:32.482: INFO: Created: latency-svc-l9vw9
Jul 22 20:33:32.488: INFO: Got endpoints: latency-svc-l9vw9 [287.550957ms]
Jul 22 20:33:32.491: INFO: Created: latency-svc-96wxw
Jul 22 20:33:32.494: INFO: Got endpoints: latency-svc-96wxw [290.988778ms]
Jul 22 20:33:32.498: INFO: Created: latency-svc-zfv8f
Jul 22 20:33:32.501: INFO: Got endpoints: latency-svc-zfv8f [225.317332ms]
Jul 22 20:33:32.575: INFO: Created: latency-svc-62cdf
Jul 22 20:33:32.579: INFO: Got endpoints: latency-svc-62cdf [297.063613ms]
Jul 22 20:33:32.584: INFO: Created: latency-svc-lzvxm
Jul 22 20:33:32.589: INFO: Got endpoints: latency-svc-lzvxm [300.566131ms]
Jul 22 20:33:32.591: INFO: Created: latency-svc-q78bc
Jul 22 20:33:32.596: INFO: Got endpoints: latency-svc-q78bc [304.853769ms]
Jul 22 20:33:32.597: INFO: Created: latency-svc-wb29x
Jul 22 20:33:32.603: INFO: Created: latency-svc-8j529
Jul 22 20:33:32.610: INFO: Created: latency-svc-zztqk
Jul 22 20:33:32.680: INFO: Created: latency-svc-6xkq6
Jul 22 20:33:32.680: INFO: Got endpoints: latency-svc-wb29x [369.892738ms]
Jul 22 20:33:32.684: INFO: Got endpoints: latency-svc-8j529 [373.715507ms]
Jul 22 20:33:32.686: INFO: Created: latency-svc-2vcsl
Jul 22 20:33:32.692: INFO: Created: latency-svc-rjqlc
Jul 22 20:33:32.700: INFO: Created: latency-svc-n8plc
Jul 22 20:33:32.705: INFO: Created: latency-svc-7dhg2
Jul 22 20:33:32.712: INFO: Created: latency-svc-5ngnk
Jul 22 20:33:32.780: INFO: Created: latency-svc-g64ht
Jul 22 20:33:32.784: INFO: Got endpoints: latency-svc-zztqk [408.524531ms]
Jul 22 20:33:32.786: INFO: Got endpoints: latency-svc-6xkq6 [398.253725ms]
Jul 22 20:33:32.879: INFO: Created: latency-svc-dj5xd
Jul 22 20:33:32.882: INFO: Got endpoints: latency-svc-2vcsl [491.107293ms]
Jul 22 20:33:32.884: INFO: Got endpoints: latency-svc-rjqlc [487.623736ms]
Jul 22 20:33:32.980: INFO: Created: latency-svc-f95vd
Jul 22 20:33:32.981: INFO: Got endpoints: latency-svc-n8plc [573.128197ms]
Jul 22 20:33:33.077: INFO: Created: latency-svc-4rsjw
Jul 22 20:33:33.078: INFO: Got endpoints: latency-svc-7dhg2 [602.733178ms]
Jul 22 20:33:33.079: INFO: Got endpoints: latency-svc-5ngnk [597.00466ms]
Jul 22 20:33:33.176: INFO: Created: latency-svc-wbcwh
Jul 22 20:33:33.182: INFO: Got endpoints: latency-svc-g64ht [693.443624ms]
Jul 22 20:33:33.182: INFO: Got endpoints: latency-svc-dj5xd [687.424622ms]
Jul 22 20:33:33.182: INFO: Created: latency-svc-jqgr8
Jul 22 20:33:33.276: INFO: Got endpoints: latency-svc-f95vd [774.627754ms]
Jul 22 20:33:33.280: INFO: Got endpoints: latency-svc-4rsjw [701.520861ms]
Jul 22 20:33:33.285: INFO: Created: latency-svc-qgvvj
Jul 22 20:33:33.285: INFO: Got endpoints: latency-svc-wbcwh [696.799229ms]
Jul 22 20:33:33.291: INFO: Created: latency-svc-hh24g
Jul 22 20:33:33.297: INFO: Created: latency-svc-qc9pl
Jul 22 20:33:33.382: INFO: Created: latency-svc-jwxrn
Jul 22 20:33:33.394: INFO: Created: latency-svc-cbtb9
Jul 22 20:33:33.401: INFO: Created: latency-svc-fcgjp
Jul 22 20:33:33.406: INFO: Created: latency-svc-fqxjh
Jul 22 20:33:33.410: INFO: Created: latency-svc-gzzwn
Jul 22 20:33:33.416: INFO: Created: latency-svc-4f272
Jul 22 20:33:33.477: INFO: Created: latency-svc-88clw
Jul 22 20:33:33.483: INFO: Created: latency-svc-nwjqk
Jul 22 20:33:33.487: INFO: Created: latency-svc-qzjcj
Jul 22 20:33:33.494: INFO: Created: latency-svc-cjbdr
Jul 22 20:33:33.494: INFO: Got endpoints: latency-svc-jqgr8 [897.12112ms]
Jul 22 20:33:33.496: INFO: Got endpoints: latency-svc-hh24g [812.57033ms]
Jul 22 20:33:33.496: INFO: Got endpoints: latency-svc-qgvvj [815.857813ms]
Jul 22 20:33:33.496: INFO: Got endpoints: latency-svc-qc9pl [712.761555ms]
Jul 22 20:33:33.499: INFO: Created: latency-svc-25457
Jul 22 20:33:33.577: INFO: Created: latency-svc-8t7t4
Jul 22 20:33:33.582: INFO: Got endpoints: latency-svc-jwxrn [795.834207ms]
Jul 22 20:33:33.584: INFO: Got endpoints: latency-svc-cbtb9 [702.434389ms]
Jul 22 20:33:33.585: INFO: Created: latency-svc-qkdw7
Jul 22 20:33:33.590: INFO: Created: latency-svc-4txdz
Jul 22 20:33:33.594: INFO: Created: latency-svc-lwhdd
Jul 22 20:33:33.599: INFO: Created: latency-svc-cmg6m
Jul 22 20:33:33.607: INFO: Created: latency-svc-2rdt2
Jul 22 20:33:33.635: INFO: Got endpoints: latency-svc-fcgjp [750.969414ms]
Jul 22 20:33:33.686: INFO: Got endpoints: latency-svc-fqxjh [704.890855ms]
Jul 22 20:33:33.686: INFO: Created: latency-svc-hnq5x
Jul 22 20:33:33.703: INFO: Created: latency-svc-kn2vk
Jul 22 20:33:33.733: INFO: Got endpoints: latency-svc-gzzwn [654.804553ms]
Jul 22 20:33:33.777: INFO: Created: latency-svc-f9drq
Jul 22 20:33:33.783: INFO: Got endpoints: latency-svc-4f272 [703.879532ms]
Jul 22 20:33:33.799: INFO: Created: latency-svc-j7vcd
Jul 22 20:33:33.835: INFO: Got endpoints: latency-svc-88clw [652.776948ms]
Jul 22 20:33:33.852: INFO: Created: latency-svc-7wcg2
Jul 22 20:33:33.884: INFO: Got endpoints: latency-svc-nwjqk [702.4144ms]
Jul 22 20:33:33.901: INFO: Created: latency-svc-tws67
Jul 22 20:33:33.933: INFO: Got endpoints: latency-svc-qzjcj [657.492082ms]
Jul 22 20:33:33.950: INFO: Created: latency-svc-snmx8
Jul 22 20:33:33.984: INFO: Got endpoints: latency-svc-cjbdr [703.672709ms]
Jul 22 20:33:34.001: INFO: Created: latency-svc-jxwvk
Jul 22 20:33:34.034: INFO: Got endpoints: latency-svc-25457 [748.554047ms]
Jul 22 20:33:34.051: INFO: Created: latency-svc-4vdb5
Jul 22 20:33:34.085: INFO: Got endpoints: latency-svc-8t7t4 [590.996554ms]
Jul 22 20:33:34.102: INFO: Created: latency-svc-lqw8w
Jul 22 20:33:34.134: INFO: Got endpoints: latency-svc-qkdw7 [637.560905ms]
Jul 22 20:33:34.152: INFO: Created: latency-svc-z58w5
Jul 22 20:33:34.184: INFO: Got endpoints: latency-svc-4txdz [687.838573ms]
Jul 22 20:33:34.201: INFO: Created: latency-svc-twzvj
Jul 22 20:33:34.236: INFO: Got endpoints: latency-svc-lwhdd [739.425009ms]
Jul 22 20:33:34.254: INFO: Created: latency-svc-knfr4
Jul 22 20:33:34.284: INFO: Got endpoints: latency-svc-cmg6m [702.297782ms]
Jul 22 20:33:34.307: INFO: Created: latency-svc-sbft7
Jul 22 20:33:34.333: INFO: Got endpoints: latency-svc-2rdt2 [748.346686ms]
Jul 22 20:33:34.379: INFO: Created: latency-svc-rmm2w
Jul 22 20:33:34.409: INFO: Got endpoints: latency-svc-hnq5x [774.634084ms]
Jul 22 20:33:34.433: INFO: Created: latency-svc-8vrvd
Jul 22 20:33:34.434: INFO: Got endpoints: latency-svc-kn2vk [747.825297ms]
Jul 22 20:33:34.451: INFO: Created: latency-svc-7kxpg
Jul 22 20:33:34.484: INFO: Got endpoints: latency-svc-f9drq [750.664807ms]
Jul 22 20:33:34.501: INFO: Created: latency-svc-qkh2t
Jul 22 20:33:34.535: INFO: Got endpoints: latency-svc-j7vcd [751.634018ms]
Jul 22 20:33:34.577: INFO: Created: latency-svc-fvrm8
Jul 22 20:33:34.585: INFO: Got endpoints: latency-svc-7wcg2 [750.729004ms]
Jul 22 20:33:34.602: INFO: Created: latency-svc-x95s6
Jul 22 20:33:34.634: INFO: Got endpoints: latency-svc-tws67 [749.725207ms]
Jul 22 20:33:34.678: INFO: Created: latency-svc-wcvn5
Jul 22 20:33:34.684: INFO: Got endpoints: latency-svc-snmx8 [749.976686ms]
Jul 22 20:33:34.699: INFO: Created: latency-svc-l4gxt
Jul 22 20:33:34.734: INFO: Got endpoints: latency-svc-jxwvk [749.981719ms]
Jul 22 20:33:34.751: INFO: Created: latency-svc-m7fj9
Jul 22 20:33:34.784: INFO: Got endpoints: latency-svc-4vdb5 [749.601827ms]
Jul 22 20:33:34.803: INFO: Created: latency-svc-vpl9k
Jul 22 20:33:34.835: INFO: Got endpoints: latency-svc-lqw8w [749.89598ms]
Jul 22 20:33:34.851: INFO: Created: latency-svc-f2zdq
Jul 22 20:33:34.886: INFO: Got endpoints: latency-svc-z58w5 [752.070578ms]
Jul 22 20:33:34.903: INFO: Created: latency-svc-ltvfp
Jul 22 20:33:34.933: INFO: Got endpoints: latency-svc-twzvj [748.666074ms]
Jul 22 20:33:34.949: INFO: Created: latency-svc-4kzvj
Jul 22 20:33:34.984: INFO: Got endpoints: latency-svc-knfr4 [748.291243ms]
Jul 22 20:33:35.004: INFO: Created: latency-svc-cpgwg
Jul 22 20:33:35.034: INFO: Got endpoints: latency-svc-sbft7 [749.617853ms]
Jul 22 20:33:35.050: INFO: Created: latency-svc-r94m4
Jul 22 20:33:35.084: INFO: Got endpoints: latency-svc-rmm2w [751.183459ms]
Jul 22 20:33:35.101: INFO: Created: latency-svc-6z74t
Jul 22 20:33:35.135: INFO: Got endpoints: latency-svc-8vrvd [725.121314ms]
Jul 22 20:33:35.152: INFO: Created: latency-svc-vxk85
Jul 22 20:33:35.184: INFO: Got endpoints: latency-svc-7kxpg [749.691376ms]
Jul 22 20:33:35.200: INFO: Created: latency-svc-w9nlx
Jul 22 20:33:35.234: INFO: Got endpoints: latency-svc-qkh2t [749.750022ms]
Jul 22 20:33:35.251: INFO: Created: latency-svc-dmpv2
Jul 22 20:33:35.284: INFO: Got endpoints: latency-svc-fvrm8 [749.149117ms]
Jul 22 20:33:35.301: INFO: Created: latency-svc-rpsft
Jul 22 20:33:35.334: INFO: Got endpoints: latency-svc-x95s6 [748.119543ms]
Jul 22 20:33:35.350: INFO: Created: latency-svc-lkfzq
Jul 22 20:33:35.384: INFO: Got endpoints: latency-svc-wcvn5 [750.331261ms]
Jul 22 20:33:35.401: INFO: Created: latency-svc-h4dln
Jul 22 20:33:35.434: INFO: Got endpoints: latency-svc-l4gxt [750.469536ms]
Jul 22 20:33:35.451: INFO: Created: latency-svc-gcvx8
Jul 22 20:33:35.484: INFO: Got endpoints: latency-svc-m7fj9 [749.836346ms]
Jul 22 20:33:35.501: INFO: Created: latency-svc-jnqjt
Jul 22 20:33:35.534: INFO: Got endpoints: latency-svc-vpl9k [750.28594ms]
Jul 22 20:33:35.551: INFO: Created: latency-svc-gw6mf
Jul 22 20:33:35.585: INFO: Got endpoints: latency-svc-f2zdq [749.997259ms]
Jul 22 20:33:35.602: INFO: Created: latency-svc-8lnkr
Jul 22 20:33:35.636: INFO: Got endpoints: latency-svc-ltvfp [750.308906ms]
Jul 22 20:33:35.653: INFO: Created: latency-svc-6fk9h
Jul 22 20:33:35.685: INFO: Got endpoints: latency-svc-4kzvj [751.566922ms]
Jul 22 20:33:35.702: INFO: Created: latency-svc-txrnh
Jul 22 20:33:35.735: INFO: Got endpoints: latency-svc-cpgwg [750.687921ms]
Jul 22 20:33:35.755: INFO: Created: latency-svc-dc6zz
Jul 22 20:33:35.783: INFO: Got endpoints: latency-svc-r94m4 [749.10254ms]
Jul 22 20:33:35.800: INFO: Created: latency-svc-49ndk
Jul 22 20:33:35.836: INFO: Got endpoints: latency-svc-6z74t [751.829568ms]
Jul 22 20:33:35.853: INFO: Created: latency-svc-rt6jg
Jul 22 20:33:35.888: INFO: Got endpoints: latency-svc-vxk85 [753.079228ms]
Jul 22 20:33:35.912: INFO: Created: latency-svc-df6vs
Jul 22 20:33:35.934: INFO: Got endpoints: latency-svc-w9nlx [750.464182ms]
Jul 22 20:33:35.952: INFO: Created: latency-svc-dthkb
Jul 22 20:33:35.984: INFO: Got endpoints: latency-svc-dmpv2 [750.255223ms]
Jul 22 20:33:36.002: INFO: Created: latency-svc-w77sj
Jul 22 20:33:36.034: INFO: Got endpoints: latency-svc-rpsft [750.346716ms]
Jul 22 20:33:36.052: INFO: Created: latency-svc-j9tw7
Jul 22 20:33:36.088: INFO: Got endpoints: latency-svc-lkfzq [754.561753ms]
Jul 22 20:33:36.107: INFO: Created: latency-svc-2tjfq
Jul 22 20:33:36.133: INFO: Got endpoints: latency-svc-h4dln [749.262539ms]
Jul 22 20:33:36.151: INFO: Created: latency-svc-8kgjh
Jul 22 20:33:36.184: INFO: Got endpoints: latency-svc-gcvx8 [749.617753ms]
Jul 22 20:33:36.201: INFO: Created: latency-svc-dwcgn
Jul 22 20:33:36.236: INFO: Got endpoints: latency-svc-jnqjt [752.40584ms]
Jul 22 20:33:36.255: INFO: Created: latency-svc-lpwd9
Jul 22 20:33:36.283: INFO: Got endpoints: latency-svc-gw6mf [749.191752ms]
Jul 22 20:33:36.301: INFO: Created: latency-svc-sjz2j
Jul 22 20:33:36.334: INFO: Got endpoints: latency-svc-8lnkr [748.830359ms]
Jul 22 20:33:36.352: INFO: Created: latency-svc-n2tcc
Jul 22 20:33:36.384: INFO: Got endpoints: latency-svc-6fk9h [747.169881ms]
Jul 22 20:33:36.401: INFO: Created: latency-svc-7qzrx
Jul 22 20:33:36.433: INFO: Got endpoints: latency-svc-txrnh [748.648605ms]
Jul 22 20:33:36.451: INFO: Created: latency-svc-rm4tw
Jul 22 20:33:36.484: INFO: Got endpoints: latency-svc-dc6zz [748.93441ms]
Jul 22 20:33:36.506: INFO: Created: latency-svc-5phqp
Jul 22 20:33:36.533: INFO: Got endpoints: latency-svc-49ndk [750.228082ms]
Jul 22 20:33:36.551: INFO: Created: latency-svc-l4jsv
Jul 22 20:33:36.583: INFO: Got endpoints: latency-svc-rt6jg [747.648596ms]
Jul 22 20:33:36.600: INFO: Created: latency-svc-4mq2g
Jul 22 20:33:36.633: INFO: Got endpoints: latency-svc-df6vs [745.149509ms]
Jul 22 20:33:36.651: INFO: Created: latency-svc-pl29r
Jul 22 20:33:36.684: INFO: Got endpoints: latency-svc-dthkb [749.496838ms]
Jul 22 20:33:36.700: INFO: Created: latency-svc-5w629
Jul 22 20:33:36.734: INFO: Got endpoints: latency-svc-w77sj [749.735411ms]
Jul 22 20:33:36.751: INFO: Created: latency-svc-2j2tn
Jul 22 20:33:36.784: INFO: Got endpoints: latency-svc-j9tw7 [749.917683ms]
Jul 22 20:33:36.802: INFO: Created: latency-svc-gmlnp
Jul 22 20:33:36.834: INFO: Got endpoints: latency-svc-2tjfq [745.507115ms]
Jul 22 20:33:36.851: INFO: Created: latency-svc-s9lbt
Jul 22 20:33:36.884: INFO: Got endpoints: latency-svc-8kgjh [750.238591ms]
Jul 22 20:33:36.901: INFO: Created: latency-svc-9k9w8
Jul 22 20:33:36.937: INFO: Got endpoints: latency-svc-dwcgn [753.11947ms]
Jul 22 20:33:36.955: INFO: Created: latency-svc-lq7b5
Jul 22 20:33:36.983: INFO: Got endpoints: latency-svc-lpwd9 [747.110781ms]
Jul 22 20:33:37.003: INFO: Created: latency-svc-jblhl
Jul 22 20:33:37.035: INFO: Got endpoints: latency-svc-sjz2j [751.921802ms]
Jul 22 20:33:37.053: INFO: Created: latency-svc-mskxf
Jul 22 20:33:37.087: INFO: Got endpoints: latency-svc-n2tcc [753.443854ms]
Jul 22 20:33:37.104: INFO: Created: latency-svc-mch4l
Jul 22 20:33:37.134: INFO: Got endpoints: latency-svc-7qzrx [750.238731ms]
Jul 22 20:33:37.151: INFO: Created: latency-svc-v4hlp
Jul 22 20:33:37.184: INFO: Got endpoints: latency-svc-rm4tw [750.652174ms]
Jul 22 20:33:37.201: INFO: Created: latency-svc-tvxdj
Jul 22 20:33:37.234: INFO: Got endpoints: latency-svc-5phqp [750.092498ms]
Jul 22 20:33:37.252: INFO: Created: latency-svc-d4sdn
Jul 22 20:33:37.284: INFO: Got endpoints: latency-svc-l4jsv [750.676907ms]
Jul 22 20:33:37.303: INFO: Created: latency-svc-r8mqm
Jul 22 20:33:37.336: INFO: Got endpoints: latency-svc-4mq2g [752.092155ms]
Jul 22 20:33:37.355: INFO: Created: latency-svc-wlxmk
Jul 22 20:33:37.384: INFO: Got endpoints: latency-svc-pl29r [751.351541ms]
Jul 22 20:33:37.404: INFO: Created: latency-svc-6g7k4
Jul 22 20:33:37.434: INFO: Got endpoints: latency-svc-5w629 [750.136614ms]
Jul 22 20:33:37.451: INFO: Created: latency-svc-tvfsm
Jul 22 20:33:37.483: INFO: Got endpoints: latency-svc-2j2tn [749.181504ms]
Jul 22 20:33:37.505: INFO: Created: latency-svc-6cmr8
Jul 22 20:33:37.534: INFO: Got endpoints: latency-svc-gmlnp [749.684426ms]
Jul 22 20:33:37.556: INFO: Created: latency-svc-q596t
Jul 22 20:33:37.584: INFO: Got endpoints: latency-svc-s9lbt [749.970136ms]
Jul 22 20:33:37.603: INFO: Created: latency-svc-4gltj
Jul 22 20:33:37.633: INFO: Got endpoints: latency-svc-9k9w8 [749.358315ms]
Jul 22 20:33:37.651: INFO: Created: latency-svc-cnh22
Jul 22 20:33:37.684: INFO: Got endpoints: latency-svc-lq7b5 [747.003396ms]
Jul 22 20:33:37.702: INFO: Created: latency-svc-g8jfc
Jul 22 20:33:37.734: INFO: Got endpoints: latency-svc-jblhl [750.33083ms]
Jul 22 20:33:37.751: INFO: Created: latency-svc-f78mw
Jul 22 20:33:37.784: INFO: Got endpoints: latency-svc-mskxf [748.5944ms]
Jul 22 20:33:37.802: INFO: Created: latency-svc-ms7d8
Jul 22 20:33:37.835: INFO: Got endpoints: latency-svc-mch4l [747.616417ms]
Jul 22 20:33:37.853: INFO: Created: latency-svc-cpfqr
Jul 22 20:33:37.884: INFO: Got endpoints: latency-svc-v4hlp [750.009805ms]
Jul 22 20:33:37.901: INFO: Created: latency-svc-prjlk
Jul 22 20:33:37.935: INFO: Got endpoints: latency-svc-tvxdj [750.456201ms]
Jul 22 20:33:37.959: INFO: Created: latency-svc-v4lqp
Jul 22 20:33:37.984: INFO: Got endpoints: latency-svc-d4sdn [749.704892ms]
Jul 22 20:33:38.002: INFO: Created: latency-svc-stj5k
Jul 22 20:33:38.033: INFO: Got endpoints: latency-svc-r8mqm [749.359403ms]
Jul 22 20:33:38.051: INFO: Created: latency-svc-56clh
Jul 22 20:33:38.085: INFO: Got endpoints: latency-svc-wlxmk [749.551438ms]
Jul 22 20:33:38.102: INFO: Created: latency-svc-xkjmm
Jul 22 20:33:38.134: INFO: Got endpoints: latency-svc-6g7k4 [749.370268ms]
Jul 22 20:33:38.152: INFO: Created: latency-svc-swlzm
Jul 22 20:33:38.184: INFO: Got endpoints: latency-svc-tvfsm [750.115688ms]
Jul 22 20:33:38.201: INFO: Created: latency-svc-8vdmj
Jul 22 20:33:38.234: INFO: Got endpoints: latency-svc-6cmr8 [750.999729ms]
Jul 22 20:33:38.252: INFO: Created: latency-svc-qfqxp
Jul 22 20:33:38.283: INFO: Got endpoints: latency-svc-q596t [749.236551ms]
Jul 22 20:33:38.301: INFO: Created: latency-svc-nj869
Jul 22 20:33:38.333: INFO: Got endpoints: latency-svc-4gltj [748.96941ms]
Jul 22 20:33:38.350: INFO: Created: latency-svc-mcrbf
Jul 22 20:33:38.384: INFO: Got endpoints: latency-svc-cnh22 [750.926839ms]
Jul 22 20:33:38.403: INFO: Created: latency-svc-ztscr
Jul 22 20:33:38.434: INFO: Got endpoints: latency-svc-g8jfc [749.714098ms]
Jul 22 20:33:38.451: INFO: Created: latency-svc-fntcz
Jul 22 20:33:38.483: INFO: Got endpoints: latency-svc-f78mw [749.360472ms]
Jul 22 20:33:38.502: INFO: Created: latency-svc-nt7hr
Jul 22 20:33:38.533: INFO: Got endpoints: latency-svc-ms7d8 [748.754236ms]
Jul 22 20:33:38.550: INFO: Created: latency-svc-pbtlf
Jul 22 20:33:38.586: INFO: Got endpoints: latency-svc-cpfqr [751.623296ms]
Jul 22 20:33:38.603: INFO: Created: latency-svc-n9w87
Jul 22 20:33:38.633: INFO: Got endpoints: latency-svc-prjlk [748.976658ms]
Jul 22 20:33:38.654: INFO: Created: latency-svc-6x7pb
Jul 22 20:33:38.684: INFO: Got endpoints: latency-svc-v4lqp [748.89472ms]
Jul 22 20:33:38.700: INFO: Created: latency-svc-cp5wz
Jul 22 20:33:38.734: INFO: Got endpoints: latency-svc-stj5k [749.919751ms]
Jul 22 20:33:38.752: INFO: Created: latency-svc-lsb9r
Jul 22 20:33:38.784: INFO: Got endpoints: latency-svc-56clh [750.280822ms]
Jul 22 20:33:38.801: INFO: Created: latency-svc-gmjtf
Jul 22 20:33:38.835: INFO: Got endpoints: latency-svc-xkjmm [749.409293ms]
Jul 22 20:33:38.856: INFO: Created: latency-svc-fj86s
Jul 22 20:33:38.884: INFO: Got endpoints: latency-svc-swlzm [749.92443ms]
Jul 22 20:33:38.901: INFO: Created: latency-svc-qrrb5
Jul 22 20:33:38.933: INFO: Got endpoints: latency-svc-8vdmj [748.761728ms]
Jul 22 20:33:38.951: INFO: Created: latency-svc-7dwvr
Jul 22 20:33:38.984: INFO: Got endpoints: latency-svc-qfqxp [749.364734ms]
Jul 22 20:33:39.002: INFO: Created: latency-svc-dnw96
Jul 22 20:33:39.033: INFO: Got endpoints: latency-svc-nj869 [749.881181ms]
Jul 22 20:33:39.051: INFO: Created: latency-svc-nnrdv
Jul 22 20:33:39.084: INFO: Got endpoints: latency-svc-mcrbf [751.509781ms]
Jul 22 20:33:39.102: INFO: Created: latency-svc-sfxmr
Jul 22 20:33:39.136: INFO: Got endpoints: latency-svc-ztscr [751.838055ms]
Jul 22 20:33:39.160: INFO: Created: latency-svc-cmd47
Jul 22 20:33:39.185: INFO: Got endpoints: latency-svc-fntcz [751.532501ms]
Jul 22 20:33:39.203: INFO: Created: latency-svc-dftcx
Jul 22 20:33:39.234: INFO: Got endpoints: latency-svc-nt7hr [750.803832ms]
Jul 22 20:33:39.251: INFO: Created: latency-svc-28m6v
Jul 22 20:33:39.285: INFO: Got endpoints: latency-svc-pbtlf [751.68073ms]
Jul 22 20:33:39.305: INFO: Created: latency-svc-ljng2
Jul 22 20:33:39.333: INFO: Got endpoints: latency-svc-n9w87 [747.014663ms]
Jul 22 20:33:39.351: INFO: Created: latency-svc-dl99x
Jul 22 20:33:39.385: INFO: Got endpoints: latency-svc-6x7pb [751.593105ms]
Jul 22 20:33:39.402: INFO: Created: latency-svc-gwt9m
Jul 22 20:33:39.434: INFO: Got endpoints: latency-svc-cp5wz [750.350169ms]
Jul 22 20:33:39.450: INFO: Created: latency-svc-7f4np
Jul 22 20:33:39.484: INFO: Got endpoints: latency-svc-lsb9r [749.730518ms]
Jul 22 20:33:39.502: INFO: Created: latency-svc-44rh8
Jul 22 20:33:39.532: INFO: Got endpoints: latency-svc-gmjtf [748.629889ms]
Jul 22 20:33:39.584: INFO: Got endpoints: latency-svc-fj86s [749.453546ms]
Jul 22 20:33:39.633: INFO: Got endpoints: latency-svc-qrrb5 [749.504912ms]
Jul 22 20:33:39.684: INFO: Got endpoints: latency-svc-7dwvr [750.851467ms]
Jul 22 20:33:39.733: INFO: Got endpoints: latency-svc-dnw96 [749.703851ms]
Jul 22 20:33:39.784: INFO: Got endpoints: latency-svc-nnrdv [750.274897ms]
Jul 22 20:33:39.833: INFO: Got endpoints: latency-svc-sfxmr [748.979473ms]
Jul 22 20:33:39.884: INFO: Got endpoints: latency-svc-cmd47 [747.379797ms]
Jul 22 20:33:39.934: INFO: Got endpoints: latency-svc-dftcx [748.57755ms]
Jul 22 20:33:39.984: INFO: Got endpoints: latency-svc-28m6v [750.107262ms]
Jul 22 20:33:40.033: INFO: Got endpoints: latency-svc-ljng2 [748.607723ms]
Jul 22 20:33:40.083: INFO: Got endpoints: latency-svc-dl99x [749.429651ms]
Jul 22 20:33:40.135: INFO: Got endpoints: latency-svc-gwt9m [749.981084ms]
Jul 22 20:33:40.184: INFO: Got endpoints: latency-svc-7f4np [750.207459ms]
Jul 22 20:33:40.234: INFO: Got endpoints: latency-svc-44rh8 [750.335971ms]
Jul 22 20:33:40.234: INFO: Latencies: [21.885881ms 26.33262ms 33.07272ms 41.430777ms 96.312989ms 101.793331ms 108.469622ms 112.802205ms 115.994119ms 191.724924ms 198.091407ms 205.528538ms 211.077408ms 216.140279ms 221.023466ms 225.317332ms 268.195635ms 269.242386ms 271.071784ms 272.545156ms 287.550957ms 290.653591ms 290.988778ms 294.828717ms 296.319178ms 297.063613ms 297.105705ms 297.508522ms 297.843216ms 298.224005ms 299.891266ms 300.566131ms 301.294549ms 302.437056ms 303.103018ms 304.728869ms 304.853769ms 306.663014ms 307.848359ms 308.531189ms 309.017308ms 310.544141ms 312.459669ms 316.420066ms 321.271188ms 369.892738ms 370.636514ms 373.715507ms 381.139078ms 398.253725ms 408.524531ms 487.623736ms 491.107293ms 573.128197ms 590.996554ms 597.00466ms 602.733178ms 637.560905ms 652.776948ms 654.804553ms 657.492082ms 687.424622ms 687.838573ms 693.443624ms 696.799229ms 701.520861ms 702.297782ms 702.4144ms 702.434389ms 703.672709ms 703.879532ms 704.890855ms 712.761555ms 725.121314ms 739.425009ms 745.149509ms 745.507115ms 747.003396ms 747.014663ms 747.110781ms 747.169881ms 747.379797ms 747.616417ms 747.648596ms 747.825297ms 748.119543ms 748.291243ms 748.346686ms 748.554047ms 748.57755ms 748.5944ms 748.607723ms 748.629889ms 748.648605ms 748.666074ms 748.754236ms 748.761728ms 748.830359ms 748.89472ms 748.93441ms 748.96941ms 748.976658ms 748.979473ms 749.10254ms 749.149117ms 749.181504ms 749.191752ms 749.236551ms 749.262539ms 749.358315ms 749.359403ms 749.360472ms 749.364734ms 749.370268ms 749.409293ms 749.429651ms 749.453546ms 749.496838ms 749.504912ms 749.551438ms 749.601827ms 749.617753ms 749.617853ms 749.684426ms 749.691376ms 749.703851ms 749.704892ms 749.714098ms 749.725207ms 749.730518ms 749.735411ms 749.750022ms 749.836346ms 749.881181ms 749.89598ms 749.917683ms 749.919751ms 749.92443ms 749.970136ms 749.976686ms 749.981084ms 749.981719ms 749.997259ms 750.009805ms 750.092498ms 750.107262ms 750.115688ms 750.136614ms 750.207459ms 750.228082ms 750.238591ms 750.238731ms 750.255223ms 750.274897ms 750.280822ms 750.28594ms 750.308906ms 750.33083ms 750.331261ms 750.335971ms 750.346716ms 750.350169ms 750.456201ms 750.464182ms 750.469536ms 750.652174ms 750.664807ms 750.676907ms 750.687921ms 750.729004ms 750.803832ms 750.851467ms 750.926839ms 750.969414ms 750.999729ms 751.183459ms 751.351541ms 751.509781ms 751.532501ms 751.566922ms 751.593105ms 751.623296ms 751.634018ms 751.68073ms 751.829568ms 751.838055ms 751.921802ms 752.070578ms 752.092155ms 752.40584ms 753.079228ms 753.11947ms 753.443854ms 754.561753ms 774.627754ms 774.634084ms 795.834207ms 812.57033ms 815.857813ms 897.12112ms]
Jul 22 20:33:40.234: INFO: 50 %ile: 748.96941ms
Jul 22 20:33:40.234: INFO: 90 %ile: 751.593105ms
Jul 22 20:33:40.234: INFO: 99 %ile: 815.857813ms
Jul 22 20:33:40.234: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:33:40.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3429" for this suite.
•{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":339,"completed":143,"skipped":2555,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:33:40.271: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6975
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6975 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6975;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6975 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6975;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6975.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6975.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6975.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6975.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6975.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6975.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6975.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6975.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6975.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6975.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6975.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 50.110.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.110.50_udp@PTR;check="$$(dig +tcp +noall +answer +search 50.110.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.110.50_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6975 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6975;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6975 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6975;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6975.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6975.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6975.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6975.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6975.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6975.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6975.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6975.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6975.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6975.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6975.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6975.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 50.110.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.110.50_udp@PTR;check="$$(dig +tcp +noall +answer +search 50.110.70.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.70.110.50_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 20:33:44.652: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.666: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.680: INFO: Unable to read wheezy_udp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.726: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.740: INFO: Unable to read wheezy_udp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.753: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.768: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.782: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.892: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.910: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.930: INFO: Unable to read jessie_udp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.945: INFO: Unable to read jessie_tcp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.960: INFO: Unable to read jessie_udp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.974: INFO: Unable to read jessie_tcp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:44.988: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:45.001: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:45.104: INFO: Lookups using dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6975 wheezy_tcp@dns-test-service.dns-6975 wheezy_udp@dns-test-service.dns-6975.svc wheezy_tcp@dns-test-service.dns-6975.svc wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6975 jessie_tcp@dns-test-service.dns-6975 jessie_udp@dns-test-service.dns-6975.svc jessie_tcp@dns-test-service.dns-6975.svc jessie_udp@_http._tcp.dns-test-service.dns-6975.svc jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc]

Jul 22 20:33:50.120: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.134: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.147: INFO: Unable to read wheezy_udp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.201: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.215: INFO: Unable to read wheezy_udp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.228: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.242: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.280: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.405: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.420: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.475: INFO: Unable to read jessie_udp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.488: INFO: Unable to read jessie_tcp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.501: INFO: Unable to read jessie_udp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.515: INFO: Unable to read jessie_tcp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.528: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.541: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:50.633: INFO: Lookups using dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6975 wheezy_tcp@dns-test-service.dns-6975 wheezy_udp@dns-test-service.dns-6975.svc wheezy_tcp@dns-test-service.dns-6975.svc wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6975 jessie_tcp@dns-test-service.dns-6975 jessie_udp@dns-test-service.dns-6975.svc jessie_tcp@dns-test-service.dns-6975.svc jessie_udp@_http._tcp.dns-test-service.dns-6975.svc jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc]

Jul 22 20:33:55.119: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.133: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.147: INFO: Unable to read wheezy_udp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.206: INFO: Unable to read wheezy_udp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.220: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.233: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.248: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.348: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.362: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.377: INFO: Unable to read jessie_udp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.391: INFO: Unable to read jessie_tcp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.405: INFO: Unable to read jessie_udp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.419: INFO: Unable to read jessie_tcp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.433: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.446: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:33:55.530: INFO: Lookups using dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6975 wheezy_tcp@dns-test-service.dns-6975 wheezy_udp@dns-test-service.dns-6975.svc wheezy_tcp@dns-test-service.dns-6975.svc wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6975 jessie_tcp@dns-test-service.dns-6975 jessie_udp@dns-test-service.dns-6975.svc jessie_tcp@dns-test-service.dns-6975.svc jessie_udp@_http._tcp.dns-test-service.dns-6975.svc jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc]

Jul 22 20:34:00.119: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.132: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.146: INFO: Unable to read wheezy_udp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.192: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.206: INFO: Unable to read wheezy_udp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.220: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.232: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.246: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.341: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.355: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.369: INFO: Unable to read jessie_udp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.382: INFO: Unable to read jessie_tcp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.396: INFO: Unable to read jessie_udp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.410: INFO: Unable to read jessie_tcp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.424: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.438: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:00.519: INFO: Lookups using dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6975 wheezy_tcp@dns-test-service.dns-6975 wheezy_udp@dns-test-service.dns-6975.svc wheezy_tcp@dns-test-service.dns-6975.svc wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6975 jessie_tcp@dns-test-service.dns-6975 jessie_udp@dns-test-service.dns-6975.svc jessie_tcp@dns-test-service.dns-6975.svc jessie_udp@_http._tcp.dns-test-service.dns-6975.svc jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc]

Jul 22 20:34:05.119: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.164: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.178: INFO: Unable to read wheezy_udp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.191: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.205: INFO: Unable to read wheezy_udp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.219: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.233: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.247: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.342: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.356: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.370: INFO: Unable to read jessie_udp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.384: INFO: Unable to read jessie_tcp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.397: INFO: Unable to read jessie_udp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.411: INFO: Unable to read jessie_tcp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.425: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.438: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:05.520: INFO: Lookups using dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6975 wheezy_tcp@dns-test-service.dns-6975 wheezy_udp@dns-test-service.dns-6975.svc wheezy_tcp@dns-test-service.dns-6975.svc wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6975 jessie_tcp@dns-test-service.dns-6975 jessie_udp@dns-test-service.dns-6975.svc jessie_tcp@dns-test-service.dns-6975.svc jessie_udp@_http._tcp.dns-test-service.dns-6975.svc jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc]

Jul 22 20:34:10.119: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.135: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.149: INFO: Unable to read wheezy_udp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.205: INFO: Unable to read wheezy_udp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.219: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.232: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.245: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.341: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.355: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.369: INFO: Unable to read jessie_udp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.383: INFO: Unable to read jessie_tcp@dns-test-service.dns-6975 from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.396: INFO: Unable to read jessie_udp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.410: INFO: Unable to read jessie_tcp@dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.424: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.437: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc from pod dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8: the server could not find the requested resource (get pods dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8)
Jul 22 20:34:10.519: INFO: Lookups using dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6975 wheezy_tcp@dns-test-service.dns-6975 wheezy_udp@dns-test-service.dns-6975.svc wheezy_tcp@dns-test-service.dns-6975.svc wheezy_udp@_http._tcp.dns-test-service.dns-6975.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6975.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6975 jessie_tcp@dns-test-service.dns-6975 jessie_udp@dns-test-service.dns-6975.svc jessie_tcp@dns-test-service.dns-6975.svc jessie_udp@_http._tcp.dns-test-service.dns-6975.svc jessie_tcp@_http._tcp.dns-test-service.dns-6975.svc]

Jul 22 20:34:15.575: INFO: DNS probes using dns-6975/dns-test-32561e4b-82d3-411f-b496-ff45b71eaed8 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:34:15.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6975" for this suite.
•{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":339,"completed":144,"skipped":2575,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:34:15.998: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2787
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:34:16.213: INFO: Waiting up to 5m0s for pod "downwardapi-volume-504e552c-100d-4059-8b79-f84b11e1b6f8" in namespace "downward-api-2787" to be "Succeeded or Failed"
Jul 22 20:34:16.223: INFO: Pod "downwardapi-volume-504e552c-100d-4059-8b79-f84b11e1b6f8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.46571ms
Jul 22 20:34:18.235: INFO: Pod "downwardapi-volume-504e552c-100d-4059-8b79-f84b11e1b6f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022494596s
Jul 22 20:34:20.251: INFO: Pod "downwardapi-volume-504e552c-100d-4059-8b79-f84b11e1b6f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038123332s
STEP: Saw pod success
Jul 22 20:34:20.251: INFO: Pod "downwardapi-volume-504e552c-100d-4059-8b79-f84b11e1b6f8" satisfied condition "Succeeded or Failed"
Jul 22 20:34:20.262: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-504e552c-100d-4059-8b79-f84b11e1b6f8 container client-container: <nil>
STEP: delete the pod
Jul 22 20:34:20.340: INFO: Waiting for pod downwardapi-volume-504e552c-100d-4059-8b79-f84b11e1b6f8 to disappear
Jul 22 20:34:20.351: INFO: Pod downwardapi-volume-504e552c-100d-4059-8b79-f84b11e1b6f8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:34:20.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2787" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":145,"skipped":2582,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:34:20.383: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3829
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:34:20.571: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul 22 20:34:20.603: INFO: The status of Pod pod-exec-websocket-e4fee48e-c781-499c-8bfe-bd2ea96b10ab is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:34:22.615: INFO: The status of Pod pod-exec-websocket-e4fee48e-c781-499c-8bfe-bd2ea96b10ab is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:34:22.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3829" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":339,"completed":146,"skipped":2607,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:34:22.823: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6937
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:34:23.013: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:34:26.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6937" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":339,"completed":147,"skipped":2611,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:34:26.596: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:34:26.867: INFO: Waiting up to 5m0s for pod "downwardapi-volume-405e4e11-e06a-4f1f-8155-b99a237f1449" in namespace "downward-api-7263" to be "Succeeded or Failed"
Jul 22 20:34:26.879: INFO: Pod "downwardapi-volume-405e4e11-e06a-4f1f-8155-b99a237f1449": Phase="Pending", Reason="", readiness=false. Elapsed: 11.64179ms
Jul 22 20:34:28.891: INFO: Pod "downwardapi-volume-405e4e11-e06a-4f1f-8155-b99a237f1449": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023698963s
Jul 22 20:34:30.903: INFO: Pod "downwardapi-volume-405e4e11-e06a-4f1f-8155-b99a237f1449": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036290964s
STEP: Saw pod success
Jul 22 20:34:30.903: INFO: Pod "downwardapi-volume-405e4e11-e06a-4f1f-8155-b99a237f1449" satisfied condition "Succeeded or Failed"
Jul 22 20:34:30.914: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-405e4e11-e06a-4f1f-8155-b99a237f1449 container client-container: <nil>
STEP: delete the pod
Jul 22 20:34:30.963: INFO: Waiting for pod downwardapi-volume-405e4e11-e06a-4f1f-8155-b99a237f1449 to disappear
Jul 22 20:34:30.974: INFO: Pod downwardapi-volume-405e4e11-e06a-4f1f-8155-b99a237f1449 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:34:30.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7263" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":148,"skipped":2614,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:34:31.018: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 22 20:34:33.269: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:34:33.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-363" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":149,"skipped":2638,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:34:33.329: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:34:34.487: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:34:37.540: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:34:37.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1753" for this suite.
STEP: Destroying namespace "webhook-1753-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":339,"completed":150,"skipped":2643,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:34:37.906: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3239
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jul 22 20:34:38.113: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3239  1232b1e1-c0b5-4573-a68a-ed8ed858e203 23921 0 2021-07-22 20:34:38 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-07-22 20:34:38 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h84q4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h84q4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:34:38.124: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:34:40.137: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:34:42.141: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jul 22 20:34:42.141: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3239 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:34:42.141: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Verifying customized DNS server is configured on pod...
Jul 22 20:34:42.417: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3239 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:34:42.417: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:34:42.773: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:34:42.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3239" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":339,"completed":151,"skipped":2647,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:34:42.831: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6322
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-4ebbc88c-40e1-43c8-a151-0096d5f2bae8
STEP: Creating a pod to test consume secrets
Jul 22 20:34:43.287: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a1a02982-a68a-4bd2-af50-60bfd252b9af" in namespace "projected-6322" to be "Succeeded or Failed"
Jul 22 20:34:43.299: INFO: Pod "pod-projected-secrets-a1a02982-a68a-4bd2-af50-60bfd252b9af": Phase="Pending", Reason="", readiness=false. Elapsed: 11.799507ms
Jul 22 20:34:45.311: INFO: Pod "pod-projected-secrets-a1a02982-a68a-4bd2-af50-60bfd252b9af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024162828s
STEP: Saw pod success
Jul 22 20:34:45.311: INFO: Pod "pod-projected-secrets-a1a02982-a68a-4bd2-af50-60bfd252b9af" satisfied condition "Succeeded or Failed"
Jul 22 20:34:45.322: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-secrets-a1a02982-a68a-4bd2-af50-60bfd252b9af container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:34:45.396: INFO: Waiting for pod pod-projected-secrets-a1a02982-a68a-4bd2-af50-60bfd252b9af to disappear
Jul 22 20:34:45.406: INFO: Pod pod-projected-secrets-a1a02982-a68a-4bd2-af50-60bfd252b9af no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:34:45.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6322" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":152,"skipped":2663,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:34:45.438: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-1658
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:34:45.647: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-eaf8d80e-a2cc-4abe-a421-51d3b1861cfc" in namespace "security-context-test-1658" to be "Succeeded or Failed"
Jul 22 20:34:45.658: INFO: Pod "alpine-nnp-false-eaf8d80e-a2cc-4abe-a421-51d3b1861cfc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.9316ms
Jul 22 20:34:47.670: INFO: Pod "alpine-nnp-false-eaf8d80e-a2cc-4abe-a421-51d3b1861cfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023189015s
Jul 22 20:34:49.683: INFO: Pod "alpine-nnp-false-eaf8d80e-a2cc-4abe-a421-51d3b1861cfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036158692s
Jul 22 20:34:49.684: INFO: Pod "alpine-nnp-false-eaf8d80e-a2cc-4abe-a421-51d3b1861cfc" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:34:49.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1658" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":153,"skipped":2684,"failed":0}

------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:34:49.740: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7636
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jul 22 20:34:54.494: INFO: Successfully updated pod "adopt-release-8m2zd"
STEP: Checking that the Job readopts the Pod
Jul 22 20:34:54.494: INFO: Waiting up to 15m0s for pod "adopt-release-8m2zd" in namespace "job-7636" to be "adopted"
Jul 22 20:34:54.507: INFO: Pod "adopt-release-8m2zd": Phase="Running", Reason="", readiness=true. Elapsed: 12.925781ms
Jul 22 20:34:54.507: INFO: Pod "adopt-release-8m2zd" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jul 22 20:34:55.034: INFO: Successfully updated pod "adopt-release-8m2zd"
STEP: Checking that the Job releases the Pod
Jul 22 20:34:55.034: INFO: Waiting up to 15m0s for pod "adopt-release-8m2zd" in namespace "job-7636" to be "released"
Jul 22 20:34:55.045: INFO: Pod "adopt-release-8m2zd": Phase="Running", Reason="", readiness=true. Elapsed: 10.418167ms
Jul 22 20:34:55.045: INFO: Pod "adopt-release-8m2zd" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:34:55.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7636" for this suite.
•{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":339,"completed":154,"skipped":2684,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:34:55.101: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-ce58c1ff-5e25-4825-a566-ead8c72c7f59
STEP: Creating a pod to test consume secrets
Jul 22 20:34:55.354: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b8e58ffb-89f1-4234-b053-adb365c9be1e" in namespace "projected-7021" to be "Succeeded or Failed"
Jul 22 20:34:55.365: INFO: Pod "pod-projected-secrets-b8e58ffb-89f1-4234-b053-adb365c9be1e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.150623ms
Jul 22 20:34:57.378: INFO: Pod "pod-projected-secrets-b8e58ffb-89f1-4234-b053-adb365c9be1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023700441s
STEP: Saw pod success
Jul 22 20:34:57.378: INFO: Pod "pod-projected-secrets-b8e58ffb-89f1-4234-b053-adb365c9be1e" satisfied condition "Succeeded or Failed"
Jul 22 20:34:57.389: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-secrets-b8e58ffb-89f1-4234-b053-adb365c9be1e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:34:57.424: INFO: Waiting for pod pod-projected-secrets-b8e58ffb-89f1-4234-b053-adb365c9be1e to disappear
Jul 22 20:34:57.435: INFO: Pod pod-projected-secrets-b8e58ffb-89f1-4234-b053-adb365c9be1e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:34:57.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7021" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":155,"skipped":2698,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:34:57.466: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3080
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-3080
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3080 to expose endpoints map[]
Jul 22 20:34:57.711: INFO: successfully validated that service multi-endpoint-test in namespace services-3080 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3080
Jul 22 20:34:57.743: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:34:59.756: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:35:01.754: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3080 to expose endpoints map[pod1:[100]]
Jul 22 20:35:01.808: INFO: successfully validated that service multi-endpoint-test in namespace services-3080 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3080
Jul 22 20:35:01.837: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:35:03.851: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:35:05.851: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3080 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 22 20:35:05.917: INFO: successfully validated that service multi-endpoint-test in namespace services-3080 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-3080
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3080 to expose endpoints map[pod2:[101]]
Jul 22 20:35:05.978: INFO: successfully validated that service multi-endpoint-test in namespace services-3080 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3080
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3080 to expose endpoints map[]
Jul 22 20:35:06.096: INFO: successfully validated that service multi-endpoint-test in namespace services-3080 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:35:06.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3080" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":339,"completed":156,"skipped":2702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:35:06.201: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1462
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-bd1dc113-f989-47c1-91a1-347a56b757d1
Jul 22 20:35:06.491: INFO: Pod name my-hostname-basic-bd1dc113-f989-47c1-91a1-347a56b757d1: Found 0 pods out of 1
Jul 22 20:35:11.504: INFO: Pod name my-hostname-basic-bd1dc113-f989-47c1-91a1-347a56b757d1: Found 1 pods out of 1
Jul 22 20:35:11.504: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-bd1dc113-f989-47c1-91a1-347a56b757d1" are running
Jul 22 20:35:11.515: INFO: Pod "my-hostname-basic-bd1dc113-f989-47c1-91a1-347a56b757d1-5w4gl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 20:35:06 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 20:35:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 20:35:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-22 20:35:06 +0000 UTC Reason: Message:}])
Jul 22 20:35:11.515: INFO: Trying to dial the pod
Jul 22 20:35:16.608: INFO: Controller my-hostname-basic-bd1dc113-f989-47c1-91a1-347a56b757d1: Got expected result from replica 1 [my-hostname-basic-bd1dc113-f989-47c1-91a1-347a56b757d1-5w4gl]: "my-hostname-basic-bd1dc113-f989-47c1-91a1-347a56b757d1-5w4gl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:35:16.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1462" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":157,"skipped":2767,"failed":0}
SSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:35:16.640: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-5359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jul 22 20:35:16.852: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jul 22 20:35:16.873: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 22 20:35:16.873: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jul 22 20:35:16.901: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 22 20:35:16.901: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jul 22 20:35:16.930: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jul 22 20:35:16.930: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jul 22 20:35:24.043: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:35:24.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5359" for this suite.
•{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":339,"completed":158,"skipped":2770,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:35:24.097: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5636
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jul 22 20:35:24.315: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:35:26.327: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:35:28.327: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 22 20:35:29.397: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:35:29.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5636" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":339,"completed":159,"skipped":2772,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:35:29.598: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6832
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:35:37.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6832" for this suite.
•{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":339,"completed":160,"skipped":2783,"failed":0}
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:35:37.909: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2761
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:35:38.131: INFO: The status of Pod busybox-host-aliases8643416b-348c-4e64-913b-0992c7c6b1d1 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:35:40.142: INFO: The status of Pod busybox-host-aliases8643416b-348c-4e64-913b-0992c7c6b1d1 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:35:42.144: INFO: The status of Pod busybox-host-aliases8643416b-348c-4e64-913b-0992c7c6b1d1 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:35:42.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2761" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":161,"skipped":2790,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:35:42.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1974.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1974.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1974.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1974.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1974.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1974.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1974.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 160.24.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.24.160_udp@PTR;check="$$(dig +tcp +noall +answer +search 160.24.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.24.160_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1974.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1974.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1974.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1974.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1974.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1974.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1974.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1974.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 160.24.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.24.160_udp@PTR;check="$$(dig +tcp +noall +answer +search 160.24.67.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.67.24.160_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 20:35:44.642: INFO: Unable to read wheezy_udp@dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:44.657: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:44.671: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:44.716: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:44.825: INFO: Unable to read jessie_udp@dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:44.841: INFO: Unable to read jessie_tcp@dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:44.859: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:44.878: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:44.985: INFO: Lookups using dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145 failed for: [wheezy_udp@dns-test-service.dns-1974.svc.cluster.local wheezy_tcp@dns-test-service.dns-1974.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local jessie_udp@dns-test-service.dns-1974.svc.cluster.local jessie_tcp@dns-test-service.dns-1974.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local]

Jul 22 20:35:50.059: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:50.076: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:50.201: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:50.215: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:50.296: INFO: Lookups using dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local]

Jul 22 20:35:55.059: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:55.073: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:55.202: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:55.216: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:35:55.297: INFO: Lookups using dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local]

Jul 22 20:36:00.182: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:36:00.279: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:36:00.519: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:36:00.532: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:36:00.614: INFO: Lookups using dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local]

Jul 22 20:36:05.058: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:36:05.071: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:36:05.224: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:36:05.238: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:36:05.326: INFO: Lookups using dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local]

Jul 22 20:36:10.059: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:36:10.073: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:36:10.226: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:36:10.240: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local from pod dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145: the server could not find the requested resource (get pods dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145)
Jul 22 20:36:10.332: INFO: Lookups using dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1974.svc.cluster.local]

Jul 22 20:36:15.297: INFO: DNS probes using dns-1974/dns-test-3f371f5d-3bef-44c9-ade9-6a1d777cf145 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:36:15.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1974" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":339,"completed":162,"skipped":2810,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:36:15.387: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-1470/secret-test-5b1c1a14-dcbd-47d6-8d3b-bd55cf4e73ad
STEP: Creating a pod to test consume secrets
Jul 22 20:36:15.611: INFO: Waiting up to 5m0s for pod "pod-configmaps-3a781009-1231-4474-b990-88ace3171f8a" in namespace "secrets-1470" to be "Succeeded or Failed"
Jul 22 20:36:15.622: INFO: Pod "pod-configmaps-3a781009-1231-4474-b990-88ace3171f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.843144ms
Jul 22 20:36:17.635: INFO: Pod "pod-configmaps-3a781009-1231-4474-b990-88ace3171f8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023130203s
STEP: Saw pod success
Jul 22 20:36:17.635: INFO: Pod "pod-configmaps-3a781009-1231-4474-b990-88ace3171f8a" satisfied condition "Succeeded or Failed"
Jul 22 20:36:17.646: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-configmaps-3a781009-1231-4474-b990-88ace3171f8a container env-test: <nil>
STEP: delete the pod
Jul 22 20:36:17.710: INFO: Waiting for pod pod-configmaps-3a781009-1231-4474-b990-88ace3171f8a to disappear
Jul 22 20:36:17.721: INFO: Pod pod-configmaps-3a781009-1231-4474-b990-88ace3171f8a no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:36:17.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1470" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":163,"skipped":2821,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:36:17.753: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4034
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:36:17.985: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 22 20:36:22.997: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jul 22 20:36:23.021: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jul 22 20:36:23.074: INFO: observed ReplicaSet test-rs in namespace replicaset-4034 with ReadyReplicas 1, AvailableReplicas 1
Jul 22 20:36:23.080: INFO: observed ReplicaSet test-rs in namespace replicaset-4034 with ReadyReplicas 1, AvailableReplicas 1
Jul 22 20:36:23.095: INFO: observed ReplicaSet test-rs in namespace replicaset-4034 with ReadyReplicas 1, AvailableReplicas 1
Jul 22 20:36:23.100: INFO: observed ReplicaSet test-rs in namespace replicaset-4034 with ReadyReplicas 1, AvailableReplicas 1
Jul 22 20:36:24.317: INFO: observed ReplicaSet test-rs in namespace replicaset-4034 with ReadyReplicas 2, AvailableReplicas 2
Jul 22 20:36:25.673: INFO: observed Replicaset test-rs in namespace replicaset-4034 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:36:25.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4034" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":339,"completed":164,"skipped":2821,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:36:25.706: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-613
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-hbs7
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 20:36:25.978: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-hbs7" in namespace "subpath-613" to be "Succeeded or Failed"
Jul 22 20:36:25.989: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.766718ms
Jul 22 20:36:28.001: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Running", Reason="", readiness=true. Elapsed: 2.022848141s
Jul 22 20:36:30.014: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Running", Reason="", readiness=true. Elapsed: 4.035545571s
Jul 22 20:36:32.027: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Running", Reason="", readiness=true. Elapsed: 6.04852118s
Jul 22 20:36:34.038: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Running", Reason="", readiness=true. Elapsed: 8.060403487s
Jul 22 20:36:36.052: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Running", Reason="", readiness=true. Elapsed: 10.07419146s
Jul 22 20:36:38.065: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Running", Reason="", readiness=true. Elapsed: 12.086623056s
Jul 22 20:36:40.076: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Running", Reason="", readiness=true. Elapsed: 14.098130681s
Jul 22 20:36:42.089: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Running", Reason="", readiness=true. Elapsed: 16.111221271s
Jul 22 20:36:44.101: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Running", Reason="", readiness=true. Elapsed: 18.123349656s
Jul 22 20:36:46.114: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Running", Reason="", readiness=true. Elapsed: 20.135983997s
Jul 22 20:36:48.127: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Running", Reason="", readiness=true. Elapsed: 22.149155634s
Jul 22 20:36:50.139: INFO: Pod "pod-subpath-test-projected-hbs7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.16143718s
STEP: Saw pod success
Jul 22 20:36:50.140: INFO: Pod "pod-subpath-test-projected-hbs7" satisfied condition "Succeeded or Failed"
Jul 22 20:36:50.150: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-subpath-test-projected-hbs7 container test-container-subpath-projected-hbs7: <nil>
STEP: delete the pod
Jul 22 20:36:50.202: INFO: Waiting for pod pod-subpath-test-projected-hbs7 to disappear
Jul 22 20:36:50.212: INFO: Pod pod-subpath-test-projected-hbs7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-hbs7
Jul 22 20:36:50.213: INFO: Deleting pod "pod-subpath-test-projected-hbs7" in namespace "subpath-613"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:36:50.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-613" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":339,"completed":165,"skipped":2845,"failed":0}
SSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:36:50.256: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3488
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:36:52.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3488" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":339,"completed":166,"skipped":2848,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:36:52.542: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7547
STEP: creating service affinity-nodeport in namespace services-7547
STEP: creating replication controller affinity-nodeport in namespace services-7547
I0722 20:36:52.766086    5669 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-7547, replica count: 3
I0722 20:36:55.817045    5669 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:36:55.859: INFO: Creating new exec pod
Jul 22 20:36:59.088: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7547 exec execpod-affinityhmk5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jul 22 20:36:59.488: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jul 22 20:36:59.488: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 20:36:59.488: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7547 exec execpod-affinityhmk5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.67.155.109 80'
Jul 22 20:36:59.938: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.67.155.109 80\nConnection to 100.67.155.109 80 port [tcp/http] succeeded!\n"
Jul 22 20:36:59.938: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 20:36:59.939: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7547 exec execpod-affinityhmk5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.3 31482'
Jul 22 20:37:00.312: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.3 31482\nConnection to 10.250.0.3 31482 port [tcp/*] succeeded!\n"
Jul 22 20:37:00.312: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 20:37:00.312: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7547 exec execpod-affinityhmk5w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.2 31482'
Jul 22 20:37:00.665: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.2 31482\nConnection to 10.250.0.2 31482 port [tcp/*] succeeded!\n"
Jul 22 20:37:00.666: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 20:37:00.666: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-7547 exec execpod-affinityhmk5w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.3:31482/ ; done'
Jul 22 20:37:01.143: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31482/\n"
Jul 22 20:37:01.143: INFO: stdout: "\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2\naffinity-nodeport-7ktx2"
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Received response from host: affinity-nodeport-7ktx2
Jul 22 20:37:01.143: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7547, will wait for the garbage collector to delete the pods
Jul 22 20:37:01.235: INFO: Deleting ReplicationController affinity-nodeport took: 13.05349ms
Jul 22 20:37:01.335: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.217497ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:37:17.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7547" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":167,"skipped":2883,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:37:17.097: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4974
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jul 22 20:37:17.387: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 20:37:17.387: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 20:37:17.387: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 20:37:17.387: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 20:37:17.392: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 20:37:17.392: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 20:37:17.414: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 20:37:17.414: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 22 20:37:19.611: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul 22 20:37:19.611: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul 22 20:37:19.627: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jul 22 20:37:19.651: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 0
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:19.674: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:19.679: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:19.679: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:19.685: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
Jul 22 20:37:19.685: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
Jul 22 20:37:19.691: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
Jul 22 20:37:19.691: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
Jul 22 20:37:21.704: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:21.704: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:21.783: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
STEP: listing Deployments
Jul 22 20:37:21.796: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jul 22 20:37:21.820: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jul 22 20:37:21.874: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 20:37:21.874: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 20:37:21.874: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 20:37:21.910: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 20:37:22.008: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 20:37:24.365: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 20:37:24.382: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 20:37:24.396: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 20:37:24.420: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 22 20:37:26.455: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jul 22 20:37:26.519: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
Jul 22 20:37:26.519: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
Jul 22 20:37:26.519: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
Jul 22 20:37:26.519: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
Jul 22 20:37:26.519: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 1
Jul 22 20:37:26.520: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:26.520: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 3
Jul 22 20:37:26.520: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:26.520: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 2
Jul 22 20:37:26.520: INFO: observed Deployment test-deployment in namespace deployment-4974 with ReadyReplicas 3
STEP: deleting the Deployment
Jul 22 20:37:26.573: INFO: observed event type MODIFIED
Jul 22 20:37:26.573: INFO: observed event type MODIFIED
Jul 22 20:37:26.573: INFO: observed event type MODIFIED
Jul 22 20:37:26.574: INFO: observed event type MODIFIED
Jul 22 20:37:26.574: INFO: observed event type MODIFIED
Jul 22 20:37:26.574: INFO: observed event type MODIFIED
Jul 22 20:37:26.574: INFO: observed event type MODIFIED
Jul 22 20:37:26.574: INFO: observed event type MODIFIED
Jul 22 20:37:26.574: INFO: observed event type MODIFIED
Jul 22 20:37:26.574: INFO: observed event type MODIFIED
Jul 22 20:37:26.575: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 22 20:37:26.586: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:37:26.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4974" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":339,"completed":168,"skipped":2900,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:37:26.622: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8291
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Jul 22 20:37:26.834: INFO: Waiting up to 5m0s for pod "var-expansion-ba9f481b-bb59-4d5b-9fe6-2d5830408654" in namespace "var-expansion-8291" to be "Succeeded or Failed"
Jul 22 20:37:26.845: INFO: Pod "var-expansion-ba9f481b-bb59-4d5b-9fe6-2d5830408654": Phase="Pending", Reason="", readiness=false. Elapsed: 10.278636ms
Jul 22 20:37:28.857: INFO: Pod "var-expansion-ba9f481b-bb59-4d5b-9fe6-2d5830408654": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022778695s
Jul 22 20:37:30.869: INFO: Pod "var-expansion-ba9f481b-bb59-4d5b-9fe6-2d5830408654": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034650182s
STEP: Saw pod success
Jul 22 20:37:30.869: INFO: Pod "var-expansion-ba9f481b-bb59-4d5b-9fe6-2d5830408654" satisfied condition "Succeeded or Failed"
Jul 22 20:37:30.881: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod var-expansion-ba9f481b-bb59-4d5b-9fe6-2d5830408654 container dapi-container: <nil>
STEP: delete the pod
Jul 22 20:37:30.921: INFO: Waiting for pod var-expansion-ba9f481b-bb59-4d5b-9fe6-2d5830408654 to disappear
Jul 22 20:37:30.932: INFO: Pod var-expansion-ba9f481b-bb59-4d5b-9fe6-2d5830408654 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:37:30.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8291" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":339,"completed":169,"skipped":2908,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:37:30.964: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5208
STEP: creating service affinity-nodeport-transition in namespace services-5208
STEP: creating replication controller affinity-nodeport-transition in namespace services-5208
I0722 20:37:31.189273    5669 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-5208, replica count: 3
I0722 20:37:34.241038    5669 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 20:37:34.283: INFO: Creating new exec pod
Jul 22 20:37:37.345: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5208 exec execpod-affinityslj94 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jul 22 20:37:37.826: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jul 22 20:37:37.827: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 20:37:37.827: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5208 exec execpod-affinityslj94 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.64.1.119 80'
Jul 22 20:37:38.259: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.64.1.119 80\nConnection to 100.64.1.119 80 port [tcp/http] succeeded!\n"
Jul 22 20:37:38.259: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 20:37:38.259: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5208 exec execpod-affinityslj94 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.3 31567'
Jul 22 20:37:38.633: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.3 31567\nConnection to 10.250.0.3 31567 port [tcp/*] succeeded!\n"
Jul 22 20:37:38.633: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 20:37:38.633: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5208 exec execpod-affinityslj94 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.2 31567'
Jul 22 20:37:39.051: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.2 31567\nConnection to 10.250.0.2 31567 port [tcp/*] succeeded!\n"
Jul 22 20:37:39.051: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 20:37:39.078: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5208 exec execpod-affinityslj94 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.3:31567/ ; done'
Jul 22 20:37:39.508: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n"
Jul 22 20:37:39.508: INFO: stdout: "\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s"
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:37:39.508: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:09.509: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5208 exec execpod-affinityslj94 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.3:31567/ ; done'
Jul 22 20:38:09.980: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n"
Jul 22 20:38:09.980: INFO: stdout: "\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-nqs5t\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-nqs5t\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-nqs5t\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-nqs5t\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75"
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-nqs5t
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-nqs5t
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-nqs5t
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-nqs5t
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:09.980: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:10.006: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5208 exec execpod-affinityslj94 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.3:31567/ ; done'
Jul 22 20:38:10.437: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n"
Jul 22 20:38:10.437: INFO: stdout: "\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-nqs5t\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-gnv8s\naffinity-nodeport-transition-nqs5t\naffinity-nodeport-transition-nqs5t\naffinity-nodeport-transition-nqs5t\naffinity-nodeport-transition-nqs5t\naffinity-nodeport-transition-gnv8s"
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-nqs5t
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-nqs5t
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-nqs5t
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-nqs5t
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-nqs5t
Jul 22 20:38:10.438: INFO: Received response from host: affinity-nodeport-transition-gnv8s
Jul 22 20:38:40.439: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-5208 exec execpod-affinityslj94 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.3:31567/ ; done'
Jul 22 20:38:40.973: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31567/\n"
Jul 22 20:38:40.973: INFO: stdout: "\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75\naffinity-nodeport-transition-z8m75"
Jul 22 20:38:40.973: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.973: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.973: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.973: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.973: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.973: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.974: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.974: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.974: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.974: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.974: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.974: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.974: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.974: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.974: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.974: INFO: Received response from host: affinity-nodeport-transition-z8m75
Jul 22 20:38:40.974: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5208, will wait for the garbage collector to delete the pods
Jul 22 20:38:41.065: INFO: Deleting ReplicationController affinity-nodeport-transition took: 13.027071ms
Jul 22 20:38:41.166: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.702403ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:38:56.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5208" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":170,"skipped":2912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:38:57.022: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-978
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Jul 22 20:38:57.242: INFO: The status of Pod pod-hostip-326f4328-7026-4da5-b325-6e867ef267a2 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:38:59.253: INFO: The status of Pod pod-hostip-326f4328-7026-4da5-b325-6e867ef267a2 is Running (Ready = true)
Jul 22 20:38:59.275: INFO: Pod pod-hostip-326f4328-7026-4da5-b325-6e867ef267a2 has hostIP: 10.250.0.2
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:38:59.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-978" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":339,"completed":171,"skipped":2952,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:38:59.308: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5850
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5850.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5850.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 20:39:01.645: INFO: DNS probes using dns-test-e95a3eb1-4f02-4b79-b3ac-273974203eea succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5850.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5850.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 20:39:03.764: INFO: File wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local from pod  dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 20:39:03.779: INFO: File jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local from pod  dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 20:39:03.779: INFO: Lookups using dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 failed for: [wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local]

Jul 22 20:39:08.794: INFO: File wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local from pod  dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 20:39:08.841: INFO: File jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local from pod  dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 20:39:08.841: INFO: Lookups using dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 failed for: [wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local]

Jul 22 20:39:13.805: INFO: File wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local from pod  dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 20:39:13.818: INFO: File jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local from pod  dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 20:39:13.818: INFO: Lookups using dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 failed for: [wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local]

Jul 22 20:39:18.793: INFO: File wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local from pod  dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 20:39:18.807: INFO: File jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local from pod  dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 20:39:18.807: INFO: Lookups using dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 failed for: [wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local]

Jul 22 20:39:23.793: INFO: File wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local from pod  dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 20:39:23.807: INFO: File jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local from pod  dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 20:39:23.807: INFO: Lookups using dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 failed for: [wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local]

Jul 22 20:39:28.795: INFO: File wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local from pod  dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 20:39:28.845: INFO: File jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local from pod  dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 22 20:39:28.845: INFO: Lookups using dns-5850/dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 failed for: [wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local]

Jul 22 20:39:33.811: INFO: DNS probes using dns-test-f2c1b84b-8cac-4e63-be9e-1436b35fb0d4 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5850.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5850.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5850.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5850.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 20:39:36.049: INFO: DNS probes using dns-test-cc0b44f1-6091-4c92-8d78-1e5ca8bab5d6 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:39:36.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5850" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":339,"completed":172,"skipped":2956,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:39:36.119: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9443
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-2a2b559e-61f8-485c-8579-379eb3a501f7
STEP: Creating a pod to test consume configMaps
Jul 22 20:39:36.337: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9e06c53b-d590-4a2a-8e50-8103520721d8" in namespace "projected-9443" to be "Succeeded or Failed"
Jul 22 20:39:36.347: INFO: Pod "pod-projected-configmaps-9e06c53b-d590-4a2a-8e50-8103520721d8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.610102ms
Jul 22 20:39:38.360: INFO: Pod "pod-projected-configmaps-9e06c53b-d590-4a2a-8e50-8103520721d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022803124s
STEP: Saw pod success
Jul 22 20:39:38.360: INFO: Pod "pod-projected-configmaps-9e06c53b-d590-4a2a-8e50-8103520721d8" satisfied condition "Succeeded or Failed"
Jul 22 20:39:38.371: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-configmaps-9e06c53b-d590-4a2a-8e50-8103520721d8 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:39:38.444: INFO: Waiting for pod pod-projected-configmaps-9e06c53b-d590-4a2a-8e50-8103520721d8 to disappear
Jul 22 20:39:38.455: INFO: Pod pod-projected-configmaps-9e06c53b-d590-4a2a-8e50-8103520721d8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:39:38.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9443" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":173,"skipped":2974,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:39:38.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7436
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:39:38.695: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1223f86-1191-4efc-ae5e-3264401e35b8" in namespace "downward-api-7436" to be "Succeeded or Failed"
Jul 22 20:39:38.705: INFO: Pod "downwardapi-volume-f1223f86-1191-4efc-ae5e-3264401e35b8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.515924ms
Jul 22 20:39:40.718: INFO: Pod "downwardapi-volume-f1223f86-1191-4efc-ae5e-3264401e35b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02287548s
STEP: Saw pod success
Jul 22 20:39:40.718: INFO: Pod "downwardapi-volume-f1223f86-1191-4efc-ae5e-3264401e35b8" satisfied condition "Succeeded or Failed"
Jul 22 20:39:40.729: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-f1223f86-1191-4efc-ae5e-3264401e35b8 container client-container: <nil>
STEP: delete the pod
Jul 22 20:39:40.763: INFO: Waiting for pod downwardapi-volume-f1223f86-1191-4efc-ae5e-3264401e35b8 to disappear
Jul 22 20:39:40.773: INFO: Pod downwardapi-volume-f1223f86-1191-4efc-ae5e-3264401e35b8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:39:40.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7436" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":174,"skipped":2979,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:39:40.808: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6798
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:39:41.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6798" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":339,"completed":175,"skipped":2981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:39:41.188: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8277
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 22 20:39:43.979: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-8277 pod-service-account-c06fd31d-7f12-4658-bafc-335f2e929351 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 22 20:39:44.388: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-8277 pod-service-account-c06fd31d-7f12-4658-bafc-335f2e929351 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 22 20:39:44.770: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-8277 pod-service-account-c06fd31d-7f12-4658-bafc-335f2e929351 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:39:45.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8277" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":339,"completed":176,"skipped":3034,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:39:45.234: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:40:13.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9003" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":339,"completed":177,"skipped":3040,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:40:13.488: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Jul 22 20:40:13.701: INFO: Waiting up to 5m0s for pod "test-pod-e113856a-f0ff-40d6-9d39-78b5d3f1264f" in namespace "svcaccounts-8992" to be "Succeeded or Failed"
Jul 22 20:40:13.712: INFO: Pod "test-pod-e113856a-f0ff-40d6-9d39-78b5d3f1264f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.246538ms
Jul 22 20:40:15.724: INFO: Pod "test-pod-e113856a-f0ff-40d6-9d39-78b5d3f1264f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022455681s
STEP: Saw pod success
Jul 22 20:40:15.724: INFO: Pod "test-pod-e113856a-f0ff-40d6-9d39-78b5d3f1264f" satisfied condition "Succeeded or Failed"
Jul 22 20:40:15.735: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod test-pod-e113856a-f0ff-40d6-9d39-78b5d3f1264f container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:40:15.771: INFO: Waiting for pod test-pod-e113856a-f0ff-40d6-9d39-78b5d3f1264f to disappear
Jul 22 20:40:15.781: INFO: Pod test-pod-e113856a-f0ff-40d6-9d39-78b5d3f1264f no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:40:15.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8992" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":339,"completed":178,"skipped":3042,"failed":0}

------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:40:15.814: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9429
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jul 22 20:40:16.037: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:40:20.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9429" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":339,"completed":179,"skipped":3042,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:40:20.051: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-6c7e4a06-dca9-4526-b267-20db2e3e43d8
STEP: Creating a pod to test consume configMaps
Jul 22 20:40:20.271: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5a85d6ba-c83b-4526-a31f-5df209ddd11c" in namespace "projected-6021" to be "Succeeded or Failed"
Jul 22 20:40:20.283: INFO: Pod "pod-projected-configmaps-5a85d6ba-c83b-4526-a31f-5df209ddd11c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.519696ms
Jul 22 20:40:22.296: INFO: Pod "pod-projected-configmaps-5a85d6ba-c83b-4526-a31f-5df209ddd11c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024537771s
STEP: Saw pod success
Jul 22 20:40:22.296: INFO: Pod "pod-projected-configmaps-5a85d6ba-c83b-4526-a31f-5df209ddd11c" satisfied condition "Succeeded or Failed"
Jul 22 20:40:22.307: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-configmaps-5a85d6ba-c83b-4526-a31f-5df209ddd11c container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:40:22.343: INFO: Waiting for pod pod-projected-configmaps-5a85d6ba-c83b-4526-a31f-5df209ddd11c to disappear
Jul 22 20:40:22.354: INFO: Pod pod-projected-configmaps-5a85d6ba-c83b-4526-a31f-5df209ddd11c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:40:22.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6021" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":180,"skipped":3048,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:40:22.387: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5699
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jul 22 20:40:22.675: INFO: The status of Pod labelsupdate40b43f3c-be28-48c8-996a-f8c1bfde960f is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:40:24.687: INFO: The status of Pod labelsupdate40b43f3c-be28-48c8-996a-f8c1bfde960f is Running (Ready = true)
Jul 22 20:40:25.475: INFO: Successfully updated pod "labelsupdate40b43f3c-be28-48c8-996a-f8c1bfde960f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:40:27.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5699" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":181,"skipped":3050,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:40:27.574: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5725
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5725
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 20:40:27.761: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 22 20:40:27.832: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:40:29.844: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:40:31.846: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:40:33.846: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:40:35.845: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:40:37.845: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:40:39.844: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:40:41.846: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:40:43.844: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:40:45.845: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 22 20:40:47.845: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 22 20:40:47.866: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Jul 22 20:40:51.931: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jul 22 20:40:51.931: INFO: Breadth first check of 100.96.0.77 on host 10.250.0.3...
Jul 22 20:40:51.942: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.226:9080/dial?request=hostname&protocol=udp&host=100.96.0.77&port=8081&tries=1'] Namespace:pod-network-test-5725 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:40:51.942: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:40:52.274: INFO: Waiting for responses: map[]
Jul 22 20:40:52.274: INFO: reached 100.96.0.77 after 0/1 tries
Jul 22 20:40:52.274: INFO: Breadth first check of 100.96.1.225 on host 10.250.0.2...
Jul 22 20:40:52.286: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.226:9080/dial?request=hostname&protocol=udp&host=100.96.1.225&port=8081&tries=1'] Namespace:pod-network-test-5725 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 22 20:40:52.286: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 20:40:52.567: INFO: Waiting for responses: map[]
Jul 22 20:40:52.568: INFO: reached 100.96.1.225 after 0/1 tries
Jul 22 20:40:52.568: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:40:52.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5725" for this suite.
•{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":339,"completed":182,"skipped":3062,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:40:52.602: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:40:53.318: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:40:56.369: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:40:56.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4256" for this suite.
STEP: Destroying namespace "webhook-4256-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":339,"completed":183,"skipped":3077,"failed":0}
SSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:40:56.972: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-5780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:149
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 22 20:40:57.253: INFO: starting watch
STEP: patching
STEP: updating
Jul 22 20:40:57.286: INFO: waiting for watch events with expected annotations
Jul 22 20:40:57.286: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:40:57.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-5780" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":339,"completed":184,"skipped":3085,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:40:57.369: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6161
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:40:57.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6161" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":339,"completed":185,"skipped":3095,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:40:57.590: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4554
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:40:57.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4554" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":339,"completed":186,"skipped":3096,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:40:57.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-696
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-965
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:41:04.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1089" for this suite.
STEP: Destroying namespace "nsdeletetest-696" for this suite.
Jul 22 20:41:04.553: INFO: Namespace nsdeletetest-696 was already deleted
STEP: Destroying namespace "nsdeletetest-965" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":339,"completed":187,"skipped":3117,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:41:04.565: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2611
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:41:20.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2611" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":339,"completed":188,"skipped":3123,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:41:20.984: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1683
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 22 20:41:21.197: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:41:27.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1683" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":339,"completed":189,"skipped":3129,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:41:27.194: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9241
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-9241/configmap-test-eccc7105-7680-4fce-802f-ff8c7865a528
STEP: Creating a pod to test consume configMaps
Jul 22 20:41:27.416: INFO: Waiting up to 5m0s for pod "pod-configmaps-be49e85b-6aca-46eb-bb8d-fc4aa06b2296" in namespace "configmap-9241" to be "Succeeded or Failed"
Jul 22 20:41:27.426: INFO: Pod "pod-configmaps-be49e85b-6aca-46eb-bb8d-fc4aa06b2296": Phase="Pending", Reason="", readiness=false. Elapsed: 10.064013ms
Jul 22 20:41:29.438: INFO: Pod "pod-configmaps-be49e85b-6aca-46eb-bb8d-fc4aa06b2296": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02215055s
STEP: Saw pod success
Jul 22 20:41:29.438: INFO: Pod "pod-configmaps-be49e85b-6aca-46eb-bb8d-fc4aa06b2296" satisfied condition "Succeeded or Failed"
Jul 22 20:41:29.449: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-configmaps-be49e85b-6aca-46eb-bb8d-fc4aa06b2296 container env-test: <nil>
STEP: delete the pod
Jul 22 20:41:29.481: INFO: Waiting for pod pod-configmaps-be49e85b-6aca-46eb-bb8d-fc4aa06b2296 to disappear
Jul 22 20:41:29.492: INFO: Pod pod-configmaps-be49e85b-6aca-46eb-bb8d-fc4aa06b2296 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:41:29.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9241" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":190,"skipped":3134,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:41:29.526: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8712
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8712, will wait for the garbage collector to delete the pods
Jul 22 20:41:33.815: INFO: Deleting Job.batch foo took: 12.862093ms
Jul 22 20:41:33.915: INFO: Terminating Job.batch foo pods took: 100.424281ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:42:17.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8712" for this suite.
•{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":339,"completed":191,"skipped":3150,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:42:17.059: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3993
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Jul 22 20:42:17.249: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3993 cluster-info'
Jul 22 20:42:17.371: INFO: stderr: ""
Jul 22 20:42:17.371: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:42:17.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3993" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":339,"completed":192,"skipped":3165,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:42:17.395: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8453
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-5970182f-cf87-42c0-a2cd-00cc4909d6ed-7195
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:42:17.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8453" for this suite.
STEP: Destroying namespace "nspatchtest-5970182f-cf87-42c0-a2cd-00cc4909d6ed-7195" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":339,"completed":193,"skipped":3166,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:42:17.809: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-5412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jul 22 20:42:18.013: INFO: Waiting up to 5m0s for pod "security-context-30a45e9c-43e8-42a0-8d0f-8ee9cb84fea4" in namespace "security-context-5412" to be "Succeeded or Failed"
Jul 22 20:42:18.026: INFO: Pod "security-context-30a45e9c-43e8-42a0-8d0f-8ee9cb84fea4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.156246ms
Jul 22 20:42:20.037: INFO: Pod "security-context-30a45e9c-43e8-42a0-8d0f-8ee9cb84fea4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024256945s
STEP: Saw pod success
Jul 22 20:42:20.037: INFO: Pod "security-context-30a45e9c-43e8-42a0-8d0f-8ee9cb84fea4" satisfied condition "Succeeded or Failed"
Jul 22 20:42:20.048: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod security-context-30a45e9c-43e8-42a0-8d0f-8ee9cb84fea4 container test-container: <nil>
STEP: delete the pod
Jul 22 20:42:20.121: INFO: Waiting for pod security-context-30a45e9c-43e8-42a0-8d0f-8ee9cb84fea4 to disappear
Jul 22 20:42:20.132: INFO: Pod security-context-30a45e9c-43e8-42a0-8d0f-8ee9cb84fea4 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:42:20.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5412" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":194,"skipped":3223,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:42:20.165: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5298
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:42:20.357: INFO: Creating simple deployment test-new-deployment
Jul 22 20:42:20.392: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 22 20:42:22.591: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-5298  f0b4cde8-80b9-49f2-a726-e6148e2af201 28170 3 2021-07-22 20:42:20 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-07-22 20:42:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002da5d08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-07-22 20:42:22 +0000 UTC,LastTransitionTime:2021-07-22 20:42:20 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-22 20:42:22 +0000 UTC,LastTransitionTime:2021-07-22 20:42:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 22 20:42:22.670: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-5298  8dde6b49-1d83-4ee6-acc6-18cfe7fcdf34 28175 3 2021-07-22 20:42:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment f0b4cde8-80b9-49f2-a726-e6148e2af201 0xc0033d6827 0xc0033d6828}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0b4cde8-80b9-49f2-a726-e6148e2af201\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033d6898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:42:22.682: INFO: Pod "test-new-deployment-847dcfb7fb-7vrrt" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-7vrrt test-new-deployment-847dcfb7fb- deployment-5298  da322485-f6e1-43f6-b207-8901c4013098 28158 0 2021-07-22 20:42:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[cni.projectcalico.org/podIP:100.96.1.233/32 cni.projectcalico.org/podIPs:100.96.1.233/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 8dde6b49-1d83-4ee6-acc6-18cfe7fcdf34 0xc002e8ad17 0xc002e8ad18}] []  [{kube-controller-manager Update v1 2021-07-22 20:42:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8dde6b49-1d83-4ee6-acc6-18cfe7fcdf34\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:42:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:42:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-544mx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-544mx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:42:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:42:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.233,StartTime:2021-07-22 20:42:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:42:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:docker://49b8ec1a853e7d7ecd00d92684bff25bd1c6ef9339c772013021c812273ce952,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:42:22.682: INFO: Pod "test-new-deployment-847dcfb7fb-9sxkr" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-9sxkr test-new-deployment-847dcfb7fb- deployment-5298  1084638f-1666-4131-bdab-06bb9c1db6a0 28179 0 2021-07-22 20:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 8dde6b49-1d83-4ee6-acc6-18cfe7fcdf34 0xc002e8b100 0xc002e8b101}] []  [{kube-controller-manager Update v1 2021-07-22 20:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8dde6b49-1d83-4ee6-acc6-18cfe7fcdf34\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s7pvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s7pvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:42:22.682: INFO: Pod "test-new-deployment-847dcfb7fb-b26jw" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-b26jw test-new-deployment-847dcfb7fb- deployment-5298  14063a47-24c7-488f-bd8a-b9dd4a0efd60 28181 0 2021-07-22 20:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 8dde6b49-1d83-4ee6-acc6-18cfe7fcdf34 0xc002e8b2c0 0xc002e8b2c1}] []  [{kube-controller-manager Update v1 2021-07-22 20:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8dde6b49-1d83-4ee6-acc6-18cfe7fcdf34\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xvf2m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xvf2m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 22 20:42:22.682: INFO: Pod "test-new-deployment-847dcfb7fb-k4gks" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-k4gks test-new-deployment-847dcfb7fb- deployment-5298  b8630a88-b100-4ffa-bc4c-983bc5b8683e 28174 0 2021-07-22 20:42:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 8dde6b49-1d83-4ee6-acc6-18cfe7fcdf34 0xc002e8b557 0xc002e8b558}] []  [{kube-controller-manager Update v1 2021-07-22 20:42:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8dde6b49-1d83-4ee6-acc6-18cfe7fcdf34\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-22 20:42:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzncn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzncn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:42:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:42:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:42:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:42:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2021-07-22 20:42:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:42:22.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5298" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":339,"completed":195,"skipped":3229,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:42:22.707: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2708
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 20:42:23.109: INFO: Waiting up to 5m0s for pod "downwardapi-volume-995b9c53-5f86-47d8-a925-1d015d388ed0" in namespace "downward-api-2708" to be "Succeeded or Failed"
Jul 22 20:42:23.120: INFO: Pod "downwardapi-volume-995b9c53-5f86-47d8-a925-1d015d388ed0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.285399ms
Jul 22 20:42:25.132: INFO: Pod "downwardapi-volume-995b9c53-5f86-47d8-a925-1d015d388ed0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023667679s
STEP: Saw pod success
Jul 22 20:42:25.132: INFO: Pod "downwardapi-volume-995b9c53-5f86-47d8-a925-1d015d388ed0" satisfied condition "Succeeded or Failed"
Jul 22 20:42:25.143: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-995b9c53-5f86-47d8-a925-1d015d388ed0 container client-container: <nil>
STEP: delete the pod
Jul 22 20:42:25.193: INFO: Waiting for pod downwardapi-volume-995b9c53-5f86-47d8-a925-1d015d388ed0 to disappear
Jul 22 20:42:25.203: INFO: Pod downwardapi-volume-995b9c53-5f86-47d8-a925-1d015d388ed0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:42:25.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2708" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":196,"skipped":3261,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:42:25.289: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:42:25.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7614" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":339,"completed":197,"skipped":3270,"failed":0}
SSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:42:25.536: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6185
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-ade71f78-72ca-4185-ae42-a405c9377c9e in namespace container-probe-6185
Jul 22 20:42:27.772: INFO: Started pod busybox-ade71f78-72ca-4185-ae42-a405c9377c9e in namespace container-probe-6185
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 20:42:27.783: INFO: Initial restart count of pod busybox-ade71f78-72ca-4185-ae42-a405c9377c9e is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:46:29.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6185" for this suite.
•{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":198,"skipped":3275,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:46:29.353: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9872
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-9872
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jul 22 20:46:29.578: INFO: Found 0 stateful pods, waiting for 3
Jul 22 20:46:39.591: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 20:46:39.648: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 20:46:39.649: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Jul 22 20:46:39.786: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 22 20:46:49.869: INFO: Updating stateful set ss2
Jul 22 20:46:49.898: INFO: Waiting for Pod statefulset-9872/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Jul 22 20:46:59.971: INFO: Found 2 stateful pods, waiting for 3
Jul 22 20:47:09.985: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 20:47:09.985: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 20:47:09.985: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 22 20:47:10.043: INFO: Updating stateful set ss2
Jul 22 20:47:10.065: INFO: Waiting for Pod statefulset-9872/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 22 20:47:20.125: INFO: Updating stateful set ss2
Jul 22 20:47:20.147: INFO: Waiting for StatefulSet statefulset-9872/ss2 to complete update
Jul 22 20:47:20.147: INFO: Waiting for Pod statefulset-9872/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul 22 20:47:30.173: INFO: Deleting all statefulset in ns statefulset-9872
Jul 22 20:47:30.183: INFO: Scaling statefulset ss2 to 0
Jul 22 20:47:40.234: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 20:47:40.245: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:47:40.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9872" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":339,"completed":199,"skipped":3294,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:47:40.312: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-4574
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
W0722 20:47:40.516217    5669 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:49:00.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4574" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":339,"completed":200,"skipped":3314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:49:00.596: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2864
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:49:00.818: INFO: created pod
Jul 22 20:49:00.818: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2864" to be "Succeeded or Failed"
Jul 22 20:49:00.828: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.65053ms
Jul 22 20:49:02.841: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023283359s
Jul 22 20:49:04.853: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034896833s
STEP: Saw pod success
Jul 22 20:49:04.853: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jul 22 20:49:34.854: INFO: polling logs
Jul 22 20:49:34.882: INFO: Pod logs: 
2021/07/22 20:49:02 OK: Got token
2021/07/22 20:49:02 validating with in-cluster discovery
2021/07/22 20:49:02 OK: got issuer https://api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com
2021/07/22 20:49:02 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-2864:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1626987541, NotBefore:1626986941, IssuedAt:1626986941, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2864", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e8019ac3-b9ed-4dd4-9aeb-0498456dd675"}}}
2021/07/22 20:49:02 OK: Constructed OIDC provider for issuer https://api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com
2021/07/22 20:49:02 OK: Validated signature on JWT
2021/07/22 20:49:02 OK: Got valid claims from token!
2021/07/22 20:49:02 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com", Subject:"system:serviceaccount:svcaccounts-2864:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1626987541, NotBefore:1626986941, IssuedAt:1626986941, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2864", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e8019ac3-b9ed-4dd4-9aeb-0498456dd675"}}}

Jul 22 20:49:34.882: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:49:34.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2864" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":339,"completed":201,"skipped":3341,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:49:34.927: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6635.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6635.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6635.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6635.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6635.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6635.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 20:49:39.338: INFO: DNS probes using dns-6635/dns-test-23427dca-308c-4d2a-ba53-e88b3a807251 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:49:39.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6635" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":339,"completed":202,"skipped":3350,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:49:39.387: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1029
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:49:39.600: INFO: The status of Pod busybox-scheduling-044cdb06-5022-444b-8d72-29dd2edad56f is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:49:41.612: INFO: The status of Pod busybox-scheduling-044cdb06-5022-444b-8d72-29dd2edad56f is Pending, waiting for it to be Running (with Ready = true)
Jul 22 20:49:43.614: INFO: The status of Pod busybox-scheduling-044cdb06-5022-444b-8d72-29dd2edad56f is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:49:43.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1029" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":339,"completed":203,"skipped":3368,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:49:43.717: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-3395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 22 20:49:43.940: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 22 20:50:44.047: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:50:44.057: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-6574
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:50:44.285: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jul 22 20:50:44.297: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:50:44.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-6574" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:50:44.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3395" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":339,"completed":204,"skipped":3376,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:50:44.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8645
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-8645
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-8645
Jul 22 20:50:44.715: INFO: Found 0 stateful pods, waiting for 1
Jul 22 20:50:54.728: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul 22 20:50:54.807: INFO: Deleting all statefulset in ns statefulset-8645
Jul 22 20:50:54.817: INFO: Scaling statefulset ss to 0
Jul 22 20:51:14.865: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 20:51:14.876: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:51:14.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8645" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":339,"completed":205,"skipped":3383,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:51:14.945: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3682
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:51:19.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3682" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":339,"completed":206,"skipped":3405,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:51:19.305: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7169
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-47887294-c355-4654-bed6-d313c490ab81
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:51:21.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7169" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":207,"skipped":3436,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:51:21.676: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-785
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:51:49.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-785" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":339,"completed":208,"skipped":3452,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:51:50.000: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-46c9c078-66b5-4c06-a54d-8ff2a37c9c12
STEP: Creating a pod to test consume configMaps
Jul 22 20:51:50.223: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-be97d6d2-e65b-4207-ac9d-be3f9d723f30" in namespace "projected-8657" to be "Succeeded or Failed"
Jul 22 20:51:50.233: INFO: Pod "pod-projected-configmaps-be97d6d2-e65b-4207-ac9d-be3f9d723f30": Phase="Pending", Reason="", readiness=false. Elapsed: 10.443743ms
Jul 22 20:51:52.247: INFO: Pod "pod-projected-configmaps-be97d6d2-e65b-4207-ac9d-be3f9d723f30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024094612s
STEP: Saw pod success
Jul 22 20:51:52.247: INFO: Pod "pod-projected-configmaps-be97d6d2-e65b-4207-ac9d-be3f9d723f30" satisfied condition "Succeeded or Failed"
Jul 22 20:51:52.258: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-configmaps-be97d6d2-e65b-4207-ac9d-be3f9d723f30 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:51:52.336: INFO: Waiting for pod pod-projected-configmaps-be97d6d2-e65b-4207-ac9d-be3f9d723f30 to disappear
Jul 22 20:51:52.347: INFO: Pod pod-projected-configmaps-be97d6d2-e65b-4207-ac9d-be3f9d723f30 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:51:52.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8657" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":209,"skipped":3466,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:51:52.378: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1514
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jul 22 20:51:52.564: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3773 run e2e-test-httpd-pod --restart=Never --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Jul 22 20:51:52.949: INFO: stderr: ""
Jul 22 20:51:52.949: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1518
Jul 22 20:51:52.960: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3773 delete pods e2e-test-httpd-pod'
Jul 22 20:52:06.938: INFO: stderr: ""
Jul 22 20:52:06.938: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:52:06.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3773" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":339,"completed":210,"skipped":3475,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:52:06.971: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4942
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jul 22 20:52:07.181: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 22 20:52:12.195: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:52:12.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4942" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":339,"completed":211,"skipped":3476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:52:12.316: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9832
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:52:12.581: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 22 20:52:17.593: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 22 20:52:17.593: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 22 20:52:19.718: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9832  0653eba4-d061-4e4b-8608-8afd3b971192 32310 1 2021-07-22 20:52:17 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-07-22 20:52:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-22 20:52:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007e52ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-22 20:52:17 +0000 UTC,LastTransitionTime:2021-07-22 20:52:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5b4d99b59b" has successfully progressed.,LastUpdateTime:2021-07-22 20:52:19 +0000 UTC,LastTransitionTime:2021-07-22 20:52:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 22 20:52:19.729: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-9832  5babe57b-7455-4807-86de-2e872b9ce0db 32303 1 2021-07-22 20:52:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 0653eba4-d061-4e4b-8608-8afd3b971192 0xc007e52e77 0xc007e52e78}] []  [{kube-controller-manager Update apps/v1 2021-07-22 20:52:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0653eba4-d061-4e4b-8608-8afd3b971192\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007e52f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 22 20:52:19.740: INFO: Pod "test-cleanup-deployment-5b4d99b59b-zm9l2" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-zm9l2 test-cleanup-deployment-5b4d99b59b- deployment-9832  8e771c9b-89d1-44d4-bc9b-64e528bf4173 32302 0 2021-07-22 20:52:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[cni.projectcalico.org/podIP:100.96.1.254/32 cni.projectcalico.org/podIPs:100.96.1.254/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b 5babe57b-7455-4807-86de-2e872b9ce0db 0xc007e532a7 0xc007e532a8}] []  [{kube-controller-manager Update v1 2021-07-22 20:52:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5babe57b-7455-4807-86de-2e872b9ce0db\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-07-22 20:52:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-07-22 20:52:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.254\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gl22x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:KUBERNETES_SERVICE_HOST,Value:api.tmy8a-9u6.it.internal.staging.k8s.ondemand.com,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gl22x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:52:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:52:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:52:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-22 20:52:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.1.254,StartTime:2021-07-22 20:52:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-22 20:52:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:docker://d75d2e5eb30f4133866e91d59540338eddd7415d5cb7484f0ee52caec159a9e5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.254,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:52:19.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9832" for this suite.
•{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":339,"completed":212,"skipped":3500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:52:19.774: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 22 20:52:19.966: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 20:52:19.990: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 20:52:20.027: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 before test
Jul 22 20:52:20.051: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6d48bc7cdd-cctqm from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul 22 20:52:20.051: INFO: apiserver-proxy-666sf from kube-system started at 2021-07-22 19:51:18 +0000 UTC (2 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container proxy ready: true, restart count 0
Jul 22 20:52:20.051: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 20:52:20.051: INFO: calico-kube-controllers-6f857b9885-2ktk5 from kube-system started at 2021-07-22 19:51:18 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 22 20:52:20.051: INFO: calico-node-kdx2r from kube-system started at 2021-07-22 19:52:34 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 20:52:20.051: INFO: calico-node-vertical-autoscaler-785b5f968-2667d from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:52:20.051: INFO: calico-typha-deploy-b7996d4cf-mqt22 from kube-system started at 2021-07-22 19:51:26 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container calico-typha ready: true, restart count 0
Jul 22 20:52:20.051: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-tj7hs from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:52:20.051: INFO: calico-typha-vertical-autoscaler-5c9655cddd-9mb9p from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:52:20.051: INFO: coredns-5d966c6cdc-bb8pm from kube-system started at 2021-07-22 19:51:46 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container coredns ready: true, restart count 0
Jul 22 20:52:20.051: INFO: coredns-5d966c6cdc-sbsmp from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container coredns ready: true, restart count 0
Jul 22 20:52:20.051: INFO: csi-driver-node-x8gk6 from kube-system started at 2021-07-22 19:51:18 +0000 UTC (3 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container csi-driver ready: true, restart count 0
Jul 22 20:52:20.051: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul 22 20:52:20.051: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jul 22 20:52:20.051: INFO: kube-proxy-7mqpv from kube-system started at 2021-07-22 20:36:07 +0000 UTC (2 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 20:52:20.051: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:52:20.051: INFO: metrics-server-7774b47fd5-95ldg from kube-system started at 2021-07-22 19:51:47 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container metrics-server ready: true, restart count 0
Jul 22 20:52:20.051: INFO: node-exporter-slf8j from kube-system started at 2021-07-22 19:51:18 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 20:52:20.051: INFO: node-problem-detector-82tqx from kube-system started at 2021-07-22 20:19:08 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 20:52:20.051: INFO: vpn-shoot-7ff9f78fff-c8rk2 from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul 22 20:52:20.051: INFO: dashboard-metrics-scraper-6dd4bbbc68-kdk6t from kubernetes-dashboard started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 22 20:52:20.051: INFO: kubernetes-dashboard-565499686d-dfd8j from kubernetes-dashboard started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.051: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
Jul 22 20:52:20.051: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 before test
Jul 22 20:52:20.067: INFO: test-cleanup-deployment-5b4d99b59b-zm9l2 from deployment-9832 started at 2021-07-22 20:52:17 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.067: INFO: 	Container agnhost ready: true, restart count 0
Jul 22 20:52:20.067: INFO: addons-nginx-ingress-controller-56b97f886d-t8r78 from kube-system started at 2021-07-22 19:54:57 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.067: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 22 20:52:20.067: INFO: apiserver-proxy-65bs6 from kube-system started at 2021-07-22 19:51:37 +0000 UTC (2 container statuses recorded)
Jul 22 20:52:20.067: INFO: 	Container proxy ready: true, restart count 0
Jul 22 20:52:20.067: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 20:52:20.067: INFO: blackbox-exporter-859b5d9c8c-694bg from kube-system started at 2021-07-22 19:58:57 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.067: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul 22 20:52:20.067: INFO: calico-node-hxcn8 from kube-system started at 2021-07-22 19:51:37 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.067: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 20:52:20.067: INFO: csi-driver-node-mmj64 from kube-system started at 2021-07-22 19:51:37 +0000 UTC (3 container statuses recorded)
Jul 22 20:52:20.067: INFO: 	Container csi-driver ready: true, restart count 0
Jul 22 20:52:20.067: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul 22 20:52:20.067: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jul 22 20:52:20.067: INFO: kube-proxy-nghjt from kube-system started at 2021-07-22 20:36:07 +0000 UTC (2 container statuses recorded)
Jul 22 20:52:20.067: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 20:52:20.067: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:52:20.067: INFO: node-exporter-4gh9r from kube-system started at 2021-07-22 19:51:37 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.067: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 20:52:20.067: INFO: node-problem-detector-mttv7 from kube-system started at 2021-07-22 20:19:06 +0000 UTC (1 container statuses recorded)
Jul 22 20:52:20.067: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c557f915-23fc-411c-b156-cbfae35601f9 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.0.2 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-c557f915-23fc-411c-b156-cbfae35601f9 off the node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c557f915-23fc-411c-b156-cbfae35601f9
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:57:28.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5984" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.567 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":339,"completed":213,"skipped":3533,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:57:28.341: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9444
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 22 20:57:38.741: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0722 20:57:38.741098    5669 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0722 20:57:38.741133    5669 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0722 20:57:38.741141    5669 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 22 20:57:38.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5htg" in namespace "gc-9444"
Jul 22 20:57:38.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbzm8" in namespace "gc-9444"
Jul 22 20:57:38.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-ch6v9" in namespace "gc-9444"
Jul 22 20:57:38.786: INFO: Deleting pod "simpletest-rc-to-be-deleted-frnd7" in namespace "gc-9444"
Jul 22 20:57:38.801: INFO: Deleting pod "simpletest-rc-to-be-deleted-hf4hp" in namespace "gc-9444"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:57:38.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9444" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":339,"completed":214,"skipped":3561,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:57:38.841: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7541
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-0eadcb2a-3d0d-4170-863a-2892c56ddf3a
STEP: Creating a pod to test consume configMaps
Jul 22 20:57:39.055: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1931e44e-1bff-4c44-95cf-68118bdefc47" in namespace "projected-7541" to be "Succeeded or Failed"
Jul 22 20:57:39.065: INFO: Pod "pod-projected-configmaps-1931e44e-1bff-4c44-95cf-68118bdefc47": Phase="Pending", Reason="", readiness=false. Elapsed: 10.212051ms
Jul 22 20:57:41.078: INFO: Pod "pod-projected-configmaps-1931e44e-1bff-4c44-95cf-68118bdefc47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023041824s
Jul 22 20:57:43.090: INFO: Pod "pod-projected-configmaps-1931e44e-1bff-4c44-95cf-68118bdefc47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035265769s
STEP: Saw pod success
Jul 22 20:57:43.090: INFO: Pod "pod-projected-configmaps-1931e44e-1bff-4c44-95cf-68118bdefc47" satisfied condition "Succeeded or Failed"
Jul 22 20:57:43.102: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-configmaps-1931e44e-1bff-4c44-95cf-68118bdefc47 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 20:57:43.141: INFO: Waiting for pod pod-projected-configmaps-1931e44e-1bff-4c44-95cf-68118bdefc47 to disappear
Jul 22 20:57:43.151: INFO: Pod pod-projected-configmaps-1931e44e-1bff-4c44-95cf-68118bdefc47 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:57:43.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7541" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":215,"skipped":3607,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:57:43.184: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-b3aa19f0-ae5a-45d9-bcc0-bc82de28bd7d
STEP: Creating a pod to test consume secrets
Jul 22 20:57:43.409: INFO: Waiting up to 5m0s for pod "pod-secrets-f8687704-c00b-4a7c-92e0-7a23e1c99211" in namespace "secrets-6587" to be "Succeeded or Failed"
Jul 22 20:57:43.419: INFO: Pod "pod-secrets-f8687704-c00b-4a7c-92e0-7a23e1c99211": Phase="Pending", Reason="", readiness=false. Elapsed: 10.586523ms
Jul 22 20:57:45.430: INFO: Pod "pod-secrets-f8687704-c00b-4a7c-92e0-7a23e1c99211": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021863474s
STEP: Saw pod success
Jul 22 20:57:45.430: INFO: Pod "pod-secrets-f8687704-c00b-4a7c-92e0-7a23e1c99211" satisfied condition "Succeeded or Failed"
Jul 22 20:57:45.441: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-secrets-f8687704-c00b-4a7c-92e0-7a23e1c99211 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 20:57:45.514: INFO: Waiting for pod pod-secrets-f8687704-c00b-4a7c-92e0-7a23e1c99211 to disappear
Jul 22 20:57:45.524: INFO: Pod pod-secrets-f8687704-c00b-4a7c-92e0-7a23e1c99211 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:57:45.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6587" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":216,"skipped":3609,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:57:45.556: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5935
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 22 20:57:45.744: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 20:57:45.768: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 20:57:45.778: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 before test
Jul 22 20:57:45.803: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6d48bc7cdd-cctqm from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul 22 20:57:45.803: INFO: apiserver-proxy-666sf from kube-system started at 2021-07-22 19:51:18 +0000 UTC (2 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container proxy ready: true, restart count 0
Jul 22 20:57:45.803: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 20:57:45.803: INFO: calico-kube-controllers-6f857b9885-2ktk5 from kube-system started at 2021-07-22 19:51:18 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 22 20:57:45.803: INFO: calico-node-kdx2r from kube-system started at 2021-07-22 19:52:34 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 20:57:45.803: INFO: calico-node-vertical-autoscaler-785b5f968-2667d from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:57:45.803: INFO: calico-typha-deploy-b7996d4cf-mqt22 from kube-system started at 2021-07-22 19:51:26 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container calico-typha ready: true, restart count 0
Jul 22 20:57:45.803: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-tj7hs from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:57:45.803: INFO: calico-typha-vertical-autoscaler-5c9655cddd-9mb9p from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 20:57:45.803: INFO: coredns-5d966c6cdc-bb8pm from kube-system started at 2021-07-22 19:51:46 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container coredns ready: true, restart count 0
Jul 22 20:57:45.803: INFO: coredns-5d966c6cdc-sbsmp from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container coredns ready: true, restart count 0
Jul 22 20:57:45.803: INFO: csi-driver-node-x8gk6 from kube-system started at 2021-07-22 19:51:18 +0000 UTC (3 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container csi-driver ready: true, restart count 0
Jul 22 20:57:45.803: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul 22 20:57:45.803: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jul 22 20:57:45.803: INFO: kube-proxy-7mqpv from kube-system started at 2021-07-22 20:36:07 +0000 UTC (2 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 20:57:45.803: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:57:45.803: INFO: metrics-server-7774b47fd5-95ldg from kube-system started at 2021-07-22 19:51:47 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container metrics-server ready: true, restart count 0
Jul 22 20:57:45.803: INFO: node-exporter-slf8j from kube-system started at 2021-07-22 19:51:18 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 20:57:45.803: INFO: node-problem-detector-82tqx from kube-system started at 2021-07-22 20:19:08 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 20:57:45.803: INFO: vpn-shoot-7ff9f78fff-c8rk2 from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul 22 20:57:45.803: INFO: dashboard-metrics-scraper-6dd4bbbc68-kdk6t from kubernetes-dashboard started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 22 20:57:45.803: INFO: kubernetes-dashboard-565499686d-dfd8j from kubernetes-dashboard started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.803: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
Jul 22 20:57:45.803: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 before test
Jul 22 20:57:45.819: INFO: addons-nginx-ingress-controller-56b97f886d-t8r78 from kube-system started at 2021-07-22 19:54:57 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.819: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 22 20:57:45.819: INFO: apiserver-proxy-65bs6 from kube-system started at 2021-07-22 19:51:37 +0000 UTC (2 container statuses recorded)
Jul 22 20:57:45.819: INFO: 	Container proxy ready: true, restart count 0
Jul 22 20:57:45.819: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 20:57:45.819: INFO: blackbox-exporter-859b5d9c8c-694bg from kube-system started at 2021-07-22 19:58:57 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.819: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul 22 20:57:45.819: INFO: calico-node-hxcn8 from kube-system started at 2021-07-22 19:51:37 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.819: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 20:57:45.819: INFO: csi-driver-node-mmj64 from kube-system started at 2021-07-22 19:51:37 +0000 UTC (3 container statuses recorded)
Jul 22 20:57:45.819: INFO: 	Container csi-driver ready: true, restart count 0
Jul 22 20:57:45.819: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul 22 20:57:45.819: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jul 22 20:57:45.819: INFO: kube-proxy-nghjt from kube-system started at 2021-07-22 20:36:07 +0000 UTC (2 container statuses recorded)
Jul 22 20:57:45.819: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 20:57:45.819: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 20:57:45.819: INFO: node-exporter-4gh9r from kube-system started at 2021-07-22 19:51:37 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.819: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 20:57:45.819: INFO: node-problem-detector-mttv7 from kube-system started at 2021-07-22 20:19:06 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.820: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 20:57:45.820: INFO: pod4 from sched-pred-5984 started at 2021-07-22 20:52:24 +0000 UTC (1 container statuses recorded)
Jul 22 20:57:45.820: INFO: 	Container agnhost ready: false, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1694387ae67755fd], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:57:46.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5935" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":339,"completed":217,"skipped":3616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:57:46.928: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1536
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jul 22 20:57:47.867: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0722 20:57:47.867672    5669 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0722 20:57:47.867698    5669 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0722 20:57:47.867703    5669 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:57:47.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1536" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":339,"completed":218,"skipped":3646,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:57:47.893: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul 22 20:57:48.101: INFO: Waiting up to 5m0s for pod "downward-api-d261bcee-f7d9-441f-82a1-d9f3fbc14c3d" in namespace "downward-api-4350" to be "Succeeded or Failed"
Jul 22 20:57:48.113: INFO: Pod "downward-api-d261bcee-f7d9-441f-82a1-d9f3fbc14c3d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.757698ms
Jul 22 20:57:50.125: INFO: Pod "downward-api-d261bcee-f7d9-441f-82a1-d9f3fbc14c3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024267739s
Jul 22 20:57:52.138: INFO: Pod "downward-api-d261bcee-f7d9-441f-82a1-d9f3fbc14c3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036531206s
STEP: Saw pod success
Jul 22 20:57:52.138: INFO: Pod "downward-api-d261bcee-f7d9-441f-82a1-d9f3fbc14c3d" satisfied condition "Succeeded or Failed"
Jul 22 20:57:52.190: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downward-api-d261bcee-f7d9-441f-82a1-d9f3fbc14c3d container dapi-container: <nil>
STEP: delete the pod
Jul 22 20:57:52.225: INFO: Waiting for pod downward-api-d261bcee-f7d9-441f-82a1-d9f3fbc14c3d to disappear
Jul 22 20:57:52.235: INFO: Pod downward-api-d261bcee-f7d9-441f-82a1-d9f3fbc14c3d no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:57:52.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4350" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":339,"completed":219,"skipped":3649,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:57:52.288: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-605
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:57:53.008: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584272, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584272, loc:(*time.Location)(0x9dde5a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-78988fc6cd\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584272, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584272, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:57:55.020: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584272, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584272, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584273, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584272, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:57:58.043: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 20:57:58.055: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1516-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:58:00.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-605" for this suite.
STEP: Destroying namespace "webhook-605-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":339,"completed":220,"skipped":3653,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:58:01.189: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 20:58:01.962: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584281, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584281, loc:(*time.Location)(0x9dde5a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-78988fc6cd\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584281, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584281, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jul 22 20:58:03.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584281, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584281, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584281, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584281, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 20:58:06.993: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 20:58:07.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-336" for this suite.
STEP: Destroying namespace "webhook-336-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":339,"completed":221,"skipped":3668,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 20:58:07.290: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Jul 22 21:00:08.058: INFO: Successfully updated pod "var-expansion-e310a403-3a06-47b5-ad39-49142c1f83dc"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jul 22 21:00:10.081: INFO: Deleting pod "var-expansion-e310a403-3a06-47b5-ad39-49142c1f83dc" in namespace "var-expansion-1803"
Jul 22 21:00:10.094: INFO: Wait up to 5m0s for pod "var-expansion-e310a403-3a06-47b5-ad39-49142c1f83dc" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:00:48.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1803" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":339,"completed":222,"skipped":3668,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:00:48.151: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2964
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-acfa8172-db3d-4762-ae99-db3c76a5d851 in namespace container-probe-2964
Jul 22 21:00:50.407: INFO: Started pod test-webserver-acfa8172-db3d-4762-ae99-db3c76a5d851 in namespace container-probe-2964
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 21:00:50.419: INFO: Initial restart count of pod test-webserver-acfa8172-db3d-4762-ae99-db3c76a5d851 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:04:52.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2964" for this suite.
•{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":223,"skipped":3677,"failed":0}

------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:04:52.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3038
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-c9db8bac-a7f9-464a-8e56-9b7fb0a2dc63 in namespace container-probe-3038
Jul 22 21:04:54.335: INFO: Started pod liveness-c9db8bac-a7f9-464a-8e56-9b7fb0a2dc63 in namespace container-probe-3038
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 21:04:54.346: INFO: Initial restart count of pod liveness-c9db8bac-a7f9-464a-8e56-9b7fb0a2dc63 is 0
Jul 22 21:05:14.481: INFO: Restart count of pod container-probe-3038/liveness-c9db8bac-a7f9-464a-8e56-9b7fb0a2dc63 is now 1 (20.134508376s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:05:14.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3038" for this suite.
•{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":224,"skipped":3677,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:05:14.530: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:05:15.274: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:05:18.324: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:05:18.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8837" for this suite.
STEP: Destroying namespace "webhook-8837-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":339,"completed":225,"skipped":3692,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:05:18.732: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-8693
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8693
STEP: Creating statefulset with conflicting port in namespace statefulset-8693
STEP: Waiting until pod test-pod will start running in namespace statefulset-8693
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8693
Jul 22 21:05:23.082: INFO: Observed stateful pod in namespace: statefulset-8693, name: ss-0, uid: 3dabf050-7c24-46e9-91bf-14b05782e458, status phase: Pending. Waiting for statefulset controller to delete.
Jul 22 21:05:23.093: INFO: Observed stateful pod in namespace: statefulset-8693, name: ss-0, uid: 3dabf050-7c24-46e9-91bf-14b05782e458, status phase: Failed. Waiting for statefulset controller to delete.
Jul 22 21:05:23.126: INFO: Observed stateful pod in namespace: statefulset-8693, name: ss-0, uid: 3dabf050-7c24-46e9-91bf-14b05782e458, status phase: Failed. Waiting for statefulset controller to delete.
Jul 22 21:05:23.129: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8693
STEP: Removing pod with conflicting port in namespace statefulset-8693
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8693 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul 22 21:05:25.166: INFO: Deleting all statefulset in ns statefulset-8693
Jul 22 21:05:25.177: INFO: Scaling statefulset ss to 0
Jul 22 21:05:35.224: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:05:35.236: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:05:35.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8693" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":339,"completed":226,"skipped":3712,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:05:35.303: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-71ecddd2-b604-4730-9a72-70ac5b22d9c5
STEP: Creating a pod to test consume secrets
Jul 22 21:05:35.524: INFO: Waiting up to 5m0s for pod "pod-secrets-7c2e19ce-3785-4696-a2b4-6741c501fd97" in namespace "secrets-3769" to be "Succeeded or Failed"
Jul 22 21:05:35.534: INFO: Pod "pod-secrets-7c2e19ce-3785-4696-a2b4-6741c501fd97": Phase="Pending", Reason="", readiness=false. Elapsed: 10.299516ms
Jul 22 21:05:37.590: INFO: Pod "pod-secrets-7c2e19ce-3785-4696-a2b4-6741c501fd97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.066171375s
STEP: Saw pod success
Jul 22 21:05:37.590: INFO: Pod "pod-secrets-7c2e19ce-3785-4696-a2b4-6741c501fd97" satisfied condition "Succeeded or Failed"
Jul 22 21:05:37.601: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-secrets-7c2e19ce-3785-4696-a2b4-6741c501fd97 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:05:37.703: INFO: Waiting for pod pod-secrets-7c2e19ce-3785-4696-a2b4-6741c501fd97 to disappear
Jul 22 21:05:37.713: INFO: Pod pod-secrets-7c2e19ce-3785-4696-a2b4-6741c501fd97 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:05:37.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3769" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":227,"skipped":3733,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:05:37.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-1169/configmap-test-42de642d-9e6c-433b-ae20-315609834b04
STEP: Creating a pod to test consume configMaps
Jul 22 21:05:38.001: INFO: Waiting up to 5m0s for pod "pod-configmaps-7746e39d-5928-4386-9f70-4a675c75c162" in namespace "configmap-1169" to be "Succeeded or Failed"
Jul 22 21:05:38.012: INFO: Pod "pod-configmaps-7746e39d-5928-4386-9f70-4a675c75c162": Phase="Pending", Reason="", readiness=false. Elapsed: 10.925257ms
Jul 22 21:05:40.024: INFO: Pod "pod-configmaps-7746e39d-5928-4386-9f70-4a675c75c162": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022969299s
STEP: Saw pod success
Jul 22 21:05:40.024: INFO: Pod "pod-configmaps-7746e39d-5928-4386-9f70-4a675c75c162" satisfied condition "Succeeded or Failed"
Jul 22 21:05:40.035: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-configmaps-7746e39d-5928-4386-9f70-4a675c75c162 container env-test: <nil>
STEP: delete the pod
Jul 22 21:05:40.073: INFO: Waiting for pod pod-configmaps-7746e39d-5928-4386-9f70-4a675c75c162 to disappear
Jul 22 21:05:40.083: INFO: Pod pod-configmaps-7746e39d-5928-4386-9f70-4a675c75c162 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:05:40.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1169" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":339,"completed":228,"skipped":3743,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:05:40.114: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 22 21:05:40.423: INFO: Number of nodes with available pods: 0
Jul 22 21:05:40.423: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:05:41.455: INFO: Number of nodes with available pods: 0
Jul 22 21:05:41.455: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:05:42.458: INFO: Number of nodes with available pods: 1
Jul 22 21:05:42.458: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:05:43.456: INFO: Number of nodes with available pods: 2
Jul 22 21:05:43.456: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 22 21:05:43.517: INFO: Number of nodes with available pods: 1
Jul 22 21:05:43.517: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:05:44.555: INFO: Number of nodes with available pods: 1
Jul 22 21:05:44.556: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:05:45.549: INFO: Number of nodes with available pods: 2
Jul 22 21:05:45.549: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8172, will wait for the garbage collector to delete the pods
Jul 22 21:05:45.647: INFO: Deleting DaemonSet.extensions daemon-set took: 13.422427ms
Jul 22 21:05:45.748: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.184309ms
Jul 22 21:05:57.860: INFO: Number of nodes with available pods: 0
Jul 22 21:05:57.860: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 21:05:57.870: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37856"},"items":null}

Jul 22 21:05:57.881: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37856"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:05:57.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8172" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":339,"completed":229,"skipped":3747,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:05:57.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1215
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul 22 21:05:58.170: INFO: Waiting up to 5m0s for pod "downward-api-9fec4c29-20ac-409c-b181-c3dd86718e71" in namespace "downward-api-1215" to be "Succeeded or Failed"
Jul 22 21:05:58.181: INFO: Pod "downward-api-9fec4c29-20ac-409c-b181-c3dd86718e71": Phase="Pending", Reason="", readiness=false. Elapsed: 10.719014ms
Jul 22 21:06:00.193: INFO: Pod "downward-api-9fec4c29-20ac-409c-b181-c3dd86718e71": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022713107s
Jul 22 21:06:02.207: INFO: Pod "downward-api-9fec4c29-20ac-409c-b181-c3dd86718e71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036175296s
STEP: Saw pod success
Jul 22 21:06:02.207: INFO: Pod "downward-api-9fec4c29-20ac-409c-b181-c3dd86718e71" satisfied condition "Succeeded or Failed"
Jul 22 21:06:02.217: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downward-api-9fec4c29-20ac-409c-b181-c3dd86718e71 container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:06:02.257: INFO: Waiting for pod downward-api-9fec4c29-20ac-409c-b181-c3dd86718e71 to disappear
Jul 22 21:06:02.268: INFO: Pod downward-api-9fec4c29-20ac-409c-b181-c3dd86718e71 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:06:02.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1215" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":339,"completed":230,"skipped":3764,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:06:02.302: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-5472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:06:03.210: INFO: Checking APIGroup: apiregistration.k8s.io
Jul 22 21:06:03.219: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jul 22 21:06:03.219: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.219: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jul 22 21:06:03.219: INFO: Checking APIGroup: apps
Jul 22 21:06:03.228: INFO: PreferredVersion.GroupVersion: apps/v1
Jul 22 21:06:03.228: INFO: Versions found [{apps/v1 v1}]
Jul 22 21:06:03.228: INFO: apps/v1 matches apps/v1
Jul 22 21:06:03.228: INFO: Checking APIGroup: events.k8s.io
Jul 22 21:06:03.237: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jul 22 21:06:03.237: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.237: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jul 22 21:06:03.237: INFO: Checking APIGroup: authentication.k8s.io
Jul 22 21:06:03.247: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jul 22 21:06:03.247: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.247: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jul 22 21:06:03.247: INFO: Checking APIGroup: authorization.k8s.io
Jul 22 21:06:03.258: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jul 22 21:06:03.258: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.265: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jul 22 21:06:03.265: INFO: Checking APIGroup: autoscaling
Jul 22 21:06:03.285: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jul 22 21:06:03.285: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jul 22 21:06:03.285: INFO: autoscaling/v1 matches autoscaling/v1
Jul 22 21:06:03.285: INFO: Checking APIGroup: batch
Jul 22 21:06:03.385: INFO: PreferredVersion.GroupVersion: batch/v1
Jul 22 21:06:03.385: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jul 22 21:06:03.385: INFO: batch/v1 matches batch/v1
Jul 22 21:06:03.385: INFO: Checking APIGroup: certificates.k8s.io
Jul 22 21:06:03.394: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jul 22 21:06:03.394: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.394: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jul 22 21:06:03.394: INFO: Checking APIGroup: networking.k8s.io
Jul 22 21:06:03.486: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jul 22 21:06:03.486: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.486: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jul 22 21:06:03.486: INFO: Checking APIGroup: extensions
Jul 22 21:06:03.586: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jul 22 21:06:03.586: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jul 22 21:06:03.586: INFO: extensions/v1beta1 matches extensions/v1beta1
Jul 22 21:06:03.586: INFO: Checking APIGroup: policy
Jul 22 21:06:03.596: INFO: PreferredVersion.GroupVersion: policy/v1
Jul 22 21:06:03.596: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jul 22 21:06:03.596: INFO: policy/v1 matches policy/v1
Jul 22 21:06:03.596: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jul 22 21:06:03.686: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jul 22 21:06:03.686: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.686: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jul 22 21:06:03.686: INFO: Checking APIGroup: storage.k8s.io
Jul 22 21:06:03.695: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jul 22 21:06:03.695: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.695: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jul 22 21:06:03.695: INFO: Checking APIGroup: admissionregistration.k8s.io
Jul 22 21:06:03.704: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jul 22 21:06:03.704: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.704: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jul 22 21:06:03.704: INFO: Checking APIGroup: apiextensions.k8s.io
Jul 22 21:06:03.714: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jul 22 21:06:03.714: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.714: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jul 22 21:06:03.714: INFO: Checking APIGroup: scheduling.k8s.io
Jul 22 21:06:03.723: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jul 22 21:06:03.723: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.723: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jul 22 21:06:03.723: INFO: Checking APIGroup: coordination.k8s.io
Jul 22 21:06:03.732: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jul 22 21:06:03.732: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.732: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jul 22 21:06:03.732: INFO: Checking APIGroup: node.k8s.io
Jul 22 21:06:03.741: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jul 22 21:06:03.741: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.741: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jul 22 21:06:03.741: INFO: Checking APIGroup: discovery.k8s.io
Jul 22 21:06:03.750: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jul 22 21:06:03.750: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.750: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jul 22 21:06:03.750: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jul 22 21:06:03.785: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Jul 22 21:06:03.785: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.785: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Jul 22 21:06:03.785: INFO: Checking APIGroup: autoscaling.k8s.io
Jul 22 21:06:03.794: INFO: PreferredVersion.GroupVersion: autoscaling.k8s.io/v1
Jul 22 21:06:03.794: INFO: Versions found [{autoscaling.k8s.io/v1 v1} {autoscaling.k8s.io/v1beta2 v1beta2}]
Jul 22 21:06:03.794: INFO: autoscaling.k8s.io/v1 matches autoscaling.k8s.io/v1
Jul 22 21:06:03.794: INFO: Checking APIGroup: crd.projectcalico.org
Jul 22 21:06:03.804: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jul 22 21:06:03.804: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jul 22 21:06:03.804: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jul 22 21:06:03.804: INFO: Checking APIGroup: cert.gardener.cloud
Jul 22 21:06:03.813: INFO: PreferredVersion.GroupVersion: cert.gardener.cloud/v1alpha1
Jul 22 21:06:03.813: INFO: Versions found [{cert.gardener.cloud/v1alpha1 v1alpha1}]
Jul 22 21:06:03.813: INFO: cert.gardener.cloud/v1alpha1 matches cert.gardener.cloud/v1alpha1
Jul 22 21:06:03.813: INFO: Checking APIGroup: dns.gardener.cloud
Jul 22 21:06:03.822: INFO: PreferredVersion.GroupVersion: dns.gardener.cloud/v1alpha1
Jul 22 21:06:03.823: INFO: Versions found [{dns.gardener.cloud/v1alpha1 v1alpha1}]
Jul 22 21:06:03.823: INFO: dns.gardener.cloud/v1alpha1 matches dns.gardener.cloud/v1alpha1
Jul 22 21:06:03.823: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jul 22 21:06:03.832: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1beta1
Jul 22 21:06:03.832: INFO: Versions found [{snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.832: INFO: snapshot.storage.k8s.io/v1beta1 matches snapshot.storage.k8s.io/v1beta1
Jul 22 21:06:03.832: INFO: Checking APIGroup: metrics.k8s.io
Jul 22 21:06:03.841: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jul 22 21:06:03.841: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jul 22 21:06:03.841: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:06:03.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5472" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":339,"completed":231,"skipped":3772,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:06:03.874: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4747
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:06:15.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4747" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":339,"completed":232,"skipped":3776,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:06:15.186: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3231
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 22 21:06:15.416: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3231  34d44119-c435-4154-ae1d-e5a71ed795f0 38018 0 2021-07-22 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:06:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:06:15.416: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3231  34d44119-c435-4154-ae1d-e5a71ed795f0 38018 0 2021-07-22 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:06:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 22 21:06:25.440: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3231  34d44119-c435-4154-ae1d-e5a71ed795f0 38081 0 2021-07-22 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:06:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:06:25.440: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3231  34d44119-c435-4154-ae1d-e5a71ed795f0 38081 0 2021-07-22 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:06:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 22 21:06:35.465: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3231  34d44119-c435-4154-ae1d-e5a71ed795f0 38133 0 2021-07-22 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:06:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:06:35.465: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3231  34d44119-c435-4154-ae1d-e5a71ed795f0 38133 0 2021-07-22 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:06:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 22 21:06:45.479: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3231  34d44119-c435-4154-ae1d-e5a71ed795f0 38189 0 2021-07-22 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:06:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:06:45.480: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3231  34d44119-c435-4154-ae1d-e5a71ed795f0 38189 0 2021-07-22 21:06:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-22 21:06:25 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 22 21:06:55.502: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3231  e6d80e50-f5fd-4503-b80a-69eb308de7ef 38243 0 2021-07-22 21:06:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-22 21:06:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:06:55.502: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3231  e6d80e50-f5fd-4503-b80a-69eb308de7ef 38243 0 2021-07-22 21:06:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-22 21:06:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 22 21:07:05.516: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3231  e6d80e50-f5fd-4503-b80a-69eb308de7ef 38320 0 2021-07-22 21:06:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-22 21:06:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 22 21:07:05.516: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3231  e6d80e50-f5fd-4503-b80a-69eb308de7ef 38320 0 2021-07-22 21:06:55 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-22 21:06:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:07:15.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3231" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":339,"completed":233,"skipped":3800,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:07:15.551: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:07:15.766: INFO: Waiting up to 5m0s for pod "downwardapi-volume-906499c5-e793-409f-9b27-b04e54ecd890" in namespace "downward-api-634" to be "Succeeded or Failed"
Jul 22 21:07:15.776: INFO: Pod "downwardapi-volume-906499c5-e793-409f-9b27-b04e54ecd890": Phase="Pending", Reason="", readiness=false. Elapsed: 10.37751ms
Jul 22 21:07:17.790: INFO: Pod "downwardapi-volume-906499c5-e793-409f-9b27-b04e54ecd890": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023835251s
STEP: Saw pod success
Jul 22 21:07:17.790: INFO: Pod "downwardapi-volume-906499c5-e793-409f-9b27-b04e54ecd890" satisfied condition "Succeeded or Failed"
Jul 22 21:07:17.801: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-906499c5-e793-409f-9b27-b04e54ecd890 container client-container: <nil>
STEP: delete the pod
Jul 22 21:07:17.836: INFO: Waiting for pod downwardapi-volume-906499c5-e793-409f-9b27-b04e54ecd890 to disappear
Jul 22 21:07:17.847: INFO: Pod downwardapi-volume-906499c5-e793-409f-9b27-b04e54ecd890 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:07:17.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-634" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":234,"skipped":3805,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:07:17.879: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-294
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 22 21:07:18.090: INFO: Waiting up to 5m0s for pod "pod-5948d9ee-14e8-44ff-ab82-83efef7a8bda" in namespace "emptydir-294" to be "Succeeded or Failed"
Jul 22 21:07:18.100: INFO: Pod "pod-5948d9ee-14e8-44ff-ab82-83efef7a8bda": Phase="Pending", Reason="", readiness=false. Elapsed: 10.474829ms
Jul 22 21:07:20.112: INFO: Pod "pod-5948d9ee-14e8-44ff-ab82-83efef7a8bda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021953467s
Jul 22 21:07:22.125: INFO: Pod "pod-5948d9ee-14e8-44ff-ab82-83efef7a8bda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034842399s
STEP: Saw pod success
Jul 22 21:07:22.125: INFO: Pod "pod-5948d9ee-14e8-44ff-ab82-83efef7a8bda" satisfied condition "Succeeded or Failed"
Jul 22 21:07:22.135: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-5948d9ee-14e8-44ff-ab82-83efef7a8bda container test-container: <nil>
STEP: delete the pod
Jul 22 21:07:22.211: INFO: Waiting for pod pod-5948d9ee-14e8-44ff-ab82-83efef7a8bda to disappear
Jul 22 21:07:22.221: INFO: Pod pod-5948d9ee-14e8-44ff-ab82-83efef7a8bda no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:07:22.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-294" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":235,"skipped":3809,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:07:22.253: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9265
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:07:22.874: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 22 21:07:24.940: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584842, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584842, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584842, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762584842, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:07:27.976: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:07:38.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9265" for this suite.
STEP: Destroying namespace "webhook-9265-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":339,"completed":236,"skipped":3813,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:07:39.292: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3059
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:07:39.712: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e5f34562-dea9-481c-b503-f7ae1ae759a2", Controller:(*bool)(0xc0049c225a), BlockOwnerDeletion:(*bool)(0xc0049c225b)}}
Jul 22 21:07:39.728: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b4df7f50-ee5c-4757-aca4-1ccfb30a65ac", Controller:(*bool)(0xc0049c24d6), BlockOwnerDeletion:(*bool)(0xc0049c24d7)}}
Jul 22 21:07:39.744: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a3b22eb8-69b4-4739-a1d1-77f644f436e8", Controller:(*bool)(0xc004c101f6), BlockOwnerDeletion:(*bool)(0xc004c101f7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:07:44.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3059" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":339,"completed":237,"skipped":3827,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:07:44.802: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3544
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:07:45.024: INFO: Waiting up to 5m0s for pod "downwardapi-volume-768dd0a9-3008-40d7-aafe-fddb91b5334c" in namespace "projected-3544" to be "Succeeded or Failed"
Jul 22 21:07:45.085: INFO: Pod "downwardapi-volume-768dd0a9-3008-40d7-aafe-fddb91b5334c": Phase="Pending", Reason="", readiness=false. Elapsed: 61.144011ms
Jul 22 21:07:47.101: INFO: Pod "downwardapi-volume-768dd0a9-3008-40d7-aafe-fddb91b5334c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077174648s
Jul 22 21:07:49.114: INFO: Pod "downwardapi-volume-768dd0a9-3008-40d7-aafe-fddb91b5334c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.090227525s
STEP: Saw pod success
Jul 22 21:07:49.114: INFO: Pod "downwardapi-volume-768dd0a9-3008-40d7-aafe-fddb91b5334c" satisfied condition "Succeeded or Failed"
Jul 22 21:07:49.125: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-768dd0a9-3008-40d7-aafe-fddb91b5334c container client-container: <nil>
STEP: delete the pod
Jul 22 21:07:49.202: INFO: Waiting for pod downwardapi-volume-768dd0a9-3008-40d7-aafe-fddb91b5334c to disappear
Jul 22 21:07:49.213: INFO: Pod downwardapi-volume-768dd0a9-3008-40d7-aafe-fddb91b5334c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:07:49.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3544" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":339,"completed":238,"skipped":3846,"failed":0}
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:07:49.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8303
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jul 22 21:07:49.462: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:07:51.487: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jul 22 21:07:51.705: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:07:53.717: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jul 22 21:07:53.741: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:07:53.753: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:07:55.753: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:07:55.764: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:07:57.753: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:07:57.766: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:07:59.754: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:07:59.766: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:08:01.754: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:08:01.766: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:08:03.754: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:08:03.765: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:08:05.754: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:08:05.766: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 21:08:07.754: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 21:08:07.765: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:08:07.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8303" for this suite.
•{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":339,"completed":239,"skipped":3849,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:08:07.816: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-5581f3f7-8902-4b52-ba9f-5cd4636b9c21
STEP: Creating a pod to test consume configMaps
Jul 22 21:08:08.039: INFO: Waiting up to 5m0s for pod "pod-configmaps-a4377825-dd61-4cf8-9949-4e9df2c41bc5" in namespace "configmap-3477" to be "Succeeded or Failed"
Jul 22 21:08:08.050: INFO: Pod "pod-configmaps-a4377825-dd61-4cf8-9949-4e9df2c41bc5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.853159ms
Jul 22 21:08:10.062: INFO: Pod "pod-configmaps-a4377825-dd61-4cf8-9949-4e9df2c41bc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022835356s
STEP: Saw pod success
Jul 22 21:08:10.062: INFO: Pod "pod-configmaps-a4377825-dd61-4cf8-9949-4e9df2c41bc5" satisfied condition "Succeeded or Failed"
Jul 22 21:08:10.073: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-configmaps-a4377825-dd61-4cf8-9949-4e9df2c41bc5 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:08:10.109: INFO: Waiting for pod pod-configmaps-a4377825-dd61-4cf8-9949-4e9df2c41bc5 to disappear
Jul 22 21:08:10.119: INFO: Pod pod-configmaps-a4377825-dd61-4cf8-9949-4e9df2c41bc5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:08:10.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3477" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":240,"skipped":3855,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:08:10.151: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4079
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-4079
Jul 22 21:08:10.391: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:08:12.403: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jul 22 21:08:12.415: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4079 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 22 21:08:13.038: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul 22 21:08:13.038: INFO: stdout: "iptables"
Jul 22 21:08:13.038: INFO: proxyMode: iptables
Jul 22 21:08:13.055: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 22 21:08:13.084: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-4079
STEP: creating replication controller affinity-nodeport-timeout in namespace services-4079
I0722 21:08:13.116191    5669 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-4079, replica count: 3
I0722 21:08:16.168113    5669 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 21:08:16.210: INFO: Creating new exec pod
Jul 22 21:08:19.270: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4079 exec execpod-affinityl8tx2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jul 22 21:08:19.679: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jul 22 21:08:19.679: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 21:08:19.679: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4079 exec execpod-affinityl8tx2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.66.22.177 80'
Jul 22 21:08:20.106: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.66.22.177 80\nConnection to 100.66.22.177 80 port [tcp/http] succeeded!\n"
Jul 22 21:08:20.106: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 21:08:20.106: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4079 exec execpod-affinityl8tx2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.3 31552'
Jul 22 21:08:20.506: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.250.0.3 31552\nConnection to 10.250.0.3 31552 port [tcp/*] succeeded!\n"
Jul 22 21:08:20.506: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 21:08:20.506: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4079 exec execpod-affinityl8tx2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.250.0.2 31552'
Jul 22 21:08:20.922: INFO: stderr: "+ nc -v -t -w 2 10.250.0.2 31552\nConnection to 10.250.0.2 31552 port [tcp/*] succeeded!\n+ echo hostName\n"
Jul 22 21:08:20.922: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 21:08:20.922: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4079 exec execpod-affinityl8tx2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.250.0.3:31552/ ; done'
Jul 22 21:08:21.414: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n"
Jul 22 21:08:21.414: INFO: stdout: "\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks\naffinity-nodeport-timeout-hjxks"
Jul 22 21:08:21.414: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.414: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.414: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.414: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.414: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.414: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.414: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.414: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.415: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.415: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.415: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.415: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.415: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.415: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.415: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.415: INFO: Received response from host: affinity-nodeport-timeout-hjxks
Jul 22 21:08:21.415: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4079 exec execpod-affinityl8tx2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.0.3:31552/'
Jul 22 21:08:21.738: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n"
Jul 22 21:08:21.738: INFO: stdout: "affinity-nodeport-timeout-hjxks"
Jul 22 21:08:41.739: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4079 exec execpod-affinityl8tx2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.0.3:31552/'
Jul 22 21:08:42.102: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n"
Jul 22 21:08:42.102: INFO: stdout: "affinity-nodeport-timeout-hjxks"
Jul 22 21:09:02.105: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-4079 exec execpod-affinityl8tx2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.250.0.3:31552/'
Jul 22 21:09:02.463: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.250.0.3:31552/\n"
Jul 22 21:09:02.463: INFO: stdout: "affinity-nodeport-timeout-r89qt"
Jul 22 21:09:02.463: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-4079, will wait for the garbage collector to delete the pods
Jul 22 21:09:02.555: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 12.800375ms
Jul 22 21:09:02.656: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.408689ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:09:17.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4079" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":241,"skipped":3858,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:09:17.912: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1767
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Jul 22 21:09:18.101: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-1767 proxy --unix-socket=/tmp/kubectl-proxy-unix146431639/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:09:18.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1767" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":339,"completed":242,"skipped":3866,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:09:18.209: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6605
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-6ff90587-0cf6-4170-a9cf-b062c98f35ca
STEP: Creating a pod to test consume secrets
Jul 22 21:09:18.427: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-750aa62d-20ed-4ff7-b08a-10ca8fc7a392" in namespace "projected-6605" to be "Succeeded or Failed"
Jul 22 21:09:18.438: INFO: Pod "pod-projected-secrets-750aa62d-20ed-4ff7-b08a-10ca8fc7a392": Phase="Pending", Reason="", readiness=false. Elapsed: 10.421468ms
Jul 22 21:09:20.449: INFO: Pod "pod-projected-secrets-750aa62d-20ed-4ff7-b08a-10ca8fc7a392": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021788478s
STEP: Saw pod success
Jul 22 21:09:20.449: INFO: Pod "pod-projected-secrets-750aa62d-20ed-4ff7-b08a-10ca8fc7a392" satisfied condition "Succeeded or Failed"
Jul 22 21:09:20.460: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-secrets-750aa62d-20ed-4ff7-b08a-10ca8fc7a392 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:09:20.503: INFO: Waiting for pod pod-projected-secrets-750aa62d-20ed-4ff7-b08a-10ca8fc7a392 to disappear
Jul 22 21:09:20.514: INFO: Pod pod-projected-secrets-750aa62d-20ed-4ff7-b08a-10ca8fc7a392 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:09:20.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6605" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":243,"skipped":3870,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:09:20.546: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:09:24.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7484" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":339,"completed":244,"skipped":3892,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:09:24.919: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-x88b
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 21:09:25.150: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-x88b" in namespace "subpath-5264" to be "Succeeded or Failed"
Jul 22 21:09:25.161: INFO: Pod "pod-subpath-test-downwardapi-x88b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.901658ms
Jul 22 21:09:27.174: INFO: Pod "pod-subpath-test-downwardapi-x88b": Phase="Running", Reason="", readiness=true. Elapsed: 2.023858746s
Jul 22 21:09:29.187: INFO: Pod "pod-subpath-test-downwardapi-x88b": Phase="Running", Reason="", readiness=true. Elapsed: 4.036608835s
Jul 22 21:09:31.199: INFO: Pod "pod-subpath-test-downwardapi-x88b": Phase="Running", Reason="", readiness=true. Elapsed: 6.048843897s
Jul 22 21:09:33.213: INFO: Pod "pod-subpath-test-downwardapi-x88b": Phase="Running", Reason="", readiness=true. Elapsed: 8.062615628s
Jul 22 21:09:35.226: INFO: Pod "pod-subpath-test-downwardapi-x88b": Phase="Running", Reason="", readiness=true. Elapsed: 10.075237395s
Jul 22 21:09:37.239: INFO: Pod "pod-subpath-test-downwardapi-x88b": Phase="Running", Reason="", readiness=true. Elapsed: 12.08824286s
Jul 22 21:09:39.286: INFO: Pod "pod-subpath-test-downwardapi-x88b": Phase="Running", Reason="", readiness=true. Elapsed: 14.136073695s
Jul 22 21:09:41.299: INFO: Pod "pod-subpath-test-downwardapi-x88b": Phase="Running", Reason="", readiness=true. Elapsed: 16.148506145s
Jul 22 21:09:43.311: INFO: Pod "pod-subpath-test-downwardapi-x88b": Phase="Running", Reason="", readiness=true. Elapsed: 18.16108959s
Jul 22 21:09:45.324: INFO: Pod "pod-subpath-test-downwardapi-x88b": Phase="Running", Reason="", readiness=true. Elapsed: 20.173866121s
Jul 22 21:09:47.337: INFO: Pod "pod-subpath-test-downwardapi-x88b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.186405647s
STEP: Saw pod success
Jul 22 21:09:47.337: INFO: Pod "pod-subpath-test-downwardapi-x88b" satisfied condition "Succeeded or Failed"
Jul 22 21:09:47.348: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-subpath-test-downwardapi-x88b container test-container-subpath-downwardapi-x88b: <nil>
STEP: delete the pod
Jul 22 21:09:47.382: INFO: Waiting for pod pod-subpath-test-downwardapi-x88b to disappear
Jul 22 21:09:47.393: INFO: Pod pod-subpath-test-downwardapi-x88b no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-x88b
Jul 22 21:09:47.393: INFO: Deleting pod "pod-subpath-test-downwardapi-x88b" in namespace "subpath-5264"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:09:47.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5264" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":339,"completed":245,"skipped":3899,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:09:47.437: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8283
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-097cfaa9-ee09-4072-8184-1d26c413fa06 in namespace container-probe-8283
Jul 22 21:09:51.678: INFO: Started pod liveness-097cfaa9-ee09-4072-8184-1d26c413fa06 in namespace container-probe-8283
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 21:09:51.690: INFO: Initial restart count of pod liveness-097cfaa9-ee09-4072-8184-1d26c413fa06 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:13:53.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8283" for this suite.
•{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":339,"completed":246,"skipped":3913,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:13:53.362: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-s6l9
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 21:14:27.343: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-s6l9" in namespace "subpath-8781" to be "Succeeded or Failed"
Jul 22 21:14:27.354: INFO: Pod "pod-subpath-test-configmap-s6l9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.041324ms
Jul 22 21:14:29.367: INFO: Pod "pod-subpath-test-configmap-s6l9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0233367s
Jul 22 21:14:31.380: INFO: Pod "pod-subpath-test-configmap-s6l9": Phase="Running", Reason="", readiness=true. Elapsed: 4.0362846s
Jul 22 21:14:33.483: INFO: Pod "pod-subpath-test-configmap-s6l9": Phase="Running", Reason="", readiness=true. Elapsed: 6.139896343s
Jul 22 21:14:35.496: INFO: Pod "pod-subpath-test-configmap-s6l9": Phase="Running", Reason="", readiness=true. Elapsed: 8.152552285s
Jul 22 21:14:37.508: INFO: Pod "pod-subpath-test-configmap-s6l9": Phase="Running", Reason="", readiness=true. Elapsed: 10.165048711s
Jul 22 21:14:39.521: INFO: Pod "pod-subpath-test-configmap-s6l9": Phase="Running", Reason="", readiness=true. Elapsed: 12.177414557s
Jul 22 21:14:41.533: INFO: Pod "pod-subpath-test-configmap-s6l9": Phase="Running", Reason="", readiness=true. Elapsed: 14.189304487s
Jul 22 21:14:43.545: INFO: Pod "pod-subpath-test-configmap-s6l9": Phase="Running", Reason="", readiness=true. Elapsed: 16.201720923s
Jul 22 21:14:45.558: INFO: Pod "pod-subpath-test-configmap-s6l9": Phase="Running", Reason="", readiness=true. Elapsed: 18.21443612s
Jul 22 21:14:47.571: INFO: Pod "pod-subpath-test-configmap-s6l9": Phase="Running", Reason="", readiness=true. Elapsed: 20.227511858s
Jul 22 21:14:49.583: INFO: Pod "pod-subpath-test-configmap-s6l9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.239977175s
STEP: Saw pod success
Jul 22 21:14:49.583: INFO: Pod "pod-subpath-test-configmap-s6l9" satisfied condition "Succeeded or Failed"
Jul 22 21:14:49.594: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-subpath-test-configmap-s6l9 container test-container-subpath-configmap-s6l9: <nil>
STEP: delete the pod
Jul 22 21:14:49.635: INFO: Waiting for pod pod-subpath-test-configmap-s6l9 to disappear
Jul 22 21:14:49.645: INFO: Pod pod-subpath-test-configmap-s6l9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-s6l9
Jul 22 21:14:49.645: INFO: Deleting pod "pod-subpath-test-configmap-s6l9" in namespace "subpath-8781"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:14:49.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8781" for this suite.
•{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":339,"completed":247,"skipped":3916,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:14:49.692: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6713
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 22 21:14:49.910: INFO: Waiting up to 5m0s for pod "pod-173d4666-76f5-4d33-b319-076a8eeb7b58" in namespace "emptydir-6713" to be "Succeeded or Failed"
Jul 22 21:14:49.920: INFO: Pod "pod-173d4666-76f5-4d33-b319-076a8eeb7b58": Phase="Pending", Reason="", readiness=false. Elapsed: 10.381035ms
Jul 22 21:14:51.931: INFO: Pod "pod-173d4666-76f5-4d33-b319-076a8eeb7b58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021759707s
Jul 22 21:14:53.944: INFO: Pod "pod-173d4666-76f5-4d33-b319-076a8eeb7b58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034275123s
STEP: Saw pod success
Jul 22 21:14:53.944: INFO: Pod "pod-173d4666-76f5-4d33-b319-076a8eeb7b58" satisfied condition "Succeeded or Failed"
Jul 22 21:14:53.955: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-173d4666-76f5-4d33-b319-076a8eeb7b58 container test-container: <nil>
STEP: delete the pod
Jul 22 21:14:53.991: INFO: Waiting for pod pod-173d4666-76f5-4d33-b319-076a8eeb7b58 to disappear
Jul 22 21:14:54.002: INFO: Pod pod-173d4666-76f5-4d33-b319-076a8eeb7b58 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:14:54.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6713" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":248,"skipped":3917,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:14:54.033: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:15:05.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7553" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":339,"completed":249,"skipped":3941,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:15:05.427: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6798
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-6163
STEP: Creating secret with name secret-test-520a119c-1344-45e3-8c2f-d5e7ef22c917
STEP: Creating a pod to test consume secrets
Jul 22 21:15:05.836: INFO: Waiting up to 5m0s for pod "pod-secrets-099deedd-023f-48af-8b5c-f88ff1d61723" in namespace "secrets-6798" to be "Succeeded or Failed"
Jul 22 21:15:05.846: INFO: Pod "pod-secrets-099deedd-023f-48af-8b5c-f88ff1d61723": Phase="Pending", Reason="", readiness=false. Elapsed: 10.408269ms
Jul 22 21:15:07.859: INFO: Pod "pod-secrets-099deedd-023f-48af-8b5c-f88ff1d61723": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022794926s
STEP: Saw pod success
Jul 22 21:15:07.859: INFO: Pod "pod-secrets-099deedd-023f-48af-8b5c-f88ff1d61723" satisfied condition "Succeeded or Failed"
Jul 22 21:15:07.869: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-secrets-099deedd-023f-48af-8b5c-f88ff1d61723 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:15:07.906: INFO: Waiting for pod pod-secrets-099deedd-023f-48af-8b5c-f88ff1d61723 to disappear
Jul 22 21:15:07.916: INFO: Pod pod-secrets-099deedd-023f-48af-8b5c-f88ff1d61723 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:15:07.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6798" for this suite.
STEP: Destroying namespace "secret-namespace-6163" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":339,"completed":250,"skipped":3941,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:15:07.963: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1308
STEP: creating the pod
Jul 22 21:15:08.162: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4467 create -f -'
Jul 22 21:15:08.569: INFO: stderr: ""
Jul 22 21:15:08.569: INFO: stdout: "pod/pause created\n"
Jul 22 21:15:08.569: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 22 21:15:08.569: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4467" to be "running and ready"
Jul 22 21:15:08.580: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.776631ms
Jul 22 21:15:10.591: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.022372531s
Jul 22 21:15:10.591: INFO: Pod "pause" satisfied condition "running and ready"
Jul 22 21:15:10.591: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 22 21:15:10.592: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4467 label pods pause testing-label=testing-label-value'
Jul 22 21:15:10.738: INFO: stderr: ""
Jul 22 21:15:10.738: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 22 21:15:10.739: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4467 get pod pause -L testing-label'
Jul 22 21:15:10.864: INFO: stderr: ""
Jul 22 21:15:10.864: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 22 21:15:10.864: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4467 label pods pause testing-label-'
Jul 22 21:15:10.988: INFO: stderr: ""
Jul 22 21:15:10.988: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 22 21:15:10.988: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4467 get pod pause -L testing-label'
Jul 22 21:15:11.097: INFO: stderr: ""
Jul 22 21:15:11.097: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: using delete to clean up resources
Jul 22 21:15:11.097: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4467 delete --grace-period=0 --force -f -'
Jul 22 21:15:11.233: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 21:15:11.233: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 22 21:15:11.233: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4467 get rc,svc -l name=pause --no-headers'
Jul 22 21:15:11.375: INFO: stderr: "No resources found in kubectl-4467 namespace.\n"
Jul 22 21:15:11.375: INFO: stdout: ""
Jul 22 21:15:11.375: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-4467 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 21:15:11.482: INFO: stderr: ""
Jul 22 21:15:11.482: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:15:11.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4467" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":339,"completed":251,"skipped":3976,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:15:11.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8515
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-e44ca53e-3c4d-4d26-9f77-2b4697fa55ef
STEP: Creating configMap with name cm-test-opt-upd-e3ce7ed9-5038-4059-98f0-20bc5a1a2ef6
STEP: Creating the pod
Jul 22 21:15:11.778: INFO: The status of Pod pod-projected-configmaps-fdb2eacd-5aba-43b6-a6f7-57ab02e686d5 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:15:13.790: INFO: The status of Pod pod-projected-configmaps-fdb2eacd-5aba-43b6-a6f7-57ab02e686d5 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:15:15.791: INFO: The status of Pod pod-projected-configmaps-fdb2eacd-5aba-43b6-a6f7-57ab02e686d5 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-e44ca53e-3c4d-4d26-9f77-2b4697fa55ef
STEP: Updating configmap cm-test-opt-upd-e3ce7ed9-5038-4059-98f0-20bc5a1a2ef6
STEP: Creating configMap with name cm-test-opt-create-20ba7c1d-bcdd-4619-834a-af04ace38a20
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:16:22.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8515" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":252,"skipped":3982,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:16:22.793: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 22 21:16:23.029: INFO: Waiting up to 5m0s for pod "pod-241dff18-fa93-434b-ac1c-3080494fa3dc" in namespace "emptydir-6108" to be "Succeeded or Failed"
Jul 22 21:16:23.040: INFO: Pod "pod-241dff18-fa93-434b-ac1c-3080494fa3dc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.986262ms
Jul 22 21:16:25.052: INFO: Pod "pod-241dff18-fa93-434b-ac1c-3080494fa3dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02340306s
STEP: Saw pod success
Jul 22 21:16:25.052: INFO: Pod "pod-241dff18-fa93-434b-ac1c-3080494fa3dc" satisfied condition "Succeeded or Failed"
Jul 22 21:16:25.064: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-241dff18-fa93-434b-ac1c-3080494fa3dc container test-container: <nil>
STEP: delete the pod
Jul 22 21:16:25.100: INFO: Waiting for pod pod-241dff18-fa93-434b-ac1c-3080494fa3dc to disappear
Jul 22 21:16:25.111: INFO: Pod pod-241dff18-fa93-434b-ac1c-3080494fa3dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:16:25.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6108" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":253,"skipped":3995,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:16:25.143: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 22 21:16:25.366: INFO: Waiting up to 5m0s for pod "pod-7586e961-dff2-4edd-9f5a-758694cb5fc9" in namespace "emptydir-1704" to be "Succeeded or Failed"
Jul 22 21:16:25.377: INFO: Pod "pod-7586e961-dff2-4edd-9f5a-758694cb5fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.860112ms
Jul 22 21:16:27.428: INFO: Pod "pod-7586e961-dff2-4edd-9f5a-758694cb5fc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.061736959s
STEP: Saw pod success
Jul 22 21:16:27.428: INFO: Pod "pod-7586e961-dff2-4edd-9f5a-758694cb5fc9" satisfied condition "Succeeded or Failed"
Jul 22 21:16:27.438: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-7586e961-dff2-4edd-9f5a-758694cb5fc9 container test-container: <nil>
STEP: delete the pod
Jul 22 21:16:27.475: INFO: Waiting for pod pod-7586e961-dff2-4edd-9f5a-758694cb5fc9 to disappear
Jul 22 21:16:27.486: INFO: Pod pod-7586e961-dff2-4edd-9f5a-758694cb5fc9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:16:27.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1704" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":254,"skipped":4000,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:16:27.518: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8100
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 22 21:16:27.746: INFO: Waiting up to 5m0s for pod "pod-6a52d19d-827d-47ae-8de2-716bd9f4d1ce" in namespace "emptydir-8100" to be "Succeeded or Failed"
Jul 22 21:16:27.757: INFO: Pod "pod-6a52d19d-827d-47ae-8de2-716bd9f4d1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 10.749613ms
Jul 22 21:16:29.769: INFO: Pod "pod-6a52d19d-827d-47ae-8de2-716bd9f4d1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02283935s
Jul 22 21:16:31.782: INFO: Pod "pod-6a52d19d-827d-47ae-8de2-716bd9f4d1ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035487968s
STEP: Saw pod success
Jul 22 21:16:31.782: INFO: Pod "pod-6a52d19d-827d-47ae-8de2-716bd9f4d1ce" satisfied condition "Succeeded or Failed"
Jul 22 21:16:31.793: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-6a52d19d-827d-47ae-8de2-716bd9f4d1ce container test-container: <nil>
STEP: delete the pod
Jul 22 21:16:31.830: INFO: Waiting for pod pod-6a52d19d-827d-47ae-8de2-716bd9f4d1ce to disappear
Jul 22 21:16:31.841: INFO: Pod pod-6a52d19d-827d-47ae-8de2-716bd9f4d1ce no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:16:31.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8100" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":255,"skipped":4025,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:16:31.878: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3063
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jul 22 21:16:32.993: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0722 21:16:32.993388    5669 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0722 21:16:32.993419    5669 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0722 21:16:32.993424    5669 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:16:32.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3063" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":339,"completed":256,"skipped":4049,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:16:33.099: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3052
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jul 22 21:16:33.400: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Jul 22 21:16:38.179: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:16:54.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3052" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":339,"completed":257,"skipped":4054,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:16:54.486: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9849
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:17:01.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9849" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":339,"completed":258,"skipped":4065,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:17:01.759: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3457
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:17:02.056: INFO: Create a RollingUpdate DaemonSet
Jul 22 21:17:02.068: INFO: Check that daemon pods launch on every node of the cluster
Jul 22 21:17:02.090: INFO: Number of nodes with available pods: 0
Jul 22 21:17:02.090: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:17:03.123: INFO: Number of nodes with available pods: 0
Jul 22 21:17:03.123: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:17:04.122: INFO: Number of nodes with available pods: 0
Jul 22 21:17:04.122: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:17:05.122: INFO: Number of nodes with available pods: 2
Jul 22 21:17:05.122: INFO: Number of running nodes: 2, number of available pods: 2
Jul 22 21:17:05.122: INFO: Update the DaemonSet to trigger a rollout
Jul 22 21:17:05.146: INFO: Updating DaemonSet daemon-set
Jul 22 21:17:09.200: INFO: Roll back the DaemonSet before rollout is complete
Jul 22 21:17:09.224: INFO: Updating DaemonSet daemon-set
Jul 22 21:17:09.224: INFO: Make sure DaemonSet rollback is complete
Jul 22 21:17:18.261: INFO: Pod daemon-set-4j7gs is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3457, will wait for the garbage collector to delete the pods
Jul 22 21:17:18.382: INFO: Deleting DaemonSet.extensions daemon-set took: 13.106532ms
Jul 22 21:17:18.483: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.861683ms
Jul 22 21:17:26.996: INFO: Number of nodes with available pods: 0
Jul 22 21:17:26.996: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 21:17:27.006: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42759"},"items":null}

Jul 22 21:17:27.017: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42759"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:17:27.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3457" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":339,"completed":259,"skipped":4072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:17:27.084: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-9134
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 22 21:17:27.309: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 22 21:18:27.414: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:18:27.425: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-9165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jul 22 21:18:31.703: INFO: found a healthy node: shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:18:45.885: INFO: pods created so far: [1 1 1]
Jul 22 21:18:45.886: INFO: length of pods created so far: 3
Jul 22 21:19:02.197: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:19:09.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9165" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:19:09.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9134" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":339,"completed":260,"skipped":4113,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:19:09.390: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6895
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6895
Jul 22 21:19:09.623: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:19:11.635: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jul 22 21:19:11.646: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6895 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 22 21:19:12.278: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul 22 21:19:12.278: INFO: stdout: "iptables"
Jul 22 21:19:12.278: INFO: proxyMode: iptables
Jul 22 21:19:12.295: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 22 21:19:12.306: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-6895
STEP: creating replication controller affinity-clusterip-timeout in namespace services-6895
I0722 21:19:12.334890    5669 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-6895, replica count: 3
I0722 21:19:15.385689    5669 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 21:19:15.406: INFO: Creating new exec pod
Jul 22 21:19:18.447: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6895 exec execpod-affinityr9p5j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jul 22 21:19:18.857: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout 80\n+ echo hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jul 22 21:19:18.857: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 21:19:18.857: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6895 exec execpod-affinityr9p5j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.66.219.128 80'
Jul 22 21:19:19.187: INFO: stderr: "+ nc -v -t -w 2 100.66.219.128 80\nConnection to 100.66.219.128 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jul 22 21:19:19.187: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 21:19:19.187: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6895 exec execpod-affinityr9p5j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.66.219.128:80/ ; done'
Jul 22 21:19:19.626: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n"
Jul 22 21:19:19.627: INFO: stdout: "\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr\naffinity-clusterip-timeout-bh8vr"
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Received response from host: affinity-clusterip-timeout-bh8vr
Jul 22 21:19:19.627: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6895 exec execpod-affinityr9p5j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.66.219.128:80/'
Jul 22 21:19:20.053: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n"
Jul 22 21:19:20.053: INFO: stdout: "affinity-clusterip-timeout-bh8vr"
Jul 22 21:19:40.054: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6895 exec execpod-affinityr9p5j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.66.219.128:80/'
Jul 22 21:19:40.426: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.66.219.128:80/\n"
Jul 22 21:19:40.427: INFO: stdout: "affinity-clusterip-timeout-6k8gq"
Jul 22 21:19:40.427: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-6895, will wait for the garbage collector to delete the pods
Jul 22 21:19:40.518: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 12.490709ms
Jul 22 21:19:40.619: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.476181ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:19:47.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6895" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":261,"skipped":4121,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:19:47.972: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3261
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:19:48.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3261" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":339,"completed":262,"skipped":4167,"failed":0}
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:19:48.274: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-4218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jul 22 21:19:48.503: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Jul 22 21:19:48.941: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 22 21:19:51.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:19:53.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:19:55.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:19:57.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:19:59.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762585588, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:20:02.923: INFO: Waited 1.859639143s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jul 22 21:20:03.358: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:20:04.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4218" for this suite.
•{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":339,"completed":263,"skipped":4168,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:20:04.253: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-7852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:20:04.899: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 22 21:20:04.923: INFO: Number of nodes with available pods: 0
Jul 22 21:20:04.923: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 22 21:20:04.996: INFO: Number of nodes with available pods: 0
Jul 22 21:20:04.996: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:06.008: INFO: Number of nodes with available pods: 0
Jul 22 21:20:06.008: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:07.009: INFO: Number of nodes with available pods: 1
Jul 22 21:20:07.009: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 22 21:20:07.064: INFO: Number of nodes with available pods: 0
Jul 22 21:20:07.064: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 22 21:20:07.090: INFO: Number of nodes with available pods: 0
Jul 22 21:20:07.090: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:08.102: INFO: Number of nodes with available pods: 0
Jul 22 21:20:08.102: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:09.101: INFO: Number of nodes with available pods: 0
Jul 22 21:20:09.101: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:10.101: INFO: Number of nodes with available pods: 0
Jul 22 21:20:10.101: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:11.103: INFO: Number of nodes with available pods: 0
Jul 22 21:20:11.103: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:12.102: INFO: Number of nodes with available pods: 0
Jul 22 21:20:12.102: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:13.102: INFO: Number of nodes with available pods: 0
Jul 22 21:20:13.102: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:14.102: INFO: Number of nodes with available pods: 0
Jul 22 21:20:14.102: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:15.102: INFO: Number of nodes with available pods: 0
Jul 22 21:20:15.102: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:16.103: INFO: Number of nodes with available pods: 0
Jul 22 21:20:16.103: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:17.102: INFO: Number of nodes with available pods: 0
Jul 22 21:20:17.102: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:18.102: INFO: Number of nodes with available pods: 0
Jul 22 21:20:18.102: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:19.102: INFO: Number of nodes with available pods: 0
Jul 22 21:20:19.102: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:20.102: INFO: Number of nodes with available pods: 0
Jul 22 21:20:20.102: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:20:21.102: INFO: Number of nodes with available pods: 1
Jul 22 21:20:21.102: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7852, will wait for the garbage collector to delete the pods
Jul 22 21:20:21.201: INFO: Deleting DaemonSet.extensions daemon-set took: 12.809873ms
Jul 22 21:20:21.302: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.554309ms
Jul 22 21:20:24.314: INFO: Number of nodes with available pods: 0
Jul 22 21:20:24.314: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 21:20:24.324: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"44113"},"items":null}

Jul 22 21:20:24.335: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"44113"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:20:24.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7852" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":339,"completed":264,"skipped":4179,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:20:24.425: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-3410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jul 22 21:20:24.631: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 22 21:21:24.733: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:21:24.744: INFO: Starting informer...
STEP: Starting pod...
Jul 22 21:21:24.787: INFO: Pod is running on shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jul 22 21:21:24.828: INFO: Pod wasn't evicted. Proceeding
Jul 22 21:21:24.828: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jul 22 21:22:39.867: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:22:39.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3410" for this suite.
•{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":339,"completed":265,"skipped":4217,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:22:39.901: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-7653
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:22:40.430: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:22:43.482: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:22:43.494: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:22:47.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7653" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":339,"completed":266,"skipped":4239,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:22:47.511: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-80
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 22 21:22:47.734: INFO: Waiting up to 5m0s for pod "pod-4e698f11-695d-4d68-85f8-c73ef4512e55" in namespace "emptydir-80" to be "Succeeded or Failed"
Jul 22 21:22:47.744: INFO: Pod "pod-4e698f11-695d-4d68-85f8-c73ef4512e55": Phase="Pending", Reason="", readiness=false. Elapsed: 10.452749ms
Jul 22 21:22:49.757: INFO: Pod "pod-4e698f11-695d-4d68-85f8-c73ef4512e55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023378019s
STEP: Saw pod success
Jul 22 21:22:49.757: INFO: Pod "pod-4e698f11-695d-4d68-85f8-c73ef4512e55" satisfied condition "Succeeded or Failed"
Jul 22 21:22:49.768: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-4e698f11-695d-4d68-85f8-c73ef4512e55 container test-container: <nil>
STEP: delete the pod
Jul 22 21:22:49.841: INFO: Waiting for pod pod-4e698f11-695d-4d68-85f8-c73ef4512e55 to disappear
Jul 22 21:22:49.852: INFO: Pod pod-4e698f11-695d-4d68-85f8-c73ef4512e55 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:22:49.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-80" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":267,"skipped":4275,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:22:49.911: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8694
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-0606b25c-0a73-42f8-82a6-346815ddf6cd
STEP: Creating secret with name s-test-opt-upd-cd115690-4a62-44e8-847b-60737be0012d
STEP: Creating the pod
Jul 22 21:22:50.223: INFO: The status of Pod pod-projected-secrets-e1ea4c00-0f47-4bed-9dfe-2c75d4d1dde7 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:22:52.235: INFO: The status of Pod pod-projected-secrets-e1ea4c00-0f47-4bed-9dfe-2c75d4d1dde7 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:22:54.237: INFO: The status of Pod pod-projected-secrets-e1ea4c00-0f47-4bed-9dfe-2c75d4d1dde7 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-0606b25c-0a73-42f8-82a6-346815ddf6cd
STEP: Updating secret s-test-opt-upd-cd115690-4a62-44e8-847b-60737be0012d
STEP: Creating secret with name s-test-opt-create-e2a58c6e-d8de-4412-b7da-d884de51f21b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:22:56.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8694" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":268,"skipped":4318,"failed":0}
SS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:22:56.565: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3910
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Jul 22 21:22:56.781: INFO: Waiting up to 5m0s for pod "client-containers-a1087427-3472-4e63-810b-6efde56e9387" in namespace "containers-3910" to be "Succeeded or Failed"
Jul 22 21:22:56.799: INFO: Pod "client-containers-a1087427-3472-4e63-810b-6efde56e9387": Phase="Pending", Reason="", readiness=false. Elapsed: 16.120554ms
Jul 22 21:22:58.811: INFO: Pod "client-containers-a1087427-3472-4e63-810b-6efde56e9387": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028430249s
Jul 22 21:23:00.823: INFO: Pod "client-containers-a1087427-3472-4e63-810b-6efde56e9387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04069333s
STEP: Saw pod success
Jul 22 21:23:00.823: INFO: Pod "client-containers-a1087427-3472-4e63-810b-6efde56e9387" satisfied condition "Succeeded or Failed"
Jul 22 21:23:00.835: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod client-containers-a1087427-3472-4e63-810b-6efde56e9387 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:23:00.871: INFO: Waiting for pod client-containers-a1087427-3472-4e63-810b-6efde56e9387 to disappear
Jul 22 21:23:00.881: INFO: Pod client-containers-a1087427-3472-4e63-810b-6efde56e9387 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:23:00.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3910" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":339,"completed":269,"skipped":4320,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:23:00.913: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3302
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-2rldc in namespace proxy-3302
I0722 21:23:01.145019    5669 runners.go:190] Created replication controller with name: proxy-service-2rldc, namespace: proxy-3302, replica count: 1
I0722 21:23:02.196473    5669 runners.go:190] proxy-service-2rldc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 21:23:03.197315    5669 runners.go:190] proxy-service-2rldc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 21:23:04.198320    5669 runners.go:190] proxy-service-2rldc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 21:23:05.198489    5669 runners.go:190] proxy-service-2rldc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 21:23:06.198666    5669 runners.go:190] proxy-service-2rldc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 21:23:07.198827    5669 runners.go:190] proxy-service-2rldc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 21:23:08.199078    5669 runners.go:190] proxy-service-2rldc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 21:23:09.199369    5669 runners.go:190] proxy-service-2rldc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 21:23:10.199655    5669 runners.go:190] proxy-service-2rldc Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 21:23:11.200188    5669 runners.go:190] proxy-service-2rldc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 21:23:11.211: INFO: setup took 10.095993063s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 22 21:23:11.245: INFO: (0) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 34.445248ms)
Jul 22 21:23:11.246: INFO: (0) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 34.93068ms)
Jul 22 21:23:11.246: INFO: (0) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 34.956551ms)
Jul 22 21:23:11.246: INFO: (0) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 35.064506ms)
Jul 22 21:23:11.288: INFO: (0) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 76.803702ms)
Jul 22 21:23:11.288: INFO: (0) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 76.866687ms)
Jul 22 21:23:11.288: INFO: (0) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 76.917014ms)
Jul 22 21:23:11.288: INFO: (0) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 77.04359ms)
Jul 22 21:23:11.288: INFO: (0) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 76.834425ms)
Jul 22 21:23:11.288: INFO: (0) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 76.872318ms)
Jul 22 21:23:11.288: INFO: (0) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 76.875036ms)
Jul 22 21:23:11.292: INFO: (0) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 81.40282ms)
Jul 22 21:23:11.292: INFO: (0) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 81.328385ms)
Jul 22 21:23:11.292: INFO: (0) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 81.304726ms)
Jul 22 21:23:11.292: INFO: (0) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 81.277073ms)
Jul 22 21:23:11.306: INFO: (0) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 94.909445ms)
Jul 22 21:23:11.321: INFO: (1) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 14.998817ms)
Jul 22 21:23:11.321: INFO: (1) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 14.978718ms)
Jul 22 21:23:11.326: INFO: (1) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 19.945267ms)
Jul 22 21:23:11.326: INFO: (1) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 19.935189ms)
Jul 22 21:23:11.326: INFO: (1) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 19.959693ms)
Jul 22 21:23:11.326: INFO: (1) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 19.973284ms)
Jul 22 21:23:11.326: INFO: (1) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 19.970937ms)
Jul 22 21:23:11.326: INFO: (1) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 20.144254ms)
Jul 22 21:23:11.326: INFO: (1) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 20.104179ms)
Jul 22 21:23:11.326: INFO: (1) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 20.081865ms)
Jul 22 21:23:11.326: INFO: (1) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 20.082281ms)
Jul 22 21:23:11.326: INFO: (1) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 20.100972ms)
Jul 22 21:23:11.379: INFO: (1) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 73.514871ms)
Jul 22 21:23:11.379: INFO: (1) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 73.615664ms)
Jul 22 21:23:11.380: INFO: (1) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 73.548997ms)
Jul 22 21:23:11.380: INFO: (1) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 73.756301ms)
Jul 22 21:23:11.395: INFO: (2) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 15.363519ms)
Jul 22 21:23:11.395: INFO: (2) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 15.285567ms)
Jul 22 21:23:11.395: INFO: (2) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 15.349598ms)
Jul 22 21:23:11.395: INFO: (2) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 15.329744ms)
Jul 22 21:23:11.395: INFO: (2) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 15.317686ms)
Jul 22 21:23:11.396: INFO: (2) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 16.473123ms)
Jul 22 21:23:11.396: INFO: (2) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 16.567714ms)
Jul 22 21:23:11.396: INFO: (2) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 16.510912ms)
Jul 22 21:23:11.400: INFO: (2) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 20.190854ms)
Jul 22 21:23:11.400: INFO: (2) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 20.094328ms)
Jul 22 21:23:11.400: INFO: (2) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 20.146034ms)
Jul 22 21:23:11.400: INFO: (2) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 20.157725ms)
Jul 22 21:23:11.401: INFO: (2) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 20.934591ms)
Jul 22 21:23:11.401: INFO: (2) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 20.839905ms)
Jul 22 21:23:11.401: INFO: (2) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 20.812584ms)
Jul 22 21:23:11.401: INFO: (2) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 20.785432ms)
Jul 22 21:23:11.421: INFO: (3) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 20.191447ms)
Jul 22 21:23:11.421: INFO: (3) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 20.227978ms)
Jul 22 21:23:11.421: INFO: (3) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 20.577157ms)
Jul 22 21:23:11.421: INFO: (3) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 20.60394ms)
Jul 22 21:23:11.421: INFO: (3) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 20.571856ms)
Jul 22 21:23:11.421: INFO: (3) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 20.537974ms)
Jul 22 21:23:11.421: INFO: (3) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 20.733688ms)
Jul 22 21:23:11.488: INFO: (3) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 87.000964ms)
Jul 22 21:23:11.488: INFO: (3) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 87.048286ms)
Jul 22 21:23:11.488: INFO: (3) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 87.255332ms)
Jul 22 21:23:11.488: INFO: (3) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 86.980419ms)
Jul 22 21:23:11.488: INFO: (3) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 87.043482ms)
Jul 22 21:23:11.488: INFO: (3) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 87.172761ms)
Jul 22 21:23:11.488: INFO: (3) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 87.124607ms)
Jul 22 21:23:11.488: INFO: (3) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 87.020797ms)
Jul 22 21:23:11.488: INFO: (3) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 87.146376ms)
Jul 22 21:23:11.504: INFO: (4) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 15.32349ms)
Jul 22 21:23:11.504: INFO: (4) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 15.466684ms)
Jul 22 21:23:11.504: INFO: (4) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 15.483989ms)
Jul 22 21:23:11.504: INFO: (4) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 15.431175ms)
Jul 22 21:23:11.504: INFO: (4) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 15.467413ms)
Jul 22 21:23:11.504: INFO: (4) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 15.486254ms)
Jul 22 21:23:11.504: INFO: (4) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 15.450126ms)
Jul 22 21:23:11.504: INFO: (4) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 15.470429ms)
Jul 22 21:23:11.508: INFO: (4) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 19.837894ms)
Jul 22 21:23:11.508: INFO: (4) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 19.900613ms)
Jul 22 21:23:11.508: INFO: (4) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 20.015174ms)
Jul 22 21:23:11.508: INFO: (4) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 19.837687ms)
Jul 22 21:23:11.509: INFO: (4) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 20.516741ms)
Jul 22 21:23:11.509: INFO: (4) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 20.548603ms)
Jul 22 21:23:11.509: INFO: (4) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 20.699279ms)
Jul 22 21:23:11.511: INFO: (4) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 23.391447ms)
Jul 22 21:23:11.531: INFO: (5) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 19.069161ms)
Jul 22 21:23:11.531: INFO: (5) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 19.20568ms)
Jul 22 21:23:11.531: INFO: (5) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 19.151375ms)
Jul 22 21:23:11.531: INFO: (5) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 19.126105ms)
Jul 22 21:23:11.531: INFO: (5) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 19.215066ms)
Jul 22 21:23:11.531: INFO: (5) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 19.173897ms)
Jul 22 21:23:11.534: INFO: (5) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 22.414017ms)
Jul 22 21:23:11.534: INFO: (5) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 22.401316ms)
Jul 22 21:23:11.534: INFO: (5) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 22.5386ms)
Jul 22 21:23:11.534: INFO: (5) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 22.633783ms)
Jul 22 21:23:11.534: INFO: (5) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 22.537637ms)
Jul 22 21:23:11.534: INFO: (5) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 22.698433ms)
Jul 22 21:23:11.588: INFO: (5) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 76.081105ms)
Jul 22 21:23:11.588: INFO: (5) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 75.988151ms)
Jul 22 21:23:11.588: INFO: (5) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 75.929832ms)
Jul 22 21:23:11.588: INFO: (5) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 75.957348ms)
Jul 22 21:23:11.603: INFO: (6) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 15.150444ms)
Jul 22 21:23:11.603: INFO: (6) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 15.192212ms)
Jul 22 21:23:11.603: INFO: (6) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 15.061616ms)
Jul 22 21:23:11.603: INFO: (6) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 15.175851ms)
Jul 22 21:23:11.603: INFO: (6) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 15.129826ms)
Jul 22 21:23:11.603: INFO: (6) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 15.134668ms)
Jul 22 21:23:11.603: INFO: (6) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 15.229577ms)
Jul 22 21:23:11.603: INFO: (6) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 15.193113ms)
Jul 22 21:23:11.603: INFO: (6) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 15.215715ms)
Jul 22 21:23:11.604: INFO: (6) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 16.185007ms)
Jul 22 21:23:11.604: INFO: (6) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 16.128606ms)
Jul 22 21:23:11.604: INFO: (6) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 16.150334ms)
Jul 22 21:23:11.607: INFO: (6) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 19.504209ms)
Jul 22 21:23:11.608: INFO: (6) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 20.108022ms)
Jul 22 21:23:11.609: INFO: (6) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 21.168342ms)
Jul 22 21:23:11.609: INFO: (6) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 21.523221ms)
Jul 22 21:23:11.629: INFO: (7) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 19.99382ms)
Jul 22 21:23:11.630: INFO: (7) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 20.122495ms)
Jul 22 21:23:11.629: INFO: (7) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 20.094091ms)
Jul 22 21:23:11.629: INFO: (7) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 20.157532ms)
Jul 22 21:23:11.630: INFO: (7) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 20.188466ms)
Jul 22 21:23:11.630: INFO: (7) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 20.088534ms)
Jul 22 21:23:11.637: INFO: (7) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 28.005977ms)
Jul 22 21:23:11.637: INFO: (7) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 28.016658ms)
Jul 22 21:23:11.637: INFO: (7) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 28.040808ms)
Jul 22 21:23:11.637: INFO: (7) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 27.978234ms)
Jul 22 21:23:11.638: INFO: (7) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 28.031076ms)
Jul 22 21:23:11.638: INFO: (7) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 28.202333ms)
Jul 22 21:23:11.640: INFO: (7) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 30.949916ms)
Jul 22 21:23:11.640: INFO: (7) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 31.000766ms)
Jul 22 21:23:11.640: INFO: (7) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 30.988031ms)
Jul 22 21:23:11.640: INFO: (7) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 31.053213ms)
Jul 22 21:23:11.691: INFO: (8) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 50.425877ms)
Jul 22 21:23:11.691: INFO: (8) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 50.579054ms)
Jul 22 21:23:11.691: INFO: (8) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 50.563761ms)
Jul 22 21:23:11.691: INFO: (8) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 50.673862ms)
Jul 22 21:23:11.691: INFO: (8) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 50.457067ms)
Jul 22 21:23:11.691: INFO: (8) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 50.4743ms)
Jul 22 21:23:11.691: INFO: (8) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 50.492579ms)
Jul 22 21:23:11.691: INFO: (8) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 50.542402ms)
Jul 22 21:23:11.691: INFO: (8) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 50.688285ms)
Jul 22 21:23:11.691: INFO: (8) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 50.541873ms)
Jul 22 21:23:11.696: INFO: (8) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 55.296514ms)
Jul 22 21:23:11.696: INFO: (8) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 55.343583ms)
Jul 22 21:23:11.696: INFO: (8) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 55.267115ms)
Jul 22 21:23:11.696: INFO: (8) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 55.302635ms)
Jul 22 21:23:11.696: INFO: (8) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 55.28979ms)
Jul 22 21:23:11.696: INFO: (8) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 55.3277ms)
Jul 22 21:23:11.711: INFO: (9) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 14.417785ms)
Jul 22 21:23:11.711: INFO: (9) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 14.497491ms)
Jul 22 21:23:11.711: INFO: (9) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 14.473755ms)
Jul 22 21:23:11.711: INFO: (9) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 14.47102ms)
Jul 22 21:23:11.711: INFO: (9) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 15.058618ms)
Jul 22 21:23:11.711: INFO: (9) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 15.158544ms)
Jul 22 21:23:11.711: INFO: (9) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 15.130278ms)
Jul 22 21:23:11.717: INFO: (9) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 20.460994ms)
Jul 22 21:23:11.717: INFO: (9) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 20.349417ms)
Jul 22 21:23:11.717: INFO: (9) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 20.396255ms)
Jul 22 21:23:11.717: INFO: (9) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 20.531842ms)
Jul 22 21:23:11.717: INFO: (9) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 20.48965ms)
Jul 22 21:23:11.740: INFO: (9) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 43.635989ms)
Jul 22 21:23:11.740: INFO: (9) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 43.441842ms)
Jul 22 21:23:11.740: INFO: (9) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 43.549566ms)
Jul 22 21:23:11.740: INFO: (9) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 43.518406ms)
Jul 22 21:23:11.788: INFO: (10) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 47.641119ms)
Jul 22 21:23:11.788: INFO: (10) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 47.851702ms)
Jul 22 21:23:11.788: INFO: (10) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 47.848184ms)
Jul 22 21:23:11.788: INFO: (10) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 47.718582ms)
Jul 22 21:23:11.788: INFO: (10) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 47.714289ms)
Jul 22 21:23:11.788: INFO: (10) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 47.761458ms)
Jul 22 21:23:11.788: INFO: (10) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 47.8726ms)
Jul 22 21:23:11.788: INFO: (10) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 47.747954ms)
Jul 22 21:23:11.788: INFO: (10) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 47.725461ms)
Jul 22 21:23:11.788: INFO: (10) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 47.783772ms)
Jul 22 21:23:11.788: INFO: (10) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 47.762793ms)
Jul 22 21:23:11.788: INFO: (10) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 47.75704ms)
Jul 22 21:23:11.797: INFO: (10) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 56.928242ms)
Jul 22 21:23:11.797: INFO: (10) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 56.929822ms)
Jul 22 21:23:11.797: INFO: (10) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 57.049479ms)
Jul 22 21:23:11.798: INFO: (10) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 57.836188ms)
Jul 22 21:23:11.813: INFO: (11) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 15.315954ms)
Jul 22 21:23:11.813: INFO: (11) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 15.256581ms)
Jul 22 21:23:11.813: INFO: (11) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 15.276592ms)
Jul 22 21:23:11.813: INFO: (11) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 15.38107ms)
Jul 22 21:23:11.813: INFO: (11) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 15.276302ms)
Jul 22 21:23:11.814: INFO: (11) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 16.140539ms)
Jul 22 21:23:11.814: INFO: (11) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 16.290883ms)
Jul 22 21:23:11.814: INFO: (11) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 16.267045ms)
Jul 22 21:23:11.814: INFO: (11) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 16.192989ms)
Jul 22 21:23:11.814: INFO: (11) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 16.145203ms)
Jul 22 21:23:11.817: INFO: (11) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 19.500085ms)
Jul 22 21:23:11.818: INFO: (11) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 19.688598ms)
Jul 22 21:23:11.818: INFO: (11) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 19.638693ms)
Jul 22 21:23:11.818: INFO: (11) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 19.842279ms)
Jul 22 21:23:11.818: INFO: (11) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 19.750876ms)
Jul 22 21:23:11.908: INFO: (11) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 109.911414ms)
Jul 22 21:23:11.922: INFO: (12) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 14.527011ms)
Jul 22 21:23:11.923: INFO: (12) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 15.147063ms)
Jul 22 21:23:11.923: INFO: (12) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 15.384571ms)
Jul 22 21:23:11.923: INFO: (12) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 15.205029ms)
Jul 22 21:23:11.923: INFO: (12) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 15.370799ms)
Jul 22 21:23:11.923: INFO: (12) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 15.278557ms)
Jul 22 21:23:11.928: INFO: (12) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 19.8574ms)
Jul 22 21:23:11.928: INFO: (12) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 19.850346ms)
Jul 22 21:23:11.928: INFO: (12) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 19.843465ms)
Jul 22 21:23:11.928: INFO: (12) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 19.837228ms)
Jul 22 21:23:11.928: INFO: (12) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 19.796472ms)
Jul 22 21:23:11.928: INFO: (12) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 19.844685ms)
Jul 22 21:23:11.928: INFO: (12) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 19.846439ms)
Jul 22 21:23:11.930: INFO: (12) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 22.143206ms)
Jul 22 21:23:11.930: INFO: (12) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 22.217226ms)
Jul 22 21:23:11.930: INFO: (12) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 22.275841ms)
Jul 22 21:23:11.945: INFO: (13) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 14.399238ms)
Jul 22 21:23:11.945: INFO: (13) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 14.593679ms)
Jul 22 21:23:11.946: INFO: (13) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 15.060272ms)
Jul 22 21:23:11.946: INFO: (13) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 15.246795ms)
Jul 22 21:23:11.946: INFO: (13) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 15.18187ms)
Jul 22 21:23:11.950: INFO: (13) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 20.007188ms)
Jul 22 21:23:11.950: INFO: (13) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 19.935292ms)
Jul 22 21:23:11.950: INFO: (13) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 19.998397ms)
Jul 22 21:23:11.950: INFO: (13) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 19.995148ms)
Jul 22 21:23:11.950: INFO: (13) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 20.052157ms)
Jul 22 21:23:11.950: INFO: (13) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 20.083156ms)
Jul 22 21:23:11.950: INFO: (13) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 20.124058ms)
Jul 22 21:23:11.953: INFO: (13) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 23.044299ms)
Jul 22 21:23:11.953: INFO: (13) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 23.095754ms)
Jul 22 21:23:11.953: INFO: (13) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 23.176575ms)
Jul 22 21:23:11.954: INFO: (13) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 23.143118ms)
Jul 22 21:23:11.996: INFO: (14) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 41.866829ms)
Jul 22 21:23:11.996: INFO: (14) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 41.940815ms)
Jul 22 21:23:11.996: INFO: (14) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 42.014691ms)
Jul 22 21:23:11.996: INFO: (14) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 41.933726ms)
Jul 22 21:23:11.996: INFO: (14) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 42.010539ms)
Jul 22 21:23:11.996: INFO: (14) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 41.959086ms)
Jul 22 21:23:11.999: INFO: (14) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 45.298263ms)
Jul 22 21:23:11.999: INFO: (14) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 45.242102ms)
Jul 22 21:23:11.999: INFO: (14) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 45.349635ms)
Jul 22 21:23:12.001: INFO: (14) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 46.916754ms)
Jul 22 21:23:12.038: INFO: (14) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 83.875949ms)
Jul 22 21:23:12.038: INFO: (14) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 83.920957ms)
Jul 22 21:23:12.038: INFO: (14) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 83.823848ms)
Jul 22 21:23:12.038: INFO: (14) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 83.928293ms)
Jul 22 21:23:12.038: INFO: (14) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 83.865418ms)
Jul 22 21:23:12.038: INFO: (14) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 83.89387ms)
Jul 22 21:23:12.052: INFO: (15) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 14.283675ms)
Jul 22 21:23:12.052: INFO: (15) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 14.316269ms)
Jul 22 21:23:12.052: INFO: (15) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 14.389591ms)
Jul 22 21:23:12.052: INFO: (15) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 14.457772ms)
Jul 22 21:23:12.052: INFO: (15) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 14.363564ms)
Jul 22 21:23:12.052: INFO: (15) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 14.481551ms)
Jul 22 21:23:12.052: INFO: (15) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 14.628299ms)
Jul 22 21:23:12.058: INFO: (15) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 19.940018ms)
Jul 22 21:23:12.058: INFO: (15) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 20.092569ms)
Jul 22 21:23:12.058: INFO: (15) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 19.971493ms)
Jul 22 21:23:12.058: INFO: (15) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 19.999772ms)
Jul 22 21:23:12.058: INFO: (15) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 20.155288ms)
Jul 22 21:23:12.058: INFO: (15) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 20.036266ms)
Jul 22 21:23:12.058: INFO: (15) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 20.057499ms)
Jul 22 21:23:12.061: INFO: (15) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 22.931523ms)
Jul 22 21:23:12.061: INFO: (15) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 22.986173ms)
Jul 22 21:23:12.076: INFO: (16) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 14.792829ms)
Jul 22 21:23:12.076: INFO: (16) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 14.78167ms)
Jul 22 21:23:12.076: INFO: (16) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 14.744876ms)
Jul 22 21:23:12.076: INFO: (16) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 14.744674ms)
Jul 22 21:23:12.076: INFO: (16) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 14.837863ms)
Jul 22 21:23:12.076: INFO: (16) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 14.716935ms)
Jul 22 21:23:12.076: INFO: (16) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 14.89676ms)
Jul 22 21:23:12.076: INFO: (16) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 14.759763ms)
Jul 22 21:23:12.076: INFO: (16) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 14.803085ms)
Jul 22 21:23:12.077: INFO: (16) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 15.592732ms)
Jul 22 21:23:12.087: INFO: (16) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 26.301728ms)
Jul 22 21:23:12.087: INFO: (16) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 26.159187ms)
Jul 22 21:23:12.134: INFO: (16) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 73.276746ms)
Jul 22 21:23:12.134: INFO: (16) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 73.207448ms)
Jul 22 21:23:12.134: INFO: (16) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 73.172402ms)
Jul 22 21:23:12.135: INFO: (16) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 73.587718ms)
Jul 22 21:23:12.149: INFO: (17) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 14.455243ms)
Jul 22 21:23:12.149: INFO: (17) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 14.625663ms)
Jul 22 21:23:12.150: INFO: (17) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 14.690058ms)
Jul 22 21:23:12.150: INFO: (17) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 14.660567ms)
Jul 22 21:23:12.150: INFO: (17) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 14.721263ms)
Jul 22 21:23:12.151: INFO: (17) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 15.933019ms)
Jul 22 21:23:12.151: INFO: (17) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 16.062492ms)
Jul 22 21:23:12.151: INFO: (17) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 15.92923ms)
Jul 22 21:23:12.151: INFO: (17) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 15.873319ms)
Jul 22 21:23:12.151: INFO: (17) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 15.992786ms)
Jul 22 21:23:12.151: INFO: (17) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 16.031848ms)
Jul 22 21:23:12.154: INFO: (17) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 19.074767ms)
Jul 22 21:23:12.154: INFO: (17) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 19.197879ms)
Jul 22 21:23:12.154: INFO: (17) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 19.152212ms)
Jul 22 21:23:12.154: INFO: (17) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 19.223114ms)
Jul 22 21:23:12.154: INFO: (17) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 19.243337ms)
Jul 22 21:23:12.169: INFO: (18) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 14.336777ms)
Jul 22 21:23:12.169: INFO: (18) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 14.350005ms)
Jul 22 21:23:12.169: INFO: (18) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 14.413814ms)
Jul 22 21:23:12.169: INFO: (18) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 14.381502ms)
Jul 22 21:23:12.169: INFO: (18) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 14.397853ms)
Jul 22 21:23:12.169: INFO: (18) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 14.444005ms)
Jul 22 21:23:12.169: INFO: (18) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 14.385472ms)
Jul 22 21:23:12.174: INFO: (18) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 19.967716ms)
Jul 22 21:23:12.174: INFO: (18) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 19.859596ms)
Jul 22 21:23:12.174: INFO: (18) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 19.897808ms)
Jul 22 21:23:12.174: INFO: (18) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 19.975469ms)
Jul 22 21:23:12.174: INFO: (18) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 20.024348ms)
Jul 22 21:23:12.174: INFO: (18) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 20.131752ms)
Jul 22 21:23:12.174: INFO: (18) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 20.018625ms)
Jul 22 21:23:12.174: INFO: (18) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 20.112803ms)
Jul 22 21:23:12.174: INFO: (18) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 20.117153ms)
Jul 22 21:23:12.239: INFO: (19) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:443/proxy/tlsrewritem... (200; 65.016395ms)
Jul 22 21:23:12.239: INFO: (19) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:1080/proxy/rewriteme">test<... (200; 65.003797ms)
Jul 22 21:23:12.239: INFO: (19) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:162/proxy/: bar (200; 65.115307ms)
Jul 22 21:23:12.240: INFO: (19) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2/proxy/rewriteme">test</a> (200; 65.005875ms)
Jul 22 21:23:12.241: INFO: (19) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:462/proxy/: tls qux (200; 66.519252ms)
Jul 22 21:23:12.241: INFO: (19) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/: <a href="/api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:1080/proxy/rewriteme">... (200; 66.530221ms)
Jul 22 21:23:12.241: INFO: (19) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:160/proxy/: foo (200; 66.490513ms)
Jul 22 21:23:12.242: INFO: (19) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname2/proxy/: tls qux (200; 67.007124ms)
Jul 22 21:23:12.242: INFO: (19) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname1/proxy/: foo (200; 67.555339ms)
Jul 22 21:23:12.242: INFO: (19) /api/v1/namespaces/proxy-3302/pods/proxy-service-2rldc-n47v2:160/proxy/: foo (200; 67.564308ms)
Jul 22 21:23:12.242: INFO: (19) /api/v1/namespaces/proxy-3302/pods/http:proxy-service-2rldc-n47v2:162/proxy/: bar (200; 67.492008ms)
Jul 22 21:23:12.242: INFO: (19) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname2/proxy/: bar (200; 67.552853ms)
Jul 22 21:23:12.243: INFO: (19) /api/v1/namespaces/proxy-3302/services/http:proxy-service-2rldc:portname1/proxy/: foo (200; 68.570694ms)
Jul 22 21:23:12.243: INFO: (19) /api/v1/namespaces/proxy-3302/services/proxy-service-2rldc:portname2/proxy/: bar (200; 68.636324ms)
Jul 22 21:23:12.244: INFO: (19) /api/v1/namespaces/proxy-3302/services/https:proxy-service-2rldc:tlsportname1/proxy/: tls baz (200; 69.029625ms)
Jul 22 21:23:12.244: INFO: (19) /api/v1/namespaces/proxy-3302/pods/https:proxy-service-2rldc-n47v2:460/proxy/: tls baz (200; 69.022609ms)
STEP: deleting ReplicationController proxy-service-2rldc in namespace proxy-3302, will wait for the garbage collector to delete the pods
Jul 22 21:23:12.319: INFO: Deleting ReplicationController proxy-service-2rldc took: 13.483941ms
Jul 22 21:23:12.420: INFO: Terminating ReplicationController proxy-service-2rldc pods took: 100.442541ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:23:17.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3302" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":339,"completed":270,"skipped":4332,"failed":0}
S
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:23:17.053: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-9399
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:23:17.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-9399" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":339,"completed":271,"skipped":4333,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:23:17.419: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jul 22 21:23:17.612: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9566 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Jul 22 21:23:17.734: INFO: stderr: ""
Jul 22 21:23:17.734: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jul 22 21:23:17.734: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9566 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Jul 22 21:23:18.136: INFO: stderr: ""
Jul 22 21:23:18.136: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jul 22 21:23:18.148: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9566 delete pods e2e-test-httpd-pod'
Jul 22 21:23:26.949: INFO: stderr: ""
Jul 22 21:23:26.949: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:23:26.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9566" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":339,"completed":272,"skipped":4337,"failed":0}
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:23:26.982: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4871
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 22 21:23:30.256: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:23:30.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4871" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":273,"skipped":4338,"failed":0}

------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:23:30.318: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-6607
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6607
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6607
Jul 22 21:23:30.561: INFO: Found 0 stateful pods, waiting for 1
Jul 22 21:23:40.574: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 22 21:23:40.586: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-6607 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 21:23:40.995: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 21:23:40.995: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 21:23:40.995: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 21:23:41.007: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 22 21:23:51.021: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 21:23:51.021: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:23:51.067: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999968s
Jul 22 21:23:52.079: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.988432797s
Jul 22 21:23:53.092: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.975950111s
Jul 22 21:23:54.104: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.96320312s
Jul 22 21:23:55.116: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.951436801s
Jul 22 21:23:56.129: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.93968831s
Jul 22 21:23:57.140: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.926873226s
Jul 22 21:23:58.153: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.915106833s
Jul 22 21:23:59.164: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.903021775s
Jul 22 21:24:00.177: INFO: Verifying statefulset ss doesn't scale past 1 for another 891.42517ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6607
Jul 22 21:24:01.189: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-6607 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 21:24:01.554: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 21:24:01.554: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 21:24:01.554: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 21:24:01.566: INFO: Found 1 stateful pods, waiting for 3
Jul 22 21:24:11.582: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:24:11.582: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:24:11.582: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 22 21:24:11.605: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-6607 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 21:24:12.016: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 21:24:12.016: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 21:24:12.016: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 21:24:12.016: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-6607 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 21:24:12.370: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 21:24:12.370: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 21:24:12.370: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 21:24:12.370: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-6607 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 21:24:12.740: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 21:24:12.740: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 21:24:12.740: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 21:24:12.740: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:24:12.752: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul 22 21:24:22.776: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 21:24:22.776: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 21:24:22.776: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 21:24:22.815: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999708s
Jul 22 21:24:23.829: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987974556s
Jul 22 21:24:24.844: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973367752s
Jul 22 21:24:25.857: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.957748756s
Jul 22 21:24:26.870: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.945345549s
Jul 22 21:24:27.883: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.931861776s
Jul 22 21:24:28.896: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.91923074s
Jul 22 21:24:29.909: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.906092246s
Jul 22 21:24:30.922: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.893396144s
Jul 22 21:24:31.935: INFO: Verifying statefulset ss doesn't scale past 3 for another 880.772497ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6607
Jul 22 21:24:32.949: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-6607 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 21:24:33.329: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 21:24:33.329: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 21:24:33.329: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 21:24:33.329: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-6607 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 21:24:33.749: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 21:24:33.749: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 21:24:33.749: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 21:24:33.749: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-6607 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 21:24:34.153: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 21:24:34.153: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 21:24:34.153: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 21:24:34.153: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul 22 21:25:14.203: INFO: Deleting all statefulset in ns statefulset-6607
Jul 22 21:25:14.214: INFO: Scaling statefulset ss to 0
Jul 22 21:25:14.249: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:25:14.260: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:25:14.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6607" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":339,"completed":274,"skipped":4338,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:25:14.326: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4358
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:25:14.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4358" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":339,"completed":275,"skipped":4356,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:25:14.650: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2947
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:25:14.875: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21336718-a4c5-4c4c-8fb4-933da2706a95" in namespace "downward-api-2947" to be "Succeeded or Failed"
Jul 22 21:25:14.894: INFO: Pod "downwardapi-volume-21336718-a4c5-4c4c-8fb4-933da2706a95": Phase="Pending", Reason="", readiness=false. Elapsed: 18.632821ms
Jul 22 21:25:16.908: INFO: Pod "downwardapi-volume-21336718-a4c5-4c4c-8fb4-933da2706a95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032712452s
STEP: Saw pod success
Jul 22 21:25:16.908: INFO: Pod "downwardapi-volume-21336718-a4c5-4c4c-8fb4-933da2706a95" satisfied condition "Succeeded or Failed"
Jul 22 21:25:16.919: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-21336718-a4c5-4c4c-8fb4-933da2706a95 container client-container: <nil>
STEP: delete the pod
Jul 22 21:25:17.010: INFO: Waiting for pod downwardapi-volume-21336718-a4c5-4c4c-8fb4-933da2706a95 to disappear
Jul 22 21:25:17.021: INFO: Pod downwardapi-volume-21336718-a4c5-4c4c-8fb4-933da2706a95 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:25:17.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2947" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":339,"completed":276,"skipped":4356,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:25:17.053: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5251
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-3da6ff9b-772d-4d11-bb45-8d26018198b9
STEP: Creating a pod to test consume secrets
Jul 22 21:25:17.281: INFO: Waiting up to 5m0s for pod "pod-secrets-2771959d-0d85-402b-971e-5bf7cdbda22d" in namespace "secrets-5251" to be "Succeeded or Failed"
Jul 22 21:25:17.292: INFO: Pod "pod-secrets-2771959d-0d85-402b-971e-5bf7cdbda22d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.945049ms
Jul 22 21:25:19.306: INFO: Pod "pod-secrets-2771959d-0d85-402b-971e-5bf7cdbda22d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024965363s
STEP: Saw pod success
Jul 22 21:25:19.307: INFO: Pod "pod-secrets-2771959d-0d85-402b-971e-5bf7cdbda22d" satisfied condition "Succeeded or Failed"
Jul 22 21:25:19.317: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-secrets-2771959d-0d85-402b-971e-5bf7cdbda22d container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:25:19.357: INFO: Waiting for pod pod-secrets-2771959d-0d85-402b-971e-5bf7cdbda22d to disappear
Jul 22 21:25:19.368: INFO: Pod pod-secrets-2771959d-0d85-402b-971e-5bf7cdbda22d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:25:19.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5251" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":277,"skipped":4363,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:25:19.400: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-adbac8a6-5153-40a7-8ab6-578f1508e195
STEP: Creating a pod to test consume secrets
Jul 22 21:25:19.644: INFO: Waiting up to 5m0s for pod "pod-secrets-e52c2151-a868-47f8-a701-d2e9aba006c7" in namespace "secrets-1689" to be "Succeeded or Failed"
Jul 22 21:25:19.655: INFO: Pod "pod-secrets-e52c2151-a868-47f8-a701-d2e9aba006c7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.004099ms
Jul 22 21:25:21.667: INFO: Pod "pod-secrets-e52c2151-a868-47f8-a701-d2e9aba006c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023078479s
STEP: Saw pod success
Jul 22 21:25:21.667: INFO: Pod "pod-secrets-e52c2151-a868-47f8-a701-d2e9aba006c7" satisfied condition "Succeeded or Failed"
Jul 22 21:25:21.678: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-secrets-e52c2151-a868-47f8-a701-d2e9aba006c7 container secret-env-test: <nil>
STEP: delete the pod
Jul 22 21:25:21.722: INFO: Waiting for pod pod-secrets-e52c2151-a868-47f8-a701-d2e9aba006c7 to disappear
Jul 22 21:25:21.733: INFO: Pod pod-secrets-e52c2151-a868-47f8-a701-d2e9aba006c7 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:25:21.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1689" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":339,"completed":278,"skipped":4373,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:25:21.766: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-3625
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0722 21:25:21.970119    5669 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:30:22.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3625" for this suite.

• [SLOW TEST:300.299 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":339,"completed":279,"skipped":4388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:30:22.066: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Jul 22 21:30:22.299: INFO: Waiting up to 5m0s for pod "var-expansion-bafe9f9e-889b-451f-995b-ff0129c70c27" in namespace "var-expansion-259" to be "Succeeded or Failed"
Jul 22 21:30:22.310: INFO: Pod "var-expansion-bafe9f9e-889b-451f-995b-ff0129c70c27": Phase="Pending", Reason="", readiness=false. Elapsed: 10.670344ms
Jul 22 21:30:24.321: INFO: Pod "var-expansion-bafe9f9e-889b-451f-995b-ff0129c70c27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022243532s
STEP: Saw pod success
Jul 22 21:30:24.321: INFO: Pod "var-expansion-bafe9f9e-889b-451f-995b-ff0129c70c27" satisfied condition "Succeeded or Failed"
Jul 22 21:30:24.332: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod var-expansion-bafe9f9e-889b-451f-995b-ff0129c70c27 container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:30:24.375: INFO: Waiting for pod var-expansion-bafe9f9e-889b-451f-995b-ff0129c70c27 to disappear
Jul 22 21:30:24.386: INFO: Pod var-expansion-bafe9f9e-889b-451f-995b-ff0129c70c27 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:30:24.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-259" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":339,"completed":280,"skipped":4423,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:30:24.419: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-658
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:30:24.613: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:30:24.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-658" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":339,"completed":281,"skipped":4423,"failed":0}
SSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:30:24.755: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jul 22 21:30:25.326: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:30:25.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4340" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":339,"completed":282,"skipped":4431,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:30:25.534: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:30:26.823: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 22 21:30:28.861: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586226, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586226, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586226, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586226, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:30:31.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:30:44.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8" for this suite.
STEP: Destroying namespace "webhook-8-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":339,"completed":283,"skipped":4453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:30:44.508: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-7411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:30:44.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7411" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":339,"completed":284,"skipped":4475,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:30:44.850: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8065
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:30:45.062: INFO: Waiting up to 5m0s for pod "downwardapi-volume-312996b5-6b7e-4986-9578-23c8e29c5424" in namespace "projected-8065" to be "Succeeded or Failed"
Jul 22 21:30:45.073: INFO: Pod "downwardapi-volume-312996b5-6b7e-4986-9578-23c8e29c5424": Phase="Pending", Reason="", readiness=false. Elapsed: 10.541403ms
Jul 22 21:30:47.085: INFO: Pod "downwardapi-volume-312996b5-6b7e-4986-9578-23c8e29c5424": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022450367s
Jul 22 21:30:49.097: INFO: Pod "downwardapi-volume-312996b5-6b7e-4986-9578-23c8e29c5424": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034936811s
STEP: Saw pod success
Jul 22 21:30:49.097: INFO: Pod "downwardapi-volume-312996b5-6b7e-4986-9578-23c8e29c5424" satisfied condition "Succeeded or Failed"
Jul 22 21:30:49.108: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-312996b5-6b7e-4986-9578-23c8e29c5424 container client-container: <nil>
STEP: delete the pod
Jul 22 21:30:49.182: INFO: Waiting for pod downwardapi-volume-312996b5-6b7e-4986-9578-23c8e29c5424 to disappear
Jul 22 21:30:49.192: INFO: Pod downwardapi-volume-312996b5-6b7e-4986-9578-23c8e29c5424 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:30:49.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8065" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":285,"skipped":4500,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:30:49.225: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1715
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:30:49.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1715" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":339,"completed":286,"skipped":4518,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:30:49.666: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9518
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:30:50.518: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 22 21:30:52.552: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586250, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586250, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586250, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586250, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:30:55.581: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:30:56.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9518" for this suite.
STEP: Destroying namespace "webhook-9518-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":339,"completed":287,"skipped":4554,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:30:56.134: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3886
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:30:57.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586257, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586257, loc:(*time.Location)(0x9dde5a0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-78988fc6cd\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586257, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586257, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jul 22 21:30:59.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586257, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586257, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586257, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586257, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:31:02.140: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:31:02.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3886" for this suite.
STEP: Destroying namespace "webhook-3886-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":339,"completed":288,"skipped":4563,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:31:02.557: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3221
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-0e963bd5-5b85-49ae-8cd8-e81c28443373
STEP: Creating a pod to test consume configMaps
Jul 22 21:31:02.780: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd4ce371-1eee-4700-ad76-77010b39ff38" in namespace "configmap-3221" to be "Succeeded or Failed"
Jul 22 21:31:02.790: INFO: Pod "pod-configmaps-cd4ce371-1eee-4700-ad76-77010b39ff38": Phase="Pending", Reason="", readiness=false. Elapsed: 10.771552ms
Jul 22 21:31:04.802: INFO: Pod "pod-configmaps-cd4ce371-1eee-4700-ad76-77010b39ff38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022731196s
STEP: Saw pod success
Jul 22 21:31:04.802: INFO: Pod "pod-configmaps-cd4ce371-1eee-4700-ad76-77010b39ff38" satisfied condition "Succeeded or Failed"
Jul 22 21:31:04.814: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-configmaps-cd4ce371-1eee-4700-ad76-77010b39ff38 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:31:04.847: INFO: Waiting for pod pod-configmaps-cd4ce371-1eee-4700-ad76-77010b39ff38 to disappear
Jul 22 21:31:04.858: INFO: Pod pod-configmaps-cd4ce371-1eee-4700-ad76-77010b39ff38 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:31:04.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3221" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":289,"skipped":4572,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:31:04.890: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jul 22 21:31:05.107: INFO: The status of Pod labelsupdatecc94e180-d748-4514-855d-dfd0f4223090 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:31:07.120: INFO: The status of Pod labelsupdatecc94e180-d748-4514-855d-dfd0f4223090 is Running (Ready = true)
Jul 22 21:31:07.724: INFO: Successfully updated pod "labelsupdatecc94e180-d748-4514-855d-dfd0f4223090"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:31:09.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8750" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":290,"skipped":4577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:31:09.798: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-6808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Jul 22 21:31:10.001: INFO: Major version: 1
STEP: Confirm minor version
Jul 22 21:31:10.001: INFO: cleanMinorVersion: 21
Jul 22 21:31:10.001: INFO: Minor version: 21
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:31:10.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6808" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":339,"completed":291,"skipped":4635,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:31:10.036: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Jul 22 21:31:10.313: INFO: created test-pod-1
Jul 22 21:31:10.330: INFO: created test-pod-2
Jul 22 21:31:10.346: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:31:10.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6857" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":339,"completed":292,"skipped":4666,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:31:10.421: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3351
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-88b9d1b1-bb4a-4da2-99a9-838511017b20
STEP: Creating a pod to test consume secrets
Jul 22 21:31:10.645: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a33fcdc9-5242-4ce7-9157-e4a6b91856fb" in namespace "projected-3351" to be "Succeeded or Failed"
Jul 22 21:31:10.655: INFO: Pod "pod-projected-secrets-a33fcdc9-5242-4ce7-9157-e4a6b91856fb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.314311ms
Jul 22 21:31:12.668: INFO: Pod "pod-projected-secrets-a33fcdc9-5242-4ce7-9157-e4a6b91856fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.022858968s
Jul 22 21:31:14.680: INFO: Pod "pod-projected-secrets-a33fcdc9-5242-4ce7-9157-e4a6b91856fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034723524s
STEP: Saw pod success
Jul 22 21:31:14.680: INFO: Pod "pod-projected-secrets-a33fcdc9-5242-4ce7-9157-e4a6b91856fb" satisfied condition "Succeeded or Failed"
Jul 22 21:31:14.691: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-secrets-a33fcdc9-5242-4ce7-9157-e4a6b91856fb container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:31:14.729: INFO: Waiting for pod pod-projected-secrets-a33fcdc9-5242-4ce7-9157-e4a6b91856fb to disappear
Jul 22 21:31:14.740: INFO: Pod pod-projected-secrets-a33fcdc9-5242-4ce7-9157-e4a6b91856fb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:31:14.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3351" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":293,"skipped":4721,"failed":0}
SS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:31:14.772: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-1106
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jul 22 21:31:14.993: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 22 21:32:15.094: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:32:15.105: INFO: Starting informer...
STEP: Starting pods...
Jul 22 21:32:15.145: INFO: Pod1 is running on shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9. Tainting Node
Jul 22 21:32:19.202: INFO: Pod2 is running on shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jul 22 21:32:36.949: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul 22 21:32:56.952: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:32:56.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1106" for this suite.
•{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":339,"completed":294,"skipped":4723,"failed":0}
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:32:57.015: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2892
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-2892
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2892 to expose endpoints map[]
Jul 22 21:32:57.267: INFO: successfully validated that service endpoint-test2 in namespace services-2892 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2892
Jul 22 21:32:57.333: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:32:59.344: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2892 to expose endpoints map[pod1:[80]]
Jul 22 21:32:59.396: INFO: successfully validated that service endpoint-test2 in namespace services-2892 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-2892
Jul 22 21:32:59.424: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:33:01.437: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2892 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 22 21:33:01.499: INFO: successfully validated that service endpoint-test2 in namespace services-2892 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-2892
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2892 to expose endpoints map[pod2:[80]]
Jul 22 21:33:01.556: INFO: successfully validated that service endpoint-test2 in namespace services-2892 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-2892
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2892 to expose endpoints map[]
Jul 22 21:33:01.601: INFO: successfully validated that service endpoint-test2 in namespace services-2892 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:33:01.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2892" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":339,"completed":295,"skipped":4724,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:33:01.653: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4014
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Jul 22 21:33:01.949: INFO: Waiting up to 5m0s for pod "var-expansion-26e77d78-9dbf-4564-9697-bec6057a5777" in namespace "var-expansion-4014" to be "Succeeded or Failed"
Jul 22 21:33:01.959: INFO: Pod "var-expansion-26e77d78-9dbf-4564-9697-bec6057a5777": Phase="Pending", Reason="", readiness=false. Elapsed: 10.458958ms
Jul 22 21:33:03.971: INFO: Pod "var-expansion-26e77d78-9dbf-4564-9697-bec6057a5777": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022242311s
STEP: Saw pod success
Jul 22 21:33:03.971: INFO: Pod "var-expansion-26e77d78-9dbf-4564-9697-bec6057a5777" satisfied condition "Succeeded or Failed"
Jul 22 21:33:03.981: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod var-expansion-26e77d78-9dbf-4564-9697-bec6057a5777 container dapi-container: <nil>
STEP: delete the pod
Jul 22 21:33:04.056: INFO: Waiting for pod var-expansion-26e77d78-9dbf-4564-9697-bec6057a5777 to disappear
Jul 22 21:33:04.066: INFO: Pod var-expansion-26e77d78-9dbf-4564-9697-bec6057a5777 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:33:04.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4014" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":339,"completed":296,"skipped":4738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:33:04.099: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:33:04.285: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption-2
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-4367
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-2828
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:33:04.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-4367" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:33:04.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2828" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":339,"completed":297,"skipped":4761,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:33:04.640: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2169
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 22 21:33:04.830: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 21:33:04.853: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 21:33:04.863: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 before test
Jul 22 21:33:04.888: INFO: addons-nginx-ingress-controller-56b97f886d-995vz from kube-system started at 2021-07-22 21:21:25 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Jul 22 21:33:04.888: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6d48bc7cdd-cctqm from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Jul 22 21:33:04.888: INFO: apiserver-proxy-666sf from kube-system started at 2021-07-22 19:51:18 +0000 UTC (2 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container proxy ready: true, restart count 0
Jul 22 21:33:04.888: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 21:33:04.888: INFO: calico-kube-controllers-6f857b9885-2ktk5 from kube-system started at 2021-07-22 19:51:18 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 22 21:33:04.888: INFO: calico-node-kdx2r from kube-system started at 2021-07-22 19:52:34 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 21:33:04.888: INFO: calico-node-vertical-autoscaler-785b5f968-2667d from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 21:33:04.888: INFO: calico-typha-deploy-b7996d4cf-mqt22 from kube-system started at 2021-07-22 19:51:26 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container calico-typha ready: true, restart count 0
Jul 22 21:33:04.888: INFO: calico-typha-horizontal-autoscaler-5b58bb446c-tj7hs from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 21:33:04.888: INFO: calico-typha-vertical-autoscaler-5c9655cddd-9mb9p from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 21:33:04.888: INFO: coredns-77bd5b684d-lsdxl from kube-system started at 2021-07-22 21:21:25 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container coredns ready: true, restart count 0
Jul 22 21:33:04.888: INFO: coredns-77bd5b684d-p8pfz from kube-system started at 2021-07-22 21:21:25 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container coredns ready: true, restart count 0
Jul 22 21:33:04.888: INFO: csi-driver-node-x8gk6 from kube-system started at 2021-07-22 19:51:18 +0000 UTC (3 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container csi-driver ready: true, restart count 0
Jul 22 21:33:04.888: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul 22 21:33:04.888: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jul 22 21:33:04.888: INFO: kube-proxy-7mqpv from kube-system started at 2021-07-22 20:36:07 +0000 UTC (2 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 21:33:04.888: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 21:33:04.888: INFO: metrics-server-7774b47fd5-95ldg from kube-system started at 2021-07-22 19:51:47 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container metrics-server ready: true, restart count 0
Jul 22 21:33:04.888: INFO: node-exporter-slf8j from kube-system started at 2021-07-22 19:51:18 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 21:33:04.888: INFO: node-problem-detector-82tqx from kube-system started at 2021-07-22 20:19:08 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container node-problem-detector ready: true, restart count 0
Jul 22 21:33:04.888: INFO: vpn-shoot-7ff9f78fff-c8rk2 from kube-system started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.888: INFO: 	Container vpn-shoot ready: true, restart count 0
Jul 22 21:33:04.889: INFO: dashboard-metrics-scraper-6dd4bbbc68-kdk6t from kubernetes-dashboard started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.889: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jul 22 21:33:04.889: INFO: kubernetes-dashboard-565499686d-dfd8j from kubernetes-dashboard started at 2021-07-22 19:51:48 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.889: INFO: 	Container kubernetes-dashboard ready: true, restart count 2
Jul 22 21:33:04.889: INFO: 
Logging pods the apiserver thinks is on node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 before test
Jul 22 21:33:04.904: INFO: apiserver-proxy-65bs6 from kube-system started at 2021-07-22 19:51:37 +0000 UTC (2 container statuses recorded)
Jul 22 21:33:04.904: INFO: 	Container proxy ready: true, restart count 0
Jul 22 21:33:04.904: INFO: 	Container sidecar ready: true, restart count 0
Jul 22 21:33:04.904: INFO: blackbox-exporter-859b5d9c8c-694bg from kube-system started at 2021-07-22 19:58:57 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.904: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jul 22 21:33:04.904: INFO: calico-node-hxcn8 from kube-system started at 2021-07-22 19:51:37 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.904: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 21:33:04.904: INFO: csi-driver-node-mmj64 from kube-system started at 2021-07-22 19:51:37 +0000 UTC (3 container statuses recorded)
Jul 22 21:33:04.904: INFO: 	Container csi-driver ready: true, restart count 0
Jul 22 21:33:04.904: INFO: 	Container csi-liveness-probe ready: true, restart count 0
Jul 22 21:33:04.904: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jul 22 21:33:04.904: INFO: kube-proxy-nghjt from kube-system started at 2021-07-22 20:36:07 +0000 UTC (2 container statuses recorded)
Jul 22 21:33:04.904: INFO: 	Container conntrack-fix ready: true, restart count 0
Jul 22 21:33:04.904: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 21:33:04.904: INFO: node-exporter-4gh9r from kube-system started at 2021-07-22 19:51:37 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.904: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 21:33:04.904: INFO: node-problem-detector-mttv7 from kube-system started at 2021-07-22 20:19:06 +0000 UTC (1 container statuses recorded)
Jul 22 21:33:04.904: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ae6188b6-0720-4e52-935e-1c38d243c455 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-ae6188b6-0720-4e52-935e-1c38d243c455 off the node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ae6188b6-0720-4e52-935e-1c38d243c455
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:33:11.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2169" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":339,"completed":298,"skipped":4795,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:33:11.126: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1529
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul 22 21:33:11.362: INFO: The status of Pod pod-update-activedeadlineseconds-1569a0b0-9b8d-4d79-8d66-64cdd6c788fa is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:33:13.374: INFO: The status of Pod pod-update-activedeadlineseconds-1569a0b0-9b8d-4d79-8d66-64cdd6c788fa is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 22 21:33:13.924: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1569a0b0-9b8d-4d79-8d66-64cdd6c788fa"
Jul 22 21:33:13.924: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1569a0b0-9b8d-4d79-8d66-64cdd6c788fa" in namespace "pods-1529" to be "terminated due to deadline exceeded"
Jul 22 21:33:13.935: INFO: Pod "pod-update-activedeadlineseconds-1569a0b0-9b8d-4d79-8d66-64cdd6c788fa": Phase="Running", Reason="", readiness=true. Elapsed: 10.881754ms
Jul 22 21:33:15.947: INFO: Pod "pod-update-activedeadlineseconds-1569a0b0-9b8d-4d79-8d66-64cdd6c788fa": Phase="Running", Reason="", readiness=true. Elapsed: 2.023440016s
Jul 22 21:33:17.959: INFO: Pod "pod-update-activedeadlineseconds-1569a0b0-9b8d-4d79-8d66-64cdd6c788fa": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.03536758s
Jul 22 21:33:17.959: INFO: Pod "pod-update-activedeadlineseconds-1569a0b0-9b8d-4d79-8d66-64cdd6c788fa" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:33:17.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1529" for this suite.
•{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":339,"completed":299,"skipped":4803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:33:17.991: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2056
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-1a2c22c8-e83a-46e0-8d54-2884cf57b15a
STEP: Creating a pod to test consume configMaps
Jul 22 21:33:18.224: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f6eb4c3b-15db-495c-b4e9-e2a9f8bb46b9" in namespace "projected-2056" to be "Succeeded or Failed"
Jul 22 21:33:18.234: INFO: Pod "pod-projected-configmaps-f6eb4c3b-15db-495c-b4e9-e2a9f8bb46b9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.672415ms
Jul 22 21:33:20.246: INFO: Pod "pod-projected-configmaps-f6eb4c3b-15db-495c-b4e9-e2a9f8bb46b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022640925s
Jul 22 21:33:22.258: INFO: Pod "pod-projected-configmaps-f6eb4c3b-15db-495c-b4e9-e2a9f8bb46b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034395607s
STEP: Saw pod success
Jul 22 21:33:22.258: INFO: Pod "pod-projected-configmaps-f6eb4c3b-15db-495c-b4e9-e2a9f8bb46b9" satisfied condition "Succeeded or Failed"
Jul 22 21:33:22.268: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-configmaps-f6eb4c3b-15db-495c-b4e9-e2a9f8bb46b9 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:33:22.349: INFO: Waiting for pod pod-projected-configmaps-f6eb4c3b-15db-495c-b4e9-e2a9f8bb46b9 to disappear
Jul 22 21:33:22.359: INFO: Pod pod-projected-configmaps-f6eb4c3b-15db-495c-b4e9-e2a9f8bb46b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:33:22.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2056" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":300,"skipped":4844,"failed":0}
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:33:22.391: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-1127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 22 21:33:23.146: INFO: starting watch
STEP: patching
STEP: updating
Jul 22 21:33:23.179: INFO: waiting for watch events with expected annotations
Jul 22 21:33:23.179: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:33:23.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1127" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":339,"completed":301,"skipped":4845,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:33:23.365: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:33:23.571: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6615146f-d525-4c43-b9c0-31fc13bef377" in namespace "projected-5732" to be "Succeeded or Failed"
Jul 22 21:33:23.582: INFO: Pod "downwardapi-volume-6615146f-d525-4c43-b9c0-31fc13bef377": Phase="Pending", Reason="", readiness=false. Elapsed: 10.581194ms
Jul 22 21:33:25.594: INFO: Pod "downwardapi-volume-6615146f-d525-4c43-b9c0-31fc13bef377": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02275286s
STEP: Saw pod success
Jul 22 21:33:25.594: INFO: Pod "downwardapi-volume-6615146f-d525-4c43-b9c0-31fc13bef377" satisfied condition "Succeeded or Failed"
Jul 22 21:33:25.604: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-6615146f-d525-4c43-b9c0-31fc13bef377 container client-container: <nil>
STEP: delete the pod
Jul 22 21:33:25.678: INFO: Waiting for pod downwardapi-volume-6615146f-d525-4c43-b9c0-31fc13bef377 to disappear
Jul 22 21:33:25.688: INFO: Pod downwardapi-volume-6615146f-d525-4c43-b9c0-31fc13bef377 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:33:25.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5732" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":302,"skipped":4857,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:33:25.721: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9509
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-9509
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-9509
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9509
Jul 22 21:33:25.960: INFO: Found 0 stateful pods, waiting for 1
Jul 22 21:33:35.972: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 22 21:33:35.983: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9509 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 21:33:36.610: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 21:33:36.610: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 21:33:36.610: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 21:33:36.621: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 22 21:33:46.634: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 21:33:46.634: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:33:46.680: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jul 22 21:33:46.680: INFO: ss-0  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:25 +0000 UTC  }]
Jul 22 21:33:46.680: INFO: 
Jul 22 21:33:46.680: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 22 21:33:47.692: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.987143485s
Jul 22 21:33:48.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.975326981s
Jul 22 21:33:49.717: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.962484999s
Jul 22 21:33:50.730: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.950284831s
Jul 22 21:33:51.742: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.938110757s
Jul 22 21:33:52.755: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.925358721s
Jul 22 21:33:53.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.912472538s
Jul 22 21:33:54.779: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.900878658s
Jul 22 21:33:55.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 888.345773ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9509
Jul 22 21:33:56.804: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9509 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 21:33:57.167: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 21:33:57.167: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 21:33:57.168: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 21:33:57.168: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9509 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 21:33:57.652: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 22 21:33:57.652: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 21:33:57.652: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 21:33:57.652: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9509 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 21:33:58.073: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 22 21:33:58.073: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 21:33:58.073: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 21:33:58.084: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:33:58.084: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:33:58.084: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 22 21:33:58.096: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9509 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 21:33:58.498: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 21:33:58.498: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 21:33:58.498: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 21:33:58.498: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9509 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 21:33:58.890: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 21:33:58.890: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 21:33:58.890: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 21:33:58.890: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-9509 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 21:33:59.325: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 21:33:59.325: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 21:33:59.325: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 21:33:59.325: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:33:59.336: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 22 21:34:09.362: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 21:34:09.362: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 21:34:09.362: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 21:34:09.397: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jul 22 21:34:09.397: INFO: ss-0  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:25 +0000 UTC  }]
Jul 22 21:34:09.398: INFO: ss-1  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  }]
Jul 22 21:34:09.398: INFO: ss-2  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  }]
Jul 22 21:34:09.398: INFO: 
Jul 22 21:34:09.398: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 21:34:10.410: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jul 22 21:34:10.410: INFO: ss-0  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:25 +0000 UTC  }]
Jul 22 21:34:10.410: INFO: ss-1  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  }]
Jul 22 21:34:10.410: INFO: ss-2  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  }]
Jul 22 21:34:10.410: INFO: 
Jul 22 21:34:10.410: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 21:34:11.423: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jul 22 21:34:11.423: INFO: ss-0  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:58 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:25 +0000 UTC  }]
Jul 22 21:34:11.423: INFO: ss-1  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  }]
Jul 22 21:34:11.423: INFO: ss-2  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  }]
Jul 22 21:34:11.423: INFO: 
Jul 22 21:34:11.423: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 21:34:12.436: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jul 22 21:34:12.436: INFO: ss-1  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  }]
Jul 22 21:34:12.436: INFO: 
Jul 22 21:34:12.436: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 22 21:34:13.448: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jul 22 21:34:13.448: INFO: ss-1  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  }]
Jul 22 21:34:13.448: INFO: 
Jul 22 21:34:13.448: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 22 21:34:14.459: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jul 22 21:34:14.459: INFO: ss-1  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  }]
Jul 22 21:34:14.459: INFO: 
Jul 22 21:34:14.459: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 22 21:34:15.471: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jul 22 21:34:15.471: INFO: ss-1  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  }]
Jul 22 21:34:15.471: INFO: 
Jul 22 21:34:15.471: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 22 21:34:16.483: INFO: POD   NODE                                          PHASE    GRACE  CONDITIONS
Jul 22 21:34:16.483: INFO: ss-1  shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:59 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-22 21:33:46 +0000 UTC  }]
Jul 22 21:34:16.483: INFO: 
Jul 22 21:34:16.483: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 22 21:34:17.495: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.900878355s
Jul 22 21:34:18.506: INFO: Verifying statefulset ss doesn't scale past 0 for another 889.666525ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9509
Jul 22 21:34:19.518: INFO: Scaling statefulset ss to 0
Jul 22 21:34:19.553: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul 22 21:34:19.563: INFO: Deleting all statefulset in ns statefulset-9509
Jul 22 21:34:19.574: INFO: Scaling statefulset ss to 0
Jul 22 21:34:19.608: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:34:19.619: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:34:19.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9509" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":339,"completed":303,"skipped":4870,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:34:19.683: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-643
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:34:36.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-643" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":339,"completed":304,"skipped":4874,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:34:37.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3522
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3522.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3522.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 21:34:41.504: INFO: DNS probes using dns-3522/dns-test-87f500c6-2c14-4694-97ea-97518ff0b6e5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:34:41.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3522" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":339,"completed":305,"skipped":4878,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:34:41.551: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7063
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jul 22 21:34:41.747: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:35:03.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7063" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":339,"completed":306,"skipped":4882,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:35:03.814: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9332
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-9e023812-8612-4515-8fb2-45505416f2ef
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:35:04.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9332" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":339,"completed":307,"skipped":4894,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:35:04.038: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-71f4d474-3f66-466a-970d-40325e337cd8 in namespace container-probe-4582
Jul 22 21:35:06.270: INFO: Started pod liveness-71f4d474-3f66-466a-970d-40325e337cd8 in namespace container-probe-4582
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 21:35:06.280: INFO: Initial restart count of pod liveness-71f4d474-3f66-466a-970d-40325e337cd8 is 0
Jul 22 21:35:26.412: INFO: Restart count of pod container-probe-4582/liveness-71f4d474-3f66-466a-970d-40325e337cd8 is now 1 (20.132065052s elapsed)
Jul 22 21:35:46.530: INFO: Restart count of pod container-probe-4582/liveness-71f4d474-3f66-466a-970d-40325e337cd8 is now 2 (40.249903481s elapsed)
Jul 22 21:36:06.652: INFO: Restart count of pod container-probe-4582/liveness-71f4d474-3f66-466a-970d-40325e337cd8 is now 3 (1m0.372335623s elapsed)
Jul 22 21:36:24.763: INFO: Restart count of pod container-probe-4582/liveness-71f4d474-3f66-466a-970d-40325e337cd8 is now 4 (1m18.483045678s elapsed)
Jul 22 21:37:37.194: INFO: Restart count of pod container-probe-4582/liveness-71f4d474-3f66-466a-970d-40325e337cd8 is now 5 (2m30.913626714s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:37:37.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4582" for this suite.
•{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":339,"completed":308,"skipped":4908,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:37:37.242: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9080
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-a3538606-eb70-45e2-9e67-c79f714acaf3
STEP: Creating the pod
Jul 22 21:37:37.516: INFO: The status of Pod pod-configmaps-37af6d7b-db48-4e9a-9e7f-5c2e15c56f06 is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:37:39.527: INFO: The status of Pod pod-configmaps-37af6d7b-db48-4e9a-9e7f-5c2e15c56f06 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-a3538606-eb70-45e2-9e67-c79f714acaf3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:37:41.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9080" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":309,"skipped":4933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:37:41.682: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4864
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 22 21:37:41.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1d0d1b9-0b42-4121-b47e-dda785839f82" in namespace "projected-4864" to be "Succeeded or Failed"
Jul 22 21:37:41.909: INFO: Pod "downwardapi-volume-d1d0d1b9-0b42-4121-b47e-dda785839f82": Phase="Pending", Reason="", readiness=false. Elapsed: 10.717338ms
Jul 22 21:37:43.921: INFO: Pod "downwardapi-volume-d1d0d1b9-0b42-4121-b47e-dda785839f82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022798675s
Jul 22 21:37:45.933: INFO: Pod "downwardapi-volume-d1d0d1b9-0b42-4121-b47e-dda785839f82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035083523s
STEP: Saw pod success
Jul 22 21:37:45.933: INFO: Pod "downwardapi-volume-d1d0d1b9-0b42-4121-b47e-dda785839f82" satisfied condition "Succeeded or Failed"
Jul 22 21:37:45.944: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod downwardapi-volume-d1d0d1b9-0b42-4121-b47e-dda785839f82 container client-container: <nil>
STEP: delete the pod
Jul 22 21:37:45.982: INFO: Waiting for pod downwardapi-volume-d1d0d1b9-0b42-4121-b47e-dda785839f82 to disappear
Jul 22 21:37:45.993: INFO: Pod downwardapi-volume-d1d0d1b9-0b42-4121-b47e-dda785839f82 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:37:45.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4864" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":310,"skipped":4997,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:37:46.025: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2561
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:37:46.253: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 22 21:37:50.790: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2561 --namespace=crd-publish-openapi-2561 create -f -'
Jul 22 21:37:51.444: INFO: stderr: ""
Jul 22 21:37:51.444: INFO: stdout: "e2e-test-crd-publish-openapi-9576-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 22 21:37:51.444: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2561 --namespace=crd-publish-openapi-2561 delete e2e-test-crd-publish-openapi-9576-crds test-cr'
Jul 22 21:37:51.582: INFO: stderr: ""
Jul 22 21:37:51.582: INFO: stdout: "e2e-test-crd-publish-openapi-9576-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul 22 21:37:51.582: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2561 --namespace=crd-publish-openapi-2561 apply -f -'
Jul 22 21:37:52.076: INFO: stderr: ""
Jul 22 21:37:52.076: INFO: stdout: "e2e-test-crd-publish-openapi-9576-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 22 21:37:52.076: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2561 --namespace=crd-publish-openapi-2561 delete e2e-test-crd-publish-openapi-9576-crds test-cr'
Jul 22 21:37:52.211: INFO: stderr: ""
Jul 22 21:37:52.211: INFO: stdout: "e2e-test-crd-publish-openapi-9576-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 22 21:37:52.211: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-2561 explain e2e-test-crd-publish-openapi-9576-crds'
Jul 22 21:37:52.595: INFO: stderr: ""
Jul 22 21:37:52.595: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9576-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:37:57.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2561" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":339,"completed":311,"skipped":5016,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:37:57.053: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5793
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 22 21:37:57.322: INFO: Waiting up to 5m0s for pod "pod-50a6d499-5aec-401b-aaf6-afe0cd0d66e1" in namespace "emptydir-5793" to be "Succeeded or Failed"
Jul 22 21:37:57.333: INFO: Pod "pod-50a6d499-5aec-401b-aaf6-afe0cd0d66e1": Phase="Pending", Reason="", readiness=false. Elapsed: 11.564154ms
Jul 22 21:37:59.344: INFO: Pod "pod-50a6d499-5aec-401b-aaf6-afe0cd0d66e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022532554s
STEP: Saw pod success
Jul 22 21:37:59.344: INFO: Pod "pod-50a6d499-5aec-401b-aaf6-afe0cd0d66e1" satisfied condition "Succeeded or Failed"
Jul 22 21:37:59.355: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-50a6d499-5aec-401b-aaf6-afe0cd0d66e1 container test-container: <nil>
STEP: delete the pod
Jul 22 21:37:59.416: INFO: Waiting for pod pod-50a6d499-5aec-401b-aaf6-afe0cd0d66e1 to disappear
Jul 22 21:37:59.427: INFO: Pod pod-50a6d499-5aec-401b-aaf6-afe0cd0d66e1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:37:59.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5793" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":312,"skipped":5035,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:37:59.459: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-d513384f-4ac8-49c1-bd89-25ab2b9b4a07
STEP: Creating secret with name secret-projected-all-test-volume-e131b8c8-2147-4484-bd3d-52e4924ab390
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 22 21:37:59.692: INFO: Waiting up to 5m0s for pod "projected-volume-2a692285-4021-4135-81af-4aecc685ffa5" in namespace "projected-9687" to be "Succeeded or Failed"
Jul 22 21:37:59.703: INFO: Pod "projected-volume-2a692285-4021-4135-81af-4aecc685ffa5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.304153ms
Jul 22 21:38:01.714: INFO: Pod "projected-volume-2a692285-4021-4135-81af-4aecc685ffa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021332801s
STEP: Saw pod success
Jul 22 21:38:01.714: INFO: Pod "projected-volume-2a692285-4021-4135-81af-4aecc685ffa5" satisfied condition "Succeeded or Failed"
Jul 22 21:38:01.724: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod projected-volume-2a692285-4021-4135-81af-4aecc685ffa5 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 22 21:38:01.759: INFO: Waiting for pod projected-volume-2a692285-4021-4135-81af-4aecc685ffa5 to disappear
Jul 22 21:38:01.770: INFO: Pod projected-volume-2a692285-4021-4135-81af-4aecc685ffa5 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:38:01.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9687" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":339,"completed":313,"skipped":5077,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:38:01.802: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2337
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jul 22 21:38:01.996: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2337 create -f -'
Jul 22 21:38:02.359: INFO: stderr: ""
Jul 22 21:38:02.359: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 22 21:38:03.371: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:38:03.371: INFO: Found 0 / 1
Jul 22 21:38:04.372: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:38:04.372: INFO: Found 1 / 1
Jul 22 21:38:04.372: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 22 21:38:04.383: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:38:04.383: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 22 21:38:04.383: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2337 patch pod agnhost-primary-6wjzg -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 22 21:38:04.527: INFO: stderr: ""
Jul 22 21:38:04.527: INFO: stdout: "pod/agnhost-primary-6wjzg patched\n"
STEP: checking annotations
Jul 22 21:38:04.538: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 22 21:38:04.538: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:38:04.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2337" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":339,"completed":314,"skipped":5080,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:38:04.571: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4457
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-f607c1e4-8262-4e72-9dc0-c3c91c933152
STEP: Creating a pod to test consume secrets
Jul 22 21:38:04.852: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9f2a57d8-9072-402b-a0aa-50d9bc5693d2" in namespace "projected-4457" to be "Succeeded or Failed"
Jul 22 21:38:04.863: INFO: Pod "pod-projected-secrets-9f2a57d8-9072-402b-a0aa-50d9bc5693d2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.929114ms
Jul 22 21:38:06.874: INFO: Pod "pod-projected-secrets-9f2a57d8-9072-402b-a0aa-50d9bc5693d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021834819s
Jul 22 21:38:08.886: INFO: Pod "pod-projected-secrets-9f2a57d8-9072-402b-a0aa-50d9bc5693d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033822378s
STEP: Saw pod success
Jul 22 21:38:08.886: INFO: Pod "pod-projected-secrets-9f2a57d8-9072-402b-a0aa-50d9bc5693d2" satisfied condition "Succeeded or Failed"
Jul 22 21:38:08.897: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-secrets-9f2a57d8-9072-402b-a0aa-50d9bc5693d2 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 21:38:08.933: INFO: Waiting for pod pod-projected-secrets-9f2a57d8-9072-402b-a0aa-50d9bc5693d2 to disappear
Jul 22 21:38:08.944: INFO: Pod pod-projected-secrets-9f2a57d8-9072-402b-a0aa-50d9bc5693d2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:38:08.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4457" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":315,"skipped":5083,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:38:08.976: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:38:09.710: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 22 21:38:11.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586689, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586689, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586689, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63762586689, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:38:14.774: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:38:14.785: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-18-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:38:18.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9533" for this suite.
STEP: Destroying namespace "webhook-9533-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":339,"completed":316,"skipped":5097,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:38:18.539: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3068
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3068
STEP: creating replication controller externalsvc in namespace services-3068
I0722 21:38:18.774278    5669 runners.go:190] Created replication controller with name: externalsvc, namespace: services-3068, replica count: 2
I0722 21:38:21.825830    5669 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jul 22 21:38:21.870: INFO: Creating new exec pod
Jul 22 21:38:23.908: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-3068 exec execpod5x7px -- /bin/sh -x -c nslookup clusterip-service.services-3068.svc.cluster.local'
Jul 22 21:38:24.380: INFO: stderr: "+ nslookup clusterip-service.services-3068.svc.cluster.local\n"
Jul 22 21:38:24.380: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nclusterip-service.services-3068.svc.cluster.local\tcanonical name = externalsvc.services-3068.svc.cluster.local.\nName:\texternalsvc.services-3068.svc.cluster.local\nAddress: 100.66.144.181\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3068, will wait for the garbage collector to delete the pods
Jul 22 21:38:24.455: INFO: Deleting ReplicationController externalsvc took: 13.178019ms
Jul 22 21:38:24.556: INFO: Terminating ReplicationController externalsvc pods took: 100.74233ms
Jul 22 21:38:37.076: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:38:37.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3068" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":339,"completed":317,"skipped":5100,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:38:37.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1711
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 22 21:38:37.405: INFO: Number of nodes with available pods: 0
Jul 22 21:38:37.405: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:38:38.438: INFO: Number of nodes with available pods: 0
Jul 22 21:38:38.438: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:38:39.437: INFO: Number of nodes with available pods: 1
Jul 22 21:38:39.437: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-4s7z5 is running more than one daemon pod
Jul 22 21:38:40.437: INFO: Number of nodes with available pods: 2
Jul 22 21:38:40.437: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 22 21:38:40.495: INFO: Number of nodes with available pods: 1
Jul 22 21:38:40.495: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 is running more than one daemon pod
Jul 22 21:38:41.526: INFO: Number of nodes with available pods: 1
Jul 22 21:38:41.526: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 is running more than one daemon pod
Jul 22 21:38:42.527: INFO: Number of nodes with available pods: 1
Jul 22 21:38:42.527: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 is running more than one daemon pod
Jul 22 21:38:43.527: INFO: Number of nodes with available pods: 1
Jul 22 21:38:43.527: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 is running more than one daemon pod
Jul 22 21:38:44.526: INFO: Number of nodes with available pods: 1
Jul 22 21:38:44.526: INFO: Node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 is running more than one daemon pod
Jul 22 21:38:45.525: INFO: Number of nodes with available pods: 2
Jul 22 21:38:45.525: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1711, will wait for the garbage collector to delete the pods
Jul 22 21:38:45.610: INFO: Deleting DaemonSet.extensions daemon-set took: 12.426277ms
Jul 22 21:38:45.711: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.117863ms
Jul 22 21:38:57.922: INFO: Number of nodes with available pods: 0
Jul 22 21:38:57.922: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 21:38:57.933: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"52317"},"items":null}

Jul 22 21:38:57.943: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"52317"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:38:57.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1711" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":339,"completed":318,"skipped":5179,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:38:58.009: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-414
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-c94443c8-bfca-4e07-bb0f-6b0a95a51419
STEP: Creating a pod to test consume configMaps
Jul 22 21:38:58.227: INFO: Waiting up to 5m0s for pod "pod-configmaps-e298fffa-fbfe-4891-8a46-ee0e32e372a0" in namespace "configmap-414" to be "Succeeded or Failed"
Jul 22 21:38:58.238: INFO: Pod "pod-configmaps-e298fffa-fbfe-4891-8a46-ee0e32e372a0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.133925ms
Jul 22 21:39:00.250: INFO: Pod "pod-configmaps-e298fffa-fbfe-4891-8a46-ee0e32e372a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02223522s
STEP: Saw pod success
Jul 22 21:39:00.250: INFO: Pod "pod-configmaps-e298fffa-fbfe-4891-8a46-ee0e32e372a0" satisfied condition "Succeeded or Failed"
Jul 22 21:39:00.260: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-configmaps-e298fffa-fbfe-4891-8a46-ee0e32e372a0 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:39:00.338: INFO: Waiting for pod pod-configmaps-e298fffa-fbfe-4891-8a46-ee0e32e372a0 to disappear
Jul 22 21:39:00.348: INFO: Pod pod-configmaps-e298fffa-fbfe-4891-8a46-ee0e32e372a0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:39:00.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-414" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":319,"skipped":5184,"failed":0}
SSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:39:00.380: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-6771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jul 22 21:39:00.654: INFO: running pods: 0 < 3
Jul 22 21:39:02.667: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:39:04.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6771" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":339,"completed":320,"skipped":5190,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:39:04.710: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-7692
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 22 21:39:04.940: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 22 21:40:05.045: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Jul 22 21:40:05.095: INFO: Created pod: pod0-sched-preemption-low-priority
Jul 22 21:40:05.129: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:40:21.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7692" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78
•{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":339,"completed":321,"skipped":5198,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:40:21.351: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8766
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8766
I0722 21:40:21.593597    5669 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8766, replica count: 2
Jul 22 21:40:24.644: INFO: Creating new exec pod
I0722 21:40:24.644492    5669 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 21:40:27.685: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8766 exec execpodm6wxw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 22 21:40:28.109: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 22 21:40:28.109: INFO: stdout: ""
Jul 22 21:40:29.109: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8766 exec execpodm6wxw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 22 21:40:29.497: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 22 21:40:29.497: INFO: stdout: ""
Jul 22 21:40:30.110: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8766 exec execpodm6wxw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 22 21:40:30.531: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 22 21:40:30.531: INFO: stdout: ""
Jul 22 21:40:31.109: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8766 exec execpodm6wxw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 22 21:40:31.476: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 22 21:40:31.476: INFO: stdout: "externalname-service-l97hd"
Jul 22 21:40:31.476: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8766 exec execpodm6wxw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.66.59.241 80'
Jul 22 21:40:31.797: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.66.59.241 80\nConnection to 100.66.59.241 80 port [tcp/http] succeeded!\n"
Jul 22 21:40:31.797: INFO: stdout: ""
Jul 22 21:40:32.797: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8766 exec execpodm6wxw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.66.59.241 80'
Jul 22 21:40:33.181: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.66.59.241 80\nConnection to 100.66.59.241 80 port [tcp/http] succeeded!\n"
Jul 22 21:40:33.181: INFO: stdout: ""
Jul 22 21:40:33.798: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-8766 exec execpodm6wxw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.66.59.241 80'
Jul 22 21:40:34.162: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.66.59.241 80\nConnection to 100.66.59.241 80 port [tcp/http] succeeded!\n"
Jul 22 21:40:34.162: INFO: stdout: "externalname-service-l97hd"
Jul 22 21:40:34.162: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:40:34.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8766" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":339,"completed":322,"skipped":5215,"failed":0}
S
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:40:34.213: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-9391
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Jul 22 21:40:34.418: INFO: created test-podtemplate-1
Jul 22 21:40:34.431: INFO: created test-podtemplate-2
Jul 22 21:40:34.442: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jul 22 21:40:34.453: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jul 22 21:40:34.472: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:40:34.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9391" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":339,"completed":323,"skipped":5216,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:40:34.506: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-326
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Jul 22 21:40:34.714: INFO: Waiting up to 5m0s for pod "client-containers-b02d2f69-27cd-4b44-9471-619ac3e84778" in namespace "containers-326" to be "Succeeded or Failed"
Jul 22 21:40:34.725: INFO: Pod "client-containers-b02d2f69-27cd-4b44-9471-619ac3e84778": Phase="Pending", Reason="", readiness=false. Elapsed: 10.369431ms
Jul 22 21:40:36.737: INFO: Pod "client-containers-b02d2f69-27cd-4b44-9471-619ac3e84778": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022819239s
Jul 22 21:40:38.750: INFO: Pod "client-containers-b02d2f69-27cd-4b44-9471-619ac3e84778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035490763s
STEP: Saw pod success
Jul 22 21:40:38.750: INFO: Pod "client-containers-b02d2f69-27cd-4b44-9471-619ac3e84778" satisfied condition "Succeeded or Failed"
Jul 22 21:40:38.761: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod client-containers-b02d2f69-27cd-4b44-9471-619ac3e84778 container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:40:38.834: INFO: Waiting for pod client-containers-b02d2f69-27cd-4b44-9471-619ac3e84778 to disappear
Jul 22 21:40:38.845: INFO: Pod client-containers-b02d2f69-27cd-4b44-9471-619ac3e84778 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:40:38.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-326" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":339,"completed":324,"skipped":5229,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:40:38.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-71320296-0024-4e3b-aae8-469729eebc02
STEP: Creating a pod to test consume configMaps
Jul 22 21:40:39.101: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fbc9e9b2-9bc5-4190-b561-96a44d9eef7a" in namespace "projected-8902" to be "Succeeded or Failed"
Jul 22 21:40:39.111: INFO: Pod "pod-projected-configmaps-fbc9e9b2-9bc5-4190-b561-96a44d9eef7a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.269278ms
Jul 22 21:40:41.123: INFO: Pod "pod-projected-configmaps-fbc9e9b2-9bc5-4190-b561-96a44d9eef7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022008797s
Jul 22 21:40:43.135: INFO: Pod "pod-projected-configmaps-fbc9e9b2-9bc5-4190-b561-96a44d9eef7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033662591s
STEP: Saw pod success
Jul 22 21:40:43.135: INFO: Pod "pod-projected-configmaps-fbc9e9b2-9bc5-4190-b561-96a44d9eef7a" satisfied condition "Succeeded or Failed"
Jul 22 21:40:43.146: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-projected-configmaps-fbc9e9b2-9bc5-4190-b561-96a44d9eef7a container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:40:43.225: INFO: Waiting for pod pod-projected-configmaps-fbc9e9b2-9bc5-4190-b561-96a44d9eef7a to disappear
Jul 22 21:40:43.236: INFO: Pod pod-projected-configmaps-fbc9e9b2-9bc5-4190-b561-96a44d9eef7a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:40:43.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8902" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":325,"skipped":5267,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:40:43.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5406
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 22 21:40:46.535: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:40:46.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5406" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":326,"skipped":5276,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:40:46.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6155
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:40:50.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6155" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":339,"completed":327,"skipped":5290,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:40:50.862: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-5999
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:40:51.051: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating first CR 
Jul 22 21:40:53.244: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-22T21:40:53Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-22T21:40:53Z]] name:name1 resourceVersion:53268 uid:bc6b7526-f71f-44f6-9cd9-cf2d0629e980] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jul 22 21:41:03.256: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-22T21:41:03Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-22T21:41:03Z]] name:name2 resourceVersion:53354 uid:172f8e54-721d-406e-befc-ec398e941525] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jul 22 21:41:13.270: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-22T21:40:53Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-22T21:41:13Z]] name:name1 resourceVersion:53406 uid:bc6b7526-f71f-44f6-9cd9-cf2d0629e980] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jul 22 21:41:23.283: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-22T21:41:03Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-22T21:41:23Z]] name:name2 resourceVersion:53461 uid:172f8e54-721d-406e-befc-ec398e941525] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jul 22 21:41:33.297: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-22T21:40:53Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-22T21:41:13Z]] name:name1 resourceVersion:53515 uid:bc6b7526-f71f-44f6-9cd9-cf2d0629e980] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jul 22 21:41:43.311: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-22T21:41:03Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-22T21:41:23Z]] name:name2 resourceVersion:53570 uid:172f8e54-721d-406e-befc-ec398e941525] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:41:53.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5999" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":339,"completed":328,"skipped":5301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:41:53.875: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-1788
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:41:54.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1788" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":329,"skipped":5328,"failed":0}
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:41:54.333: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-6622
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jul 22 21:41:54.623: INFO: Found 0 stateful pods, waiting for 3
Jul 22 21:42:04.636: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:42:04.636: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:42:04.636: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 21:42:04.669: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-6622 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 21:42:05.087: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 21:42:05.087: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 21:42:05.087: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Jul 22 21:42:15.166: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 22 21:42:25.221: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-6622 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 21:42:25.657: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 21:42:25.657: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 21:42:25.657: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 21:42:35.726: INFO: Waiting for StatefulSet statefulset-6622/ss2 to complete update
Jul 22 21:42:35.726: INFO: Waiting for Pod statefulset-6622/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 22 21:42:35.726: INFO: Waiting for Pod statefulset-6622/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 22 21:42:35.726: INFO: Waiting for Pod statefulset-6622/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 22 21:42:45.749: INFO: Waiting for StatefulSet statefulset-6622/ss2 to complete update
Jul 22 21:42:45.749: INFO: Waiting for Pod statefulset-6622/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 22 21:42:45.749: INFO: Waiting for Pod statefulset-6622/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Rolling back to a previous revision
Jul 22 21:42:55.750: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-6622 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 22 21:42:56.250: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 22 21:42:56.251: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 22 21:42:56.251: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 22 21:43:06.330: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 22 21:43:16.384: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-6622 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 22 21:43:16.775: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 22 21:43:16.775: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 22 21:43:16.775: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 22 21:43:26.843: INFO: Waiting for StatefulSet statefulset-6622/ss2 to complete update
Jul 22 21:43:26.843: INFO: Waiting for Pod statefulset-6622/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Jul 22 21:43:26.843: INFO: Waiting for Pod statefulset-6622/ss2-1 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Jul 22 21:43:26.843: INFO: Waiting for Pod statefulset-6622/ss2-2 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Jul 22 21:43:36.868: INFO: Waiting for StatefulSet statefulset-6622/ss2 to complete update
Jul 22 21:43:36.868: INFO: Waiting for Pod statefulset-6622/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Jul 22 21:43:46.866: INFO: Waiting for StatefulSet statefulset-6622/ss2 to complete update
Jul 22 21:43:46.867: INFO: Waiting for Pod statefulset-6622/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul 22 21:43:56.866: INFO: Deleting all statefulset in ns statefulset-6622
Jul 22 21:43:56.877: INFO: Scaling statefulset ss2 to 0
Jul 22 21:44:36.924: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 21:44:36.934: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:44:36.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6622" for this suite.
•{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":339,"completed":330,"skipped":5332,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:44:36.999: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3697
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 22 21:44:37.185: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:44:37.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3697" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":339,"completed":331,"skipped":5356,"failed":0}
S
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:44:38.027: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jul 22 21:44:38.264: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:44:43.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-154" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":339,"completed":332,"skipped":5357,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:44:43.415: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5715
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Jul 22 21:44:43.602: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-5715 api-versions'
Jul 22 21:44:43.712: INFO: stderr: ""
Jul 22 21:44:43.712: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling.k8s.io/v1\nautoscaling.k8s.io/v1beta2\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\ndns.gardener.cloud/v1alpha1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:44:43.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5715" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":339,"completed":333,"skipped":5366,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:44:43.735: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6537
STEP: creating service affinity-clusterip in namespace services-6537
STEP: creating replication controller affinity-clusterip in namespace services-6537
I0722 21:44:43.955168    5669 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-6537, replica count: 3
I0722 21:44:47.006667    5669 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 21:44:47.028: INFO: Creating new exec pod
Jul 22 21:44:52.067: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6537 exec execpod-affinity66c6h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jul 22 21:44:52.520: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jul 22 21:44:52.520: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 21:44:52.520: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6537 exec execpod-affinity66c6h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.68.233.164 80'
Jul 22 21:44:52.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.68.233.164 80\nConnection to 100.68.233.164 80 port [tcp/http] succeeded!\n"
Jul 22 21:44:52.856: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 22 21:44:52.856: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmy8a-9u6.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=services-6537 exec execpod-affinity66c6h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.68.233.164:80/ ; done'
Jul 22 21:44:53.298: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.68.233.164:80/\n"
Jul 22 21:44:53.298: INFO: stdout: "\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4\naffinity-clusterip-nzqw4"
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Received response from host: affinity-clusterip-nzqw4
Jul 22 21:44:53.298: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6537, will wait for the garbage collector to delete the pods
Jul 22 21:44:53.388: INFO: Deleting ReplicationController affinity-clusterip took: 11.974488ms
Jul 22 21:44:53.489: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.212005ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:45:07.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6537" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":334,"skipped":5378,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:45:07.038: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6184
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-6a52ff52-c8a5-42f4-8c2b-666e1c116509
STEP: Creating a pod to test consume configMaps
Jul 22 21:45:07.260: INFO: Waiting up to 5m0s for pod "pod-configmaps-faa61d85-0284-476d-a3ab-d7318c256fcf" in namespace "configmap-6184" to be "Succeeded or Failed"
Jul 22 21:45:07.271: INFO: Pod "pod-configmaps-faa61d85-0284-476d-a3ab-d7318c256fcf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.413326ms
Jul 22 21:45:09.283: INFO: Pod "pod-configmaps-faa61d85-0284-476d-a3ab-d7318c256fcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022384583s
Jul 22 21:45:11.294: INFO: Pod "pod-configmaps-faa61d85-0284-476d-a3ab-d7318c256fcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033931523s
STEP: Saw pod success
Jul 22 21:45:11.294: INFO: Pod "pod-configmaps-faa61d85-0284-476d-a3ab-d7318c256fcf" satisfied condition "Succeeded or Failed"
Jul 22 21:45:11.305: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-configmaps-faa61d85-0284-476d-a3ab-d7318c256fcf container agnhost-container: <nil>
STEP: delete the pod
Jul 22 21:45:11.351: INFO: Waiting for pod pod-configmaps-faa61d85-0284-476d-a3ab-d7318c256fcf to disappear
Jul 22 21:45:11.361: INFO: Pod pod-configmaps-faa61d85-0284-476d-a3ab-d7318c256fcf no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:45:11.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6184" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":335,"skipped":5393,"failed":0}
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:45:11.393: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jul 22 21:45:11.615: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:45:13.626: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:45:15.626: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jul 22 21:45:15.666: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:45:17.679: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:45:19.678: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 22 21:45:19.759: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 21:45:19.770: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 21:45:21.771: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 21:45:21.782: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 21:45:23.771: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 21:45:23.782: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 21:45:25.771: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 21:45:25.782: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 21:45:27.770: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 21:45:27.782: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:45:27.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8504" for this suite.
•{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":339,"completed":336,"skipped":5396,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:45:27.813: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5457
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Jul 22 21:45:28.034: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:45:30.045: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jul 22 21:45:32.044: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:45:33.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5457" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":339,"completed":337,"skipped":5397,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:45:33.124: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 22 21:45:33.810: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 22 21:45:36.864: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:45:36.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7878" for this suite.
STEP: Destroying namespace "webhook-7878-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":339,"completed":338,"skipped":5406,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 22 21:45:36.999: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 22 21:45:37.206: INFO: Waiting up to 5m0s for pod "pod-e2dac02e-b977-4f25-8e7b-24a559d142d0" in namespace "emptydir-3920" to be "Succeeded or Failed"
Jul 22 21:45:37.216: INFO: Pod "pod-e2dac02e-b977-4f25-8e7b-24a559d142d0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024075ms
Jul 22 21:45:39.228: INFO: Pod "pod-e2dac02e-b977-4f25-8e7b-24a559d142d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021938167s
Jul 22 21:45:41.240: INFO: Pod "pod-e2dac02e-b977-4f25-8e7b-24a559d142d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034010871s
STEP: Saw pod success
Jul 22 21:45:41.240: INFO: Pod "pod-e2dac02e-b977-4f25-8e7b-24a559d142d0" satisfied condition "Succeeded or Failed"
Jul 22 21:45:41.251: INFO: Trying to get logs from node shoot--it--tmy8a-9u6-worker-1-z1-5d845-cr8n9 pod pod-e2dac02e-b977-4f25-8e7b-24a559d142d0 container test-container: <nil>
STEP: delete the pod
Jul 22 21:45:41.322: INFO: Waiting for pod pod-e2dac02e-b977-4f25-8e7b-24a559d142d0 to disappear
Jul 22 21:45:41.332: INFO: Pod pod-e2dac02e-b977-4f25-8e7b-24a559d142d0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 22 21:45:41.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3920" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":339,"skipped":5430,"failed":0}
SSJul 22 21:45:41.363: INFO: Running AfterSuite actions on all nodes
Jul 22 21:45:41.363: INFO: Running AfterSuite actions on node 1
Jul 22 21:45:41.363: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/e2e/artifacts/1626983901/junit_01.xml
{"msg":"Test Suite completed","total":339,"completed":339,"skipped":5432,"failed":0}

Ran 339 of 5771 Specs in 6437.481 seconds
SUCCESS! -- 339 Passed | 0 Failed | 0 Flaked | 0 Pending | 5432 Skipped
PASS

Ginkgo ran 1 suite in 1h47m19.04356995s
Test Suite Passed
