I0927 15:43:34.096011      23 e2e.go:129] Starting e2e run "fb189d23-a8b0-43e8-8d06-9673ddd11c59" on Ginkgo node 1
{"msg":"Test Suite starting","total":339,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1632757412 - Will randomize all specs
Will run 339 of 5771 specs

Sep 27 15:43:34.106: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 15:43:34.108: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 27 15:43:34.122: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 27 15:43:34.157: INFO: 28 / 28 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 27 15:43:34.157: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Sep 27 15:43:34.157: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 27 15:43:34.165: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'antrea-agent' (0 seconds elapsed)
Sep 27 15:43:34.165: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep 27 15:43:34.165: INFO: e2e test version: v1.21.2
Sep 27 15:43:34.166: INFO: kube-apiserver version: v1.21.2+vmware.1
Sep 27 15:43:34.166: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 15:43:34.171: INFO: Cluster IP family: ipv4
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:43:34.171: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
Sep 27 15:43:34.205: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
W0927 15:43:34.205433      23 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-f4d3b0de-d960-4f9e-aa71-1181f31d5fbb
STEP: Creating a pod to test consume secrets
Sep 27 15:43:34.224: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38" in namespace "projected-5972" to be "Succeeded or Failed"
Sep 27 15:43:34.227: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38": Phase="Pending", Reason="", readiness=false. Elapsed: 3.604607ms
Sep 27 15:43:36.232: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00827026s
Sep 27 15:43:38.237: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013055521s
Sep 27 15:43:40.305: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081108625s
Sep 27 15:43:42.311: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087054706s
Sep 27 15:43:44.317: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38": Phase="Pending", Reason="", readiness=false. Elapsed: 10.092920247s
Sep 27 15:43:46.322: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38": Phase="Pending", Reason="", readiness=false. Elapsed: 12.097922809s
Sep 27 15:43:48.327: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38": Phase="Pending", Reason="", readiness=false. Elapsed: 14.102890328s
Sep 27 15:43:50.331: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38": Phase="Pending", Reason="", readiness=false. Elapsed: 16.107663073s
Sep 27 15:43:52.337: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38": Phase="Pending", Reason="", readiness=false. Elapsed: 18.112961092s
Sep 27 15:43:54.341: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38": Phase="Pending", Reason="", readiness=false. Elapsed: 20.11783547s
Sep 27 15:43:56.347: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.123546929s
STEP: Saw pod success
Sep 27 15:43:56.347: INFO: Pod "pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38" satisfied condition "Succeeded or Failed"
Sep 27 15:43:56.351: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 15:43:56.383: INFO: Waiting for pod pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38 to disappear
Sep 27 15:43:56.386: INFO: Pod pod-projected-secrets-9aa3b72d-bbb6-488b-8ac3-9afbf0821d38 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:43:56.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5972" for this suite.

• [SLOW TEST:22.226 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":1,"skipped":7,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:43:56.398: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 27 15:43:56.455: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7477  845ab4a1-0093-4fe7-a8a7-5e752e1aa495 1683809 0 2021-09-27 15:43:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-27 15:43:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 15:43:56.455: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7477  845ab4a1-0093-4fe7-a8a7-5e752e1aa495 1683810 0 2021-09-27 15:43:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-27 15:43:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 15:43:56.456: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7477  845ab4a1-0093-4fe7-a8a7-5e752e1aa495 1683811 0 2021-09-27 15:43:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-27 15:43:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 27 15:44:06.489: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7477  845ab4a1-0093-4fe7-a8a7-5e752e1aa495 1683874 0 2021-09-27 15:43:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-27 15:43:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 15:44:06.489: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7477  845ab4a1-0093-4fe7-a8a7-5e752e1aa495 1683875 0 2021-09-27 15:43:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-27 15:43:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 15:44:06.489: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7477  845ab4a1-0093-4fe7-a8a7-5e752e1aa495 1683876 0 2021-09-27 15:43:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-27 15:43:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:44:06.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7477" for this suite.

• [SLOW TEST:10.103 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":339,"completed":2,"skipped":34,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:44:06.501: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:44:06.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5971" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":339,"completed":3,"skipped":44,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:44:06.600: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 15:44:10.648: INFO: Deleting pod "var-expansion-044d7593-30c0-4852-ac5a-1b8d93bb7aee" in namespace "var-expansion-4239"
Sep 27 15:44:10.657: INFO: Wait up to 5m0s for pod "var-expansion-044d7593-30c0-4852-ac5a-1b8d93bb7aee" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:44:20.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4239" for this suite.

• [SLOW TEST:14.080 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":339,"completed":4,"skipped":44,"failed":0}
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:44:20.680: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-3971
STEP: creating service affinity-nodeport-transition in namespace services-3971
STEP: creating replication controller affinity-nodeport-transition in namespace services-3971
I0927 15:44:20.745412      23 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-3971, replica count: 3
I0927 15:44:23.796131      23 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 15:44:26.796873      23 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 15:44:26.809: INFO: Creating new exec pod
Sep 27 15:44:29.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-3971 exec execpod-affinity85k77 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Sep 27 15:44:30.350: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Sep 27 15:44:30.350: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 15:44:30.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-3971 exec execpod-affinity85k77 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.65.252.92 80'
Sep 27 15:44:30.488: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.65.252.92 80\nConnection to 100.65.252.92 80 port [tcp/http] succeeded!\n"
Sep 27 15:44:30.488: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 15:44:30.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-3971 exec execpod-affinity85k77 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.95.24 32230'
Sep 27 15:44:30.632: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.95.24 32230\nConnection to 10.0.95.24 32230 port [tcp/*] succeeded!\n"
Sep 27 15:44:30.632: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 15:44:30.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-3971 exec execpod-affinity85k77 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.62.6 32230'
Sep 27 15:44:30.778: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.62.6 32230\nConnection to 10.0.62.6 32230 port [tcp/*] succeeded!\n"
Sep 27 15:44:30.778: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 15:44:30.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-3971 exec execpod-affinity85k77 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.31.225:32230/ ; done'
Sep 27 15:44:31.021: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n"
Sep 27 15:44:31.021: INFO: stdout: "\naffinity-nodeport-transition-bvnnw\naffinity-nodeport-transition-g6zsl\naffinity-nodeport-transition-g6zsl\naffinity-nodeport-transition-g6zsl\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-bvnnw\naffinity-nodeport-transition-bvnnw\naffinity-nodeport-transition-g6zsl\naffinity-nodeport-transition-g6zsl\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-g6zsl\naffinity-nodeport-transition-bvnnw\naffinity-nodeport-transition-bvnnw\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-g6zsl\naffinity-nodeport-transition-bvnnw"
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-bvnnw
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-g6zsl
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-g6zsl
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-g6zsl
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-bvnnw
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-bvnnw
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-g6zsl
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-g6zsl
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-g6zsl
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-bvnnw
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-bvnnw
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-g6zsl
Sep 27 15:44:31.021: INFO: Received response from host: affinity-nodeport-transition-bvnnw
Sep 27 15:44:31.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-3971 exec execpod-affinity85k77 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.31.225:32230/ ; done'
Sep 27 15:44:31.285: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32230/\n"
Sep 27 15:44:31.285: INFO: stdout: "\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz\naffinity-nodeport-transition-5jccz"
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Received response from host: affinity-nodeport-transition-5jccz
Sep 27 15:44:31.285: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3971, will wait for the garbage collector to delete the pods
Sep 27 15:44:31.365: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.818647ms
Sep 27 15:44:31.465: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.608078ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:44:42.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3971" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:21.532 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":5,"skipped":44,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:44:42.213: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-02d55e5e-364e-4411-87bb-e174db8b5135 in namespace container-probe-969
Sep 27 15:44:44.265: INFO: Started pod busybox-02d55e5e-364e-4411-87bb-e174db8b5135 in namespace container-probe-969
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 15:44:44.269: INFO: Initial restart count of pod busybox-02d55e5e-364e-4411-87bb-e174db8b5135 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:48:45.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-969" for this suite.

• [SLOW TEST:242.924 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":6,"skipped":73,"failed":0}
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:48:45.136: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 27 15:48:45.506: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 15:48:48.537: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 15:48:48.542: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:48:51.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3402" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.701 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":339,"completed":7,"skipped":73,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:48:51.838: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9731
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-9731
I0927 15:48:51.915823      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-9731, replica count: 2
Sep 27 15:48:54.966: INFO: Creating new exec pod
I0927 15:48:54.966563      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 15:48:57.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-9731 exec execpod4m4lj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Sep 27 15:48:58.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 27 15:48:58.153: INFO: stdout: "externalname-service-j2cbk"
Sep 27 15:48:58.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-9731 exec execpod4m4lj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.71.153.172 80'
Sep 27 15:48:58.287: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.71.153.172 80\nConnection to 100.71.153.172 80 port [tcp/http] succeeded!\n"
Sep 27 15:48:58.287: INFO: stdout: "externalname-service-5bm9r"
Sep 27 15:48:58.287: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:48:58.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9731" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:6.488 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":339,"completed":8,"skipped":96,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:48:58.325: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Sep 27 15:48:58.374: INFO: The status of Pod annotationupdate42a95df5-ef16-4866-a4e4-84973ae183a4 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 15:49:00.379: INFO: The status of Pod annotationupdate42a95df5-ef16-4866-a4e4-84973ae183a4 is Running (Ready = true)
Sep 27 15:49:00.918: INFO: Successfully updated pod "annotationupdate42a95df5-ef16-4866-a4e4-84973ae183a4"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:49:04.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9810" for this suite.

• [SLOW TEST:6.631 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":9,"skipped":100,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:49:04.957: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 15:49:05.010: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65084e45-84b2-4bf1-866f-a69fa601cd73" in namespace "downward-api-9309" to be "Succeeded or Failed"
Sep 27 15:49:05.014: INFO: Pod "downwardapi-volume-65084e45-84b2-4bf1-866f-a69fa601cd73": Phase="Pending", Reason="", readiness=false. Elapsed: 3.801312ms
Sep 27 15:49:07.019: INFO: Pod "downwardapi-volume-65084e45-84b2-4bf1-866f-a69fa601cd73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00966055s
STEP: Saw pod success
Sep 27 15:49:07.019: INFO: Pod "downwardapi-volume-65084e45-84b2-4bf1-866f-a69fa601cd73" satisfied condition "Succeeded or Failed"
Sep 27 15:49:07.023: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downwardapi-volume-65084e45-84b2-4bf1-866f-a69fa601cd73 container client-container: <nil>
STEP: delete the pod
Sep 27 15:49:07.048: INFO: Waiting for pod downwardapi-volume-65084e45-84b2-4bf1-866f-a69fa601cd73 to disappear
Sep 27 15:49:07.051: INFO: Pod downwardapi-volume-65084e45-84b2-4bf1-866f-a69fa601cd73 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:49:07.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9309" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":10,"skipped":109,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:49:07.063: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:49:07.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6700" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":339,"completed":11,"skipped":115,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:49:07.149: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-ce4a6d1d-ca65-4220-89e3-808df63d675e
STEP: Creating a pod to test consume configMaps
Sep 27 15:49:07.196: INFO: Waiting up to 5m0s for pod "pod-configmaps-e6e10987-69e0-4e19-a969-78317e6c45a2" in namespace "configmap-3079" to be "Succeeded or Failed"
Sep 27 15:49:07.200: INFO: Pod "pod-configmaps-e6e10987-69e0-4e19-a969-78317e6c45a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.575625ms
Sep 27 15:49:09.206: INFO: Pod "pod-configmaps-e6e10987-69e0-4e19-a969-78317e6c45a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009761279s
STEP: Saw pod success
Sep 27 15:49:09.206: INFO: Pod "pod-configmaps-e6e10987-69e0-4e19-a969-78317e6c45a2" satisfied condition "Succeeded or Failed"
Sep 27 15:49:09.210: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-configmaps-e6e10987-69e0-4e19-a969-78317e6c45a2 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 15:49:09.245: INFO: Waiting for pod pod-configmaps-e6e10987-69e0-4e19-a969-78317e6c45a2 to disappear
Sep 27 15:49:09.249: INFO: Pod pod-configmaps-e6e10987-69e0-4e19-a969-78317e6c45a2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:49:09.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3079" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":12,"skipped":117,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:49:09.261: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 15:49:09.314: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b387e79d-a884-4729-a939-e0cc99dcecdf" in namespace "projected-4864" to be "Succeeded or Failed"
Sep 27 15:49:09.321: INFO: Pod "downwardapi-volume-b387e79d-a884-4729-a939-e0cc99dcecdf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.763075ms
Sep 27 15:49:11.328: INFO: Pod "downwardapi-volume-b387e79d-a884-4729-a939-e0cc99dcecdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01360544s
STEP: Saw pod success
Sep 27 15:49:11.328: INFO: Pod "downwardapi-volume-b387e79d-a884-4729-a939-e0cc99dcecdf" satisfied condition "Succeeded or Failed"
Sep 27 15:49:11.332: INFO: Trying to get logs from node ip-10-0-62-6.us-east-2.compute.internal pod downwardapi-volume-b387e79d-a884-4729-a939-e0cc99dcecdf container client-container: <nil>
STEP: delete the pod
Sep 27 15:49:11.367: INFO: Waiting for pod downwardapi-volume-b387e79d-a884-4729-a939-e0cc99dcecdf to disappear
Sep 27 15:49:11.370: INFO: Pod downwardapi-volume-b387e79d-a884-4729-a939-e0cc99dcecdf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:49:11.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4864" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":13,"skipped":118,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:49:11.382: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep 27 15:49:11.418: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 15:49:16.518: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:49:36.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9688" for this suite.

• [SLOW TEST:24.633 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":339,"completed":14,"skipped":120,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:49:36.015: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:49:36.051: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-6963
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:49:42.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-8574" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:49:42.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6963" for this suite.

• [SLOW TEST:6.171 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":339,"completed":15,"skipped":133,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:49:42.186: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-6480
STEP: creating service affinity-clusterip in namespace services-6480
STEP: creating replication controller affinity-clusterip in namespace services-6480
I0927 15:49:42.242670      23 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-6480, replica count: 3
I0927 15:49:45.293983      23 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 15:49:45.303: INFO: Creating new exec pod
Sep 27 15:49:48.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-6480 exec execpod-affinitycj86n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Sep 27 15:49:48.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep 27 15:49:48.496: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 15:49:48.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-6480 exec execpod-affinitycj86n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.67.248.240 80'
Sep 27 15:49:48.641: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.67.248.240 80\nConnection to 100.67.248.240 80 port [tcp/http] succeeded!\n"
Sep 27 15:49:48.641: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 15:49:48.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-6480 exec execpod-affinitycj86n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.67.248.240:80/ ; done'
Sep 27 15:49:48.837: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.67.248.240:80/\n"
Sep 27 15:49:48.837: INFO: stdout: "\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s\naffinity-clusterip-llc9s"
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Received response from host: affinity-clusterip-llc9s
Sep 27 15:49:48.837: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6480, will wait for the garbage collector to delete the pods
Sep 27 15:49:48.918: INFO: Deleting ReplicationController affinity-clusterip took: 6.769835ms
Sep 27 15:49:49.019: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.040831ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:50:02.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6480" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:19.991 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":16,"skipped":144,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:50:02.177: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-6d7ca8dc-5f36-4a38-96e7-192ba1b4a4f2
STEP: Creating the pod
Sep 27 15:50:02.259: INFO: The status of Pod pod-configmaps-0d3b95dd-7944-4ccd-b3a3-0af8464358af is Pending, waiting for it to be Running (with Ready = true)
Sep 27 15:50:04.266: INFO: The status of Pod pod-configmaps-0d3b95dd-7944-4ccd-b3a3-0af8464358af is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-6d7ca8dc-5f36-4a38-96e7-192ba1b4a4f2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:50:08.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-23" for this suite.

• [SLOW TEST:6.147 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":17,"skipped":151,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:50:08.324: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 15:50:08.370: INFO: The status of Pod server-envvars-aabdd544-d53d-4766-9740-b64e2ca21a15 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 15:50:10.376: INFO: The status of Pod server-envvars-aabdd544-d53d-4766-9740-b64e2ca21a15 is Running (Ready = true)
Sep 27 15:50:10.408: INFO: Waiting up to 5m0s for pod "client-envvars-e4bf170d-46c4-4455-a733-8b4c82a257ce" in namespace "pods-5707" to be "Succeeded or Failed"
Sep 27 15:50:10.412: INFO: Pod "client-envvars-e4bf170d-46c4-4455-a733-8b4c82a257ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037805ms
Sep 27 15:50:12.420: INFO: Pod "client-envvars-e4bf170d-46c4-4455-a733-8b4c82a257ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012119497s
Sep 27 15:50:14.426: INFO: Pod "client-envvars-e4bf170d-46c4-4455-a733-8b4c82a257ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017960357s
STEP: Saw pod success
Sep 27 15:50:14.426: INFO: Pod "client-envvars-e4bf170d-46c4-4455-a733-8b4c82a257ce" satisfied condition "Succeeded or Failed"
Sep 27 15:50:14.429: INFO: Trying to get logs from node ip-10-0-62-6.us-east-2.compute.internal pod client-envvars-e4bf170d-46c4-4455-a733-8b4c82a257ce container env3cont: <nil>
STEP: delete the pod
Sep 27 15:50:14.455: INFO: Waiting for pod client-envvars-e4bf170d-46c4-4455-a733-8b4c82a257ce to disappear
Sep 27 15:50:14.459: INFO: Pod client-envvars-e4bf170d-46c4-4455-a733-8b4c82a257ce no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:50:14.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5707" for this suite.

• [SLOW TEST:6.147 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":339,"completed":18,"skipped":171,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:50:14.471: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Sep 27 15:50:16.548: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:50:18.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8516" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":339,"completed":19,"skipped":208,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:50:18.608: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 15:50:18.640: INFO: Creating deployment "test-recreate-deployment"
Sep 27 15:50:18.645: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 27 15:50:18.654: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep 27 15:50:20.663: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 27 15:50:20.667: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 27 15:50:20.682: INFO: Updating deployment test-recreate-deployment
Sep 27 15:50:20.682: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Sep 27 15:50:20.756: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-366  24b3b5e8-9caa-4349-86bf-44e84190b8a2 1686616 2 2021-09-27 15:50:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-27 15:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 15:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008524428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-09-27 15:50:20 +0000 UTC,LastTransitionTime:2021-09-27 15:50:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-09-27 15:50:20 +0000 UTC,LastTransitionTime:2021-09-27 15:50:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep 27 15:50:20.760: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-366  a1b1aabc-24e2-4961-8bc4-0e16543dad1c 1686614 1 2021-09-27 15:50:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 24b3b5e8-9caa-4349-86bf-44e84190b8a2 0xc0085248b0 0xc0085248b1}] []  [{kube-controller-manager Update apps/v1 2021-09-27 15:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24b3b5e8-9caa-4349-86bf-44e84190b8a2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008524968 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 15:50:20.760: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 27 15:50:20.760: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-366  3a5cc023-d47a-4674-990c-fd84c66d5017 1686605 2 2021-09-27 15:50:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 24b3b5e8-9caa-4349-86bf-44e84190b8a2 0xc0085247b7 0xc0085247b8}] []  [{kube-controller-manager Update apps/v1 2021-09-27 15:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24b3b5e8-9caa-4349-86bf-44e84190b8a2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008524848 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 15:50:20.766: INFO: Pod "test-recreate-deployment-85d47dcb4-vfvdf" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-vfvdf test-recreate-deployment-85d47dcb4- deployment-366  64c70239-b0c8-469b-8527-a4ba9fe4c57f 1686617 0 2021-09-27 15:50:20 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 a1b1aabc-24e2-4961-8bc4-0e16543dad1c 0xc008524ea0 0xc008524ea1}] []  [{kube-controller-manager Update v1 2021-09-27 15:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a1b1aabc-24e2-4961-8bc4-0e16543dad1c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 15:50:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plvdj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plvdj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-62-6.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 15:50:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 15:50:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 15:50:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 15:50:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.62.6,PodIP:,StartTime:2021-09-27 15:50:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:50:20.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-366" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":20,"skipped":234,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:50:20.777: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:50:20.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-957" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":339,"completed":21,"skipped":241,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:50:20.847: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 27 15:50:20.877: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 15:50:20.886: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 15:50:20.889: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-31-225.us-east-2.compute.internal before test
Sep 27 15:50:20.896: INFO: antrea-agent-cv4j2 from kube-system started at 2021-09-23 17:40:20 +0000 UTC (2 container statuses recorded)
Sep 27 15:50:20.896: INFO: 	Container antrea-agent ready: true, restart count 0
Sep 27 15:50:20.896: INFO: 	Container antrea-ovs ready: true, restart count 0
Sep 27 15:50:20.896: INFO: kube-proxy-sjbtj from kube-system started at 2021-09-23 17:30:51 +0000 UTC (1 container statuses recorded)
Sep 27 15:50:20.896: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 15:50:20.896: INFO: metrics-server-6c47b7847-kd7fj from kube-system started at 2021-09-23 22:19:30 +0000 UTC (1 container statuses recorded)
Sep 27 15:50:20.896: INFO: 	Container metrics-server ready: true, restart count 0
Sep 27 15:50:20.896: INFO: server-envvars-aabdd544-d53d-4766-9740-b64e2ca21a15 from pods-5707 started at 2021-09-27 15:50:08 +0000 UTC (1 container statuses recorded)
Sep 27 15:50:20.896: INFO: 	Container srv ready: false, restart count 0
Sep 27 15:50:20.896: INFO: sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-p4r29 from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 15:50:20.896: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 15:50:20.896: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 15:50:20.896: INFO: tkr-controller-manager-6bc455b5d4-m5rjt from tkr-system started at 2021-09-23 22:19:30 +0000 UTC (1 container statuses recorded)
Sep 27 15:50:20.896: INFO: 	Container manager ready: true, restart count 0
Sep 27 15:50:20.896: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-62-6.us-east-2.compute.internal before test
Sep 27 15:50:20.903: INFO: test-recreate-deployment-85d47dcb4-vfvdf from deployment-366 started at 2021-09-27 15:50:20 +0000 UTC (1 container statuses recorded)
Sep 27 15:50:20.903: INFO: 	Container httpd ready: false, restart count 0
Sep 27 15:50:20.903: INFO: antrea-agent-85fx9 from kube-system started at 2021-09-27 15:39:42 +0000 UTC (2 container statuses recorded)
Sep 27 15:50:20.903: INFO: 	Container antrea-agent ready: true, restart count 0
Sep 27 15:50:20.903: INFO: 	Container antrea-ovs ready: true, restart count 0
Sep 27 15:50:20.903: INFO: kube-proxy-882cf from kube-system started at 2021-09-27 15:39:42 +0000 UTC (1 container statuses recorded)
Sep 27 15:50:20.903: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 15:50:20.903: INFO: sonobuoy-e2e-job-87b0b9424e96441f from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 15:50:20.903: INFO: 	Container e2e ready: true, restart count 0
Sep 27 15:50:20.903: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 15:50:20.903: INFO: sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-88tn9 from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 15:50:20.903: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 15:50:20.903: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 15:50:20.903: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-95-24.us-east-2.compute.internal before test
Sep 27 15:50:20.910: INFO: pod-0 from disruption-8516 started at 2021-09-27 15:50:16 +0000 UTC (1 container statuses recorded)
Sep 27 15:50:20.910: INFO: 	Container donothing ready: true, restart count 0
Sep 27 15:50:20.910: INFO: antrea-agent-jwpnj from kube-system started at 2021-09-27 15:39:42 +0000 UTC (2 container statuses recorded)
Sep 27 15:50:20.910: INFO: 	Container antrea-agent ready: true, restart count 0
Sep 27 15:50:20.910: INFO: 	Container antrea-ovs ready: true, restart count 0
Sep 27 15:50:20.910: INFO: kube-proxy-2bm5v from kube-system started at 2021-09-27 15:39:42 +0000 UTC (1 container statuses recorded)
Sep 27 15:50:20.910: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 15:50:20.910: INFO: sonobuoy from sonobuoy started at 2021-09-27 15:43:02 +0000 UTC (1 container statuses recorded)
Sep 27 15:50:20.910: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 15:50:20.910: INFO: sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-nv46v from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 15:50:20.910: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 15:50:20.910: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c49f12e1-dba8-4eae-bbad-da5993efa6dc 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.95.24 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-c49f12e1-dba8-4eae-bbad-da5993efa6dc off the node ip-10-0-95-24.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c49f12e1-dba8-4eae-bbad-da5993efa6dc
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:55:25.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-687" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.181 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":339,"completed":22,"skipped":264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:55:25.028: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Sep 27 15:55:25.069: INFO: created test-podtemplate-1
Sep 27 15:55:25.074: INFO: created test-podtemplate-2
Sep 27 15:55:25.078: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Sep 27 15:55:25.082: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Sep 27 15:55:25.102: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 15:55:25.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-330" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":339,"completed":23,"skipped":307,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 15:55:25.118: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0927 15:55:25.152126      23 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:01:01.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4998" for this suite.

• [SLOW TEST:336.088 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":339,"completed":24,"skipped":315,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:01:01.206: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:01:01.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2392" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":25,"skipped":335,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:01:01.473: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 16:01:01.855: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 16:01:04.887: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:01:04.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4721" for this suite.
STEP: Destroying namespace "webhook-4721-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":339,"completed":26,"skipped":344,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:01:05.023: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:01:05.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2319" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":339,"completed":27,"skipped":357,"failed":0}
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:01:05.115: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Sep 27 16:01:05.169: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:01:05.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7789" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":339,"completed":28,"skipped":366,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:01:05.211: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-6bb3250b-bdf5-44f9-a21d-9ad7da60b205
STEP: Creating a pod to test consume secrets
Sep 27 16:01:05.263: INFO: Waiting up to 5m0s for pod "pod-secrets-a03db39d-db63-41b1-ae92-d252706ce50b" in namespace "secrets-9453" to be "Succeeded or Failed"
Sep 27 16:01:05.267: INFO: Pod "pod-secrets-a03db39d-db63-41b1-ae92-d252706ce50b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490977ms
Sep 27 16:01:07.272: INFO: Pod "pod-secrets-a03db39d-db63-41b1-ae92-d252706ce50b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008726299s
STEP: Saw pod success
Sep 27 16:01:07.272: INFO: Pod "pod-secrets-a03db39d-db63-41b1-ae92-d252706ce50b" satisfied condition "Succeeded or Failed"
Sep 27 16:01:07.276: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-secrets-a03db39d-db63-41b1-ae92-d252706ce50b container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 16:01:07.311: INFO: Waiting for pod pod-secrets-a03db39d-db63-41b1-ae92-d252706ce50b to disappear
Sep 27 16:01:07.316: INFO: Pod pod-secrets-a03db39d-db63-41b1-ae92-d252706ce50b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:01:07.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9453" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":29,"skipped":367,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:01:07.327: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:01:07.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5313" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":30,"skipped":392,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:01:07.376: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:01:11.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8324" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":339,"completed":31,"skipped":405,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:01:11.442: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4440
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4440
STEP: creating replication controller externalsvc in namespace services-4440
I0927 16:01:11.526994      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4440, replica count: 2
I0927 16:01:14.578194      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Sep 27 16:01:14.612: INFO: Creating new exec pod
Sep 27 16:01:16.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-4440 exec execpodn9zcs -- /bin/sh -x -c nslookup clusterip-service.services-4440.svc.cluster.local'
Sep 27 16:01:17.248: INFO: stderr: "+ nslookup clusterip-service.services-4440.svc.cluster.local\n"
Sep 27 16:01:17.249: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nclusterip-service.services-4440.svc.cluster.local\tcanonical name = externalsvc.services-4440.svc.cluster.local.\nName:\texternalsvc.services-4440.svc.cluster.local\nAddress: 100.69.7.144\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4440, will wait for the garbage collector to delete the pods
Sep 27 16:01:17.312: INFO: Deleting ReplicationController externalsvc took: 7.711864ms
Sep 27 16:01:17.412: INFO: Terminating ReplicationController externalsvc pods took: 100.124748ms
Sep 27 16:01:21.138: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:01:21.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4440" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:9.727 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":339,"completed":32,"skipped":426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:01:21.169: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-683eaa24-7d7a-4ceb-8387-767e39a65077
STEP: Creating a pod to test consume secrets
Sep 27 16:01:21.226: INFO: Waiting up to 5m0s for pod "pod-secrets-c42fec51-330b-4819-8312-c8a0cc11c25e" in namespace "secrets-3512" to be "Succeeded or Failed"
Sep 27 16:01:21.231: INFO: Pod "pod-secrets-c42fec51-330b-4819-8312-c8a0cc11c25e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.278584ms
Sep 27 16:01:23.238: INFO: Pod "pod-secrets-c42fec51-330b-4819-8312-c8a0cc11c25e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011557526s
STEP: Saw pod success
Sep 27 16:01:23.238: INFO: Pod "pod-secrets-c42fec51-330b-4819-8312-c8a0cc11c25e" satisfied condition "Succeeded or Failed"
Sep 27 16:01:23.241: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-secrets-c42fec51-330b-4819-8312-c8a0cc11c25e container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 16:01:23.274: INFO: Waiting for pod pod-secrets-c42fec51-330b-4819-8312-c8a0cc11c25e to disappear
Sep 27 16:01:23.277: INFO: Pod pod-secrets-c42fec51-330b-4819-8312-c8a0cc11c25e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:01:23.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3512" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":33,"skipped":472,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:01:23.290: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0927 16:01:29.372429      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 27 16:06:29.377: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:06:29.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1671" for this suite.

• [SLOW TEST:306.100 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":339,"completed":34,"skipped":479,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:06:29.390: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Sep 27 16:06:29.439: INFO: observed Pod pod-test in namespace pods-7231 in phase Pending with labels: map[test-pod-static:true] & conditions []
Sep 27 16:06:29.443: INFO: observed Pod pod-test in namespace pods-7231 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:06:29 +0000 UTC  }]
Sep 27 16:06:29.462: INFO: observed Pod pod-test in namespace pods-7231 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:06:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:06:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:06:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:06:29 +0000 UTC  }]
Sep 27 16:06:30.177: INFO: Found Pod pod-test in namespace pods-7231 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:06:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:06:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:06:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:06:29 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Sep 27 16:06:30.189: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Sep 27 16:06:30.223: INFO: observed event type ADDED
Sep 27 16:06:30.223: INFO: observed event type MODIFIED
Sep 27 16:06:30.223: INFO: observed event type MODIFIED
Sep 27 16:06:30.223: INFO: observed event type MODIFIED
Sep 27 16:06:30.223: INFO: observed event type MODIFIED
Sep 27 16:06:30.223: INFO: observed event type MODIFIED
Sep 27 16:06:30.223: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:06:30.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7231" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":339,"completed":35,"skipped":485,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:06:30.235: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-st6q
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 16:06:30.284: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-st6q" in namespace "subpath-2450" to be "Succeeded or Failed"
Sep 27 16:06:30.287: INFO: Pod "pod-subpath-test-configmap-st6q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534165ms
Sep 27 16:06:32.295: INFO: Pod "pod-subpath-test-configmap-st6q": Phase="Running", Reason="", readiness=true. Elapsed: 2.010982534s
Sep 27 16:06:34.302: INFO: Pod "pod-subpath-test-configmap-st6q": Phase="Running", Reason="", readiness=true. Elapsed: 4.018155545s
Sep 27 16:06:36.309: INFO: Pod "pod-subpath-test-configmap-st6q": Phase="Running", Reason="", readiness=true. Elapsed: 6.025178105s
Sep 27 16:06:38.316: INFO: Pod "pod-subpath-test-configmap-st6q": Phase="Running", Reason="", readiness=true. Elapsed: 8.032708639s
Sep 27 16:06:40.324: INFO: Pod "pod-subpath-test-configmap-st6q": Phase="Running", Reason="", readiness=true. Elapsed: 10.040435789s
Sep 27 16:06:42.332: INFO: Pod "pod-subpath-test-configmap-st6q": Phase="Running", Reason="", readiness=true. Elapsed: 12.04801815s
Sep 27 16:06:44.338: INFO: Pod "pod-subpath-test-configmap-st6q": Phase="Running", Reason="", readiness=true. Elapsed: 14.054351007s
Sep 27 16:06:46.346: INFO: Pod "pod-subpath-test-configmap-st6q": Phase="Running", Reason="", readiness=true. Elapsed: 16.062734965s
Sep 27 16:06:48.355: INFO: Pod "pod-subpath-test-configmap-st6q": Phase="Running", Reason="", readiness=true. Elapsed: 18.07105312s
Sep 27 16:06:50.363: INFO: Pod "pod-subpath-test-configmap-st6q": Phase="Running", Reason="", readiness=true. Elapsed: 20.079035182s
Sep 27 16:06:52.371: INFO: Pod "pod-subpath-test-configmap-st6q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.087318025s
STEP: Saw pod success
Sep 27 16:06:52.371: INFO: Pod "pod-subpath-test-configmap-st6q" satisfied condition "Succeeded or Failed"
Sep 27 16:06:52.375: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-subpath-test-configmap-st6q container test-container-subpath-configmap-st6q: <nil>
STEP: delete the pod
Sep 27 16:06:52.415: INFO: Waiting for pod pod-subpath-test-configmap-st6q to disappear
Sep 27 16:06:52.424: INFO: Pod pod-subpath-test-configmap-st6q no longer exists
STEP: Deleting pod pod-subpath-test-configmap-st6q
Sep 27 16:06:52.424: INFO: Deleting pod "pod-subpath-test-configmap-st6q" in namespace "subpath-2450"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:06:52.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2450" for this suite.

• [SLOW TEST:22.208 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":339,"completed":36,"skipped":502,"failed":0}
S
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:06:52.443: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Sep 27 16:06:52.486: INFO: created test-pod-1
Sep 27 16:06:52.517: INFO: created test-pod-2
Sep 27 16:06:52.527: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:06:52.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8153" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":339,"completed":37,"skipped":503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:06:52.591: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:06:52.622: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 27 16:06:57.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-1541 --namespace=crd-publish-openapi-1541 create -f -'
Sep 27 16:06:58.831: INFO: stderr: ""
Sep 27 16:06:58.831: INFO: stdout: "e2e-test-crd-publish-openapi-6668-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 27 16:06:58.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-1541 --namespace=crd-publish-openapi-1541 delete e2e-test-crd-publish-openapi-6668-crds test-cr'
Sep 27 16:06:58.933: INFO: stderr: ""
Sep 27 16:06:58.933: INFO: stdout: "e2e-test-crd-publish-openapi-6668-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep 27 16:06:58.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-1541 --namespace=crd-publish-openapi-1541 apply -f -'
Sep 27 16:06:59.357: INFO: stderr: ""
Sep 27 16:06:59.357: INFO: stdout: "e2e-test-crd-publish-openapi-6668-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep 27 16:06:59.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-1541 --namespace=crd-publish-openapi-1541 delete e2e-test-crd-publish-openapi-6668-crds test-cr'
Sep 27 16:06:59.427: INFO: stderr: ""
Sep 27 16:06:59.427: INFO: stdout: "e2e-test-crd-publish-openapi-6668-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 27 16:06:59.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-1541 explain e2e-test-crd-publish-openapi-6668-crds'
Sep 27 16:06:59.665: INFO: stderr: ""
Sep 27 16:06:59.665: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6668-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:07:04.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1541" for this suite.

• [SLOW TEST:12.187 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":339,"completed":38,"skipped":535,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:07:04.778: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:07:11.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5613" for this suite.

• [SLOW TEST:7.067 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":339,"completed":39,"skipped":541,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:07:11.845: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-83d8bfcd-f4b5-407c-b0eb-67abc3c5179d
STEP: Creating a pod to test consume secrets
Sep 27 16:07:11.896: INFO: Waiting up to 5m0s for pod "pod-secrets-0757d545-398b-4733-ba4f-8573e896c50e" in namespace "secrets-1382" to be "Succeeded or Failed"
Sep 27 16:07:11.900: INFO: Pod "pod-secrets-0757d545-398b-4733-ba4f-8573e896c50e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.529333ms
Sep 27 16:07:13.906: INFO: Pod "pod-secrets-0757d545-398b-4733-ba4f-8573e896c50e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009662574s
STEP: Saw pod success
Sep 27 16:07:13.906: INFO: Pod "pod-secrets-0757d545-398b-4733-ba4f-8573e896c50e" satisfied condition "Succeeded or Failed"
Sep 27 16:07:13.910: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-secrets-0757d545-398b-4733-ba4f-8573e896c50e container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 16:07:13.936: INFO: Waiting for pod pod-secrets-0757d545-398b-4733-ba4f-8573e896c50e to disappear
Sep 27 16:07:13.939: INFO: Pod pod-secrets-0757d545-398b-4733-ba4f-8573e896c50e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:07:13.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1382" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":40,"skipped":560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:07:13.951: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 27 16:07:16.518: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5041 pod-service-account-0f986ebb-6d92-43c0-8964-be6e9514f848 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 27 16:07:16.651: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5041 pod-service-account-0f986ebb-6d92-43c0-8964-be6e9514f848 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 27 16:07:16.774: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5041 pod-service-account-0f986ebb-6d92-43c0-8964-be6e9514f848 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:07:16.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5041" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":339,"completed":41,"skipped":589,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:07:16.914: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-6638
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 27 16:07:16.951: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 27 16:07:16.992: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:07:18.999: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:07:21.000: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:07:22.998: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:07:25.000: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:07:26.999: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:07:28.999: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:07:31.000: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:07:32.998: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:07:34.998: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:07:37.003: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 27 16:07:37.012: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 27 16:07:37.019: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 27 16:07:39.040: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep 27 16:07:39.040: INFO: Breadth first check of 100.96.1.181 on host 10.0.31.225...
Sep 27 16:07:39.043: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.4.34:9080/dial?request=hostname&protocol=udp&host=100.96.1.181&port=8081&tries=1'] Namespace:pod-network-test-6638 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 16:07:39.043: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 16:07:39.135: INFO: Waiting for responses: map[]
Sep 27 16:07:39.135: INFO: reached 100.96.1.181 after 0/1 tries
Sep 27 16:07:39.135: INFO: Breadth first check of 100.96.5.11 on host 10.0.62.6...
Sep 27 16:07:39.139: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.4.34:9080/dial?request=hostname&protocol=udp&host=100.96.5.11&port=8081&tries=1'] Namespace:pod-network-test-6638 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 16:07:39.139: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 16:07:39.226: INFO: Waiting for responses: map[]
Sep 27 16:07:39.226: INFO: reached 100.96.5.11 after 0/1 tries
Sep 27 16:07:39.226: INFO: Breadth first check of 100.96.4.33 on host 10.0.95.24...
Sep 27 16:07:39.234: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.4.34:9080/dial?request=hostname&protocol=udp&host=100.96.4.33&port=8081&tries=1'] Namespace:pod-network-test-6638 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 16:07:39.234: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 16:07:39.308: INFO: Waiting for responses: map[]
Sep 27 16:07:39.308: INFO: reached 100.96.4.33 after 0/1 tries
Sep 27 16:07:39.308: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:07:39.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6638" for this suite.

• [SLOW TEST:22.406 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":339,"completed":42,"skipped":617,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:07:39.320: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 27 16:07:39.362: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7241  0083c1d1-ea82-40f5-acbf-835d24ae77fa 1692880 0 2021-09-27 16:07:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 16:07:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 16:07:39.362: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7241  0083c1d1-ea82-40f5-acbf-835d24ae77fa 1692880 0 2021-09-27 16:07:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 16:07:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 27 16:07:49.373: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7241  0083c1d1-ea82-40f5-acbf-835d24ae77fa 1692962 0 2021-09-27 16:07:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 16:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 16:07:49.373: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7241  0083c1d1-ea82-40f5-acbf-835d24ae77fa 1692962 0 2021-09-27 16:07:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 16:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 27 16:07:59.385: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7241  0083c1d1-ea82-40f5-acbf-835d24ae77fa 1693022 0 2021-09-27 16:07:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 16:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 16:07:59.385: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7241  0083c1d1-ea82-40f5-acbf-835d24ae77fa 1693022 0 2021-09-27 16:07:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 16:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 27 16:08:09.398: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7241  0083c1d1-ea82-40f5-acbf-835d24ae77fa 1693069 0 2021-09-27 16:07:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 16:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 16:08:09.398: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7241  0083c1d1-ea82-40f5-acbf-835d24ae77fa 1693069 0 2021-09-27 16:07:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-27 16:07:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 27 16:08:19.410: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7241  d1fcfda7-bfd1-460b-a810-e68b5e32cf6f 1693122 0 2021-09-27 16:08:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-27 16:08:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 16:08:19.410: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7241  d1fcfda7-bfd1-460b-a810-e68b5e32cf6f 1693122 0 2021-09-27 16:08:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-27 16:08:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 27 16:08:29.422: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7241  d1fcfda7-bfd1-460b-a810-e68b5e32cf6f 1693173 0 2021-09-27 16:08:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-27 16:08:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 16:08:29.422: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7241  d1fcfda7-bfd1-460b-a810-e68b5e32cf6f 1693173 0 2021-09-27 16:08:19 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-27 16:08:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:08:39.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7241" for this suite.

• [SLOW TEST:60.120 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":339,"completed":43,"skipped":631,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:08:39.440: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:08:56.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7227" for this suite.

• [SLOW TEST:17.101 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":339,"completed":44,"skipped":674,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:08:56.541: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-4265
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4265
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4265
Sep 27 16:08:56.595: INFO: Found 0 stateful pods, waiting for 1
Sep 27 16:09:06.601: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 27 16:09:06.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4265 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 16:09:06.734: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 16:09:06.734: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 16:09:06.734: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 16:09:06.738: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 27 16:09:16.746: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 16:09:16.746: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 16:09:16.762: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999844s
Sep 27 16:09:17.770: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995303537s
Sep 27 16:09:18.776: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988000941s
Sep 27 16:09:19.782: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982619184s
Sep 27 16:09:20.789: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976474659s
Sep 27 16:09:21.794: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.969821082s
Sep 27 16:09:22.802: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.963683611s
Sep 27 16:09:23.807: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.956529447s
Sep 27 16:09:24.813: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.951333267s
Sep 27 16:09:25.821: INFO: Verifying statefulset ss doesn't scale past 1 for another 944.502133ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4265
Sep 27 16:09:26.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4265 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:09:26.968: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 16:09:26.968: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 16:09:26.968: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 16:09:26.972: INFO: Found 1 stateful pods, waiting for 3
Sep 27 16:09:36.978: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:09:36.979: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:09:36.979: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 27 16:09:46.981: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:09:46.981: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:09:46.981: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 27 16:09:46.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4265 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 16:09:47.143: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 16:09:47.143: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 16:09:47.143: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 16:09:47.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4265 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 16:09:47.309: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 16:09:47.309: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 16:09:47.309: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 16:09:47.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4265 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 16:09:47.467: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 16:09:47.467: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 16:09:47.467: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 16:09:47.467: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 16:09:47.472: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep 27 16:09:57.487: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 16:09:57.487: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 16:09:57.487: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 16:09:57.510: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999776s
Sep 27 16:09:58.516: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988056578s
Sep 27 16:09:59.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981303549s
Sep 27 16:10:00.530: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974596204s
Sep 27 16:10:01.535: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.968780487s
Sep 27 16:10:02.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96369147s
Sep 27 16:10:03.547: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.957784705s
Sep 27 16:10:04.553: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950922776s
Sep 27 16:10:05.559: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.946049038s
Sep 27 16:10:06.565: INFO: Verifying statefulset ss doesn't scale past 3 for another 939.399612ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4265
Sep 27 16:10:07.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4265 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:10:07.747: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 16:10:07.747: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 16:10:07.747: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 16:10:07.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4265 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:10:07.890: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 16:10:07.890: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 16:10:07.890: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 16:10:07.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4265 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:10:08.044: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 16:10:08.044: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 16:10:08.044: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 16:10:08.044: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Sep 27 16:10:18.067: INFO: Deleting all statefulset in ns statefulset-4265
Sep 27 16:10:18.071: INFO: Scaling statefulset ss to 0
Sep 27 16:10:18.084: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 16:10:18.088: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:10:18.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4265" for this suite.

• [SLOW TEST:81.579 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":339,"completed":45,"skipped":681,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:10:18.120: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 27 16:10:18.198: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:18.198: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:18.198: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:18.202: INFO: Number of nodes with available pods: 0
Sep 27 16:10:18.202: INFO: Node ip-10-0-31-225.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:19.209: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:19.209: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:19.209: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:19.214: INFO: Number of nodes with available pods: 0
Sep 27 16:10:19.214: INFO: Node ip-10-0-31-225.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:20.209: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:20.209: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:20.209: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:20.213: INFO: Number of nodes with available pods: 3
Sep 27 16:10:20.213: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 27 16:10:20.233: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:20.233: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:20.233: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:20.237: INFO: Number of nodes with available pods: 2
Sep 27 16:10:20.237: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:21.243: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:21.243: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:21.243: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:21.247: INFO: Number of nodes with available pods: 2
Sep 27 16:10:21.247: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:22.242: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:22.242: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:22.242: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:22.246: INFO: Number of nodes with available pods: 2
Sep 27 16:10:22.246: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:23.251: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:23.251: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:23.251: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:23.255: INFO: Number of nodes with available pods: 2
Sep 27 16:10:23.255: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:24.242: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:24.242: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:24.242: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:24.247: INFO: Number of nodes with available pods: 2
Sep 27 16:10:24.247: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:25.243: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:25.243: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:25.243: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:25.247: INFO: Number of nodes with available pods: 2
Sep 27 16:10:25.247: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:26.242: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:26.242: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:26.242: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:26.246: INFO: Number of nodes with available pods: 2
Sep 27 16:10:26.246: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:27.244: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:27.244: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:27.244: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:27.248: INFO: Number of nodes with available pods: 2
Sep 27 16:10:27.248: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:28.242: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:28.242: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:28.242: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:28.246: INFO: Number of nodes with available pods: 2
Sep 27 16:10:28.246: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:29.244: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:29.244: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:29.244: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:29.249: INFO: Number of nodes with available pods: 2
Sep 27 16:10:29.249: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:30.242: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:30.242: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:30.242: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:30.246: INFO: Number of nodes with available pods: 2
Sep 27 16:10:30.246: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:31.243: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:31.243: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:31.243: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:31.247: INFO: Number of nodes with available pods: 2
Sep 27 16:10:31.247: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:32.243: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:32.243: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:32.243: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:32.247: INFO: Number of nodes with available pods: 2
Sep 27 16:10:32.247: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:33.245: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:33.245: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:33.245: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:33.276: INFO: Number of nodes with available pods: 2
Sep 27 16:10:33.276: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:10:34.243: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:34.243: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:34.243: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:10:34.249: INFO: Number of nodes with available pods: 3
Sep 27 16:10:34.249: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8963, will wait for the garbage collector to delete the pods
Sep 27 16:10:34.314: INFO: Deleting DaemonSet.extensions daemon-set took: 6.725168ms
Sep 27 16:10:34.414: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.101512ms
Sep 27 16:10:42.120: INFO: Number of nodes with available pods: 0
Sep 27 16:10:42.120: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 16:10:42.124: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1694173"},"items":null}

Sep 27 16:10:42.127: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1694173"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:10:42.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8963" for this suite.

• [SLOW TEST:24.035 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":339,"completed":46,"skipped":699,"failed":0}
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:10:42.155: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-hgxj
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 16:10:42.209: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hgxj" in namespace "subpath-5582" to be "Succeeded or Failed"
Sep 27 16:10:42.213: INFO: Pod "pod-subpath-test-secret-hgxj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.501799ms
Sep 27 16:10:44.219: INFO: Pod "pod-subpath-test-secret-hgxj": Phase="Running", Reason="", readiness=true. Elapsed: 2.009973535s
Sep 27 16:10:46.227: INFO: Pod "pod-subpath-test-secret-hgxj": Phase="Running", Reason="", readiness=true. Elapsed: 4.017890397s
Sep 27 16:10:48.235: INFO: Pod "pod-subpath-test-secret-hgxj": Phase="Running", Reason="", readiness=true. Elapsed: 6.025724238s
Sep 27 16:10:50.242: INFO: Pod "pod-subpath-test-secret-hgxj": Phase="Running", Reason="", readiness=true. Elapsed: 8.033291724s
Sep 27 16:10:52.252: INFO: Pod "pod-subpath-test-secret-hgxj": Phase="Running", Reason="", readiness=true. Elapsed: 10.042470204s
Sep 27 16:10:54.259: INFO: Pod "pod-subpath-test-secret-hgxj": Phase="Running", Reason="", readiness=true. Elapsed: 12.049552417s
Sep 27 16:10:56.265: INFO: Pod "pod-subpath-test-secret-hgxj": Phase="Running", Reason="", readiness=true. Elapsed: 14.055996071s
Sep 27 16:10:58.274: INFO: Pod "pod-subpath-test-secret-hgxj": Phase="Running", Reason="", readiness=true. Elapsed: 16.064465007s
Sep 27 16:11:00.282: INFO: Pod "pod-subpath-test-secret-hgxj": Phase="Running", Reason="", readiness=true. Elapsed: 18.072734059s
Sep 27 16:11:02.287: INFO: Pod "pod-subpath-test-secret-hgxj": Phase="Running", Reason="", readiness=true. Elapsed: 20.077727137s
Sep 27 16:11:04.294: INFO: Pod "pod-subpath-test-secret-hgxj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.084857707s
STEP: Saw pod success
Sep 27 16:11:04.294: INFO: Pod "pod-subpath-test-secret-hgxj" satisfied condition "Succeeded or Failed"
Sep 27 16:11:04.298: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-subpath-test-secret-hgxj container test-container-subpath-secret-hgxj: <nil>
STEP: delete the pod
Sep 27 16:11:04.331: INFO: Waiting for pod pod-subpath-test-secret-hgxj to disappear
Sep 27 16:11:04.335: INFO: Pod pod-subpath-test-secret-hgxj no longer exists
STEP: Deleting pod pod-subpath-test-secret-hgxj
Sep 27 16:11:04.335: INFO: Deleting pod "pod-subpath-test-secret-hgxj" in namespace "subpath-5582"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:11:04.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5582" for this suite.

• [SLOW TEST:22.194 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":339,"completed":47,"skipped":699,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:11:04.349: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 27 16:11:04.412: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep 27 16:11:04.419: INFO: starting watch
STEP: patching
STEP: updating
Sep 27 16:11:04.438: INFO: waiting for watch events with expected annotations
Sep 27 16:11:04.438: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:11:04.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6568" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":339,"completed":48,"skipped":712,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:11:04.504: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Sep 27 16:11:04.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7017 create -f -'
Sep 27 16:11:05.029: INFO: stderr: ""
Sep 27 16:11:05.029: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 27 16:11:05.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7017 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 27 16:11:05.092: INFO: stderr: ""
Sep 27 16:11:05.092: INFO: stdout: "update-demo-nautilus-2tg77 update-demo-nautilus-cnlm4 "
Sep 27 16:11:05.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7017 get pods update-demo-nautilus-2tg77 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 27 16:11:05.151: INFO: stderr: ""
Sep 27 16:11:05.151: INFO: stdout: ""
Sep 27 16:11:05.151: INFO: update-demo-nautilus-2tg77 is created but not running
Sep 27 16:11:10.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7017 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 27 16:11:10.212: INFO: stderr: ""
Sep 27 16:11:10.212: INFO: stdout: "update-demo-nautilus-2tg77 update-demo-nautilus-cnlm4 "
Sep 27 16:11:10.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7017 get pods update-demo-nautilus-2tg77 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 27 16:11:10.277: INFO: stderr: ""
Sep 27 16:11:10.277: INFO: stdout: "true"
Sep 27 16:11:10.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7017 get pods update-demo-nautilus-2tg77 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 27 16:11:10.332: INFO: stderr: ""
Sep 27 16:11:10.332: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Sep 27 16:11:10.332: INFO: validating pod update-demo-nautilus-2tg77
Sep 27 16:11:10.338: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 16:11:10.338: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 16:11:10.338: INFO: update-demo-nautilus-2tg77 is verified up and running
Sep 27 16:11:10.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7017 get pods update-demo-nautilus-cnlm4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 27 16:11:10.395: INFO: stderr: ""
Sep 27 16:11:10.395: INFO: stdout: "true"
Sep 27 16:11:10.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7017 get pods update-demo-nautilus-cnlm4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 27 16:11:10.450: INFO: stderr: ""
Sep 27 16:11:10.450: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Sep 27 16:11:10.450: INFO: validating pod update-demo-nautilus-cnlm4
Sep 27 16:11:10.458: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 16:11:10.459: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 16:11:10.459: INFO: update-demo-nautilus-cnlm4 is verified up and running
STEP: using delete to clean up resources
Sep 27 16:11:10.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7017 delete --grace-period=0 --force -f -'
Sep 27 16:11:10.519: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 16:11:10.519: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 27 16:11:10.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7017 get rc,svc -l name=update-demo --no-headers'
Sep 27 16:11:10.581: INFO: stderr: "No resources found in kubectl-7017 namespace.\n"
Sep 27 16:11:10.581: INFO: stdout: ""
Sep 27 16:11:10.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7017 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 27 16:11:10.641: INFO: stderr: ""
Sep 27 16:11:10.641: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:11:10.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7017" for this suite.

• [SLOW TEST:6.149 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":339,"completed":49,"skipped":713,"failed":0}
SSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:11:10.653: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Sep 27 16:11:10.698: INFO: Waiting up to 5m0s for pod "client-containers-950943cb-a273-4a77-a5df-14f3a661e63e" in namespace "containers-1514" to be "Succeeded or Failed"
Sep 27 16:11:10.701: INFO: Pod "client-containers-950943cb-a273-4a77-a5df-14f3a661e63e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.427837ms
Sep 27 16:11:12.707: INFO: Pod "client-containers-950943cb-a273-4a77-a5df-14f3a661e63e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008757966s
STEP: Saw pod success
Sep 27 16:11:12.707: INFO: Pod "client-containers-950943cb-a273-4a77-a5df-14f3a661e63e" satisfied condition "Succeeded or Failed"
Sep 27 16:11:12.710: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod client-containers-950943cb-a273-4a77-a5df-14f3a661e63e container agnhost-container: <nil>
STEP: delete the pod
Sep 27 16:11:12.733: INFO: Waiting for pod client-containers-950943cb-a273-4a77-a5df-14f3a661e63e to disappear
Sep 27 16:11:12.737: INFO: Pod client-containers-950943cb-a273-4a77-a5df-14f3a661e63e no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:11:12.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1514" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":339,"completed":50,"skipped":719,"failed":0}
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:11:12.750: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 27 16:11:13.803: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:11:13.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4518" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":51,"skipped":723,"failed":0}

------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:11:13.835: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:149
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 27 16:11:13.898: INFO: starting watch
STEP: patching
STEP: updating
Sep 27 16:11:13.910: INFO: waiting for watch events with expected annotations
Sep 27 16:11:13.910: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:11:13.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-3471" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":339,"completed":52,"skipped":723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:11:13.956: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:11:20.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6404" for this suite.

• [SLOW TEST:6.058 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":339,"completed":53,"skipped":746,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:11:20.014: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-1829
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Sep 27 16:11:20.066: INFO: Found 0 stateful pods, waiting for 3
Sep 27 16:11:30.074: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:11:30.074: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:11:30.074: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:11:30.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-1829 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 16:11:30.240: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 16:11:30.240: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 16:11:30.240: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Sep 27 16:11:40.280: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 27 16:11:50.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-1829 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:11:50.463: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 16:11:50.463: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 16:11:50.463: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 16:12:00.489: INFO: Waiting for StatefulSet statefulset-1829/ss2 to complete update
Sep 27 16:12:00.489: INFO: Waiting for Pod statefulset-1829/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Sep 27 16:12:00.489: INFO: Waiting for Pod statefulset-1829/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Sep 27 16:12:10.502: INFO: Waiting for StatefulSet statefulset-1829/ss2 to complete update
Sep 27 16:12:10.502: INFO: Waiting for Pod statefulset-1829/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Sep 27 16:12:10.502: INFO: Waiting for Pod statefulset-1829/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Sep 27 16:12:20.500: INFO: Waiting for StatefulSet statefulset-1829/ss2 to complete update
Sep 27 16:12:20.500: INFO: Waiting for Pod statefulset-1829/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Rolling back to a previous revision
Sep 27 16:12:30.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-1829 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 16:12:30.662: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 16:12:30.662: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 16:12:30.662: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 16:12:40.702: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 27 16:12:50.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-1829 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:12:50.888: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 16:12:50.888: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 16:12:50.888: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 16:13:00.915: INFO: Waiting for StatefulSet statefulset-1829/ss2 to complete update
Sep 27 16:13:00.915: INFO: Waiting for Pod statefulset-1829/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Sep 27 16:13:00.915: INFO: Waiting for Pod statefulset-1829/ss2-1 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Sep 27 16:13:00.915: INFO: Waiting for Pod statefulset-1829/ss2-2 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Sep 27 16:13:10.926: INFO: Waiting for StatefulSet statefulset-1829/ss2 to complete update
Sep 27 16:13:10.926: INFO: Waiting for Pod statefulset-1829/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Sep 27 16:13:20.926: INFO: Deleting all statefulset in ns statefulset-1829
Sep 27 16:13:20.930: INFO: Scaling statefulset ss2 to 0
Sep 27 16:13:50.952: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 16:13:50.956: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:13:50.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1829" for this suite.

• [SLOW TEST:150.976 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":339,"completed":54,"skipped":751,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:13:50.990: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-93124d20-73c9-4c87-bf47-dc1369f2f8ef
STEP: Creating a pod to test consume configMaps
Sep 27 16:13:51.037: INFO: Waiting up to 5m0s for pod "pod-configmaps-d3bda17c-3c3e-4e4b-b739-903af75a9d7b" in namespace "configmap-8256" to be "Succeeded or Failed"
Sep 27 16:13:51.043: INFO: Pod "pod-configmaps-d3bda17c-3c3e-4e4b-b739-903af75a9d7b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.486042ms
Sep 27 16:13:53.050: INFO: Pod "pod-configmaps-d3bda17c-3c3e-4e4b-b739-903af75a9d7b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012772115s
STEP: Saw pod success
Sep 27 16:13:53.050: INFO: Pod "pod-configmaps-d3bda17c-3c3e-4e4b-b739-903af75a9d7b" satisfied condition "Succeeded or Failed"
Sep 27 16:13:53.054: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-configmaps-d3bda17c-3c3e-4e4b-b739-903af75a9d7b container agnhost-container: <nil>
STEP: delete the pod
Sep 27 16:13:53.089: INFO: Waiting for pod pod-configmaps-d3bda17c-3c3e-4e4b-b739-903af75a9d7b to disappear
Sep 27 16:13:53.092: INFO: Pod pod-configmaps-d3bda17c-3c3e-4e4b-b739-903af75a9d7b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:13:53.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8256" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":55,"skipped":783,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:13:53.105: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 16:13:53.566: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 16:13:56.592: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:13:56.598: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5756-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:13:59.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3954" for this suite.
STEP: Destroying namespace "webhook-3954-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.691 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":339,"completed":56,"skipped":785,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:13:59.795: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1128
STEP: creating service affinity-nodeport in namespace services-1128
STEP: creating replication controller affinity-nodeport in namespace services-1128
I0927 16:13:59.868725      23 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-1128, replica count: 3
I0927 16:14:02.920496      23 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 16:14:02.935: INFO: Creating new exec pod
Sep 27 16:14:05.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-1128 exec execpod-affinityldg42 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Sep 27 16:14:06.133: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Sep 27 16:14:06.133: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 16:14:06.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-1128 exec execpod-affinityldg42 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.68.38.238 80'
Sep 27 16:14:06.269: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.68.38.238 80\nConnection to 100.68.38.238 80 port [tcp/http] succeeded!\n"
Sep 27 16:14:06.269: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 16:14:06.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-1128 exec execpod-affinityldg42 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.62.6 32615'
Sep 27 16:14:06.425: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.62.6 32615\nConnection to 10.0.62.6 32615 port [tcp/*] succeeded!\n"
Sep 27 16:14:06.425: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 16:14:06.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-1128 exec execpod-affinityldg42 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.31.225 32615'
Sep 27 16:14:06.569: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.31.225 32615\nConnection to 10.0.31.225 32615 port [tcp/*] succeeded!\n"
Sep 27 16:14:06.569: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 16:14:06.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-1128 exec execpod-affinityldg42 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.31.225:32615/ ; done'
Sep 27 16:14:06.773: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:32615/\n"
Sep 27 16:14:06.773: INFO: stdout: "\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc\naffinity-nodeport-w5qsc"
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Received response from host: affinity-nodeport-w5qsc
Sep 27 16:14:06.773: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1128, will wait for the garbage collector to delete the pods
Sep 27 16:14:06.854: INFO: Deleting ReplicationController affinity-nodeport took: 7.533376ms
Sep 27 16:14:06.955: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.967597ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:14:22.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1128" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:22.411 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":57,"skipped":786,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:14:22.206: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 27 16:14:22.250: INFO: Waiting up to 5m0s for pod "pod-ad986ee2-5b68-42f6-95db-30d3f4ce161e" in namespace "emptydir-385" to be "Succeeded or Failed"
Sep 27 16:14:22.253: INFO: Pod "pod-ad986ee2-5b68-42f6-95db-30d3f4ce161e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.575868ms
Sep 27 16:14:24.259: INFO: Pod "pod-ad986ee2-5b68-42f6-95db-30d3f4ce161e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009078087s
STEP: Saw pod success
Sep 27 16:14:24.259: INFO: Pod "pod-ad986ee2-5b68-42f6-95db-30d3f4ce161e" satisfied condition "Succeeded or Failed"
Sep 27 16:14:24.263: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-ad986ee2-5b68-42f6-95db-30d3f4ce161e container test-container: <nil>
STEP: delete the pod
Sep 27 16:14:24.287: INFO: Waiting for pod pod-ad986ee2-5b68-42f6-95db-30d3f4ce161e to disappear
Sep 27 16:14:24.290: INFO: Pod pod-ad986ee2-5b68-42f6-95db-30d3f4ce161e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:14:24.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-385" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":58,"skipped":791,"failed":0}
SSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:14:24.302: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 27 16:14:26.361: INFO: &Pod{ObjectMeta:{send-events-20aa39c9-863d-4f93-9018-86a681505b94  events-9383  19fef47f-65cc-4f90-8ecc-04c87333bffc 1696260 0 2021-09-27 16:14:24 +0000 UTC <nil> <nil> map[name:foo time:335160257] map[] [] []  [{e2e.test Update v1 2021-09-27 16:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 16:14:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mq9m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mq9m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 16:14:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 16:14:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 16:14:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 16:14:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.95.24,PodIP:100.96.4.52,StartTime:2021-09-27 16:14:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 16:14:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://ff6e6f5805cff17a18c1abd42428080ab5175f0be7317b08b5873254e66ca6d7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep 27 16:14:28.368: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 27 16:14:30.374: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:14:30.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9383" for this suite.

• [SLOW TEST:6.098 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":339,"completed":59,"skipped":795,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:14:30.401: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 16:14:30.443: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ec2c517-298a-4e06-8760-2887b9da1fe3" in namespace "projected-3828" to be "Succeeded or Failed"
Sep 27 16:14:30.450: INFO: Pod "downwardapi-volume-3ec2c517-298a-4e06-8760-2887b9da1fe3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.163245ms
Sep 27 16:14:32.456: INFO: Pod "downwardapi-volume-3ec2c517-298a-4e06-8760-2887b9da1fe3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01284403s
STEP: Saw pod success
Sep 27 16:14:32.456: INFO: Pod "downwardapi-volume-3ec2c517-298a-4e06-8760-2887b9da1fe3" satisfied condition "Succeeded or Failed"
Sep 27 16:14:32.460: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod downwardapi-volume-3ec2c517-298a-4e06-8760-2887b9da1fe3 container client-container: <nil>
STEP: delete the pod
Sep 27 16:14:32.492: INFO: Waiting for pod downwardapi-volume-3ec2c517-298a-4e06-8760-2887b9da1fe3 to disappear
Sep 27 16:14:32.496: INFO: Pod downwardapi-volume-3ec2c517-298a-4e06-8760-2887b9da1fe3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:14:32.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3828" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":60,"skipped":808,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:14:32.508: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Sep 27 16:14:34.603: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:14:36.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3058" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":339,"completed":61,"skipped":852,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:14:36.627: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1684.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1684.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1684.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1684.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1684.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1684.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1684.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1684.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1684.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1684.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 27.3.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.3.27_udp@PTR;check="$$(dig +tcp +noall +answer +search 27.3.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.3.27_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1684.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1684.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1684.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1684.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1684.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1684.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1684.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1684.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1684.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1684.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1684.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 27.3.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.3.27_udp@PTR;check="$$(dig +tcp +noall +answer +search 27.3.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.3.27_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 16:14:38.714: INFO: Unable to read wheezy_udp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:38.718: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:38.722: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:38.726: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:38.754: INFO: Unable to read jessie_udp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:38.758: INFO: Unable to read jessie_tcp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:38.762: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:38.765: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:38.788: INFO: Lookups using dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038 failed for: [wheezy_udp@dns-test-service.dns-1684.svc.cluster.local wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local jessie_udp@dns-test-service.dns-1684.svc.cluster.local jessie_tcp@dns-test-service.dns-1684.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local]

Sep 27 16:14:43.792: INFO: Unable to read wheezy_udp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:43.796: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:43.800: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:43.804: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:43.831: INFO: Unable to read jessie_udp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:43.834: INFO: Unable to read jessie_tcp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:43.838: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:43.842: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:43.864: INFO: Lookups using dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038 failed for: [wheezy_udp@dns-test-service.dns-1684.svc.cluster.local wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local jessie_udp@dns-test-service.dns-1684.svc.cluster.local jessie_tcp@dns-test-service.dns-1684.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local]

Sep 27 16:14:48.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:48.797: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:48.801: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:48.805: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:48.831: INFO: Unable to read jessie_udp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:48.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:48.838: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:48.842: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:48.864: INFO: Lookups using dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038 failed for: [wheezy_udp@dns-test-service.dns-1684.svc.cluster.local wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local jessie_udp@dns-test-service.dns-1684.svc.cluster.local jessie_tcp@dns-test-service.dns-1684.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local]

Sep 27 16:14:53.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:53.797: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:53.801: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:53.805: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:53.831: INFO: Unable to read jessie_udp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:53.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:53.839: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:53.842: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:53.872: INFO: Lookups using dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038 failed for: [wheezy_udp@dns-test-service.dns-1684.svc.cluster.local wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local jessie_udp@dns-test-service.dns-1684.svc.cluster.local jessie_tcp@dns-test-service.dns-1684.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local]

Sep 27 16:14:58.794: INFO: Unable to read wheezy_udp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:58.798: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:58.802: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:58.805: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:58.832: INFO: Unable to read jessie_udp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:58.835: INFO: Unable to read jessie_tcp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:58.839: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:58.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:14:58.865: INFO: Lookups using dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038 failed for: [wheezy_udp@dns-test-service.dns-1684.svc.cluster.local wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local jessie_udp@dns-test-service.dns-1684.svc.cluster.local jessie_tcp@dns-test-service.dns-1684.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local]

Sep 27 16:15:03.793: INFO: Unable to read wheezy_udp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:15:03.797: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:15:03.801: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:15:03.805: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:15:03.831: INFO: Unable to read jessie_udp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:15:03.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:15:03.844: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:15:03.848: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local from pod dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038: the server could not find the requested resource (get pods dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038)
Sep 27 16:15:03.871: INFO: Lookups using dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038 failed for: [wheezy_udp@dns-test-service.dns-1684.svc.cluster.local wheezy_tcp@dns-test-service.dns-1684.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local jessie_udp@dns-test-service.dns-1684.svc.cluster.local jessie_tcp@dns-test-service.dns-1684.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1684.svc.cluster.local]

Sep 27 16:15:08.865: INFO: DNS probes using dns-1684/dns-test-dbcff061-6b3e-4f85-8814-9892e0ba4038 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:15:08.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1684" for this suite.

• [SLOW TEST:32.321 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":339,"completed":62,"skipped":872,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:15:08.949: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Sep 27 16:15:08.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-5356 api-versions'
Sep 27 16:15:09.059: INFO: stderr: ""
Sep 27 16:15:09.059: INFO: stdout: "acme.cert-manager.io/v1\nacme.cert-manager.io/v1alpha2\nacme.cert-manager.io/v1alpha3\nacme.cert-manager.io/v1beta1\naddons.cluster.x-k8s.io/v1alpha3\nadmissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbootstrap.cluster.x-k8s.io/v1alpha2\nbootstrap.cluster.x-k8s.io/v1alpha3\ncert-manager.io/v1\ncert-manager.io/v1alpha2\ncert-manager.io/v1alpha3\ncert-manager.io/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncluster.x-k8s.io/v1alpha2\ncluster.x-k8s.io/v1alpha3\nclusterctl.cluster.x-k8s.io/v1alpha3\nclusterinformation.antrea.tanzu.vmware.com/v1beta1\ncontrolplane.antrea.tanzu.vmware.com/v1beta1\ncontrolplane.antrea.tanzu.vmware.com/v1beta2\ncontrolplane.cluster.x-k8s.io/v1alpha3\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncore.antrea.tanzu.vmware.com/v1alpha2\ncrd.antrea.tanzu.vmware.com/v1alpha1\ndata.packaging.carvel.dev/v1alpha1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nexp.cluster.x-k8s.io/v1alpha3\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\ninfrastructure.cluster.x-k8s.io/v1alpha2\ninfrastructure.cluster.x-k8s.io/v1alpha3\ninternal.packaging.carvel.dev/v1alpha1\nkappctrl.k14s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.antrea.tanzu.vmware.com/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\nops.antrea.tanzu.vmware.com/v1alpha1\npackaging.carvel.dev/v1alpha1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrun.tanzu.vmware.com/v1alpha1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.antrea.tanzu.vmware.com/v1alpha1\nstats.antrea.tanzu.vmware.com/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nsystem.antrea.tanzu.vmware.com/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:15:09.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5356" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":339,"completed":63,"skipped":891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:15:09.073: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Sep 27 16:15:09.115: INFO: Waiting up to 5m0s for pod "var-expansion-55676e39-0e4e-4a9e-8f07-ae856cd846b0" in namespace "var-expansion-5043" to be "Succeeded or Failed"
Sep 27 16:15:09.119: INFO: Pod "var-expansion-55676e39-0e4e-4a9e-8f07-ae856cd846b0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.547174ms
Sep 27 16:15:11.125: INFO: Pod "var-expansion-55676e39-0e4e-4a9e-8f07-ae856cd846b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010171029s
STEP: Saw pod success
Sep 27 16:15:11.125: INFO: Pod "var-expansion-55676e39-0e4e-4a9e-8f07-ae856cd846b0" satisfied condition "Succeeded or Failed"
Sep 27 16:15:11.129: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod var-expansion-55676e39-0e4e-4a9e-8f07-ae856cd846b0 container dapi-container: <nil>
STEP: delete the pod
Sep 27 16:15:11.156: INFO: Waiting for pod var-expansion-55676e39-0e4e-4a9e-8f07-ae856cd846b0 to disappear
Sep 27 16:15:11.159: INFO: Pod var-expansion-55676e39-0e4e-4a9e-8f07-ae856cd846b0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:15:11.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5043" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":339,"completed":64,"skipped":913,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:15:11.171: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Sep 27 16:15:11.214: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 27 16:15:16.221: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:15:16.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2334" for this suite.

• [SLOW TEST:5.104 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":339,"completed":65,"skipped":927,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:15:16.275: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7011
STEP: creating service affinity-clusterip-transition in namespace services-7011
STEP: creating replication controller affinity-clusterip-transition in namespace services-7011
I0927 16:15:16.342589      23 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-7011, replica count: 3
I0927 16:15:19.394562      23 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 16:15:19.402: INFO: Creating new exec pod
Sep 27 16:15:22.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-7011 exec execpod-affinityfjc6v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Sep 27 16:15:22.610: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep 27 16:15:22.610: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 16:15:22.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-7011 exec execpod-affinityfjc6v -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.66.206.196 80'
Sep 27 16:15:22.747: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.66.206.196 80\nConnection to 100.66.206.196 80 port [tcp/http] succeeded!\n"
Sep 27 16:15:22.747: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 16:15:22.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-7011 exec execpod-affinityfjc6v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.66.206.196:80/ ; done'
Sep 27 16:15:22.971: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n"
Sep 27 16:15:22.971: INFO: stdout: "\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-msjt4\naffinity-clusterip-transition-msjt4\naffinity-clusterip-transition-ssz5v\naffinity-clusterip-transition-ssz5v\naffinity-clusterip-transition-msjt4\naffinity-clusterip-transition-ssz5v\naffinity-clusterip-transition-msjt4\naffinity-clusterip-transition-msjt4\naffinity-clusterip-transition-msjt4\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-msjt4\naffinity-clusterip-transition-ssz5v\naffinity-clusterip-transition-ssz5v\naffinity-clusterip-transition-qxfpm"
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-msjt4
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-msjt4
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-ssz5v
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-ssz5v
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-msjt4
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-ssz5v
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-msjt4
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-msjt4
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-msjt4
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-msjt4
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-ssz5v
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-ssz5v
Sep 27 16:15:22.971: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:22.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-7011 exec execpod-affinityfjc6v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.66.206.196:80/ ; done'
Sep 27 16:15:23.192: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n"
Sep 27 16:15:23.192: INFO: stdout: "\naffinity-clusterip-transition-ssz5v\naffinity-clusterip-transition-ssz5v\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-ssz5v\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-msjt4\naffinity-clusterip-transition-ssz5v\naffinity-clusterip-transition-msjt4\naffinity-clusterip-transition-ssz5v\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-msjt4\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-msjt4"
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-ssz5v
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-ssz5v
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-ssz5v
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-msjt4
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-ssz5v
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-msjt4
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-ssz5v
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-msjt4
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:23.192: INFO: Received response from host: affinity-clusterip-transition-msjt4
Sep 27 16:15:53.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-7011 exec execpod-affinityfjc6v -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.66.206.196:80/ ; done'
Sep 27 16:15:53.408: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.66.206.196:80/\n"
Sep 27 16:15:53.408: INFO: stdout: "\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm\naffinity-clusterip-transition-qxfpm"
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Received response from host: affinity-clusterip-transition-qxfpm
Sep 27 16:15:53.408: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7011, will wait for the garbage collector to delete the pods
Sep 27 16:15:53.497: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.651537ms
Sep 27 16:15:53.597: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.511949ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:16:02.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7011" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:45.883 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":66,"skipped":937,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:16:02.158: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:16:02.228: INFO: Got root ca configmap in namespace "svcaccounts-6442"
Sep 27 16:16:02.235: INFO: Deleted root ca configmap in namespace "svcaccounts-6442"
STEP: waiting for a new root ca configmap created
Sep 27 16:16:02.741: INFO: Recreated root ca configmap in namespace "svcaccounts-6442"
Sep 27 16:16:02.747: INFO: Updated root ca configmap in namespace "svcaccounts-6442"
STEP: waiting for the root ca configmap reconciled
Sep 27 16:16:03.252: INFO: Reconciled root ca configmap in namespace "svcaccounts-6442"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:16:03.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6442" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":339,"completed":67,"skipped":949,"failed":0}
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:16:03.265: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:16:03.299: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6347
I0927 16:16:03.309864      23 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6347, replica count: 1
I0927 16:16:04.360576      23 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 16:16:04.478: INFO: Created: latency-svc-5znqg
Sep 27 16:16:04.485: INFO: Got endpoints: latency-svc-5znqg [24.317515ms]
Sep 27 16:16:04.502: INFO: Created: latency-svc-kw2zg
Sep 27 16:16:04.510: INFO: Got endpoints: latency-svc-kw2zg [24.99181ms]
Sep 27 16:16:04.514: INFO: Created: latency-svc-t6h4j
Sep 27 16:16:04.521: INFO: Got endpoints: latency-svc-t6h4j [36.010744ms]
Sep 27 16:16:04.528: INFO: Created: latency-svc-jpvbx
Sep 27 16:16:04.535: INFO: Got endpoints: latency-svc-jpvbx [49.785126ms]
Sep 27 16:16:04.543: INFO: Created: latency-svc-rgpxv
Sep 27 16:16:04.555: INFO: Got endpoints: latency-svc-rgpxv [69.819367ms]
Sep 27 16:16:04.558: INFO: Created: latency-svc-nh8fh
Sep 27 16:16:04.568: INFO: Got endpoints: latency-svc-nh8fh [82.766834ms]
Sep 27 16:16:04.572: INFO: Created: latency-svc-tcqgm
Sep 27 16:16:04.582: INFO: Got endpoints: latency-svc-tcqgm [96.555818ms]
Sep 27 16:16:04.585: INFO: Created: latency-svc-gzjwp
Sep 27 16:16:04.594: INFO: Got endpoints: latency-svc-gzjwp [108.400456ms]
Sep 27 16:16:04.598: INFO: Created: latency-svc-ffhr8
Sep 27 16:16:04.605: INFO: Got endpoints: latency-svc-ffhr8 [119.794177ms]
Sep 27 16:16:04.608: INFO: Created: latency-svc-m8qcq
Sep 27 16:16:04.619: INFO: Got endpoints: latency-svc-m8qcq [133.102959ms]
Sep 27 16:16:04.622: INFO: Created: latency-svc-wxzlf
Sep 27 16:16:04.631: INFO: Got endpoints: latency-svc-wxzlf [145.618011ms]
Sep 27 16:16:04.633: INFO: Created: latency-svc-wxlzw
Sep 27 16:16:04.642: INFO: Got endpoints: latency-svc-wxlzw [156.255719ms]
Sep 27 16:16:04.645: INFO: Created: latency-svc-2vmkg
Sep 27 16:16:04.653: INFO: Got endpoints: latency-svc-2vmkg [166.929081ms]
Sep 27 16:16:04.657: INFO: Created: latency-svc-6f4z2
Sep 27 16:16:04.667: INFO: Got endpoints: latency-svc-6f4z2 [181.287607ms]
Sep 27 16:16:04.669: INFO: Created: latency-svc-pwqn6
Sep 27 16:16:04.679: INFO: Got endpoints: latency-svc-pwqn6 [193.371468ms]
Sep 27 16:16:04.683: INFO: Created: latency-svc-wvwkh
Sep 27 16:16:04.690: INFO: Got endpoints: latency-svc-wvwkh [204.237351ms]
Sep 27 16:16:04.693: INFO: Created: latency-svc-9sj7w
Sep 27 16:16:04.701: INFO: Got endpoints: latency-svc-9sj7w [191.106392ms]
Sep 27 16:16:04.706: INFO: Created: latency-svc-6rlx5
Sep 27 16:16:04.713: INFO: Got endpoints: latency-svc-6rlx5 [191.505428ms]
Sep 27 16:16:04.718: INFO: Created: latency-svc-zlnr7
Sep 27 16:16:04.728: INFO: Got endpoints: latency-svc-zlnr7 [192.533643ms]
Sep 27 16:16:04.730: INFO: Created: latency-svc-4hhrb
Sep 27 16:16:04.739: INFO: Got endpoints: latency-svc-4hhrb [183.844298ms]
Sep 27 16:16:04.744: INFO: Created: latency-svc-j6klf
Sep 27 16:16:04.753: INFO: Got endpoints: latency-svc-j6klf [184.708982ms]
Sep 27 16:16:04.757: INFO: Created: latency-svc-n69xt
Sep 27 16:16:04.766: INFO: Got endpoints: latency-svc-n69xt [183.727688ms]
Sep 27 16:16:04.770: INFO: Created: latency-svc-q89vp
Sep 27 16:16:04.777: INFO: Got endpoints: latency-svc-q89vp [182.958054ms]
Sep 27 16:16:04.781: INFO: Created: latency-svc-28vbj
Sep 27 16:16:04.790: INFO: Got endpoints: latency-svc-28vbj [184.325401ms]
Sep 27 16:16:04.794: INFO: Created: latency-svc-z5lwh
Sep 27 16:16:04.802: INFO: Got endpoints: latency-svc-z5lwh [183.629831ms]
Sep 27 16:16:04.805: INFO: Created: latency-svc-5gt2q
Sep 27 16:16:04.813: INFO: Got endpoints: latency-svc-5gt2q [182.197056ms]
Sep 27 16:16:04.818: INFO: Created: latency-svc-2l4wp
Sep 27 16:16:04.826: INFO: Got endpoints: latency-svc-2l4wp [184.095713ms]
Sep 27 16:16:04.828: INFO: Created: latency-svc-r4kdh
Sep 27 16:16:04.838: INFO: Got endpoints: latency-svc-r4kdh [185.389412ms]
Sep 27 16:16:04.841: INFO: Created: latency-svc-kpm57
Sep 27 16:16:04.849: INFO: Got endpoints: latency-svc-kpm57 [181.964804ms]
Sep 27 16:16:04.854: INFO: Created: latency-svc-xh9w7
Sep 27 16:16:04.863: INFO: Got endpoints: latency-svc-xh9w7 [183.912692ms]
Sep 27 16:16:04.867: INFO: Created: latency-svc-jthvw
Sep 27 16:16:04.874: INFO: Got endpoints: latency-svc-jthvw [184.557812ms]
Sep 27 16:16:04.878: INFO: Created: latency-svc-p7kct
Sep 27 16:16:04.886: INFO: Got endpoints: latency-svc-p7kct [184.751409ms]
Sep 27 16:16:04.891: INFO: Created: latency-svc-9nxd4
Sep 27 16:16:04.904: INFO: Got endpoints: latency-svc-9nxd4 [190.713176ms]
Sep 27 16:16:04.908: INFO: Created: latency-svc-k6xmc
Sep 27 16:16:04.916: INFO: Got endpoints: latency-svc-k6xmc [188.509833ms]
Sep 27 16:16:04.920: INFO: Created: latency-svc-ztggs
Sep 27 16:16:04.928: INFO: Got endpoints: latency-svc-ztggs [188.86847ms]
Sep 27 16:16:04.931: INFO: Created: latency-svc-6vr7q
Sep 27 16:16:04.938: INFO: Got endpoints: latency-svc-6vr7q [184.870829ms]
Sep 27 16:16:04.943: INFO: Created: latency-svc-r827m
Sep 27 16:16:04.950: INFO: Got endpoints: latency-svc-r827m [184.54922ms]
Sep 27 16:16:04.955: INFO: Created: latency-svc-dh7kq
Sep 27 16:16:04.963: INFO: Got endpoints: latency-svc-dh7kq [185.77102ms]
Sep 27 16:16:04.967: INFO: Created: latency-svc-5fbw9
Sep 27 16:16:04.975: INFO: Got endpoints: latency-svc-5fbw9 [185.569634ms]
Sep 27 16:16:04.978: INFO: Created: latency-svc-xc978
Sep 27 16:16:04.986: INFO: Got endpoints: latency-svc-xc978 [184.093694ms]
Sep 27 16:16:04.990: INFO: Created: latency-svc-42bqx
Sep 27 16:16:05.000: INFO: Created: latency-svc-ql45v
Sep 27 16:16:05.012: INFO: Created: latency-svc-gdnpz
Sep 27 16:16:05.024: INFO: Created: latency-svc-xzknl
Sep 27 16:16:05.034: INFO: Got endpoints: latency-svc-42bqx [221.105045ms]
Sep 27 16:16:05.038: INFO: Created: latency-svc-dxgvq
Sep 27 16:16:05.049: INFO: Created: latency-svc-cnpqn
Sep 27 16:16:05.063: INFO: Created: latency-svc-4tnlk
Sep 27 16:16:05.073: INFO: Created: latency-svc-ddg2w
Sep 27 16:16:05.085: INFO: Created: latency-svc-7hwmv
Sep 27 16:16:05.085: INFO: Got endpoints: latency-svc-ql45v [259.13854ms]
Sep 27 16:16:05.095: INFO: Created: latency-svc-s27fl
Sep 27 16:16:05.106: INFO: Created: latency-svc-h8nlt
Sep 27 16:16:05.135: INFO: Got endpoints: latency-svc-gdnpz [297.423218ms]
Sep 27 16:16:05.136: INFO: Created: latency-svc-mnkm8
Sep 27 16:16:05.150: INFO: Created: latency-svc-f9mbv
Sep 27 16:16:05.161: INFO: Created: latency-svc-cjfqp
Sep 27 16:16:05.173: INFO: Created: latency-svc-qg84j
Sep 27 16:16:05.184: INFO: Created: latency-svc-9rv87
Sep 27 16:16:05.184: INFO: Got endpoints: latency-svc-xzknl [335.667982ms]
Sep 27 16:16:05.196: INFO: Created: latency-svc-s6cm4
Sep 27 16:16:05.207: INFO: Created: latency-svc-8zl7n
Sep 27 16:16:05.222: INFO: Created: latency-svc-jq7c5
Sep 27 16:16:05.237: INFO: Got endpoints: latency-svc-dxgvq [374.016978ms]
Sep 27 16:16:05.254: INFO: Created: latency-svc-p227g
Sep 27 16:16:05.285: INFO: Got endpoints: latency-svc-cnpqn [410.30167ms]
Sep 27 16:16:05.301: INFO: Created: latency-svc-64zjh
Sep 27 16:16:05.334: INFO: Got endpoints: latency-svc-4tnlk [448.153155ms]
Sep 27 16:16:05.354: INFO: Created: latency-svc-bqghg
Sep 27 16:16:05.385: INFO: Got endpoints: latency-svc-ddg2w [480.921207ms]
Sep 27 16:16:05.400: INFO: Created: latency-svc-q79vc
Sep 27 16:16:05.435: INFO: Got endpoints: latency-svc-7hwmv [518.334905ms]
Sep 27 16:16:05.451: INFO: Created: latency-svc-695q7
Sep 27 16:16:05.483: INFO: Got endpoints: latency-svc-s27fl [554.818273ms]
Sep 27 16:16:05.505: INFO: Created: latency-svc-9n8b5
Sep 27 16:16:05.537: INFO: Got endpoints: latency-svc-h8nlt [599.559898ms]
Sep 27 16:16:05.553: INFO: Created: latency-svc-5gqww
Sep 27 16:16:05.584: INFO: Got endpoints: latency-svc-mnkm8 [633.894939ms]
Sep 27 16:16:05.599: INFO: Created: latency-svc-l5wnr
Sep 27 16:16:05.635: INFO: Got endpoints: latency-svc-f9mbv [672.591389ms]
Sep 27 16:16:05.652: INFO: Created: latency-svc-2878n
Sep 27 16:16:05.684: INFO: Got endpoints: latency-svc-cjfqp [708.914437ms]
Sep 27 16:16:05.699: INFO: Created: latency-svc-b6fhz
Sep 27 16:16:05.734: INFO: Got endpoints: latency-svc-qg84j [747.467787ms]
Sep 27 16:16:05.751: INFO: Created: latency-svc-72qxz
Sep 27 16:16:05.785: INFO: Got endpoints: latency-svc-9rv87 [750.20414ms]
Sep 27 16:16:05.801: INFO: Created: latency-svc-x9zhr
Sep 27 16:16:05.834: INFO: Got endpoints: latency-svc-s6cm4 [748.50022ms]
Sep 27 16:16:05.850: INFO: Created: latency-svc-sdmdv
Sep 27 16:16:05.884: INFO: Got endpoints: latency-svc-8zl7n [748.501715ms]
Sep 27 16:16:05.900: INFO: Created: latency-svc-tsxgd
Sep 27 16:16:05.934: INFO: Got endpoints: latency-svc-jq7c5 [749.397236ms]
Sep 27 16:16:05.949: INFO: Created: latency-svc-b7rnx
Sep 27 16:16:05.985: INFO: Got endpoints: latency-svc-p227g [747.960217ms]
Sep 27 16:16:05.999: INFO: Created: latency-svc-d827m
Sep 27 16:16:06.035: INFO: Got endpoints: latency-svc-64zjh [750.236384ms]
Sep 27 16:16:06.052: INFO: Created: latency-svc-ksj4b
Sep 27 16:16:06.084: INFO: Got endpoints: latency-svc-bqghg [749.835508ms]
Sep 27 16:16:06.103: INFO: Created: latency-svc-vxp4q
Sep 27 16:16:06.134: INFO: Got endpoints: latency-svc-q79vc [749.785379ms]
Sep 27 16:16:06.150: INFO: Created: latency-svc-qvcfv
Sep 27 16:16:06.185: INFO: Got endpoints: latency-svc-695q7 [750.19578ms]
Sep 27 16:16:06.201: INFO: Created: latency-svc-8wgzn
Sep 27 16:16:06.236: INFO: Got endpoints: latency-svc-9n8b5 [752.994745ms]
Sep 27 16:16:06.251: INFO: Created: latency-svc-24dd5
Sep 27 16:16:06.292: INFO: Got endpoints: latency-svc-5gqww [754.882546ms]
Sep 27 16:16:06.310: INFO: Created: latency-svc-jxpjc
Sep 27 16:16:06.333: INFO: Got endpoints: latency-svc-l5wnr [749.008051ms]
Sep 27 16:16:06.349: INFO: Created: latency-svc-kmlw8
Sep 27 16:16:06.383: INFO: Got endpoints: latency-svc-2878n [747.775143ms]
Sep 27 16:16:06.399: INFO: Created: latency-svc-kzp9d
Sep 27 16:16:06.433: INFO: Got endpoints: latency-svc-b6fhz [748.981209ms]
Sep 27 16:16:06.449: INFO: Created: latency-svc-p2zqs
Sep 27 16:16:06.485: INFO: Got endpoints: latency-svc-72qxz [751.024116ms]
Sep 27 16:16:06.501: INFO: Created: latency-svc-skw8j
Sep 27 16:16:06.536: INFO: Got endpoints: latency-svc-x9zhr [750.976557ms]
Sep 27 16:16:06.550: INFO: Created: latency-svc-mjw7q
Sep 27 16:16:06.586: INFO: Got endpoints: latency-svc-sdmdv [751.834075ms]
Sep 27 16:16:06.601: INFO: Created: latency-svc-cchrj
Sep 27 16:16:06.634: INFO: Got endpoints: latency-svc-tsxgd [750.479172ms]
Sep 27 16:16:06.650: INFO: Created: latency-svc-htpj4
Sep 27 16:16:06.684: INFO: Got endpoints: latency-svc-b7rnx [750.110476ms]
Sep 27 16:16:06.699: INFO: Created: latency-svc-mzfwx
Sep 27 16:16:06.736: INFO: Got endpoints: latency-svc-d827m [751.103552ms]
Sep 27 16:16:06.751: INFO: Created: latency-svc-9rwws
Sep 27 16:16:06.784: INFO: Got endpoints: latency-svc-ksj4b [748.539304ms]
Sep 27 16:16:06.800: INFO: Created: latency-svc-fdb55
Sep 27 16:16:06.833: INFO: Got endpoints: latency-svc-vxp4q [748.59533ms]
Sep 27 16:16:06.848: INFO: Created: latency-svc-jwnvf
Sep 27 16:16:06.887: INFO: Got endpoints: latency-svc-qvcfv [752.155233ms]
Sep 27 16:16:06.905: INFO: Created: latency-svc-rxbfd
Sep 27 16:16:06.935: INFO: Got endpoints: latency-svc-8wgzn [750.098375ms]
Sep 27 16:16:06.949: INFO: Created: latency-svc-9ztsk
Sep 27 16:16:06.989: INFO: Got endpoints: latency-svc-24dd5 [752.604163ms]
Sep 27 16:16:07.004: INFO: Created: latency-svc-h2wf5
Sep 27 16:16:07.033: INFO: Got endpoints: latency-svc-jxpjc [740.974299ms]
Sep 27 16:16:07.050: INFO: Created: latency-svc-gl9mj
Sep 27 16:16:07.083: INFO: Got endpoints: latency-svc-kmlw8 [749.850237ms]
Sep 27 16:16:07.099: INFO: Created: latency-svc-22wfd
Sep 27 16:16:07.136: INFO: Got endpoints: latency-svc-kzp9d [752.708845ms]
Sep 27 16:16:07.156: INFO: Created: latency-svc-dv52k
Sep 27 16:16:07.185: INFO: Got endpoints: latency-svc-p2zqs [751.270499ms]
Sep 27 16:16:07.200: INFO: Created: latency-svc-j5rfr
Sep 27 16:16:07.236: INFO: Got endpoints: latency-svc-skw8j [751.030795ms]
Sep 27 16:16:07.253: INFO: Created: latency-svc-4qn8r
Sep 27 16:16:07.284: INFO: Got endpoints: latency-svc-mjw7q [748.033492ms]
Sep 27 16:16:07.306: INFO: Created: latency-svc-p6x54
Sep 27 16:16:07.335: INFO: Got endpoints: latency-svc-cchrj [749.751759ms]
Sep 27 16:16:07.350: INFO: Created: latency-svc-7jm67
Sep 27 16:16:07.385: INFO: Got endpoints: latency-svc-htpj4 [750.70953ms]
Sep 27 16:16:07.402: INFO: Created: latency-svc-wb5nd
Sep 27 16:16:07.434: INFO: Got endpoints: latency-svc-mzfwx [749.57829ms]
Sep 27 16:16:07.450: INFO: Created: latency-svc-b8dbs
Sep 27 16:16:07.484: INFO: Got endpoints: latency-svc-9rwws [747.584284ms]
Sep 27 16:16:07.500: INFO: Created: latency-svc-zhm4l
Sep 27 16:16:07.535: INFO: Got endpoints: latency-svc-fdb55 [750.992894ms]
Sep 27 16:16:07.549: INFO: Created: latency-svc-g4wtn
Sep 27 16:16:07.585: INFO: Got endpoints: latency-svc-jwnvf [751.798196ms]
Sep 27 16:16:07.600: INFO: Created: latency-svc-d6nv6
Sep 27 16:16:07.636: INFO: Got endpoints: latency-svc-rxbfd [749.01161ms]
Sep 27 16:16:07.650: INFO: Created: latency-svc-pdcnp
Sep 27 16:16:07.684: INFO: Got endpoints: latency-svc-9ztsk [749.059537ms]
Sep 27 16:16:07.700: INFO: Created: latency-svc-jb6dt
Sep 27 16:16:07.734: INFO: Got endpoints: latency-svc-h2wf5 [744.928289ms]
Sep 27 16:16:07.766: INFO: Created: latency-svc-lnpjl
Sep 27 16:16:07.783: INFO: Got endpoints: latency-svc-gl9mj [749.579231ms]
Sep 27 16:16:07.801: INFO: Created: latency-svc-zlw6m
Sep 27 16:16:07.838: INFO: Got endpoints: latency-svc-22wfd [754.820303ms]
Sep 27 16:16:07.853: INFO: Created: latency-svc-lsh6s
Sep 27 16:16:07.887: INFO: Got endpoints: latency-svc-dv52k [750.694578ms]
Sep 27 16:16:07.904: INFO: Created: latency-svc-kq8nm
Sep 27 16:16:07.934: INFO: Got endpoints: latency-svc-j5rfr [749.64217ms]
Sep 27 16:16:07.950: INFO: Created: latency-svc-qv6sf
Sep 27 16:16:07.983: INFO: Got endpoints: latency-svc-4qn8r [746.884349ms]
Sep 27 16:16:07.998: INFO: Created: latency-svc-2d7ks
Sep 27 16:16:08.034: INFO: Got endpoints: latency-svc-p6x54 [750.435438ms]
Sep 27 16:16:08.049: INFO: Created: latency-svc-4ws2g
Sep 27 16:16:08.084: INFO: Got endpoints: latency-svc-7jm67 [748.358456ms]
Sep 27 16:16:08.098: INFO: Created: latency-svc-v97tl
Sep 27 16:16:08.136: INFO: Got endpoints: latency-svc-wb5nd [751.141104ms]
Sep 27 16:16:08.153: INFO: Created: latency-svc-x4qcz
Sep 27 16:16:08.185: INFO: Got endpoints: latency-svc-b8dbs [751.737069ms]
Sep 27 16:16:08.200: INFO: Created: latency-svc-xj677
Sep 27 16:16:08.234: INFO: Got endpoints: latency-svc-zhm4l [750.374006ms]
Sep 27 16:16:08.250: INFO: Created: latency-svc-sjcxq
Sep 27 16:16:08.284: INFO: Got endpoints: latency-svc-g4wtn [748.894193ms]
Sep 27 16:16:08.298: INFO: Created: latency-svc-x2s8d
Sep 27 16:16:08.335: INFO: Got endpoints: latency-svc-d6nv6 [750.007081ms]
Sep 27 16:16:08.349: INFO: Created: latency-svc-vhpjr
Sep 27 16:16:08.385: INFO: Got endpoints: latency-svc-pdcnp [749.622508ms]
Sep 27 16:16:08.402: INFO: Created: latency-svc-qm42w
Sep 27 16:16:08.433: INFO: Got endpoints: latency-svc-jb6dt [749.078481ms]
Sep 27 16:16:08.450: INFO: Created: latency-svc-6kd7x
Sep 27 16:16:08.484: INFO: Got endpoints: latency-svc-lnpjl [750.351791ms]
Sep 27 16:16:08.500: INFO: Created: latency-svc-kzhwz
Sep 27 16:16:08.537: INFO: Got endpoints: latency-svc-zlw6m [753.701709ms]
Sep 27 16:16:08.551: INFO: Created: latency-svc-bd4vq
Sep 27 16:16:08.584: INFO: Got endpoints: latency-svc-lsh6s [746.271493ms]
Sep 27 16:16:08.599: INFO: Created: latency-svc-z7pxn
Sep 27 16:16:08.635: INFO: Got endpoints: latency-svc-kq8nm [748.50818ms]
Sep 27 16:16:08.651: INFO: Created: latency-svc-mrkk6
Sep 27 16:16:08.684: INFO: Got endpoints: latency-svc-qv6sf [749.698311ms]
Sep 27 16:16:08.700: INFO: Created: latency-svc-5p8qq
Sep 27 16:16:08.735: INFO: Got endpoints: latency-svc-2d7ks [751.722471ms]
Sep 27 16:16:08.750: INFO: Created: latency-svc-vlbqm
Sep 27 16:16:08.786: INFO: Got endpoints: latency-svc-4ws2g [752.085523ms]
Sep 27 16:16:08.800: INFO: Created: latency-svc-qbd58
Sep 27 16:16:08.835: INFO: Got endpoints: latency-svc-v97tl [751.506412ms]
Sep 27 16:16:08.851: INFO: Created: latency-svc-dwbfc
Sep 27 16:16:08.884: INFO: Got endpoints: latency-svc-x4qcz [747.309751ms]
Sep 27 16:16:08.899: INFO: Created: latency-svc-n52cq
Sep 27 16:16:08.935: INFO: Got endpoints: latency-svc-xj677 [749.134282ms]
Sep 27 16:16:08.949: INFO: Created: latency-svc-wp7tf
Sep 27 16:16:08.984: INFO: Got endpoints: latency-svc-sjcxq [749.698048ms]
Sep 27 16:16:08.999: INFO: Created: latency-svc-4hf7x
Sep 27 16:16:09.033: INFO: Got endpoints: latency-svc-x2s8d [749.929944ms]
Sep 27 16:16:09.049: INFO: Created: latency-svc-4phj5
Sep 27 16:16:09.083: INFO: Got endpoints: latency-svc-vhpjr [748.396885ms]
Sep 27 16:16:09.097: INFO: Created: latency-svc-2d4rz
Sep 27 16:16:09.135: INFO: Got endpoints: latency-svc-qm42w [749.459654ms]
Sep 27 16:16:09.150: INFO: Created: latency-svc-lnlx4
Sep 27 16:16:09.185: INFO: Got endpoints: latency-svc-6kd7x [752.092818ms]
Sep 27 16:16:09.199: INFO: Created: latency-svc-8ljfc
Sep 27 16:16:09.234: INFO: Got endpoints: latency-svc-kzhwz [750.289533ms]
Sep 27 16:16:09.250: INFO: Created: latency-svc-cnktf
Sep 27 16:16:09.284: INFO: Got endpoints: latency-svc-bd4vq [747.012595ms]
Sep 27 16:16:09.300: INFO: Created: latency-svc-nw8hz
Sep 27 16:16:09.334: INFO: Got endpoints: latency-svc-z7pxn [749.657607ms]
Sep 27 16:16:09.350: INFO: Created: latency-svc-pwz8g
Sep 27 16:16:09.386: INFO: Got endpoints: latency-svc-mrkk6 [750.349623ms]
Sep 27 16:16:09.401: INFO: Created: latency-svc-hgcp7
Sep 27 16:16:09.434: INFO: Got endpoints: latency-svc-5p8qq [750.368172ms]
Sep 27 16:16:09.450: INFO: Created: latency-svc-zvbn4
Sep 27 16:16:09.484: INFO: Got endpoints: latency-svc-vlbqm [748.976715ms]
Sep 27 16:16:09.501: INFO: Created: latency-svc-gfhnz
Sep 27 16:16:09.533: INFO: Got endpoints: latency-svc-qbd58 [747.03324ms]
Sep 27 16:16:09.549: INFO: Created: latency-svc-g42jh
Sep 27 16:16:09.586: INFO: Got endpoints: latency-svc-dwbfc [751.126413ms]
Sep 27 16:16:09.601: INFO: Created: latency-svc-jfbl9
Sep 27 16:16:09.634: INFO: Got endpoints: latency-svc-n52cq [750.664816ms]
Sep 27 16:16:09.651: INFO: Created: latency-svc-h5gpv
Sep 27 16:16:09.684: INFO: Got endpoints: latency-svc-wp7tf [749.791388ms]
Sep 27 16:16:09.699: INFO: Created: latency-svc-lgfh9
Sep 27 16:16:09.734: INFO: Got endpoints: latency-svc-4hf7x [750.141474ms]
Sep 27 16:16:09.749: INFO: Created: latency-svc-tbw5s
Sep 27 16:16:09.784: INFO: Got endpoints: latency-svc-4phj5 [750.785093ms]
Sep 27 16:16:09.802: INFO: Created: latency-svc-j4ct8
Sep 27 16:16:09.833: INFO: Got endpoints: latency-svc-2d4rz [750.19596ms]
Sep 27 16:16:09.850: INFO: Created: latency-svc-4s5tz
Sep 27 16:16:09.886: INFO: Got endpoints: latency-svc-lnlx4 [751.093355ms]
Sep 27 16:16:09.902: INFO: Created: latency-svc-8d2b8
Sep 27 16:16:09.934: INFO: Got endpoints: latency-svc-8ljfc [749.112835ms]
Sep 27 16:16:09.950: INFO: Created: latency-svc-5xq8m
Sep 27 16:16:09.985: INFO: Got endpoints: latency-svc-cnktf [750.956195ms]
Sep 27 16:16:10.001: INFO: Created: latency-svc-g5cn6
Sep 27 16:16:10.034: INFO: Got endpoints: latency-svc-nw8hz [750.425659ms]
Sep 27 16:16:10.053: INFO: Created: latency-svc-njmtl
Sep 27 16:16:10.084: INFO: Got endpoints: latency-svc-pwz8g [750.389045ms]
Sep 27 16:16:10.100: INFO: Created: latency-svc-t4dsd
Sep 27 16:16:10.134: INFO: Got endpoints: latency-svc-hgcp7 [748.629785ms]
Sep 27 16:16:10.149: INFO: Created: latency-svc-bbrj4
Sep 27 16:16:10.184: INFO: Got endpoints: latency-svc-zvbn4 [749.792629ms]
Sep 27 16:16:10.199: INFO: Created: latency-svc-7qbhj
Sep 27 16:16:10.236: INFO: Got endpoints: latency-svc-gfhnz [751.885798ms]
Sep 27 16:16:10.250: INFO: Created: latency-svc-6pw8l
Sep 27 16:16:10.288: INFO: Got endpoints: latency-svc-g42jh [754.517851ms]
Sep 27 16:16:10.302: INFO: Created: latency-svc-p99vr
Sep 27 16:16:10.333: INFO: Got endpoints: latency-svc-jfbl9 [746.531331ms]
Sep 27 16:16:10.348: INFO: Created: latency-svc-wpbvm
Sep 27 16:16:10.385: INFO: Got endpoints: latency-svc-h5gpv [750.325179ms]
Sep 27 16:16:10.401: INFO: Created: latency-svc-jg2dk
Sep 27 16:16:10.434: INFO: Got endpoints: latency-svc-lgfh9 [749.659635ms]
Sep 27 16:16:10.450: INFO: Created: latency-svc-wp9vg
Sep 27 16:16:10.484: INFO: Got endpoints: latency-svc-tbw5s [750.379188ms]
Sep 27 16:16:10.499: INFO: Created: latency-svc-whrbh
Sep 27 16:16:10.535: INFO: Got endpoints: latency-svc-j4ct8 [751.080157ms]
Sep 27 16:16:10.551: INFO: Created: latency-svc-2j4jp
Sep 27 16:16:10.587: INFO: Got endpoints: latency-svc-4s5tz [754.068583ms]
Sep 27 16:16:10.602: INFO: Created: latency-svc-cdvcw
Sep 27 16:16:10.636: INFO: Got endpoints: latency-svc-8d2b8 [749.926256ms]
Sep 27 16:16:10.653: INFO: Created: latency-svc-n4bns
Sep 27 16:16:10.686: INFO: Got endpoints: latency-svc-5xq8m [751.83229ms]
Sep 27 16:16:10.704: INFO: Created: latency-svc-kb9xj
Sep 27 16:16:10.733: INFO: Got endpoints: latency-svc-g5cn6 [748.170409ms]
Sep 27 16:16:10.747: INFO: Created: latency-svc-vpb5h
Sep 27 16:16:10.784: INFO: Got endpoints: latency-svc-njmtl [749.462751ms]
Sep 27 16:16:10.799: INFO: Created: latency-svc-7k57x
Sep 27 16:16:10.834: INFO: Got endpoints: latency-svc-t4dsd [749.785677ms]
Sep 27 16:16:10.848: INFO: Created: latency-svc-s296h
Sep 27 16:16:10.884: INFO: Got endpoints: latency-svc-bbrj4 [749.817595ms]
Sep 27 16:16:10.900: INFO: Created: latency-svc-8bvrj
Sep 27 16:16:10.933: INFO: Got endpoints: latency-svc-7qbhj [749.121858ms]
Sep 27 16:16:10.949: INFO: Created: latency-svc-9mmzn
Sep 27 16:16:10.984: INFO: Got endpoints: latency-svc-6pw8l [747.979635ms]
Sep 27 16:16:10.999: INFO: Created: latency-svc-9gslp
Sep 27 16:16:11.035: INFO: Got endpoints: latency-svc-p99vr [747.121359ms]
Sep 27 16:16:11.049: INFO: Created: latency-svc-9qmv5
Sep 27 16:16:11.085: INFO: Got endpoints: latency-svc-wpbvm [752.179133ms]
Sep 27 16:16:11.099: INFO: Created: latency-svc-mhp9x
Sep 27 16:16:11.135: INFO: Got endpoints: latency-svc-jg2dk [749.951451ms]
Sep 27 16:16:11.149: INFO: Created: latency-svc-5x67d
Sep 27 16:16:11.185: INFO: Got endpoints: latency-svc-wp9vg [751.160446ms]
Sep 27 16:16:11.201: INFO: Created: latency-svc-xpns4
Sep 27 16:16:11.235: INFO: Got endpoints: latency-svc-whrbh [750.889122ms]
Sep 27 16:16:11.250: INFO: Created: latency-svc-gmstt
Sep 27 16:16:11.284: INFO: Got endpoints: latency-svc-2j4jp [748.715356ms]
Sep 27 16:16:11.300: INFO: Created: latency-svc-zwxhf
Sep 27 16:16:11.335: INFO: Got endpoints: latency-svc-cdvcw [747.47127ms]
Sep 27 16:16:11.350: INFO: Created: latency-svc-ncqtk
Sep 27 16:16:11.385: INFO: Got endpoints: latency-svc-n4bns [749.582872ms]
Sep 27 16:16:11.403: INFO: Created: latency-svc-svpn6
Sep 27 16:16:11.435: INFO: Got endpoints: latency-svc-kb9xj [748.752365ms]
Sep 27 16:16:11.450: INFO: Created: latency-svc-4zrts
Sep 27 16:16:11.484: INFO: Got endpoints: latency-svc-vpb5h [751.038686ms]
Sep 27 16:16:11.517: INFO: Created: latency-svc-7trzh
Sep 27 16:16:11.541: INFO: Got endpoints: latency-svc-7k57x [757.772199ms]
Sep 27 16:16:11.564: INFO: Created: latency-svc-wzcjz
Sep 27 16:16:11.585: INFO: Got endpoints: latency-svc-s296h [750.5794ms]
Sep 27 16:16:11.600: INFO: Created: latency-svc-dftl9
Sep 27 16:16:11.640: INFO: Got endpoints: latency-svc-8bvrj [755.631662ms]
Sep 27 16:16:11.665: INFO: Created: latency-svc-dclpm
Sep 27 16:16:11.691: INFO: Got endpoints: latency-svc-9mmzn [757.383172ms]
Sep 27 16:16:11.705: INFO: Created: latency-svc-jwq72
Sep 27 16:16:11.735: INFO: Got endpoints: latency-svc-9gslp [751.348371ms]
Sep 27 16:16:11.749: INFO: Created: latency-svc-wgbl2
Sep 27 16:16:11.784: INFO: Got endpoints: latency-svc-9qmv5 [748.415155ms]
Sep 27 16:16:11.799: INFO: Created: latency-svc-7wqzb
Sep 27 16:16:11.835: INFO: Got endpoints: latency-svc-mhp9x [749.552702ms]
Sep 27 16:16:11.849: INFO: Created: latency-svc-zxk2d
Sep 27 16:16:11.884: INFO: Got endpoints: latency-svc-5x67d [749.176611ms]
Sep 27 16:16:11.899: INFO: Created: latency-svc-s4nck
Sep 27 16:16:11.933: INFO: Got endpoints: latency-svc-xpns4 [747.761631ms]
Sep 27 16:16:11.949: INFO: Created: latency-svc-bt6tx
Sep 27 16:16:11.986: INFO: Got endpoints: latency-svc-gmstt [750.293314ms]
Sep 27 16:16:12.003: INFO: Created: latency-svc-9xsng
Sep 27 16:16:12.034: INFO: Got endpoints: latency-svc-zwxhf [749.45316ms]
Sep 27 16:16:12.048: INFO: Created: latency-svc-swrwh
Sep 27 16:16:12.085: INFO: Got endpoints: latency-svc-ncqtk [749.931296ms]
Sep 27 16:16:12.099: INFO: Created: latency-svc-x8rgw
Sep 27 16:16:12.134: INFO: Got endpoints: latency-svc-svpn6 [748.10097ms]
Sep 27 16:16:12.150: INFO: Created: latency-svc-xml26
Sep 27 16:16:12.183: INFO: Got endpoints: latency-svc-4zrts [748.156694ms]
Sep 27 16:16:12.198: INFO: Created: latency-svc-xx2ln
Sep 27 16:16:12.235: INFO: Got endpoints: latency-svc-7trzh [750.101611ms]
Sep 27 16:16:12.250: INFO: Created: latency-svc-6t8k4
Sep 27 16:16:12.285: INFO: Got endpoints: latency-svc-wzcjz [743.638319ms]
Sep 27 16:16:12.299: INFO: Created: latency-svc-7b26f
Sep 27 16:16:12.334: INFO: Got endpoints: latency-svc-dftl9 [749.381099ms]
Sep 27 16:16:12.385: INFO: Got endpoints: latency-svc-dclpm [745.043364ms]
Sep 27 16:16:12.434: INFO: Got endpoints: latency-svc-jwq72 [743.598022ms]
Sep 27 16:16:12.485: INFO: Got endpoints: latency-svc-wgbl2 [749.924424ms]
Sep 27 16:16:12.533: INFO: Got endpoints: latency-svc-7wqzb [749.932206ms]
Sep 27 16:16:12.585: INFO: Got endpoints: latency-svc-zxk2d [750.389058ms]
Sep 27 16:16:12.638: INFO: Got endpoints: latency-svc-s4nck [754.362364ms]
Sep 27 16:16:12.684: INFO: Got endpoints: latency-svc-bt6tx [751.037981ms]
Sep 27 16:16:12.734: INFO: Got endpoints: latency-svc-9xsng [748.643039ms]
Sep 27 16:16:12.784: INFO: Got endpoints: latency-svc-swrwh [750.203949ms]
Sep 27 16:16:12.835: INFO: Got endpoints: latency-svc-x8rgw [750.537291ms]
Sep 27 16:16:12.884: INFO: Got endpoints: latency-svc-xml26 [750.295497ms]
Sep 27 16:16:12.935: INFO: Got endpoints: latency-svc-xx2ln [751.903758ms]
Sep 27 16:16:12.986: INFO: Got endpoints: latency-svc-6t8k4 [751.158772ms]
Sep 27 16:16:13.035: INFO: Got endpoints: latency-svc-7b26f [749.451228ms]
Sep 27 16:16:13.035: INFO: Latencies: [24.99181ms 36.010744ms 49.785126ms 69.819367ms 82.766834ms 96.555818ms 108.400456ms 119.794177ms 133.102959ms 145.618011ms 156.255719ms 166.929081ms 181.287607ms 181.964804ms 182.197056ms 182.958054ms 183.629831ms 183.727688ms 183.844298ms 183.912692ms 184.093694ms 184.095713ms 184.325401ms 184.54922ms 184.557812ms 184.708982ms 184.751409ms 184.870829ms 185.389412ms 185.569634ms 185.77102ms 188.509833ms 188.86847ms 190.713176ms 191.106392ms 191.505428ms 192.533643ms 193.371468ms 204.237351ms 221.105045ms 259.13854ms 297.423218ms 335.667982ms 374.016978ms 410.30167ms 448.153155ms 480.921207ms 518.334905ms 554.818273ms 599.559898ms 633.894939ms 672.591389ms 708.914437ms 740.974299ms 743.598022ms 743.638319ms 744.928289ms 745.043364ms 746.271493ms 746.531331ms 746.884349ms 747.012595ms 747.03324ms 747.121359ms 747.309751ms 747.467787ms 747.47127ms 747.584284ms 747.761631ms 747.775143ms 747.960217ms 747.979635ms 748.033492ms 748.10097ms 748.156694ms 748.170409ms 748.358456ms 748.396885ms 748.415155ms 748.50022ms 748.501715ms 748.50818ms 748.539304ms 748.59533ms 748.629785ms 748.643039ms 748.715356ms 748.752365ms 748.894193ms 748.976715ms 748.981209ms 749.008051ms 749.01161ms 749.059537ms 749.078481ms 749.112835ms 749.121858ms 749.134282ms 749.176611ms 749.381099ms 749.397236ms 749.451228ms 749.45316ms 749.459654ms 749.462751ms 749.552702ms 749.57829ms 749.579231ms 749.582872ms 749.622508ms 749.64217ms 749.657607ms 749.659635ms 749.698048ms 749.698311ms 749.751759ms 749.785379ms 749.785677ms 749.791388ms 749.792629ms 749.817595ms 749.835508ms 749.850237ms 749.924424ms 749.926256ms 749.929944ms 749.931296ms 749.932206ms 749.951451ms 750.007081ms 750.098375ms 750.101611ms 750.110476ms 750.141474ms 750.19578ms 750.19596ms 750.203949ms 750.20414ms 750.236384ms 750.289533ms 750.293314ms 750.295497ms 750.325179ms 750.349623ms 750.351791ms 750.368172ms 750.374006ms 750.379188ms 750.389045ms 750.389058ms 750.425659ms 750.435438ms 750.479172ms 750.537291ms 750.5794ms 750.664816ms 750.694578ms 750.70953ms 750.785093ms 750.889122ms 750.956195ms 750.976557ms 750.992894ms 751.024116ms 751.030795ms 751.037981ms 751.038686ms 751.080157ms 751.093355ms 751.103552ms 751.126413ms 751.141104ms 751.158772ms 751.160446ms 751.270499ms 751.348371ms 751.506412ms 751.722471ms 751.737069ms 751.798196ms 751.83229ms 751.834075ms 751.885798ms 751.903758ms 752.085523ms 752.092818ms 752.155233ms 752.179133ms 752.604163ms 752.708845ms 752.994745ms 753.701709ms 754.068583ms 754.362364ms 754.517851ms 754.820303ms 754.882546ms 755.631662ms 757.383172ms 757.772199ms]
Sep 27 16:16:13.035: INFO: 50 %ile: 749.397236ms
Sep 27 16:16:13.035: INFO: 90 %ile: 751.83229ms
Sep 27 16:16:13.035: INFO: 99 %ile: 757.383172ms
Sep 27 16:16:13.035: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:16:13.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6347" for this suite.

• [SLOW TEST:9.783 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":339,"completed":68,"skipped":953,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:16:13.049: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Sep 27 16:16:13.081: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:16:38.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8182" for this suite.

• [SLOW TEST:25.450 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":339,"completed":69,"skipped":993,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:16:38.499: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Sep 27 16:16:38.539: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8825 proxy --unix-socket=/tmp/kubectl-proxy-unix947268654/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:16:38.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8825" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":339,"completed":70,"skipped":1007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:16:38.596: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-864c5069-b3ff-4f09-87f0-3542dd04a1a3
STEP: Creating a pod to test consume configMaps
Sep 27 16:16:38.642: INFO: Waiting up to 5m0s for pod "pod-configmaps-3add8f5d-c866-4870-9ecf-938226f6f971" in namespace "configmap-2778" to be "Succeeded or Failed"
Sep 27 16:16:38.645: INFO: Pod "pod-configmaps-3add8f5d-c866-4870-9ecf-938226f6f971": Phase="Pending", Reason="", readiness=false. Elapsed: 3.500759ms
Sep 27 16:16:40.650: INFO: Pod "pod-configmaps-3add8f5d-c866-4870-9ecf-938226f6f971": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008115805s
STEP: Saw pod success
Sep 27 16:16:40.650: INFO: Pod "pod-configmaps-3add8f5d-c866-4870-9ecf-938226f6f971" satisfied condition "Succeeded or Failed"
Sep 27 16:16:40.654: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-configmaps-3add8f5d-c866-4870-9ecf-938226f6f971 container agnhost-container: <nil>
STEP: delete the pod
Sep 27 16:16:40.689: INFO: Waiting for pod pod-configmaps-3add8f5d-c866-4870-9ecf-938226f6f971 to disappear
Sep 27 16:16:40.692: INFO: Pod pod-configmaps-3add8f5d-c866-4870-9ecf-938226f6f971 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:16:40.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2778" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":71,"skipped":1081,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:16:40.704: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 27 16:16:40.745: INFO: Waiting up to 5m0s for pod "pod-c38622f7-f37c-451b-b1f9-d263a13405a5" in namespace "emptydir-1516" to be "Succeeded or Failed"
Sep 27 16:16:40.748: INFO: Pod "pod-c38622f7-f37c-451b-b1f9-d263a13405a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.511239ms
Sep 27 16:16:42.754: INFO: Pod "pod-c38622f7-f37c-451b-b1f9-d263a13405a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008647538s
STEP: Saw pod success
Sep 27 16:16:42.754: INFO: Pod "pod-c38622f7-f37c-451b-b1f9-d263a13405a5" satisfied condition "Succeeded or Failed"
Sep 27 16:16:42.758: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-c38622f7-f37c-451b-b1f9-d263a13405a5 container test-container: <nil>
STEP: delete the pod
Sep 27 16:16:42.781: INFO: Waiting for pod pod-c38622f7-f37c-451b-b1f9-d263a13405a5 to disappear
Sep 27 16:16:42.784: INFO: Pod pod-c38622f7-f37c-451b-b1f9-d263a13405a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:16:42.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1516" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":72,"skipped":1090,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:16:42.796: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:16:55.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-361" for this suite.

• [SLOW TEST:13.131 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":339,"completed":73,"skipped":1101,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:16:55.927: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:16:55.983: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 27 16:16:55.995: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:55.995: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:55.995: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:55.998: INFO: Number of nodes with available pods: 0
Sep 27 16:16:55.998: INFO: Node ip-10-0-31-225.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:16:57.004: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:57.004: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:57.004: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:57.008: INFO: Number of nodes with available pods: 1
Sep 27 16:16:57.008: INFO: Node ip-10-0-31-225.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:16:58.006: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:58.007: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:58.007: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:58.011: INFO: Number of nodes with available pods: 3
Sep 27 16:16:58.011: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 27 16:16:58.049: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:16:58.049: INFO: Wrong image for pod: daemon-set-9fjcm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:16:58.049: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:16:58.053: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:58.053: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:58.053: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:59.058: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:16:59.058: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:16:59.062: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:59.062: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:16:59.062: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:00.060: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:00.060: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:00.065: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:00.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:00.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:01.058: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:01.058: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:01.065: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:01.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:01.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:02.060: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:02.060: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:02.064: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:02.064: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:02.064: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:03.059: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:03.059: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:03.064: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:03.064: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:03.064: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:04.060: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:04.060: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:04.065: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:04.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:04.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:05.059: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:05.059: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:05.063: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:05.064: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:05.064: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:06.061: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:06.061: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:06.066: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:06.066: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:06.066: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:07.059: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:07.059: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:07.063: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:07.063: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:07.063: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:08.060: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:08.060: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:08.065: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:08.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:08.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:09.058: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:09.058: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:09.063: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:09.063: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:09.063: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:10.060: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:10.060: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:10.064: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:10.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:10.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:11.059: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:11.059: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:11.064: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:11.064: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:11.064: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:12.064: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:12.064: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:12.069: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:12.069: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:12.069: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:13.059: INFO: Wrong image for pod: daemon-set-4cddq. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:13.059: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:13.059: INFO: Pod daemon-set-w9979 is not available
Sep 27 16:17:13.064: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:13.064: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:13.064: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:14.060: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:14.064: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:14.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:14.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:15.059: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:15.063: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:15.063: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:15.063: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:16.060: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:16.065: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:16.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:16.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:17.060: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:17.065: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:17.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:17.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:18.060: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:18.064: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:18.064: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:18.064: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:19.059: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:19.063: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:19.063: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:19.063: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:20.060: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:20.065: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:20.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:20.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:21.058: INFO: Pod daemon-set-5hkjk is not available
Sep 27 16:17:21.059: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:21.063: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:21.063: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:21.063: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:22.060: INFO: Pod daemon-set-5hkjk is not available
Sep 27 16:17:22.060: INFO: Wrong image for pod: daemon-set-qxk6f. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Sep 27 16:17:22.065: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:22.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:22.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:23.063: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:23.063: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:23.063: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:24.064: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:24.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:24.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:25.064: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:25.064: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:25.064: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:26.065: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:26.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:26.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:27.063: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:27.064: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:27.064: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:28.065: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:28.065: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:28.065: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:29.063: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:29.063: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:29.063: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:30.064: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:30.064: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:30.064: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:31.064: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:31.064: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:31.064: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:32.067: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:32.067: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:32.067: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:33.064: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:33.064: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:33.064: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 27 16:17:33.068: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:33.068: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:33.068: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:17:33.072: INFO: Number of nodes with available pods: 3
Sep 27 16:17:33.072: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3074, will wait for the garbage collector to delete the pods
Sep 27 16:17:33.153: INFO: Deleting DaemonSet.extensions daemon-set took: 7.715346ms
Sep 27 16:17:33.254: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.969367ms
Sep 27 16:17:42.160: INFO: Number of nodes with available pods: 0
Sep 27 16:17:42.160: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 16:17:42.163: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1699595"},"items":null}

Sep 27 16:17:42.166: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1699595"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:17:42.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3074" for this suite.

• [SLOW TEST:46.267 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":339,"completed":74,"skipped":1117,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:17:42.194: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Sep 27 16:17:42.228: INFO: namespace kubectl-7754
Sep 27 16:17:42.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7754 create -f -'
Sep 27 16:17:43.004: INFO: stderr: ""
Sep 27 16:17:43.004: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 27 16:17:44.009: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 16:17:44.009: INFO: Found 0 / 1
Sep 27 16:17:45.010: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 16:17:45.010: INFO: Found 1 / 1
Sep 27 16:17:45.010: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 27 16:17:45.013: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 16:17:45.013: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 27 16:17:45.013: INFO: wait on agnhost-primary startup in kubectl-7754 
Sep 27 16:17:45.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7754 logs agnhost-primary-88dxv agnhost-primary'
Sep 27 16:17:45.085: INFO: stderr: ""
Sep 27 16:17:45.085: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep 27 16:17:45.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7754 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep 27 16:17:45.171: INFO: stderr: ""
Sep 27 16:17:45.171: INFO: stdout: "service/rm2 exposed\n"
Sep 27 16:17:45.177: INFO: Service rm2 in namespace kubectl-7754 found.
STEP: exposing service
Sep 27 16:17:47.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-7754 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep 27 16:17:47.272: INFO: stderr: ""
Sep 27 16:17:47.272: INFO: stdout: "service/rm3 exposed\n"
Sep 27 16:17:47.278: INFO: Service rm3 in namespace kubectl-7754 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:17:49.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7754" for this suite.

• [SLOW TEST:7.106 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1223
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":339,"completed":75,"skipped":1142,"failed":0}
SSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:17:49.300: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-3b7d0c87-d085-49e3-8815-0cea14d54c4b
STEP: Creating a pod to test consume secrets
Sep 27 16:17:49.346: INFO: Waiting up to 5m0s for pod "pod-secrets-bffe995c-30d8-4413-8d8d-52e4b1b755e4" in namespace "secrets-6034" to be "Succeeded or Failed"
Sep 27 16:17:49.349: INFO: Pod "pod-secrets-bffe995c-30d8-4413-8d8d-52e4b1b755e4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.462949ms
Sep 27 16:17:51.354: INFO: Pod "pod-secrets-bffe995c-30d8-4413-8d8d-52e4b1b755e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008795847s
STEP: Saw pod success
Sep 27 16:17:51.354: INFO: Pod "pod-secrets-bffe995c-30d8-4413-8d8d-52e4b1b755e4" satisfied condition "Succeeded or Failed"
Sep 27 16:17:51.358: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-secrets-bffe995c-30d8-4413-8d8d-52e4b1b755e4 container secret-env-test: <nil>
STEP: delete the pod
Sep 27 16:17:51.383: INFO: Waiting for pod pod-secrets-bffe995c-30d8-4413-8d8d-52e4b1b755e4 to disappear
Sep 27 16:17:51.386: INFO: Pod pod-secrets-bffe995c-30d8-4413-8d8d-52e4b1b755e4 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:17:51.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6034" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":339,"completed":76,"skipped":1145,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:17:51.398: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:17:51.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6434" for this suite.
STEP: Destroying namespace "nspatchtest-b3a2f9ef-0ee5-4a47-bf75-0440485b6c48-8188" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":339,"completed":77,"skipped":1175,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:17:51.490: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 16:17:51.951: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 16:17:54.981: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:18:07.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1303" for this suite.
STEP: Destroying namespace "webhook-1303-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.685 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":339,"completed":78,"skipped":1175,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:18:07.175: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:18:07.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6323" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":339,"completed":79,"skipped":1205,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:18:07.265: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 16:18:07.311: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b6e69c93-2412-4f03-83f4-7830bdc542d6" in namespace "downward-api-6293" to be "Succeeded or Failed"
Sep 27 16:18:07.314: INFO: Pod "downwardapi-volume-b6e69c93-2412-4f03-83f4-7830bdc542d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.694017ms
Sep 27 16:18:09.319: INFO: Pod "downwardapi-volume-b6e69c93-2412-4f03-83f4-7830bdc542d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008812761s
STEP: Saw pod success
Sep 27 16:18:09.319: INFO: Pod "downwardapi-volume-b6e69c93-2412-4f03-83f4-7830bdc542d6" satisfied condition "Succeeded or Failed"
Sep 27 16:18:09.323: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downwardapi-volume-b6e69c93-2412-4f03-83f4-7830bdc542d6 container client-container: <nil>
STEP: delete the pod
Sep 27 16:18:09.347: INFO: Waiting for pod downwardapi-volume-b6e69c93-2412-4f03-83f4-7830bdc542d6 to disappear
Sep 27 16:18:09.350: INFO: Pod downwardapi-volume-b6e69c93-2412-4f03-83f4-7830bdc542d6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:18:09.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6293" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":80,"skipped":1219,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:18:09.363: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-b757
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 16:18:09.421: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-b757" in namespace "subpath-2684" to be "Succeeded or Failed"
Sep 27 16:18:09.425: INFO: Pod "pod-subpath-test-downwardapi-b757": Phase="Pending", Reason="", readiness=false. Elapsed: 3.922982ms
Sep 27 16:18:11.430: INFO: Pod "pod-subpath-test-downwardapi-b757": Phase="Running", Reason="", readiness=true. Elapsed: 2.009450693s
Sep 27 16:18:13.436: INFO: Pod "pod-subpath-test-downwardapi-b757": Phase="Running", Reason="", readiness=true. Elapsed: 4.01461452s
Sep 27 16:18:15.442: INFO: Pod "pod-subpath-test-downwardapi-b757": Phase="Running", Reason="", readiness=true. Elapsed: 6.020694805s
Sep 27 16:18:17.447: INFO: Pod "pod-subpath-test-downwardapi-b757": Phase="Running", Reason="", readiness=true. Elapsed: 8.026458775s
Sep 27 16:18:19.452: INFO: Pod "pod-subpath-test-downwardapi-b757": Phase="Running", Reason="", readiness=true. Elapsed: 10.031422763s
Sep 27 16:18:21.459: INFO: Pod "pod-subpath-test-downwardapi-b757": Phase="Running", Reason="", readiness=true. Elapsed: 12.037597826s
Sep 27 16:18:23.464: INFO: Pod "pod-subpath-test-downwardapi-b757": Phase="Running", Reason="", readiness=true. Elapsed: 14.042769955s
Sep 27 16:18:25.470: INFO: Pod "pod-subpath-test-downwardapi-b757": Phase="Running", Reason="", readiness=true. Elapsed: 16.048759539s
Sep 27 16:18:27.475: INFO: Pod "pod-subpath-test-downwardapi-b757": Phase="Running", Reason="", readiness=true. Elapsed: 18.054194671s
Sep 27 16:18:29.480: INFO: Pod "pod-subpath-test-downwardapi-b757": Phase="Running", Reason="", readiness=true. Elapsed: 20.059157171s
Sep 27 16:18:31.486: INFO: Pod "pod-subpath-test-downwardapi-b757": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.064875093s
STEP: Saw pod success
Sep 27 16:18:31.486: INFO: Pod "pod-subpath-test-downwardapi-b757" satisfied condition "Succeeded or Failed"
Sep 27 16:18:31.490: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-subpath-test-downwardapi-b757 container test-container-subpath-downwardapi-b757: <nil>
STEP: delete the pod
Sep 27 16:18:31.514: INFO: Waiting for pod pod-subpath-test-downwardapi-b757 to disappear
Sep 27 16:18:31.517: INFO: Pod pod-subpath-test-downwardapi-b757 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-b757
Sep 27 16:18:31.517: INFO: Deleting pod "pod-subpath-test-downwardapi-b757" in namespace "subpath-2684"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:18:31.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2684" for this suite.

• [SLOW TEST:22.169 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":339,"completed":81,"skipped":1240,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:18:31.532: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0927 16:18:32.632068      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 27 16:23:32.637: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:23:32.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3104" for this suite.

• [SLOW TEST:301.120 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":339,"completed":82,"skipped":1250,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:23:32.652: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1386
STEP: creating an pod
Sep 27 16:23:32.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3602 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep 27 16:23:32.756: INFO: stderr: ""
Sep 27 16:23:32.756: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Sep 27 16:23:32.756: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep 27 16:23:32.756: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-3602" to be "running and ready, or succeeded"
Sep 27 16:23:32.770: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 13.575292ms
Sep 27 16:23:34.774: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.017715309s
Sep 27 16:23:34.774: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep 27 16:23:34.774: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep 27 16:23:34.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3602 logs logs-generator logs-generator'
Sep 27 16:23:34.858: INFO: stderr: ""
Sep 27 16:23:34.858: INFO: stdout: "I0927 16:23:33.505857       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/2rf7 204\nI0927 16:23:33.705946       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/bgrm 501\nI0927 16:23:33.905949       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/nq2 293\nI0927 16:23:34.106229       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/fdpn 400\nI0927 16:23:34.306526       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/blw 387\nI0927 16:23:34.506825       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/x2qk 227\nI0927 16:23:34.706065       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/8m7 574\n"
STEP: limiting log lines
Sep 27 16:23:34.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3602 logs logs-generator logs-generator --tail=1'
Sep 27 16:23:34.928: INFO: stderr: ""
Sep 27 16:23:34.929: INFO: stdout: "I0927 16:23:34.906362       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/972 226\n"
Sep 27 16:23:34.929: INFO: got output "I0927 16:23:34.906362       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/972 226\n"
STEP: limiting log bytes
Sep 27 16:23:34.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3602 logs logs-generator logs-generator --limit-bytes=1'
Sep 27 16:23:35.000: INFO: stderr: ""
Sep 27 16:23:35.000: INFO: stdout: "I"
Sep 27 16:23:35.000: INFO: got output "I"
STEP: exposing timestamps
Sep 27 16:23:35.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3602 logs logs-generator logs-generator --tail=1 --timestamps'
Sep 27 16:23:35.066: INFO: stderr: ""
Sep 27 16:23:35.066: INFO: stdout: "2021-09-27T16:23:34.906431902Z I0927 16:23:34.906362       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/972 226\n"
Sep 27 16:23:35.066: INFO: got output "2021-09-27T16:23:34.906431902Z I0927 16:23:34.906362       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/972 226\n"
STEP: restricting to a time range
Sep 27 16:23:37.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3602 logs logs-generator logs-generator --since=1s'
Sep 27 16:23:37.637: INFO: stderr: ""
Sep 27 16:23:37.637: INFO: stdout: "I0927 16:23:36.706897       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/zzl 267\nI0927 16:23:36.906187       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/bzv 529\nI0927 16:23:37.106485       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/rhfl 288\nI0927 16:23:37.306777       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/246v 471\nI0927 16:23:37.506012       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/rg7c 464\n"
Sep 27 16:23:37.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3602 logs logs-generator logs-generator --since=24h'
Sep 27 16:23:37.711: INFO: stderr: ""
Sep 27 16:23:37.711: INFO: stdout: "I0927 16:23:33.505857       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/2rf7 204\nI0927 16:23:33.705946       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/bgrm 501\nI0927 16:23:33.905949       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/nq2 293\nI0927 16:23:34.106229       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/fdpn 400\nI0927 16:23:34.306526       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/blw 387\nI0927 16:23:34.506825       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/x2qk 227\nI0927 16:23:34.706065       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/8m7 574\nI0927 16:23:34.906362       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/972 226\nI0927 16:23:35.106673       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/hxp7 419\nI0927 16:23:35.305906       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/zl74 516\nI0927 16:23:35.506204       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/q6b 518\nI0927 16:23:35.706494       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/65j 404\nI0927 16:23:35.906781       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/x74 439\nI0927 16:23:36.106011       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/r6v 214\nI0927 16:23:36.306317       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/hrt 209\nI0927 16:23:36.506605       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/grb 221\nI0927 16:23:36.706897       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/zzl 267\nI0927 16:23:36.906187       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/bzv 529\nI0927 16:23:37.106485       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/rhfl 288\nI0927 16:23:37.306777       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/246v 471\nI0927 16:23:37.506012       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/rg7c 464\nI0927 16:23:37.706299       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/g6z 254\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1391
Sep 27 16:23:37.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3602 delete pod logs-generator'
Sep 27 16:23:42.084: INFO: stderr: ""
Sep 27 16:23:42.084: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:23:42.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3602" for this suite.

• [SLOW TEST:9.444 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1383
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":339,"completed":83,"skipped":1278,"failed":0}
SSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:23:42.097: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0927 16:23:42.135814      23 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:25:00.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6473" for this suite.

• [SLOW TEST:78.100 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":339,"completed":84,"skipped":1281,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:25:00.196: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-9173/configmap-test-495a6f95-743e-4671-8fd1-4f08392690f7
STEP: Creating a pod to test consume configMaps
Sep 27 16:25:00.244: INFO: Waiting up to 5m0s for pod "pod-configmaps-04822f5a-b139-4b89-b8ca-9c0b2a1b8d89" in namespace "configmap-9173" to be "Succeeded or Failed"
Sep 27 16:25:00.249: INFO: Pod "pod-configmaps-04822f5a-b139-4b89-b8ca-9c0b2a1b8d89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.492367ms
Sep 27 16:25:02.255: INFO: Pod "pod-configmaps-04822f5a-b139-4b89-b8ca-9c0b2a1b8d89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011317438s
STEP: Saw pod success
Sep 27 16:25:02.255: INFO: Pod "pod-configmaps-04822f5a-b139-4b89-b8ca-9c0b2a1b8d89" satisfied condition "Succeeded or Failed"
Sep 27 16:25:02.259: INFO: Trying to get logs from node ip-10-0-62-6.us-east-2.compute.internal pod pod-configmaps-04822f5a-b139-4b89-b8ca-9c0b2a1b8d89 container env-test: <nil>
STEP: delete the pod
Sep 27 16:25:02.302: INFO: Waiting for pod pod-configmaps-04822f5a-b139-4b89-b8ca-9c0b2a1b8d89 to disappear
Sep 27 16:25:02.306: INFO: Pod pod-configmaps-04822f5a-b139-4b89-b8ca-9c0b2a1b8d89 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:25:02.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9173" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":85,"skipped":1295,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:25:02.318: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8224.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8224.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 16:25:14.447: INFO: DNS probes using dns-8224/dns-test-ad710fa7-300e-4722-9820-d41a9660e3ce succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:25:14.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8224" for this suite.

• [SLOW TEST:12.160 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":339,"completed":86,"skipped":1296,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:25:14.478: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:25:14.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5707" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":339,"completed":87,"skipped":1309,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:25:14.541: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 27 16:25:14.583: INFO: Waiting up to 5m0s for pod "pod-b2d297dc-378d-4d59-b8d3-d3dece8f28c2" in namespace "emptydir-6765" to be "Succeeded or Failed"
Sep 27 16:25:14.588: INFO: Pod "pod-b2d297dc-378d-4d59-b8d3-d3dece8f28c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.66617ms
Sep 27 16:25:16.593: INFO: Pod "pod-b2d297dc-378d-4d59-b8d3-d3dece8f28c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010230697s
STEP: Saw pod success
Sep 27 16:25:16.593: INFO: Pod "pod-b2d297dc-378d-4d59-b8d3-d3dece8f28c2" satisfied condition "Succeeded or Failed"
Sep 27 16:25:16.597: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-b2d297dc-378d-4d59-b8d3-d3dece8f28c2 container test-container: <nil>
STEP: delete the pod
Sep 27 16:25:16.628: INFO: Waiting for pod pod-b2d297dc-378d-4d59-b8d3-d3dece8f28c2 to disappear
Sep 27 16:25:16.632: INFO: Pod pod-b2d297dc-378d-4d59-b8d3-d3dece8f28c2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:25:16.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6765" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":88,"skipped":1321,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:25:16.644: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:25:16.690: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4cf98370-3b07-4a8b-86c3-09da34e4c972" in namespace "security-context-test-8408" to be "Succeeded or Failed"
Sep 27 16:25:16.695: INFO: Pod "alpine-nnp-false-4cf98370-3b07-4a8b-86c3-09da34e4c972": Phase="Pending", Reason="", readiness=false. Elapsed: 5.394709ms
Sep 27 16:25:18.702: INFO: Pod "alpine-nnp-false-4cf98370-3b07-4a8b-86c3-09da34e4c972": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012208088s
Sep 27 16:25:18.702: INFO: Pod "alpine-nnp-false-4cf98370-3b07-4a8b-86c3-09da34e4c972" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:25:18.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8408" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":89,"skipped":1330,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:25:18.740: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Sep 27 16:25:18.830: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:25:20.836: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 27 16:25:21.860: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:25:22.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8099" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":339,"completed":90,"skipped":1330,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:25:22.894: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-1388a670-0c2a-4ded-bb5a-581a662ed89a in namespace container-probe-7595
Sep 27 16:25:24.950: INFO: Started pod test-webserver-1388a670-0c2a-4ded-bb5a-581a662ed89a in namespace container-probe-7595
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 16:25:24.953: INFO: Initial restart count of pod test-webserver-1388a670-0c2a-4ded-bb5a-581a662ed89a is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:29:25.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7595" for this suite.

• [SLOW TEST:242.884 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":91,"skipped":1338,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:29:25.779: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 16:29:25.823: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68126631-e6f3-408c-81b1-543782b8eaf9" in namespace "projected-5356" to be "Succeeded or Failed"
Sep 27 16:29:25.826: INFO: Pod "downwardapi-volume-68126631-e6f3-408c-81b1-543782b8eaf9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.559223ms
Sep 27 16:29:27.833: INFO: Pod "downwardapi-volume-68126631-e6f3-408c-81b1-543782b8eaf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010348058s
STEP: Saw pod success
Sep 27 16:29:27.833: INFO: Pod "downwardapi-volume-68126631-e6f3-408c-81b1-543782b8eaf9" satisfied condition "Succeeded or Failed"
Sep 27 16:29:27.837: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downwardapi-volume-68126631-e6f3-408c-81b1-543782b8eaf9 container client-container: <nil>
STEP: delete the pod
Sep 27 16:29:27.870: INFO: Waiting for pod downwardapi-volume-68126631-e6f3-408c-81b1-543782b8eaf9 to disappear
Sep 27 16:29:27.874: INFO: Pod downwardapi-volume-68126631-e6f3-408c-81b1-543782b8eaf9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:29:27.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5356" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":92,"skipped":1342,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:29:27.887: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Sep 27 16:29:27.931: INFO: The status of Pod pod-update-activedeadlineseconds-be1b053e-75b6-4508-a426-cdd0a0a3c7c2 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:29:29.936: INFO: The status of Pod pod-update-activedeadlineseconds-be1b053e-75b6-4508-a426-cdd0a0a3c7c2 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 27 16:29:30.459: INFO: Successfully updated pod "pod-update-activedeadlineseconds-be1b053e-75b6-4508-a426-cdd0a0a3c7c2"
Sep 27 16:29:30.459: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-be1b053e-75b6-4508-a426-cdd0a0a3c7c2" in namespace "pods-4518" to be "terminated due to deadline exceeded"
Sep 27 16:29:30.463: INFO: Pod "pod-update-activedeadlineseconds-be1b053e-75b6-4508-a426-cdd0a0a3c7c2": Phase="Running", Reason="", readiness=true. Elapsed: 3.768486ms
Sep 27 16:29:32.472: INFO: Pod "pod-update-activedeadlineseconds-be1b053e-75b6-4508-a426-cdd0a0a3c7c2": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.012669153s
Sep 27 16:29:32.472: INFO: Pod "pod-update-activedeadlineseconds-be1b053e-75b6-4508-a426-cdd0a0a3c7c2" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:29:32.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4518" for this suite.
•{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":339,"completed":93,"skipped":1345,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:29:32.486: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Sep 27 16:31:33.061: INFO: Successfully updated pod "var-expansion-33b0f05f-997f-4242-80b2-ee6d846bbcac"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Sep 27 16:31:35.073: INFO: Deleting pod "var-expansion-33b0f05f-997f-4242-80b2-ee6d846bbcac" in namespace "var-expansion-3647"
Sep 27 16:31:35.081: INFO: Wait up to 5m0s for pod "var-expansion-33b0f05f-997f-4242-80b2-ee6d846bbcac" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:32:13.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3647" for this suite.

• [SLOW TEST:160.617 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":339,"completed":94,"skipped":1354,"failed":0}
S
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:32:13.103: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Sep 27 16:32:13.153: INFO: Waiting up to 5m0s for pod "security-context-52b8ecea-b9e8-4e4d-aedf-6a160e13f7b5" in namespace "security-context-2497" to be "Succeeded or Failed"
Sep 27 16:32:13.156: INFO: Pod "security-context-52b8ecea-b9e8-4e4d-aedf-6a160e13f7b5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.518837ms
Sep 27 16:32:15.162: INFO: Pod "security-context-52b8ecea-b9e8-4e4d-aedf-6a160e13f7b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009117571s
STEP: Saw pod success
Sep 27 16:32:15.162: INFO: Pod "security-context-52b8ecea-b9e8-4e4d-aedf-6a160e13f7b5" satisfied condition "Succeeded or Failed"
Sep 27 16:32:15.166: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod security-context-52b8ecea-b9e8-4e4d-aedf-6a160e13f7b5 container test-container: <nil>
STEP: delete the pod
Sep 27 16:32:15.198: INFO: Waiting for pod security-context-52b8ecea-b9e8-4e4d-aedf-6a160e13f7b5 to disappear
Sep 27 16:32:15.202: INFO: Pod security-context-52b8ecea-b9e8-4e4d-aedf-6a160e13f7b5 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:32:15.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2497" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":95,"skipped":1355,"failed":0}
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:32:15.214: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 27 16:32:17.279: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:32:17.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-202" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":339,"completed":96,"skipped":1360,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:32:17.309: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-5057
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-5057
Sep 27 16:32:17.360: INFO: Found 0 stateful pods, waiting for 1
Sep 27 16:32:27.367: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Sep 27 16:32:27.400: INFO: Deleting all statefulset in ns statefulset-5057
Sep 27 16:32:27.404: INFO: Scaling statefulset ss to 0
Sep 27 16:32:47.430: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 16:32:47.434: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:32:47.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5057" for this suite.

• [SLOW TEST:30.162 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":339,"completed":97,"skipped":1365,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:32:47.472: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:32:47.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8344 create -f -'
Sep 27 16:32:48.180: INFO: stderr: ""
Sep 27 16:32:48.180: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep 27 16:32:48.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8344 create -f -'
Sep 27 16:32:48.528: INFO: stderr: ""
Sep 27 16:32:48.528: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 27 16:32:49.534: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 16:32:49.534: INFO: Found 1 / 1
Sep 27 16:32:49.534: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 27 16:32:49.538: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 16:32:49.538: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 27 16:32:49.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8344 describe pod agnhost-primary-vv7dx'
Sep 27 16:32:49.608: INFO: stderr: ""
Sep 27 16:32:49.608: INFO: stdout: "Name:         agnhost-primary-vv7dx\nNamespace:    kubectl-8344\nPriority:     0\nNode:         ip-10-0-95-24.us-east-2.compute.internal/10.0.95.24\nStart Time:   Mon, 27 Sep 2021 16:32:48 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           100.96.4.74\nIPs:\n  IP:           100.96.4.74\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://1f2c76a66ceea901630154810a5ae463c1868763fb74a253d421a3830fed194f\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 27 Sep 2021 16:32:48 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pb8ql (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-pb8ql:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-8344/agnhost-primary-vv7dx to ip-10-0-95-24.us-east-2.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Sep 27 16:32:49.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8344 describe rc agnhost-primary'
Sep 27 16:32:49.680: INFO: stderr: ""
Sep 27 16:32:49.680: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8344\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-vv7dx\n"
Sep 27 16:32:49.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8344 describe service agnhost-primary'
Sep 27 16:32:49.751: INFO: stderr: ""
Sep 27 16:32:49.751: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8344\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.68.174.148\nIPs:               100.68.174.148\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.96.4.74:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 27 16:32:49.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8344 describe node ip-10-0-23-154.us-east-2.compute.internal'
Sep 27 16:32:49.844: INFO: stderr: ""
Sep 27 16:32:49.844: INFO: stdout: "Name:               ip-10-0-23-154.us-east-2.compute.internal\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.2xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-2\n                    failure-domain.beta.kubernetes.io/zone=us-east-2a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-23-154.us-east-2.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=m5.2xlarge\n                    topology.kubernetes.io/region=us-east-2\n                    topology.kubernetes.io/zone=us-east-2a\nAnnotations:        cluster.x-k8s.io/cluster-name: tceconform2\n                    cluster.x-k8s.io/cluster-namespace: tkg-system\n                    cluster.x-k8s.io/machine: tceconform2-control-plane-hknbm\n                    cluster.x-k8s.io/owner-kind: KubeadmControlPlane\n                    cluster.x-k8s.io/owner-name: tceconform2-control-plane\n                    kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 23 Sep 2021 17:32:02 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-23-154.us-east-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 27 Sep 2021 16:32:42 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 27 Sep 2021 16:30:14 +0000   Thu, 23 Sep 2021 17:32:02 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 27 Sep 2021 16:30:14 +0000   Thu, 23 Sep 2021 17:32:02 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 27 Sep 2021 16:30:14 +0000   Thu, 23 Sep 2021 17:32:02 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 27 Sep 2021 16:30:14 +0000   Thu, 23 Sep 2021 17:32:44 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.0.23.154\n  Hostname:     ip-10-0-23-154.us-east-2.compute.internal\n  InternalDNS:  ip-10-0-23-154.us-east-2.compute.internal\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         8\n  ephemeral-storage:           81253764Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      32411960Ki\n  pods:                        110\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         8\n  ephemeral-storage:           74883468779\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      32309560Ki\n  pods:                        110\nSystem Info:\n  Machine ID:                        ec2fb1baf0ee0c04fcf8905e7aa70adc\n  System UUID:                       ec2fb1ba-f0ee-0c04-fcf8-905e7aa70adc\n  Boot ID:                           4cc7631b-364d-4bf3-8669-fbab7e354a7f\n  Kernel Version:                    5.8.0-1038-aws\n  OS Image:                          Ubuntu 20.04.2 LTS\n  Operating System:                  linux\n  Architecture:                      amd64\n  Container Runtime Version:         containerd://1.4.6\n  Kubelet Version:                   v1.21.2+vmware.1\n  Kube-Proxy Version:                v1.21.2+vmware.1\nPodCIDR:                             100.96.2.0/24\nPodCIDRs:                            100.96.2.0/24\nProviderID:                          aws:///us-east-2a/i-059a61115a4cc0458\nNon-terminated Pods:                 (11 in total)\n  Namespace                          Name                                                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                          ----                                                                 ------------  ----------  ---------------  -------------  ---\n  capi-kubeadm-control-plane-system  capi-kubeadm-control-plane-controller-manager-857d687b9d-kv2kz       0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d22h\n  capi-system                        capi-controller-manager-778bd4dfb9-shg7c                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d22h\n  cert-manager                       cert-manager-cainjector-6bd4cff7bb-pz4v9                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d22h\n  kube-system                        antrea-agent-kcj8s                                                   400m (5%)     0 (0%)      0 (0%)           0 (0%)         3d22h\n  kube-system                        antrea-controller-587687c6c4-4zvm9                                   200m (2%)     0 (0%)      0 (0%)           0 (0%)         3d21h\n  kube-system                        etcd-ip-10-0-23-154.us-east-2.compute.internal                       100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         3d23h\n  kube-system                        kube-apiserver-ip-10-0-23-154.us-east-2.compute.internal             250m (3%)     0 (0%)      0 (0%)           0 (0%)         3d23h\n  kube-system                        kube-controller-manager-ip-10-0-23-154.us-east-2.compute.internal    200m (2%)     0 (0%)      0 (0%)           0 (0%)         3d23h\n  kube-system                        kube-proxy-z8wks                                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d23h\n  kube-system                        kube-scheduler-ip-10-0-23-154.us-east-2.compute.internal             100m (1%)     0 (0%)      0 (0%)           0 (0%)         3d23h\n  sonobuoy                           sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-lz6d8              0 (0%)        0 (0%)      0 (0%)           0 (0%)         49m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests     Limits\n  --------                    --------     ------\n  cpu                         1250m (15%)  0 (0%)\n  memory                      100Mi (0%)   0 (0%)\n  ephemeral-storage           0 (0%)       0 (0%)\n  hugepages-1Gi               0 (0%)       0 (0%)\n  hugepages-2Mi               0 (0%)       0 (0%)\n  attachable-volumes-aws-ebs  0            0\nEvents:                       <none>\n"
Sep 27 16:32:49.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8344 describe namespace kubectl-8344'
Sep 27 16:32:49.908: INFO: stderr: ""
Sep 27 16:32:49.908: INFO: stdout: "Name:         kubectl-8344\nLabels:       e2e-framework=kubectl\n              e2e-run=fb189d23-a8b0-43e8-8d06-9673ddd11c59\n              kubernetes.io/metadata.name=kubectl-8344\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:32:49.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8344" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":339,"completed":98,"skipped":1385,"failed":0}
SSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:32:49.921: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:32:49.974: INFO: The status of Pod test-webserver-a0c7ca9e-411c-42a3-b7ad-6e2710e4ef62 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:32:51.979: INFO: The status of Pod test-webserver-a0c7ca9e-411c-42a3-b7ad-6e2710e4ef62 is Running (Ready = false)
Sep 27 16:32:53.980: INFO: The status of Pod test-webserver-a0c7ca9e-411c-42a3-b7ad-6e2710e4ef62 is Running (Ready = false)
Sep 27 16:32:55.981: INFO: The status of Pod test-webserver-a0c7ca9e-411c-42a3-b7ad-6e2710e4ef62 is Running (Ready = false)
Sep 27 16:32:57.980: INFO: The status of Pod test-webserver-a0c7ca9e-411c-42a3-b7ad-6e2710e4ef62 is Running (Ready = false)
Sep 27 16:32:59.980: INFO: The status of Pod test-webserver-a0c7ca9e-411c-42a3-b7ad-6e2710e4ef62 is Running (Ready = false)
Sep 27 16:33:01.981: INFO: The status of Pod test-webserver-a0c7ca9e-411c-42a3-b7ad-6e2710e4ef62 is Running (Ready = false)
Sep 27 16:33:03.981: INFO: The status of Pod test-webserver-a0c7ca9e-411c-42a3-b7ad-6e2710e4ef62 is Running (Ready = false)
Sep 27 16:33:05.980: INFO: The status of Pod test-webserver-a0c7ca9e-411c-42a3-b7ad-6e2710e4ef62 is Running (Ready = false)
Sep 27 16:33:07.979: INFO: The status of Pod test-webserver-a0c7ca9e-411c-42a3-b7ad-6e2710e4ef62 is Running (Ready = false)
Sep 27 16:33:09.980: INFO: The status of Pod test-webserver-a0c7ca9e-411c-42a3-b7ad-6e2710e4ef62 is Running (Ready = false)
Sep 27 16:33:11.982: INFO: The status of Pod test-webserver-a0c7ca9e-411c-42a3-b7ad-6e2710e4ef62 is Running (Ready = true)
Sep 27 16:33:11.986: INFO: Container started at 2021-09-27 16:32:50 +0000 UTC, pod became ready at 2021-09-27 16:33:09 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:33:11.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4295" for this suite.

• [SLOW TEST:22.083 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":339,"completed":99,"skipped":1390,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:33:12.004: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 16:33:12.486: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 16:33:15.518: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:33:15.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9125" for this suite.
STEP: Destroying namespace "webhook-9125-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":339,"completed":100,"skipped":1409,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:33:15.641: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-ec395f7e-ed15-4c82-96e7-10e961f78429
STEP: Creating a pod to test consume configMaps
Sep 27 16:33:15.694: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-86b578d7-af7f-4b0d-9cfd-8724ed0cace9" in namespace "projected-3747" to be "Succeeded or Failed"
Sep 27 16:33:15.699: INFO: Pod "pod-projected-configmaps-86b578d7-af7f-4b0d-9cfd-8724ed0cace9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.85209ms
Sep 27 16:33:17.703: INFO: Pod "pod-projected-configmaps-86b578d7-af7f-4b0d-9cfd-8724ed0cace9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009542408s
STEP: Saw pod success
Sep 27 16:33:17.703: INFO: Pod "pod-projected-configmaps-86b578d7-af7f-4b0d-9cfd-8724ed0cace9" satisfied condition "Succeeded or Failed"
Sep 27 16:33:17.707: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-projected-configmaps-86b578d7-af7f-4b0d-9cfd-8724ed0cace9 container agnhost-container: <nil>
STEP: delete the pod
Sep 27 16:33:17.731: INFO: Waiting for pod pod-projected-configmaps-86b578d7-af7f-4b0d-9cfd-8724ed0cace9 to disappear
Sep 27 16:33:17.734: INFO: Pod pod-projected-configmaps-86b578d7-af7f-4b0d-9cfd-8724ed0cace9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:33:17.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3747" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":101,"skipped":1420,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:33:17.746: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-2dd6bb1f-6091-4f1c-9950-0e86e3ec0f4d
STEP: Creating secret with name s-test-opt-upd-48251f61-a35c-4cd7-8423-94881fe9cf14
STEP: Creating the pod
Sep 27 16:33:17.807: INFO: The status of Pod pod-secrets-e99f97ad-5220-4311-a44e-ffeaa7e3d5b7 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:33:19.812: INFO: The status of Pod pod-secrets-e99f97ad-5220-4311-a44e-ffeaa7e3d5b7 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-2dd6bb1f-6091-4f1c-9950-0e86e3ec0f4d
STEP: Updating secret s-test-opt-upd-48251f61-a35c-4cd7-8423-94881fe9cf14
STEP: Creating secret with name s-test-opt-create-f01f25e5-b2b3-4578-9715-a80257ef1bf5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:33:23.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4199" for this suite.

• [SLOW TEST:6.165 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":102,"skipped":1430,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:33:23.911: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-9d4379c6-0709-444f-a48f-ce125181722a
STEP: Creating a pod to test consume secrets
Sep 27 16:33:23.963: INFO: Waiting up to 5m0s for pod "pod-secrets-23acd7d9-c3c5-43e1-91ec-ccd3d66eae3a" in namespace "secrets-188" to be "Succeeded or Failed"
Sep 27 16:33:23.966: INFO: Pod "pod-secrets-23acd7d9-c3c5-43e1-91ec-ccd3d66eae3a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.472377ms
Sep 27 16:33:25.972: INFO: Pod "pod-secrets-23acd7d9-c3c5-43e1-91ec-ccd3d66eae3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009881778s
STEP: Saw pod success
Sep 27 16:33:25.972: INFO: Pod "pod-secrets-23acd7d9-c3c5-43e1-91ec-ccd3d66eae3a" satisfied condition "Succeeded or Failed"
Sep 27 16:33:25.976: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-secrets-23acd7d9-c3c5-43e1-91ec-ccd3d66eae3a container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 16:33:26.009: INFO: Waiting for pod pod-secrets-23acd7d9-c3c5-43e1-91ec-ccd3d66eae3a to disappear
Sep 27 16:33:26.013: INFO: Pod pod-secrets-23acd7d9-c3c5-43e1-91ec-ccd3d66eae3a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:33:26.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-188" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":103,"skipped":1450,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:33:26.024: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-74b47a9b-a54b-488a-921f-5257fea42bb2
STEP: Creating a pod to test consume configMaps
Sep 27 16:33:26.070: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a4a66072-a6a2-4dec-91e6-ad1c722989d2" in namespace "projected-3850" to be "Succeeded or Failed"
Sep 27 16:33:26.075: INFO: Pod "pod-projected-configmaps-a4a66072-a6a2-4dec-91e6-ad1c722989d2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.589061ms
Sep 27 16:33:28.081: INFO: Pod "pod-projected-configmaps-a4a66072-a6a2-4dec-91e6-ad1c722989d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011386637s
STEP: Saw pod success
Sep 27 16:33:28.081: INFO: Pod "pod-projected-configmaps-a4a66072-a6a2-4dec-91e6-ad1c722989d2" satisfied condition "Succeeded or Failed"
Sep 27 16:33:28.085: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-projected-configmaps-a4a66072-a6a2-4dec-91e6-ad1c722989d2 container agnhost-container: <nil>
STEP: delete the pod
Sep 27 16:33:28.108: INFO: Waiting for pod pod-projected-configmaps-a4a66072-a6a2-4dec-91e6-ad1c722989d2 to disappear
Sep 27 16:33:28.112: INFO: Pod pod-projected-configmaps-a4a66072-a6a2-4dec-91e6-ad1c722989d2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:33:28.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3850" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":104,"skipped":1465,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:33:28.123: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:33:28.168: INFO: The status of Pod busybox-readonly-fs35d35996-caaa-4680-b1a7-ca1ba78f128c is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:33:30.174: INFO: The status of Pod busybox-readonly-fs35d35996-caaa-4680-b1a7-ca1ba78f128c is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:33:30.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2051" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":105,"skipped":1468,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:33:30.196: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Sep 27 16:33:30.252: INFO: Found Service test-service-xgrtk in namespace services-5392 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Sep 27 16:33:30.252: INFO: Service test-service-xgrtk created
STEP: Getting /status
Sep 27 16:33:30.256: INFO: Service test-service-xgrtk has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Sep 27 16:33:30.263: INFO: observed Service test-service-xgrtk in namespace services-5392 with annotations: map[] & LoadBalancer: {[]}
Sep 27 16:33:30.263: INFO: Found Service test-service-xgrtk in namespace services-5392 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Sep 27 16:33:30.263: INFO: Service test-service-xgrtk has service status patched
STEP: updating the ServiceStatus
Sep 27 16:33:30.274: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Sep 27 16:33:30.275: INFO: Observed Service test-service-xgrtk in namespace services-5392 with annotations: map[] & Conditions: {[]}
Sep 27 16:33:30.275: INFO: Observed event: &Service{ObjectMeta:{test-service-xgrtk  services-5392  0827fd3f-3441-42cd-a45a-eff2c1d632bc 1705632 0 2021-09-27 16:33:30 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2021-09-27 16:33:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}},"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}}}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.64.145.122,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,TopologyKeys:[],IPFamilyPolicy:*SingleStack,ClusterIPs:[100.64.145.122],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:nil,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Sep 27 16:33:30.275: INFO: Found Service test-service-xgrtk in namespace services-5392 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Sep 27 16:33:30.275: INFO: Service test-service-xgrtk has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Sep 27 16:33:30.291: INFO: observed Service test-service-xgrtk in namespace services-5392 with labels: map[test-service-static:true]
Sep 27 16:33:30.291: INFO: observed Service test-service-xgrtk in namespace services-5392 with labels: map[test-service-static:true]
Sep 27 16:33:30.291: INFO: observed Service test-service-xgrtk in namespace services-5392 with labels: map[test-service-static:true]
Sep 27 16:33:30.291: INFO: Found Service test-service-xgrtk in namespace services-5392 with labels: map[test-service:patched test-service-static:true]
Sep 27 16:33:30.291: INFO: Service test-service-xgrtk patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Sep 27 16:33:30.312: INFO: Observed event: ADDED
Sep 27 16:33:30.312: INFO: Observed event: MODIFIED
Sep 27 16:33:30.312: INFO: Observed event: MODIFIED
Sep 27 16:33:30.312: INFO: Observed event: MODIFIED
Sep 27 16:33:30.312: INFO: Found Service test-service-xgrtk in namespace services-5392 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Sep 27 16:33:30.312: INFO: Service test-service-xgrtk deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:33:30.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5392" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":339,"completed":106,"skipped":1476,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:33:30.324: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 16:33:30.749: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 16:33:33.781: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:33:33.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-653" for this suite.
STEP: Destroying namespace "webhook-653-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":339,"completed":107,"skipped":1487,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:33:33.861: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Sep 27 16:33:35.917: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1047 PodName:pod-sharedvolume-ffa43ba5-642e-4ee2-9395-e62238390842 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 16:33:35.917: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 16:33:36.010: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:33:36.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1047" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":339,"completed":108,"skipped":1502,"failed":0}
SSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:33:36.027: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0927 16:33:36.065854      23 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:35:02.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-660" for this suite.

• [SLOW TEST:86.080 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":339,"completed":109,"skipped":1506,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:35:02.107: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-197eb025-2009-4150-bbdc-7275329cdb38
STEP: Creating a pod to test consume secrets
Sep 27 16:35:02.158: INFO: Waiting up to 5m0s for pod "pod-secrets-8e600ccd-dca7-4ff2-9959-c47dfacc55ae" in namespace "secrets-9453" to be "Succeeded or Failed"
Sep 27 16:35:02.162: INFO: Pod "pod-secrets-8e600ccd-dca7-4ff2-9959-c47dfacc55ae": Phase="Pending", Reason="", readiness=false. Elapsed: 3.519328ms
Sep 27 16:35:04.168: INFO: Pod "pod-secrets-8e600ccd-dca7-4ff2-9959-c47dfacc55ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009767267s
STEP: Saw pod success
Sep 27 16:35:04.168: INFO: Pod "pod-secrets-8e600ccd-dca7-4ff2-9959-c47dfacc55ae" satisfied condition "Succeeded or Failed"
Sep 27 16:35:04.172: INFO: Trying to get logs from node ip-10-0-62-6.us-east-2.compute.internal pod pod-secrets-8e600ccd-dca7-4ff2-9959-c47dfacc55ae container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 16:35:04.205: INFO: Waiting for pod pod-secrets-8e600ccd-dca7-4ff2-9959-c47dfacc55ae to disappear
Sep 27 16:35:04.209: INFO: Pod pod-secrets-8e600ccd-dca7-4ff2-9959-c47dfacc55ae no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:35:04.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9453" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":110,"skipped":1522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:35:04.220: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-7d0b1535-c764-406f-ae2a-171f278909d3
STEP: Creating secret with name s-test-opt-upd-9f90e0a5-8bd0-4077-98ee-aedb383cffcf
STEP: Creating the pod
Sep 27 16:35:04.285: INFO: The status of Pod pod-projected-secrets-aa0f1427-714f-4aa1-97fa-962a3f83e596 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:35:06.291: INFO: The status of Pod pod-projected-secrets-aa0f1427-714f-4aa1-97fa-962a3f83e596 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-7d0b1535-c764-406f-ae2a-171f278909d3
STEP: Updating secret s-test-opt-upd-9f90e0a5-8bd0-4077-98ee-aedb383cffcf
STEP: Creating secret with name s-test-opt-create-9fa2e83e-d3e3-403e-8a35-090e8ae800d1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:35:08.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6568" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":111,"skipped":1609,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:35:08.390: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 27 16:35:08.439: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 16:36:08.476: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Sep 27 16:36:08.505: INFO: Created pod: pod0-sched-preemption-low-priority
Sep 27 16:36:08.525: INFO: Created pod: pod1-sched-preemption-medium-priority
Sep 27 16:36:08.543: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:36:32.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7242" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:84.294 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":339,"completed":112,"skipped":1610,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:36:32.684: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-7598/secret-test-16702e0a-709e-4513-8eea-04d8e288121c
STEP: Creating a pod to test consume secrets
Sep 27 16:36:32.735: INFO: Waiting up to 5m0s for pod "pod-configmaps-3c276e93-ef64-4daf-969b-78909ae06ee7" in namespace "secrets-7598" to be "Succeeded or Failed"
Sep 27 16:36:32.739: INFO: Pod "pod-configmaps-3c276e93-ef64-4daf-969b-78909ae06ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520072ms
Sep 27 16:36:34.744: INFO: Pod "pod-configmaps-3c276e93-ef64-4daf-969b-78909ae06ee7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009049808s
STEP: Saw pod success
Sep 27 16:36:34.744: INFO: Pod "pod-configmaps-3c276e93-ef64-4daf-969b-78909ae06ee7" satisfied condition "Succeeded or Failed"
Sep 27 16:36:34.752: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-configmaps-3c276e93-ef64-4daf-969b-78909ae06ee7 container env-test: <nil>
STEP: delete the pod
Sep 27 16:36:34.793: INFO: Waiting for pod pod-configmaps-3c276e93-ef64-4daf-969b-78909ae06ee7 to disappear
Sep 27 16:36:34.797: INFO: Pod pod-configmaps-3c276e93-ef64-4daf-969b-78909ae06ee7 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:36:34.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7598" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":113,"skipped":1637,"failed":0}
S
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:36:34.813: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:36:34.875: INFO: Waiting up to 5m0s for pod "busybox-user-65534-3cb7ad55-78d4-4444-a3d0-bf7e8e4f87d2" in namespace "security-context-test-6451" to be "Succeeded or Failed"
Sep 27 16:36:34.928: INFO: Pod "busybox-user-65534-3cb7ad55-78d4-4444-a3d0-bf7e8e4f87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 52.691828ms
Sep 27 16:36:36.934: INFO: Pod "busybox-user-65534-3cb7ad55-78d4-4444-a3d0-bf7e8e4f87d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.059316137s
Sep 27 16:36:36.934: INFO: Pod "busybox-user-65534-3cb7ad55-78d4-4444-a3d0-bf7e8e4f87d2" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:36:36.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6451" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":114,"skipped":1638,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:36:36.947: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:36:37.003: INFO: The status of Pod pod-secrets-fda2194a-5621-4643-ac5c-79cef53c4943 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:36:39.008: INFO: The status of Pod pod-secrets-fda2194a-5621-4643-ac5c-79cef53c4943 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:36:39.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2938" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":339,"completed":115,"skipped":1639,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:36:39.056: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 16:36:39.563: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 16:36:42.591: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:36:42.596: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1185-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:36:45.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1103" for this suite.
STEP: Destroying namespace "webhook-1103-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.715 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":339,"completed":116,"skipped":1651,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:36:45.772: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-cecc3025-3b64-41ca-ba21-143d790d5440
STEP: Creating a pod to test consume secrets
Sep 27 16:36:45.831: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bf9cf64a-b32e-4a84-8e5e-e21016a81cf2" in namespace "projected-3376" to be "Succeeded or Failed"
Sep 27 16:36:45.837: INFO: Pod "pod-projected-secrets-bf9cf64a-b32e-4a84-8e5e-e21016a81cf2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.446403ms
Sep 27 16:36:47.845: INFO: Pod "pod-projected-secrets-bf9cf64a-b32e-4a84-8e5e-e21016a81cf2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014227789s
STEP: Saw pod success
Sep 27 16:36:47.845: INFO: Pod "pod-projected-secrets-bf9cf64a-b32e-4a84-8e5e-e21016a81cf2" satisfied condition "Succeeded or Failed"
Sep 27 16:36:47.849: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-projected-secrets-bf9cf64a-b32e-4a84-8e5e-e21016a81cf2 container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 16:36:47.880: INFO: Waiting for pod pod-projected-secrets-bf9cf64a-b32e-4a84-8e5e-e21016a81cf2 to disappear
Sep 27 16:36:47.883: INFO: Pod pod-projected-secrets-bf9cf64a-b32e-4a84-8e5e-e21016a81cf2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:36:47.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3376" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":117,"skipped":1667,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:36:47.896: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Sep 27 16:36:47.939: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:36:49.944: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Sep 27 16:36:49.959: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:36:51.965: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 27 16:36:51.985: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 16:36:51.989: INFO: Pod pod-with-poststart-http-hook still exists
Sep 27 16:36:53.989: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 16:36:53.996: INFO: Pod pod-with-poststart-http-hook still exists
Sep 27 16:36:55.989: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 16:36:55.994: INFO: Pod pod-with-poststart-http-hook still exists
Sep 27 16:36:57.989: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 16:36:58.000: INFO: Pod pod-with-poststart-http-hook still exists
Sep 27 16:36:59.989: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 16:36:59.994: INFO: Pod pod-with-poststart-http-hook still exists
Sep 27 16:37:01.989: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 27 16:37:01.994: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:37:01.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8533" for this suite.

• [SLOW TEST:14.111 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":339,"completed":118,"skipped":1679,"failed":0}
SSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:37:02.006: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:37:02.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9446" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":339,"completed":119,"skipped":1683,"failed":0}
SSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:37:02.100: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 27 16:37:02.160: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep 27 16:37:02.165: INFO: starting watch
STEP: patching
STEP: updating
Sep 27 16:37:02.182: INFO: waiting for watch events with expected annotations
Sep 27 16:37:02.182: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:37:02.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6117" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":339,"completed":120,"skipped":1686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:37:02.231: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:37:02.270: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:37:05.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7469" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":339,"completed":121,"skipped":1721,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:37:05.420: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-f9f581df-7eae-4e30-8ebf-22f23268163b
STEP: Creating configMap with name cm-test-opt-upd-f4f3974f-d083-411a-b0be-33b3776fc663
STEP: Creating the pod
Sep 27 16:37:05.500: INFO: The status of Pod pod-projected-configmaps-675224b4-67a6-4525-8e55-35bfa99b2b69 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:37:07.506: INFO: The status of Pod pod-projected-configmaps-675224b4-67a6-4525-8e55-35bfa99b2b69 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-f9f581df-7eae-4e30-8ebf-22f23268163b
STEP: Updating configmap cm-test-opt-upd-f4f3974f-d083-411a-b0be-33b3776fc663
STEP: Creating configMap with name cm-test-opt-create-742a8e27-5f54-47e3-9bac-7b023db3bc24
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:37:09.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6149" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":122,"skipped":1723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:37:09.608: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Sep 27 16:37:09.652: INFO: Waiting up to 5m0s for pod "var-expansion-0892b2fa-0660-4fb7-961a-60c85b103c4a" in namespace "var-expansion-6831" to be "Succeeded or Failed"
Sep 27 16:37:09.657: INFO: Pod "var-expansion-0892b2fa-0660-4fb7-961a-60c85b103c4a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.489762ms
Sep 27 16:37:11.663: INFO: Pod "var-expansion-0892b2fa-0660-4fb7-961a-60c85b103c4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011220909s
STEP: Saw pod success
Sep 27 16:37:11.663: INFO: Pod "var-expansion-0892b2fa-0660-4fb7-961a-60c85b103c4a" satisfied condition "Succeeded or Failed"
Sep 27 16:37:11.667: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod var-expansion-0892b2fa-0660-4fb7-961a-60c85b103c4a container dapi-container: <nil>
STEP: delete the pod
Sep 27 16:37:11.690: INFO: Waiting for pod var-expansion-0892b2fa-0660-4fb7-961a-60c85b103c4a to disappear
Sep 27 16:37:11.693: INFO: Pod var-expansion-0892b2fa-0660-4fb7-961a-60c85b103c4a no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:37:11.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6831" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":339,"completed":123,"skipped":1748,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:37:11.705: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-608925da-9f08-466e-847e-926aed9cb3a2
STEP: Creating a pod to test consume secrets
Sep 27 16:37:11.754: INFO: Waiting up to 5m0s for pod "pod-secrets-a5ca01a9-939e-4900-a65a-f8e75ebf2123" in namespace "secrets-4862" to be "Succeeded or Failed"
Sep 27 16:37:11.758: INFO: Pod "pod-secrets-a5ca01a9-939e-4900-a65a-f8e75ebf2123": Phase="Pending", Reason="", readiness=false. Elapsed: 3.534845ms
Sep 27 16:37:13.766: INFO: Pod "pod-secrets-a5ca01a9-939e-4900-a65a-f8e75ebf2123": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011757284s
STEP: Saw pod success
Sep 27 16:37:13.766: INFO: Pod "pod-secrets-a5ca01a9-939e-4900-a65a-f8e75ebf2123" satisfied condition "Succeeded or Failed"
Sep 27 16:37:13.770: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-secrets-a5ca01a9-939e-4900-a65a-f8e75ebf2123 container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 16:37:13.794: INFO: Waiting for pod pod-secrets-a5ca01a9-939e-4900-a65a-f8e75ebf2123 to disappear
Sep 27 16:37:13.798: INFO: Pod pod-secrets-a5ca01a9-939e-4900-a65a-f8e75ebf2123 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:37:13.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4862" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":124,"skipped":1752,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:37:13.810: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Sep 27 16:37:13.871: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:37:15.876: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.95.24 on the node which pod1 resides and expect scheduled
Sep 27 16:37:15.886: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:37:17.893: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.95.24 but use UDP protocol on the node which pod2 resides
Sep 27 16:37:17.903: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:37:19.907: INFO: The status of Pod pod3 is Running (Ready = true)
Sep 27 16:37:19.918: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:37:21.925: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Sep 27 16:37:21.929: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.95.24 http://127.0.0.1:54323/hostname] Namespace:hostport-4879 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 16:37:21.929: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.95.24, port: 54323
Sep 27 16:37:22.014: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.95.24:54323/hostname] Namespace:hostport-4879 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 16:37:22.014: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.95.24, port: 54323 UDP
Sep 27 16:37:22.055: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.95.24 54323] Namespace:hostport-4879 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 16:37:22.055: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:37:27.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-4879" for this suite.

• [SLOW TEST:13.343 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":339,"completed":125,"skipped":1762,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:37:27.153: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 16:37:27.200: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13496f21-50c1-4080-a544-641fbf330ee3" in namespace "projected-8235" to be "Succeeded or Failed"
Sep 27 16:37:27.204: INFO: Pod "downwardapi-volume-13496f21-50c1-4080-a544-641fbf330ee3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.574616ms
Sep 27 16:37:29.210: INFO: Pod "downwardapi-volume-13496f21-50c1-4080-a544-641fbf330ee3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009703779s
STEP: Saw pod success
Sep 27 16:37:29.210: INFO: Pod "downwardapi-volume-13496f21-50c1-4080-a544-641fbf330ee3" satisfied condition "Succeeded or Failed"
Sep 27 16:37:29.214: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod downwardapi-volume-13496f21-50c1-4080-a544-641fbf330ee3 container client-container: <nil>
STEP: delete the pod
Sep 27 16:37:29.237: INFO: Waiting for pod downwardapi-volume-13496f21-50c1-4080-a544-641fbf330ee3 to disappear
Sep 27 16:37:29.241: INFO: Pod downwardapi-volume-13496f21-50c1-4080-a544-641fbf330ee3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:37:29.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8235" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":126,"skipped":1783,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:37:29.253: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Sep 27 16:37:29.297: INFO: Waiting up to 5m0s for pod "var-expansion-4a027d12-ca67-4328-a8bb-836859ca65df" in namespace "var-expansion-2786" to be "Succeeded or Failed"
Sep 27 16:37:29.301: INFO: Pod "var-expansion-4a027d12-ca67-4328-a8bb-836859ca65df": Phase="Pending", Reason="", readiness=false. Elapsed: 3.702389ms
Sep 27 16:37:31.307: INFO: Pod "var-expansion-4a027d12-ca67-4328-a8bb-836859ca65df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009882219s
STEP: Saw pod success
Sep 27 16:37:31.307: INFO: Pod "var-expansion-4a027d12-ca67-4328-a8bb-836859ca65df" satisfied condition "Succeeded or Failed"
Sep 27 16:37:31.311: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod var-expansion-4a027d12-ca67-4328-a8bb-836859ca65df container dapi-container: <nil>
STEP: delete the pod
Sep 27 16:37:31.334: INFO: Waiting for pod var-expansion-4a027d12-ca67-4328-a8bb-836859ca65df to disappear
Sep 27 16:37:31.338: INFO: Pod var-expansion-4a027d12-ca67-4328-a8bb-836859ca65df no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:37:31.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2786" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":339,"completed":127,"skipped":1785,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:37:31.349: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:37:52.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1777" for this suite.

• [SLOW TEST:21.330 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":339,"completed":128,"skipped":1804,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:37:52.679: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:37:52.716: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 27 16:37:52.727: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 27 16:37:57.735: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 27 16:37:57.735: INFO: Creating deployment "test-rolling-update-deployment"
Sep 27 16:37:57.740: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 27 16:37:57.750: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 27 16:37:59.760: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 27 16:37:59.763: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Sep 27 16:37:59.774: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5098  7e461099-239f-4c50-8e22-0496744f6d39 1708052 1 2021-09-27 16:37:57 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-09-27 16:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 16:37:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e15658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-27 16:37:57 +0000 UTC,LastTransitionTime:2021-09-27 16:37:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-09-27 16:37:59 +0000 UTC,LastTransitionTime:2021-09-27 16:37:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 27 16:37:59.777: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-5098  3d8eb006-0187-471f-a2bc-9bb223ea1122 1708041 1 2021-09-27 16:37:57 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7e461099-239f-4c50-8e22-0496744f6d39 0xc00336a1b7 0xc00336a1b8}] []  [{kube-controller-manager Update apps/v1 2021-09-27 16:37:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e461099-239f-4c50-8e22-0496744f6d39\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00336a378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 27 16:37:59.777: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 27 16:37:59.777: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5098  9e733972-e17f-4323-bcd4-bd55ea3e7d6e 1708050 2 2021-09-27 16:37:52 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7e461099-239f-4c50-8e22-0496744f6d39 0xc0019e9fc7 0xc0019e9fc8}] []  [{e2e.test Update apps/v1 2021-09-27 16:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 16:37:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e461099-239f-4c50-8e22-0496744f6d39\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00336a0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 16:37:59.781: INFO: Pod "test-rolling-update-deployment-585b757574-xl6d6" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-xl6d6 test-rolling-update-deployment-585b757574- deployment-5098  2c5215f4-ea53-4271-96e1-67fd40bd3c77 1708040 0 2021-09-27 16:37:57 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 3d8eb006-0187-471f-a2bc-9bb223ea1122 0xc003e159f7 0xc003e159f8}] []  [{kube-controller-manager Update v1 2021-09-27 16:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d8eb006-0187-471f-a2bc-9bb223ea1122\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 16:37:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zjlnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zjlnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 16:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 16:37:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 16:37:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 16:37:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.31.225,PodIP:100.96.1.221,StartTime:2021-09-27 16:37:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 16:37:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://5c641f154499c3af5aac9b32eccce470b8fb9d1dc9db34ef60f9d39ca21a8757,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:37:59.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5098" for this suite.

• [SLOW TEST:7.114 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":129,"skipped":1809,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:37:59.794: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:37:59.828: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:38:00.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9773" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":339,"completed":130,"skipped":1827,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:38:00.866: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:38:00.901: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: creating the pod
STEP: submitting the pod to kubernetes
Sep 27 16:38:00.913: INFO: The status of Pod pod-exec-websocket-5f7bf8a1-b684-4c24-8d12-3333696db302 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:38:02.919: INFO: The status of Pod pod-exec-websocket-5f7bf8a1-b684-4c24-8d12-3333696db302 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:38:03.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2395" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":339,"completed":131,"skipped":1851,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:38:03.024: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 27 16:38:03.063: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:38:12.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9805" for this suite.

• [SLOW TEST:9.115 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":339,"completed":132,"skipped":1860,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:38:12.139: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-a208428c-3e63-4f5f-acbd-8822ad0aa0ae
STEP: Creating a pod to test consume configMaps
Sep 27 16:38:12.188: INFO: Waiting up to 5m0s for pod "pod-configmaps-cbfe9b03-4b91-4a0e-b16f-e71b83c64ed1" in namespace "configmap-1894" to be "Succeeded or Failed"
Sep 27 16:38:12.192: INFO: Pod "pod-configmaps-cbfe9b03-4b91-4a0e-b16f-e71b83c64ed1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.616638ms
Sep 27 16:38:14.199: INFO: Pod "pod-configmaps-cbfe9b03-4b91-4a0e-b16f-e71b83c64ed1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011398879s
STEP: Saw pod success
Sep 27 16:38:14.200: INFO: Pod "pod-configmaps-cbfe9b03-4b91-4a0e-b16f-e71b83c64ed1" satisfied condition "Succeeded or Failed"
Sep 27 16:38:14.203: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-configmaps-cbfe9b03-4b91-4a0e-b16f-e71b83c64ed1 container agnhost-container: <nil>
STEP: delete the pod
Sep 27 16:38:14.227: INFO: Waiting for pod pod-configmaps-cbfe9b03-4b91-4a0e-b16f-e71b83c64ed1 to disappear
Sep 27 16:38:14.231: INFO: Pod pod-configmaps-cbfe9b03-4b91-4a0e-b16f-e71b83c64ed1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:38:14.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1894" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":133,"skipped":1877,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:38:14.243: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f41fdfcb-2599-415f-8063-85fbc38d3d21
STEP: Creating the pod
Sep 27 16:38:14.297: INFO: The status of Pod pod-projected-configmaps-7588ddaf-564c-4eb9-842a-8fcf4b25a3b9 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:38:16.304: INFO: The status of Pod pod-projected-configmaps-7588ddaf-564c-4eb9-842a-8fcf4b25a3b9 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-f41fdfcb-2599-415f-8063-85fbc38d3d21
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:38:18.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5188" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":134,"skipped":1879,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:38:18.350: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 27 16:38:18.692: INFO: Pod name wrapped-volume-race-12098295-a5f5-4fd6-8d24-7d28e6f39b26: Found 0 pods out of 5
Sep 27 16:38:23.704: INFO: Pod name wrapped-volume-race-12098295-a5f5-4fd6-8d24-7d28e6f39b26: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-12098295-a5f5-4fd6-8d24-7d28e6f39b26 in namespace emptydir-wrapper-6783, will wait for the garbage collector to delete the pods
Sep 27 16:38:33.809: INFO: Deleting ReplicationController wrapped-volume-race-12098295-a5f5-4fd6-8d24-7d28e6f39b26 took: 7.874005ms
Sep 27 16:38:33.910: INFO: Terminating ReplicationController wrapped-volume-race-12098295-a5f5-4fd6-8d24-7d28e6f39b26 pods took: 100.348272ms
STEP: Creating RC which spawns configmap-volume pods
Sep 27 16:38:42.531: INFO: Pod name wrapped-volume-race-afeb4f36-275c-4e36-bce2-a3fe7bb0cd62: Found 0 pods out of 5
Sep 27 16:38:47.538: INFO: Pod name wrapped-volume-race-afeb4f36-275c-4e36-bce2-a3fe7bb0cd62: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-afeb4f36-275c-4e36-bce2-a3fe7bb0cd62 in namespace emptydir-wrapper-6783, will wait for the garbage collector to delete the pods
Sep 27 16:38:57.628: INFO: Deleting ReplicationController wrapped-volume-race-afeb4f36-275c-4e36-bce2-a3fe7bb0cd62 took: 8.379951ms
Sep 27 16:38:57.729: INFO: Terminating ReplicationController wrapped-volume-race-afeb4f36-275c-4e36-bce2-a3fe7bb0cd62 pods took: 101.086023ms
STEP: Creating RC which spawns configmap-volume pods
Sep 27 16:39:12.158: INFO: Pod name wrapped-volume-race-c0c54890-5aed-43e8-a040-c9dfb35bfbd5: Found 0 pods out of 5
Sep 27 16:39:17.166: INFO: Pod name wrapped-volume-race-c0c54890-5aed-43e8-a040-c9dfb35bfbd5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c0c54890-5aed-43e8-a040-c9dfb35bfbd5 in namespace emptydir-wrapper-6783, will wait for the garbage collector to delete the pods
Sep 27 16:39:27.253: INFO: Deleting ReplicationController wrapped-volume-race-c0c54890-5aed-43e8-a040-c9dfb35bfbd5 took: 7.862789ms
Sep 27 16:39:27.353: INFO: Terminating ReplicationController wrapped-volume-race-c0c54890-5aed-43e8-a040-c9dfb35bfbd5 pods took: 100.107614ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:39:42.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6783" for this suite.

• [SLOW TEST:84.160 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":339,"completed":135,"skipped":1893,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:39:42.510: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 16:39:42.925: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 16:39:45.954: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:39:46.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2586" for this suite.
STEP: Destroying namespace "webhook-2586-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":339,"completed":136,"skipped":1898,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:39:46.221: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-809
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 27 16:39:46.256: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 27 16:39:46.289: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:39:48.296: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:39:50.294: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:39:52.297: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:39:54.296: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:39:56.295: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 16:39:58.295: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 27 16:39:58.302: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 27 16:39:58.309: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 27 16:40:00.348: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep 27 16:40:00.348: INFO: Going to poll 100.96.1.224 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Sep 27 16:40:00.351: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.224 8081 | grep -v '^\s*$'] Namespace:pod-network-test-809 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 16:40:00.351: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 16:40:01.441: INFO: Found all 1 expected endpoints: [netserver-0]
Sep 27 16:40:01.441: INFO: Going to poll 100.96.5.34 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Sep 27 16:40:01.445: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.5.34 8081 | grep -v '^\s*$'] Namespace:pod-network-test-809 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 16:40:01.445: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 16:40:02.548: INFO: Found all 1 expected endpoints: [netserver-1]
Sep 27 16:40:02.548: INFO: Going to poll 100.96.4.106 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Sep 27 16:40:02.555: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.4.106 8081 | grep -v '^\s*$'] Namespace:pod-network-test-809 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 16:40:02.555: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 16:40:03.655: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:40:03.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-809" for this suite.

• [SLOW TEST:17.447 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":137,"skipped":1904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:40:03.668: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 27 16:40:03.715: INFO: Waiting up to 5m0s for pod "pod-4045c313-a06c-46d3-84b5-dceb180ed50a" in namespace "emptydir-6056" to be "Succeeded or Failed"
Sep 27 16:40:03.719: INFO: Pod "pod-4045c313-a06c-46d3-84b5-dceb180ed50a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.667068ms
Sep 27 16:40:05.725: INFO: Pod "pod-4045c313-a06c-46d3-84b5-dceb180ed50a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009294433s
STEP: Saw pod success
Sep 27 16:40:05.725: INFO: Pod "pod-4045c313-a06c-46d3-84b5-dceb180ed50a" satisfied condition "Succeeded or Failed"
Sep 27 16:40:05.729: INFO: Trying to get logs from node ip-10-0-62-6.us-east-2.compute.internal pod pod-4045c313-a06c-46d3-84b5-dceb180ed50a container test-container: <nil>
STEP: delete the pod
Sep 27 16:40:05.762: INFO: Waiting for pod pod-4045c313-a06c-46d3-84b5-dceb180ed50a to disappear
Sep 27 16:40:05.765: INFO: Pod pod-4045c313-a06c-46d3-84b5-dceb180ed50a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:40:05.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6056" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":138,"skipped":1931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:40:05.778: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:40:05.814: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:40:06.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5979" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":339,"completed":139,"skipped":1959,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:40:06.377: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 27 16:40:06.449: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:06.449: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:06.449: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:06.452: INFO: Number of nodes with available pods: 0
Sep 27 16:40:06.452: INFO: Node ip-10-0-31-225.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:40:07.458: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:07.458: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:07.458: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:07.463: INFO: Number of nodes with available pods: 1
Sep 27 16:40:07.463: INFO: Node ip-10-0-62-6.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:40:08.460: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:08.460: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:08.460: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:08.465: INFO: Number of nodes with available pods: 3
Sep 27 16:40:08.465: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 27 16:40:08.495: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:08.495: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:08.495: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:08.500: INFO: Number of nodes with available pods: 2
Sep 27 16:40:08.500: INFO: Node ip-10-0-31-225.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:40:09.505: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:09.505: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:09.505: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 16:40:09.509: INFO: Number of nodes with available pods: 3
Sep 27 16:40:09.509: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-153, will wait for the garbage collector to delete the pods
Sep 27 16:40:09.578: INFO: Deleting DaemonSet.extensions daemon-set took: 7.705076ms
Sep 27 16:40:09.678: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.609015ms
Sep 27 16:40:22.184: INFO: Number of nodes with available pods: 0
Sep 27 16:40:22.184: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 16:40:22.188: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1710086"},"items":null}

Sep 27 16:40:22.191: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1710086"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:40:22.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-153" for this suite.

• [SLOW TEST:15.843 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":339,"completed":140,"skipped":1959,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:40:22.220: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:40:27.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4181" for this suite.

• [SLOW TEST:5.410 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":339,"completed":141,"skipped":1972,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:40:27.630: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Sep 27 16:40:27.660: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:40:31.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5684" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":339,"completed":142,"skipped":1982,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:40:31.099: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-0eda67c9-fcb6-4c8c-afdc-b32775292bb2
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:40:31.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3516" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":339,"completed":143,"skipped":2002,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:40:31.144: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-3413
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3413
STEP: Creating statefulset with conflicting port in namespace statefulset-3413
STEP: Waiting until pod test-pod will start running in namespace statefulset-3413
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3413
Sep 27 16:40:33.236: INFO: Observed stateful pod in namespace: statefulset-3413, name: ss-0, uid: aa2a2b3c-9388-40cb-90e7-65537794df63, status phase: Pending. Waiting for statefulset controller to delete.
Sep 27 16:40:33.804: INFO: Observed stateful pod in namespace: statefulset-3413, name: ss-0, uid: aa2a2b3c-9388-40cb-90e7-65537794df63, status phase: Failed. Waiting for statefulset controller to delete.
Sep 27 16:40:33.817: INFO: Observed stateful pod in namespace: statefulset-3413, name: ss-0, uid: aa2a2b3c-9388-40cb-90e7-65537794df63, status phase: Failed. Waiting for statefulset controller to delete.
Sep 27 16:40:33.823: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3413
STEP: Removing pod with conflicting port in namespace statefulset-3413
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3413 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Sep 27 16:40:37.856: INFO: Deleting all statefulset in ns statefulset-3413
Sep 27 16:40:37.859: INFO: Scaling statefulset ss to 0
Sep 27 16:40:57.880: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 16:40:57.884: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:40:57.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3413" for this suite.

• [SLOW TEST:26.769 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":339,"completed":144,"skipped":2016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:40:57.914: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:40:57.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9943" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":339,"completed":145,"skipped":2060,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:40:57.980: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:40:58.020: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 27 16:41:03.025: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 27 16:41:03.025: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Sep 27 16:41:03.054: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2525  726df412-c51c-4b3b-adba-0af89b1eb693 1710624 1 2021-09-27 16:41:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-09-27 16:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c597e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep 27 16:41:03.062: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-2525  17f68df7-c850-4331-8d7a-ee83bd129e0f 1710626 1 2021-09-27 16:41:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 726df412-c51c-4b3b-adba-0af89b1eb693 0xc003c59d87 0xc003c59d88}] []  [{kube-controller-manager Update apps/v1 2021-09-27 16:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"726df412-c51c-4b3b-adba-0af89b1eb693\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c59e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 16:41:03.062: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep 27 16:41:03.062: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2525  ee38b391-6dab-443e-a229-e39192ca8d2b 1710625 1 2021-09-27 16:40:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 726df412-c51c-4b3b-adba-0af89b1eb693 0xc003c59c77 0xc003c59c78}] []  [{e2e.test Update apps/v1 2021-09-27 16:40:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 16:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"726df412-c51c-4b3b-adba-0af89b1eb693\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c59d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 27 16:41:03.067: INFO: Pod "test-cleanup-controller-d2lv9" is available:
&Pod{ObjectMeta:{test-cleanup-controller-d2lv9 test-cleanup-controller- deployment-2525  ffbd1f70-52cd-4cec-aa09-de7e875d9b11 1710597 0 2021-09-27 16:40:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller ee38b391-6dab-443e-a229-e39192ca8d2b 0xc003bd3797 0xc003bd3798}] []  [{kube-controller-manager Update v1 2021-09-27 16:40:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee38b391-6dab-443e-a229-e39192ca8d2b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 16:40:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s459t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s459t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 16:40:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 16:40:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 16:40:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 16:40:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.95.24,PodIP:100.96.4.110,StartTime:2021-09-27 16:40:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 16:40:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://418cc21112e1f375af77cb52239943f4b32684bd1a08b072983d528c03d4ffa4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 16:41:03.067: INFO: Pod "test-cleanup-deployment-5b4d99b59b-4m4wn" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-4m4wn test-cleanup-deployment-5b4d99b59b- deployment-2525  4a9b0ea5-6cd7-4aff-8167-e53374ca50f8 1710628 0 2021-09-27 16:41:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b 17f68df7-c850-4331-8d7a-ee83bd129e0f 0xc003bd3977 0xc003bd3978}] []  [{kube-controller-manager Update v1 2021-09-27 16:41:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"17f68df7-c850-4331-8d7a-ee83bd129e0f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wfhdk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wfhdk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:41:03.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2525" for this suite.

• [SLOW TEST:5.111 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":339,"completed":146,"skipped":2080,"failed":0}
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:41:03.090: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:41:03.135: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-59819908-b509-49fe-9480-4e6399319669" in namespace "security-context-test-6316" to be "Succeeded or Failed"
Sep 27 16:41:03.142: INFO: Pod "busybox-readonly-false-59819908-b509-49fe-9480-4e6399319669": Phase="Pending", Reason="", readiness=false. Elapsed: 7.122545ms
Sep 27 16:41:05.149: INFO: Pod "busybox-readonly-false-59819908-b509-49fe-9480-4e6399319669": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014006817s
Sep 27 16:41:05.149: INFO: Pod "busybox-readonly-false-59819908-b509-49fe-9480-4e6399319669" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:41:05.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6316" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":339,"completed":147,"skipped":2081,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:41:05.162: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 16:41:05.599: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 16:41:08.628: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:41:08.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3198" for this suite.
STEP: Destroying namespace "webhook-3198-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":339,"completed":148,"skipped":2088,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:41:08.771: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:41:08.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6312" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":339,"completed":149,"skipped":2091,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:41:08.821: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4759
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4759
I0927 16:41:08.891283      23 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4759, replica count: 2
Sep 27 16:41:11.943: INFO: Creating new exec pod
I0927 16:41:11.943377      23 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 16:41:14.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-4759 exec execpodzk7jt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Sep 27 16:41:15.131: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep 27 16:41:15.131: INFO: stdout: "externalname-service-gxrbk"
Sep 27 16:41:15.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-4759 exec execpodzk7jt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.68.104.95 80'
Sep 27 16:41:15.262: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.68.104.95 80\nConnection to 100.68.104.95 80 port [tcp/http] succeeded!\n"
Sep 27 16:41:15.262: INFO: stdout: "externalname-service-4zhfj"
Sep 27 16:41:15.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-4759 exec execpodzk7jt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.62.6 31802'
Sep 27 16:41:15.402: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.62.6 31802\nConnection to 10.0.62.6 31802 port [tcp/*] succeeded!\n"
Sep 27 16:41:15.403: INFO: stdout: "externalname-service-gxrbk"
Sep 27 16:41:15.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-4759 exec execpodzk7jt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.31.225 31802'
Sep 27 16:41:15.542: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.31.225 31802\nConnection to 10.0.31.225 31802 port [tcp/*] succeeded!\n"
Sep 27 16:41:15.542: INFO: stdout: "externalname-service-4zhfj"
Sep 27 16:41:15.542: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:41:15.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4759" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:6.769 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":339,"completed":150,"skipped":2115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:41:15.591: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep 27 16:41:15.626: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 16:41:20.236: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:41:39.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1121" for this suite.

• [SLOW TEST:24.159 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":339,"completed":151,"skipped":2145,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:41:39.750: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Sep 27 16:41:39.793: INFO: Waiting up to 5m0s for pod "downward-api-8788fb5b-a46f-486a-a36c-693f05bddb7a" in namespace "downward-api-7085" to be "Succeeded or Failed"
Sep 27 16:41:39.797: INFO: Pod "downward-api-8788fb5b-a46f-486a-a36c-693f05bddb7a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.54326ms
Sep 27 16:41:41.804: INFO: Pod "downward-api-8788fb5b-a46f-486a-a36c-693f05bddb7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011087601s
STEP: Saw pod success
Sep 27 16:41:41.804: INFO: Pod "downward-api-8788fb5b-a46f-486a-a36c-693f05bddb7a" satisfied condition "Succeeded or Failed"
Sep 27 16:41:41.808: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downward-api-8788fb5b-a46f-486a-a36c-693f05bddb7a container dapi-container: <nil>
STEP: delete the pod
Sep 27 16:41:41.843: INFO: Waiting for pod downward-api-8788fb5b-a46f-486a-a36c-693f05bddb7a to disappear
Sep 27 16:41:41.847: INFO: Pod downward-api-8788fb5b-a46f-486a-a36c-693f05bddb7a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:41:41.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7085" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":339,"completed":152,"skipped":2149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:41:41.859: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-2208
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Sep 27 16:41:41.911: INFO: Found 0 stateful pods, waiting for 3
Sep 27 16:41:51.920: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:41:51.920: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:41:51.920: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Sep 27 16:41:51.953: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 27 16:42:02.000: INFO: Updating stateful set ss2
Sep 27 16:42:02.008: INFO: Waiting for Pod statefulset-2208/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Sep 27 16:42:12.019: INFO: Waiting for Pod statefulset-2208/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Sep 27 16:42:22.071: INFO: Found 2 stateful pods, waiting for 3
Sep 27 16:42:32.078: INFO: Found 2 stateful pods, waiting for 3
Sep 27 16:42:42.079: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:42:42.079: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:42:42.079: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 27 16:42:42.109: INFO: Updating stateful set ss2
Sep 27 16:42:42.116: INFO: Waiting for Pod statefulset-2208/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Sep 27 16:42:52.150: INFO: Updating stateful set ss2
Sep 27 16:42:52.160: INFO: Waiting for StatefulSet statefulset-2208/ss2 to complete update
Sep 27 16:42:52.160: INFO: Waiting for Pod statefulset-2208/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Sep 27 16:43:02.171: INFO: Waiting for StatefulSet statefulset-2208/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Sep 27 16:43:12.171: INFO: Deleting all statefulset in ns statefulset-2208
Sep 27 16:43:12.175: INFO: Scaling statefulset ss2 to 0
Sep 27 16:43:42.196: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 16:43:42.200: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:43:42.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2208" for this suite.

• [SLOW TEST:120.377 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":339,"completed":153,"skipped":2207,"failed":0}
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:43:42.235: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:43:42.269: INFO: Creating ReplicaSet my-hostname-basic-91723829-2165-43fb-aa9f-783aa6edd70d
Sep 27 16:43:42.279: INFO: Pod name my-hostname-basic-91723829-2165-43fb-aa9f-783aa6edd70d: Found 0 pods out of 1
Sep 27 16:43:47.285: INFO: Pod name my-hostname-basic-91723829-2165-43fb-aa9f-783aa6edd70d: Found 1 pods out of 1
Sep 27 16:43:47.285: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-91723829-2165-43fb-aa9f-783aa6edd70d" is running
Sep 27 16:43:47.289: INFO: Pod "my-hostname-basic-91723829-2165-43fb-aa9f-783aa6edd70d-xxdb2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 16:43:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 16:43:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 16:43:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 16:43:42 +0000 UTC Reason: Message:}])
Sep 27 16:43:47.289: INFO: Trying to dial the pod
Sep 27 16:43:52.308: INFO: Controller my-hostname-basic-91723829-2165-43fb-aa9f-783aa6edd70d: Got expected result from replica 1 [my-hostname-basic-91723829-2165-43fb-aa9f-783aa6edd70d-xxdb2]: "my-hostname-basic-91723829-2165-43fb-aa9f-783aa6edd70d-xxdb2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:43:52.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8888" for this suite.

• [SLOW TEST:10.086 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":154,"skipped":2207,"failed":0}
SSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:43:52.322: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 27 16:43:52.682: INFO: starting watch
STEP: patching
STEP: updating
Sep 27 16:43:52.696: INFO: waiting for watch events with expected annotations
Sep 27 16:43:52.696: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:43:52.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-4385" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":339,"completed":155,"skipped":2214,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:43:52.775: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 16:43:52.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d5ba2d4-2bb0-4b90-be53-ee89d12b4123" in namespace "downward-api-7363" to be "Succeeded or Failed"
Sep 27 16:43:52.820: INFO: Pod "downwardapi-volume-9d5ba2d4-2bb0-4b90-be53-ee89d12b4123": Phase="Pending", Reason="", readiness=false. Elapsed: 3.558354ms
Sep 27 16:43:54.827: INFO: Pod "downwardapi-volume-9d5ba2d4-2bb0-4b90-be53-ee89d12b4123": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010413702s
STEP: Saw pod success
Sep 27 16:43:54.827: INFO: Pod "downwardapi-volume-9d5ba2d4-2bb0-4b90-be53-ee89d12b4123" satisfied condition "Succeeded or Failed"
Sep 27 16:43:54.830: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod downwardapi-volume-9d5ba2d4-2bb0-4b90-be53-ee89d12b4123 container client-container: <nil>
STEP: delete the pod
Sep 27 16:43:54.864: INFO: Waiting for pod downwardapi-volume-9d5ba2d4-2bb0-4b90-be53-ee89d12b4123 to disappear
Sep 27 16:43:54.867: INFO: Pod downwardapi-volume-9d5ba2d4-2bb0-4b90-be53-ee89d12b4123 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:43:54.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7363" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":339,"completed":156,"skipped":2224,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:43:54.882: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4487.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4487.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4487.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4487.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4487.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4487.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 16:43:56.973: INFO: DNS probes using dns-4487/dns-test-7ff32add-77e9-4ce9-9b5b-7501e26adeec succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:43:56.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4487" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":339,"completed":157,"skipped":2241,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:43:56.998: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Sep 27 16:43:57.040: INFO: Waiting up to 5m0s for pod "downward-api-40f00d01-972f-4c1d-bf9b-608b980cb077" in namespace "downward-api-2871" to be "Succeeded or Failed"
Sep 27 16:43:57.043: INFO: Pod "downward-api-40f00d01-972f-4c1d-bf9b-608b980cb077": Phase="Pending", Reason="", readiness=false. Elapsed: 3.606776ms
Sep 27 16:43:59.049: INFO: Pod "downward-api-40f00d01-972f-4c1d-bf9b-608b980cb077": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00907186s
STEP: Saw pod success
Sep 27 16:43:59.049: INFO: Pod "downward-api-40f00d01-972f-4c1d-bf9b-608b980cb077" satisfied condition "Succeeded or Failed"
Sep 27 16:43:59.052: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downward-api-40f00d01-972f-4c1d-bf9b-608b980cb077 container dapi-container: <nil>
STEP: delete the pod
Sep 27 16:43:59.083: INFO: Waiting for pod downward-api-40f00d01-972f-4c1d-bf9b-608b980cb077 to disappear
Sep 27 16:43:59.087: INFO: Pod downward-api-40f00d01-972f-4c1d-bf9b-608b980cb077 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:43:59.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2871" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":339,"completed":158,"skipped":2294,"failed":0}

------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:43:59.098: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep 27 16:44:01.661: INFO: Successfully updated pod "adopt-release-87skk"
STEP: Checking that the Job readopts the Pod
Sep 27 16:44:01.661: INFO: Waiting up to 15m0s for pod "adopt-release-87skk" in namespace "job-9034" to be "adopted"
Sep 27 16:44:01.667: INFO: Pod "adopt-release-87skk": Phase="Running", Reason="", readiness=true. Elapsed: 6.604247ms
Sep 27 16:44:03.674: INFO: Pod "adopt-release-87skk": Phase="Running", Reason="", readiness=true. Elapsed: 2.013462573s
Sep 27 16:44:03.674: INFO: Pod "adopt-release-87skk" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep 27 16:44:04.187: INFO: Successfully updated pod "adopt-release-87skk"
STEP: Checking that the Job releases the Pod
Sep 27 16:44:04.187: INFO: Waiting up to 15m0s for pod "adopt-release-87skk" in namespace "job-9034" to be "released"
Sep 27 16:44:04.190: INFO: Pod "adopt-release-87skk": Phase="Running", Reason="", readiness=true. Elapsed: 3.753614ms
Sep 27 16:44:06.197: INFO: Pod "adopt-release-87skk": Phase="Running", Reason="", readiness=true. Elapsed: 2.010557135s
Sep 27 16:44:06.197: INFO: Pod "adopt-release-87skk" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:44:06.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9034" for this suite.

• [SLOW TEST:7.112 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":339,"completed":159,"skipped":2294,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:44:06.211: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:44:06.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9685" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":160,"skipped":2319,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:44:06.325: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:44:06.382: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 27 16:44:06.391: INFO: Number of nodes with available pods: 0
Sep 27 16:44:06.392: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 27 16:44:06.411: INFO: Number of nodes with available pods: 0
Sep 27 16:44:06.411: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:07.417: INFO: Number of nodes with available pods: 1
Sep 27 16:44:07.417: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 27 16:44:07.438: INFO: Number of nodes with available pods: 1
Sep 27 16:44:07.438: INFO: Number of running nodes: 0, number of available pods: 1
Sep 27 16:44:08.444: INFO: Number of nodes with available pods: 0
Sep 27 16:44:08.444: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 27 16:44:08.458: INFO: Number of nodes with available pods: 0
Sep 27 16:44:08.458: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:09.464: INFO: Number of nodes with available pods: 0
Sep 27 16:44:09.464: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:10.463: INFO: Number of nodes with available pods: 0
Sep 27 16:44:10.463: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:11.464: INFO: Number of nodes with available pods: 0
Sep 27 16:44:11.464: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:12.466: INFO: Number of nodes with available pods: 0
Sep 27 16:44:12.466: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:13.464: INFO: Number of nodes with available pods: 0
Sep 27 16:44:13.464: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:14.465: INFO: Number of nodes with available pods: 0
Sep 27 16:44:14.465: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:15.464: INFO: Number of nodes with available pods: 0
Sep 27 16:44:15.464: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:16.464: INFO: Number of nodes with available pods: 0
Sep 27 16:44:16.464: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:17.464: INFO: Number of nodes with available pods: 0
Sep 27 16:44:17.464: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:18.466: INFO: Number of nodes with available pods: 0
Sep 27 16:44:18.466: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:19.464: INFO: Number of nodes with available pods: 0
Sep 27 16:44:19.464: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:20.464: INFO: Number of nodes with available pods: 0
Sep 27 16:44:20.464: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:21.464: INFO: Number of nodes with available pods: 0
Sep 27 16:44:21.464: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:22.465: INFO: Number of nodes with available pods: 0
Sep 27 16:44:22.465: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 16:44:23.464: INFO: Number of nodes with available pods: 1
Sep 27 16:44:23.464: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9641, will wait for the garbage collector to delete the pods
Sep 27 16:44:23.537: INFO: Deleting DaemonSet.extensions daemon-set took: 8.9077ms
Sep 27 16:44:23.637: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.76401ms
Sep 27 16:44:32.144: INFO: Number of nodes with available pods: 0
Sep 27 16:44:32.144: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 16:44:32.147: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1712595"},"items":null}

Sep 27 16:44:32.151: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1712595"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:44:32.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9641" for this suite.

• [SLOW TEST:25.862 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":339,"completed":161,"skipped":2326,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:44:32.188: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Sep 27 16:44:32.753: INFO: created pod pod-service-account-defaultsa
Sep 27 16:44:32.753: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 27 16:44:32.758: INFO: created pod pod-service-account-mountsa
Sep 27 16:44:32.758: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 27 16:44:32.765: INFO: created pod pod-service-account-nomountsa
Sep 27 16:44:32.765: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 27 16:44:32.774: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 27 16:44:32.774: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 27 16:44:32.780: INFO: created pod pod-service-account-mountsa-mountspec
Sep 27 16:44:32.780: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 27 16:44:32.788: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 27 16:44:32.788: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 27 16:44:32.795: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 27 16:44:32.795: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 27 16:44:32.804: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 27 16:44:32.804: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 27 16:44:32.811: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 27 16:44:32.811: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:44:32.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3027" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":339,"completed":162,"skipped":2335,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:44:32.826: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 16:44:33.035: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep 27 16:44:35.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768357873, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768357873, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768357873, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768357873, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 16:44:38.073: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:44:38.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7862" for this suite.
STEP: Destroying namespace "webhook-7862-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.367 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":339,"completed":163,"skipped":2335,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:44:38.193: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 27 16:44:38.236: INFO: Waiting up to 5m0s for pod "pod-106c4957-1329-4123-a42f-c12cf671f6ce" in namespace "emptydir-8461" to be "Succeeded or Failed"
Sep 27 16:44:38.242: INFO: Pod "pod-106c4957-1329-4123-a42f-c12cf671f6ce": Phase="Pending", Reason="", readiness=false. Elapsed: 5.956271ms
Sep 27 16:44:40.246: INFO: Pod "pod-106c4957-1329-4123-a42f-c12cf671f6ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009839128s
STEP: Saw pod success
Sep 27 16:44:40.246: INFO: Pod "pod-106c4957-1329-4123-a42f-c12cf671f6ce" satisfied condition "Succeeded or Failed"
Sep 27 16:44:40.249: INFO: Trying to get logs from node ip-10-0-62-6.us-east-2.compute.internal pod pod-106c4957-1329-4123-a42f-c12cf671f6ce container test-container: <nil>
STEP: delete the pod
Sep 27 16:44:40.280: INFO: Waiting for pod pod-106c4957-1329-4123-a42f-c12cf671f6ce to disappear
Sep 27 16:44:40.284: INFO: Pod pod-106c4957-1329-4123-a42f-c12cf671f6ce no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:44:40.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8461" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":164,"skipped":2352,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:44:40.297: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-ea3af5f5-221c-4fa2-b1db-6ff56c324e3c
STEP: Creating a pod to test consume secrets
Sep 27 16:44:40.342: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-36536be2-c6bd-4803-87fc-a0bb052a7c07" in namespace "projected-8895" to be "Succeeded or Failed"
Sep 27 16:44:40.347: INFO: Pod "pod-projected-secrets-36536be2-c6bd-4803-87fc-a0bb052a7c07": Phase="Pending", Reason="", readiness=false. Elapsed: 4.963927ms
Sep 27 16:44:42.353: INFO: Pod "pod-projected-secrets-36536be2-c6bd-4803-87fc-a0bb052a7c07": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011313503s
STEP: Saw pod success
Sep 27 16:44:42.353: INFO: Pod "pod-projected-secrets-36536be2-c6bd-4803-87fc-a0bb052a7c07" satisfied condition "Succeeded or Failed"
Sep 27 16:44:42.357: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-projected-secrets-36536be2-c6bd-4803-87fc-a0bb052a7c07 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 16:44:42.381: INFO: Waiting for pod pod-projected-secrets-36536be2-c6bd-4803-87fc-a0bb052a7c07 to disappear
Sep 27 16:44:42.384: INFO: Pod pod-projected-secrets-36536be2-c6bd-4803-87fc-a0bb052a7c07 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:44:42.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8895" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":165,"skipped":2352,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:44:42.396: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0927 16:45:22.483439      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 27 16:50:22.490: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Sep 27 16:50:22.490: INFO: Deleting pod "simpletest.rc-29ws2" in namespace "gc-5941"
Sep 27 16:50:22.506: INFO: Deleting pod "simpletest.rc-g9qrp" in namespace "gc-5941"
Sep 27 16:50:22.525: INFO: Deleting pod "simpletest.rc-jmj8c" in namespace "gc-5941"
Sep 27 16:50:22.540: INFO: Deleting pod "simpletest.rc-nbtbm" in namespace "gc-5941"
Sep 27 16:50:22.558: INFO: Deleting pod "simpletest.rc-qpp44" in namespace "gc-5941"
Sep 27 16:50:22.575: INFO: Deleting pod "simpletest.rc-tzwmk" in namespace "gc-5941"
Sep 27 16:50:22.589: INFO: Deleting pod "simpletest.rc-vmmx4" in namespace "gc-5941"
Sep 27 16:50:22.606: INFO: Deleting pod "simpletest.rc-xm6df" in namespace "gc-5941"
Sep 27 16:50:22.622: INFO: Deleting pod "simpletest.rc-xsmwm" in namespace "gc-5941"
Sep 27 16:50:22.634: INFO: Deleting pod "simpletest.rc-z6nnq" in namespace "gc-5941"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:50:22.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5941" for this suite.

• [SLOW TEST:340.263 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":339,"completed":166,"skipped":2352,"failed":0}
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:50:22.659: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Sep 27 16:50:22.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-6464 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Sep 27 16:50:23.260: INFO: stderr: ""
Sep 27 16:50:23.260: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Sep 27 16:50:23.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-6464 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Sep 27 16:50:23.600: INFO: stderr: ""
Sep 27 16:50:23.600: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Sep 27 16:50:23.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-6464 delete pods e2e-test-httpd-pod'
Sep 27 16:50:32.084: INFO: stderr: ""
Sep 27 16:50:32.084: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:50:32.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6464" for this suite.

• [SLOW TEST:9.437 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:903
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":339,"completed":167,"skipped":2352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:50:32.096: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Sep 27 16:50:32.143: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:50:34.150: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:50:35.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8275" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":339,"completed":168,"skipped":2413,"failed":0}
S
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:50:35.188: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-574
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-574
STEP: Deleting pre-stop pod
Sep 27 16:50:44.290: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:50:44.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-574" for this suite.

• [SLOW TEST:9.137 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":339,"completed":169,"skipped":2414,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:50:44.325: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:51:00.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-615" for this suite.

• [SLOW TEST:16.159 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":339,"completed":170,"skipped":2436,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:51:00.485: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Sep 27 16:51:00.529: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:51:02.536: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Sep 27 16:51:02.551: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:51:04.559: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Sep 27 16:51:04.571: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 16:51:04.579: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 16:51:06.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 16:51:06.585: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 16:51:08.579: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 16:51:08.584: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 16:51:10.580: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 16:51:10.586: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 27 16:51:12.581: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 27 16:51:12.587: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:51:12.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4950" for this suite.

• [SLOW TEST:12.134 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":339,"completed":171,"skipped":2445,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:51:12.619: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0927 16:51:22.693061      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 27 16:56:22.698: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:56:22.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3624" for this suite.

• [SLOW TEST:310.092 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":339,"completed":172,"skipped":2460,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:56:22.711: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Sep 27 16:56:22.755: INFO: Waiting up to 5m0s for pod "security-context-4a27ef24-4acc-401c-ab44-a9610806d792" in namespace "security-context-8738" to be "Succeeded or Failed"
Sep 27 16:56:22.758: INFO: Pod "security-context-4a27ef24-4acc-401c-ab44-a9610806d792": Phase="Pending", Reason="", readiness=false. Elapsed: 3.57406ms
Sep 27 16:56:24.763: INFO: Pod "security-context-4a27ef24-4acc-401c-ab44-a9610806d792": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008412576s
STEP: Saw pod success
Sep 27 16:56:24.763: INFO: Pod "security-context-4a27ef24-4acc-401c-ab44-a9610806d792" satisfied condition "Succeeded or Failed"
Sep 27 16:56:24.767: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod security-context-4a27ef24-4acc-401c-ab44-a9610806d792 container test-container: <nil>
STEP: delete the pod
Sep 27 16:56:24.800: INFO: Waiting for pod security-context-4a27ef24-4acc-401c-ab44-a9610806d792 to disappear
Sep 27 16:56:24.804: INFO: Pod security-context-4a27ef24-4acc-401c-ab44-a9610806d792 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:56:24.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8738" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":173,"skipped":2519,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:56:24.815: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Sep 27 16:56:24.848: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:56:51.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8350" for this suite.

• [SLOW TEST:26.946 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":339,"completed":174,"skipped":2519,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:56:51.761: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:57:19.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4267" for this suite.

• [SLOW TEST:28.111 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":339,"completed":175,"skipped":2555,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:57:19.872: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 16:57:19.906: INFO: Creating pod...
Sep 27 16:57:19.917: INFO: Pod Quantity: 1 Status: Pending
Sep 27 16:57:20.922: INFO: Pod Quantity: 1 Status: Pending
Sep 27 16:57:21.923: INFO: Pod Status: Running
Sep 27 16:57:21.923: INFO: Creating service...
Sep 27 16:57:21.938: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/pods/agnhost/proxy/some/path/with/DELETE
Sep 27 16:57:21.945: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep 27 16:57:21.945: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/pods/agnhost/proxy/some/path/with/GET
Sep 27 16:57:21.951: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep 27 16:57:21.951: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/pods/agnhost/proxy/some/path/with/HEAD
Sep 27 16:57:21.955: INFO: http.Client request:HEAD | StatusCode:200
Sep 27 16:57:21.955: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/pods/agnhost/proxy/some/path/with/OPTIONS
Sep 27 16:57:21.960: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep 27 16:57:21.960: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/pods/agnhost/proxy/some/path/with/PATCH
Sep 27 16:57:21.965: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep 27 16:57:21.965: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/pods/agnhost/proxy/some/path/with/POST
Sep 27 16:57:21.970: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep 27 16:57:21.970: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/pods/agnhost/proxy/some/path/with/PUT
Sep 27 16:57:21.974: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Sep 27 16:57:21.974: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/services/test-service/proxy/some/path/with/DELETE
Sep 27 16:57:21.981: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Sep 27 16:57:21.981: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/services/test-service/proxy/some/path/with/GET
Sep 27 16:57:21.988: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Sep 27 16:57:21.988: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/services/test-service/proxy/some/path/with/HEAD
Sep 27 16:57:21.995: INFO: http.Client request:HEAD | StatusCode:200
Sep 27 16:57:21.995: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/services/test-service/proxy/some/path/with/OPTIONS
Sep 27 16:57:22.002: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Sep 27 16:57:22.002: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/services/test-service/proxy/some/path/with/PATCH
Sep 27 16:57:22.010: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Sep 27 16:57:22.010: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/services/test-service/proxy/some/path/with/POST
Sep 27 16:57:22.017: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Sep 27 16:57:22.017: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7703/services/test-service/proxy/some/path/with/PUT
Sep 27 16:57:22.023: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:57:22.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7703" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":339,"completed":176,"skipped":2592,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:57:22.037: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 27 16:57:22.080: INFO: Waiting up to 5m0s for pod "pod-b32faeeb-9d8a-4576-89aa-73a0768ffbeb" in namespace "emptydir-1485" to be "Succeeded or Failed"
Sep 27 16:57:22.085: INFO: Pod "pod-b32faeeb-9d8a-4576-89aa-73a0768ffbeb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.325936ms
Sep 27 16:57:24.091: INFO: Pod "pod-b32faeeb-9d8a-4576-89aa-73a0768ffbeb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011393667s
STEP: Saw pod success
Sep 27 16:57:24.091: INFO: Pod "pod-b32faeeb-9d8a-4576-89aa-73a0768ffbeb" satisfied condition "Succeeded or Failed"
Sep 27 16:57:24.096: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-b32faeeb-9d8a-4576-89aa-73a0768ffbeb container test-container: <nil>
STEP: delete the pod
Sep 27 16:57:24.124: INFO: Waiting for pod pod-b32faeeb-9d8a-4576-89aa-73a0768ffbeb to disappear
Sep 27 16:57:24.128: INFO: Pod pod-b32faeeb-9d8a-4576-89aa-73a0768ffbeb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:57:24.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1485" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":177,"skipped":2594,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:57:24.141: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Sep 27 16:57:24.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3880 create -f -'
Sep 27 16:57:24.533: INFO: stderr: ""
Sep 27 16:57:24.533: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Sep 27 16:57:24.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3880 diff -f -'
Sep 27 16:57:24.798: INFO: rc: 1
Sep 27 16:57:24.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3880 delete -f -'
Sep 27 16:57:24.865: INFO: stderr: ""
Sep 27 16:57:24.865: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:57:24.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3880" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":339,"completed":178,"skipped":2598,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:57:24.878: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 16:57:24.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81f6cdee-0341-41fb-8f96-bfc16c579203" in namespace "downward-api-1881" to be "Succeeded or Failed"
Sep 27 16:57:24.927: INFO: Pod "downwardapi-volume-81f6cdee-0341-41fb-8f96-bfc16c579203": Phase="Pending", Reason="", readiness=false. Elapsed: 5.000058ms
Sep 27 16:57:26.935: INFO: Pod "downwardapi-volume-81f6cdee-0341-41fb-8f96-bfc16c579203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013031784s
STEP: Saw pod success
Sep 27 16:57:26.935: INFO: Pod "downwardapi-volume-81f6cdee-0341-41fb-8f96-bfc16c579203" satisfied condition "Succeeded or Failed"
Sep 27 16:57:26.939: INFO: Trying to get logs from node ip-10-0-62-6.us-east-2.compute.internal pod downwardapi-volume-81f6cdee-0341-41fb-8f96-bfc16c579203 container client-container: <nil>
STEP: delete the pod
Sep 27 16:57:26.969: INFO: Waiting for pod downwardapi-volume-81f6cdee-0341-41fb-8f96-bfc16c579203 to disappear
Sep 27 16:57:26.973: INFO: Pod downwardapi-volume-81f6cdee-0341-41fb-8f96-bfc16c579203 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:57:26.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1881" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":179,"skipped":2603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:57:26.985: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-49ce0f0e-35fb-456a-90c1-536ca6f80b75
STEP: Creating configMap with name cm-test-opt-upd-31a01e11-958d-4a14-b27d-188d3ff72e5e
STEP: Creating the pod
Sep 27 16:57:27.044: INFO: The status of Pod pod-configmaps-a13db0c5-5ebd-4531-8b3b-6eebbd945fc8 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:57:29.051: INFO: The status of Pod pod-configmaps-a13db0c5-5ebd-4531-8b3b-6eebbd945fc8 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-49ce0f0e-35fb-456a-90c1-536ca6f80b75
STEP: Updating configmap cm-test-opt-upd-31a01e11-958d-4a14-b27d-188d3ff72e5e
STEP: Creating configMap with name cm-test-opt-create-1c6c17cb-de2a-485f-ae7f-fb16a91c880c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:57:31.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4548" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":180,"skipped":2650,"failed":0}
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:57:31.137: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-e22a6be5-4eaa-4bd7-9589-a5cee89b4582
STEP: Creating secret with name secret-projected-all-test-volume-3a3ced50-ac05-4778-8835-1752124a51b0
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 27 16:57:31.192: INFO: Waiting up to 5m0s for pod "projected-volume-58b52beb-b3fc-4289-9c66-a6279a646c6d" in namespace "projected-2502" to be "Succeeded or Failed"
Sep 27 16:57:31.195: INFO: Pod "projected-volume-58b52beb-b3fc-4289-9c66-a6279a646c6d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.555949ms
Sep 27 16:57:33.202: INFO: Pod "projected-volume-58b52beb-b3fc-4289-9c66-a6279a646c6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01006395s
STEP: Saw pod success
Sep 27 16:57:33.202: INFO: Pod "projected-volume-58b52beb-b3fc-4289-9c66-a6279a646c6d" satisfied condition "Succeeded or Failed"
Sep 27 16:57:33.206: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod projected-volume-58b52beb-b3fc-4289-9c66-a6279a646c6d container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 27 16:57:33.229: INFO: Waiting for pod projected-volume-58b52beb-b3fc-4289-9c66-a6279a646c6d to disappear
Sep 27 16:57:33.232: INFO: Pod projected-volume-58b52beb-b3fc-4289-9c66-a6279a646c6d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:57:33.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2502" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":339,"completed":181,"skipped":2651,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:57:33.243: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Sep 27 16:57:33.288: INFO: The status of Pod labelsupdateb88eb168-0e09-43a9-ab26-d758efb28855 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 16:57:35.296: INFO: The status of Pod labelsupdateb88eb168-0e09-43a9-ab26-d758efb28855 is Running (Ready = true)
Sep 27 16:57:35.828: INFO: Successfully updated pod "labelsupdateb88eb168-0e09-43a9-ab26-d758efb28855"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 16:57:39.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7545" for this suite.

• [SLOW TEST:6.625 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":182,"skipped":2651,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 16:57:39.868: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-4085
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-4085
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4085
Sep 27 16:57:39.920: INFO: Found 0 stateful pods, waiting for 1
Sep 27 16:57:49.927: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 27 16:57:49.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 16:57:50.118: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 16:57:50.118: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 16:57:50.118: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 16:57:50.123: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 27 16:58:00.131: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 16:58:00.131: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 16:58:00.151: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep 27 16:58:00.151: INFO: ss-0  ip-10-0-31-225.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  }]
Sep 27 16:58:00.151: INFO: 
Sep 27 16:58:00.151: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 27 16:58:01.158: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996207037s
Sep 27 16:58:02.164: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990012091s
Sep 27 16:58:03.170: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983599803s
Sep 27 16:58:04.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977692566s
Sep 27 16:58:05.182: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971542662s
Sep 27 16:58:06.189: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965520556s
Sep 27 16:58:07.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95901539s
Sep 27 16:58:08.202: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.952112518s
Sep 27 16:58:09.208: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.189339ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4085
Sep 27 16:58:10.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:58:10.376: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep 27 16:58:10.376: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 16:58:10.376: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 16:58:10.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:58:10.528: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 27 16:58:10.528: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 16:58:10.528: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 16:58:10.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:58:10.682: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 27 16:58:10.682: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep 27 16:58:10.682: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep 27 16:58:10.687: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep 27 16:58:20.693: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:58:20.693: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 27 16:58:20.693: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 27 16:58:20.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 16:58:20.860: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 16:58:20.860: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 16:58:20.860: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 16:58:20.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 16:58:21.005: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 16:58:21.005: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 16:58:21.005: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 16:58:21.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep 27 16:58:21.183: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep 27 16:58:21.183: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep 27 16:58:21.183: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep 27 16:58:21.183: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 16:58:21.187: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 27 16:58:31.197: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 16:58:31.197: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 16:58:31.197: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 27 16:58:31.211: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep 27 16:58:31.211: INFO: ss-0  ip-10-0-31-225.us-east-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  }]
Sep 27 16:58:31.211: INFO: ss-1  ip-10-0-95-24.us-east-2.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:31.211: INFO: ss-2  ip-10-0-62-6.us-east-2.compute.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:31.211: INFO: 
Sep 27 16:58:31.211: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 16:58:32.217: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep 27 16:58:32.217: INFO: ss-0  ip-10-0-31-225.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  }]
Sep 27 16:58:32.217: INFO: ss-1  ip-10-0-95-24.us-east-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:32.217: INFO: ss-2  ip-10-0-62-6.us-east-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:32.217: INFO: 
Sep 27 16:58:32.217: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 16:58:33.223: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep 27 16:58:33.223: INFO: ss-0  ip-10-0-31-225.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  }]
Sep 27 16:58:33.223: INFO: ss-1  ip-10-0-95-24.us-east-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:33.223: INFO: ss-2  ip-10-0-62-6.us-east-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:33.223: INFO: 
Sep 27 16:58:33.223: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 16:58:34.229: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep 27 16:58:34.229: INFO: ss-0  ip-10-0-31-225.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  }]
Sep 27 16:58:34.229: INFO: ss-1  ip-10-0-95-24.us-east-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:34.229: INFO: ss-2  ip-10-0-62-6.us-east-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:34.229: INFO: 
Sep 27 16:58:34.229: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 16:58:35.236: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep 27 16:58:35.236: INFO: ss-0  ip-10-0-31-225.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  }]
Sep 27 16:58:35.236: INFO: ss-1  ip-10-0-95-24.us-east-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:35.236: INFO: ss-2  ip-10-0-62-6.us-east-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:35.236: INFO: 
Sep 27 16:58:35.236: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 16:58:36.243: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep 27 16:58:36.243: INFO: ss-0  ip-10-0-31-225.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  }]
Sep 27 16:58:36.243: INFO: ss-1  ip-10-0-95-24.us-east-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:36.243: INFO: ss-2  ip-10-0-62-6.us-east-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:36.243: INFO: 
Sep 27 16:58:36.243: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 16:58:37.249: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep 27 16:58:37.249: INFO: ss-0  ip-10-0-31-225.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  }]
Sep 27 16:58:37.249: INFO: ss-1  ip-10-0-95-24.us-east-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:37.249: INFO: ss-2  ip-10-0-62-6.us-east-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:37.249: INFO: 
Sep 27 16:58:37.249: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 16:58:38.256: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep 27 16:58:38.256: INFO: ss-0  ip-10-0-31-225.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  }]
Sep 27 16:58:38.256: INFO: ss-1  ip-10-0-95-24.us-east-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:38.256: INFO: ss-2  ip-10-0-62-6.us-east-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:38.256: INFO: 
Sep 27 16:58:38.256: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 16:58:39.262: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep 27 16:58:39.262: INFO: ss-0  ip-10-0-31-225.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  }]
Sep 27 16:58:39.262: INFO: ss-1  ip-10-0-95-24.us-east-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:39.262: INFO: ss-2  ip-10-0-62-6.us-east-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:39.262: INFO: 
Sep 27 16:58:39.262: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 27 16:58:40.270: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep 27 16:58:40.270: INFO: ss-0  ip-10-0-31-225.us-east-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:57:39 +0000 UTC  }]
Sep 27 16:58:40.270: INFO: ss-1  ip-10-0-95-24.us-east-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:40.270: INFO: ss-2  ip-10-0-62-6.us-east-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-27 16:58:00 +0000 UTC  }]
Sep 27 16:58:40.270: INFO: 
Sep 27 16:58:40.270: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4085
Sep 27 16:58:41.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:58:41.355: INFO: rc: 1
Sep 27 16:58:41.355: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Sep 27 16:58:51.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:58:51.419: INFO: rc: 1
Sep 27 16:58:51.419: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 16:59:01.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:59:01.494: INFO: rc: 1
Sep 27 16:59:01.494: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 16:59:11.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:59:11.557: INFO: rc: 1
Sep 27 16:59:11.557: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 16:59:21.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:59:21.620: INFO: rc: 1
Sep 27 16:59:21.620: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 16:59:31.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:59:31.693: INFO: rc: 1
Sep 27 16:59:31.693: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 16:59:41.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:59:41.758: INFO: rc: 1
Sep 27 16:59:41.758: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 16:59:51.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 16:59:51.821: INFO: rc: 1
Sep 27 16:59:51.821: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:00:01.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:00:01.889: INFO: rc: 1
Sep 27 17:00:01.889: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:00:11.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:00:11.960: INFO: rc: 1
Sep 27 17:00:11.960: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:00:21.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:00:22.027: INFO: rc: 1
Sep 27 17:00:22.027: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:00:32.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:00:32.513: INFO: rc: 1
Sep 27 17:00:32.513: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:00:42.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:00:42.578: INFO: rc: 1
Sep 27 17:00:42.578: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:00:52.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:00:52.645: INFO: rc: 1
Sep 27 17:00:52.645: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:01:02.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:01:02.713: INFO: rc: 1
Sep 27 17:01:02.713: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:01:12.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:01:12.774: INFO: rc: 1
Sep 27 17:01:12.774: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:01:22.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:01:22.848: INFO: rc: 1
Sep 27 17:01:22.848: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:01:32.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:01:32.918: INFO: rc: 1
Sep 27 17:01:32.918: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:01:42.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:01:42.984: INFO: rc: 1
Sep 27 17:01:42.984: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:01:52.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:01:53.051: INFO: rc: 1
Sep 27 17:01:53.051: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:02:03.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:02:03.117: INFO: rc: 1
Sep 27 17:02:03.117: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:02:13.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:02:13.179: INFO: rc: 1
Sep 27 17:02:13.180: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:02:23.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:02:23.252: INFO: rc: 1
Sep 27 17:02:23.252: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:02:33.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:02:33.317: INFO: rc: 1
Sep 27 17:02:33.317: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:02:43.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:02:43.383: INFO: rc: 1
Sep 27 17:02:43.383: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:02:53.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:02:53.449: INFO: rc: 1
Sep 27 17:02:53.449: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:03:03.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:03:03.517: INFO: rc: 1
Sep 27 17:03:03.517: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:03:13.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:03:13.583: INFO: rc: 1
Sep 27 17:03:13.583: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:03:23.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:03:23.677: INFO: rc: 1
Sep 27 17:03:23.677: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:03:33.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:03:33.740: INFO: rc: 1
Sep 27 17:03:33.740: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Sep 27 17:03:43.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=statefulset-4085 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep 27 17:03:43.804: INFO: rc: 1
Sep 27 17:03:43.804: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Sep 27 17:03:43.804: INFO: Scaling statefulset ss to 0
Sep 27 17:03:43.820: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Sep 27 17:03:43.824: INFO: Deleting all statefulset in ns statefulset-4085
Sep 27 17:03:43.827: INFO: Scaling statefulset ss to 0
Sep 27 17:03:43.839: INFO: Waiting for statefulset status.replicas updated to 0
Sep 27 17:03:43.842: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:03:43.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4085" for this suite.

• [SLOW TEST:364.005 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":339,"completed":183,"skipped":2665,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:03:43.873: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Sep 27 17:03:43.919: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:03:45.926: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Sep 27 17:03:45.941: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:03:47.947: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 27 17:03:47.981: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 17:03:47.985: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 27 17:03:49.986: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 17:03:49.991: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 27 17:03:51.985: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 27 17:03:51.991: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:03:51.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8500" for this suite.

• [SLOW TEST:8.134 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":339,"completed":184,"skipped":2702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:03:52.007: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:03:52.040: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 27 17:03:57.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-8875 --namespace=crd-publish-openapi-8875 create -f -'
Sep 27 17:03:58.085: INFO: stderr: ""
Sep 27 17:03:58.085: INFO: stdout: "e2e-test-crd-publish-openapi-5930-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 27 17:03:58.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-8875 --namespace=crd-publish-openapi-8875 delete e2e-test-crd-publish-openapi-5930-crds test-cr'
Sep 27 17:03:58.153: INFO: stderr: ""
Sep 27 17:03:58.153: INFO: stdout: "e2e-test-crd-publish-openapi-5930-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep 27 17:03:58.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-8875 --namespace=crd-publish-openapi-8875 apply -f -'
Sep 27 17:03:58.521: INFO: stderr: ""
Sep 27 17:03:58.521: INFO: stdout: "e2e-test-crd-publish-openapi-5930-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep 27 17:03:58.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-8875 --namespace=crd-publish-openapi-8875 delete e2e-test-crd-publish-openapi-5930-crds test-cr'
Sep 27 17:03:58.615: INFO: stderr: ""
Sep 27 17:03:58.615: INFO: stdout: "e2e-test-crd-publish-openapi-5930-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep 27 17:03:58.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-8875 explain e2e-test-crd-publish-openapi-5930-crds'
Sep 27 17:03:58.949: INFO: stderr: ""
Sep 27 17:03:58.949: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5930-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:04:04.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8875" for this suite.

• [SLOW TEST:12.556 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":339,"completed":185,"skipped":2730,"failed":0}
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:04:04.563: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:04:04.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8720" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":339,"completed":186,"skipped":2731,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:04:04.621: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:04:04.658: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep 27 17:04:09.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 --namespace=crd-publish-openapi-2750 create -f -'
Sep 27 17:04:10.581: INFO: stderr: ""
Sep 27 17:04:10.581: INFO: stdout: "e2e-test-crd-publish-openapi-8621-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 27 17:04:10.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 --namespace=crd-publish-openapi-2750 delete e2e-test-crd-publish-openapi-8621-crds test-foo'
Sep 27 17:04:10.650: INFO: stderr: ""
Sep 27 17:04:10.650: INFO: stdout: "e2e-test-crd-publish-openapi-8621-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep 27 17:04:10.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 --namespace=crd-publish-openapi-2750 apply -f -'
Sep 27 17:04:10.920: INFO: stderr: ""
Sep 27 17:04:10.920: INFO: stdout: "e2e-test-crd-publish-openapi-8621-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep 27 17:04:10.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 --namespace=crd-publish-openapi-2750 delete e2e-test-crd-publish-openapi-8621-crds test-foo'
Sep 27 17:04:11.008: INFO: stderr: ""
Sep 27 17:04:11.008: INFO: stdout: "e2e-test-crd-publish-openapi-8621-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep 27 17:04:11.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 --namespace=crd-publish-openapi-2750 create -f -'
Sep 27 17:04:11.249: INFO: rc: 1
Sep 27 17:04:11.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 --namespace=crd-publish-openapi-2750 apply -f -'
Sep 27 17:04:11.588: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep 27 17:04:11.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 --namespace=crd-publish-openapi-2750 create -f -'
Sep 27 17:04:11.830: INFO: rc: 1
Sep 27 17:04:11.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 --namespace=crd-publish-openapi-2750 apply -f -'
Sep 27 17:04:12.161: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep 27 17:04:12.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 explain e2e-test-crd-publish-openapi-8621-crds'
Sep 27 17:04:12.503: INFO: stderr: ""
Sep 27 17:04:12.503: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep 27 17:04:12.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 explain e2e-test-crd-publish-openapi-8621-crds.metadata'
Sep 27 17:04:12.831: INFO: stderr: ""
Sep 27 17:04:12.831: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep 27 17:04:12.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 explain e2e-test-crd-publish-openapi-8621-crds.spec'
Sep 27 17:04:13.178: INFO: stderr: ""
Sep 27 17:04:13.178: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep 27 17:04:13.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 explain e2e-test-crd-publish-openapi-8621-crds.spec.bars'
Sep 27 17:04:13.509: INFO: stderr: ""
Sep 27 17:04:13.509: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep 27 17:04:13.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-2750 explain e2e-test-crd-publish-openapi-8621-crds.spec.bars2'
Sep 27 17:04:13.844: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:04:18.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2750" for this suite.

• [SLOW TEST:14.353 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":339,"completed":187,"skipped":2731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:04:18.974: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-f613b1e7-cf89-4192-815b-920ef681c00a
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:04:19.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-369" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":339,"completed":188,"skipped":2767,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:04:19.021: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-279
Sep 27 17:04:19.067: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:04:21.074: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Sep 27 17:04:21.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-279 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep 27 17:04:21.238: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Sep 27 17:04:21.238: INFO: stdout: "iptables"
Sep 27 17:04:21.238: INFO: proxyMode: iptables
Sep 27 17:04:21.254: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 17:04:21.257: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-279
STEP: creating replication controller affinity-nodeport-timeout in namespace services-279
I0927 17:04:21.290712      23 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-279, replica count: 3
I0927 17:04:24.341409      23 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 17:04:24.357: INFO: Creating new exec pod
Sep 27 17:04:27.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-279 exec execpod-affinityf9qzs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Sep 27 17:04:27.526: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Sep 27 17:04:27.526: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 17:04:27.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-279 exec execpod-affinityf9qzs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.66.168.71 80'
Sep 27 17:04:27.659: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.66.168.71 80\nConnection to 100.66.168.71 80 port [tcp/http] succeeded!\n"
Sep 27 17:04:27.659: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 17:04:27.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-279 exec execpod-affinityf9qzs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.31.225 31787'
Sep 27 17:04:27.791: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.31.225 31787\nConnection to 10.0.31.225 31787 port [tcp/*] succeeded!\n"
Sep 27 17:04:27.791: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 17:04:27.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-279 exec execpod-affinityf9qzs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.62.6 31787'
Sep 27 17:04:27.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.62.6 31787\nConnection to 10.0.62.6 31787 port [tcp/*] succeeded!\n"
Sep 27 17:04:27.922: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 17:04:27.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-279 exec execpod-affinityf9qzs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.31.225:31787/ ; done'
Sep 27 17:04:28.126: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n"
Sep 27 17:04:28.127: INFO: stdout: "\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt\naffinity-nodeport-timeout-gf8bt"
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Received response from host: affinity-nodeport-timeout-gf8bt
Sep 27 17:04:28.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-279 exec execpod-affinityf9qzs -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.31.225:31787/'
Sep 27 17:04:28.276: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n"
Sep 27 17:04:28.276: INFO: stdout: "affinity-nodeport-timeout-gf8bt"
Sep 27 17:04:48.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-279 exec execpod-affinityf9qzs -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.31.225:31787/'
Sep 27 17:04:48.440: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.31.225:31787/\n"
Sep 27 17:04:48.440: INFO: stdout: "affinity-nodeport-timeout-qsnfj"
Sep 27 17:04:48.440: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-279, will wait for the garbage collector to delete the pods
Sep 27 17:04:48.519: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 7.584601ms
Sep 27 17:04:48.620: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.12046ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:05:02.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-279" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:43.145 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":189,"skipped":2773,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:05:02.166: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 27 17:05:02.216: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 17:06:02.253: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Sep 27 17:06:02.277: INFO: Created pod: pod0-sched-preemption-low-priority
Sep 27 17:06:02.289: INFO: Created pod: pod1-sched-preemption-medium-priority
Sep 27 17:06:02.307: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:06:10.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4969" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:68.250 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":339,"completed":190,"skipped":2773,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:06:10.416: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:06:10.481: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e888f8ef-4e29-47f8-b42d-67ca51e30364", Controller:(*bool)(0xc0035f01a2), BlockOwnerDeletion:(*bool)(0xc0035f01a3)}}
Sep 27 17:06:10.492: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c141c82f-289c-4dcd-ae7c-e2e36dcd37fa", Controller:(*bool)(0xc00762cbaa), BlockOwnerDeletion:(*bool)(0xc00762cbab)}}
Sep 27 17:06:10.498: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"fd601401-50cd-4aed-b768-5cb88bb32eab", Controller:(*bool)(0xc0064c3092), BlockOwnerDeletion:(*bool)(0xc0064c3093)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:06:15.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9217" for this suite.

• [SLOW TEST:5.112 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":339,"completed":191,"skipped":2784,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:06:15.528: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:06:17.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-259" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":339,"completed":192,"skipped":2793,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:06:17.614: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Sep 27 17:06:17.657: INFO: created test-event-1
Sep 27 17:06:17.662: INFO: created test-event-2
Sep 27 17:06:17.667: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Sep 27 17:06:17.671: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Sep 27 17:06:17.704: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:06:17.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7098" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":339,"completed":193,"skipped":2823,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:06:17.720: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 17:06:17.762: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91674c88-44c0-453c-8bfb-6f5bdc7da3da" in namespace "projected-3216" to be "Succeeded or Failed"
Sep 27 17:06:17.766: INFO: Pod "downwardapi-volume-91674c88-44c0-453c-8bfb-6f5bdc7da3da": Phase="Pending", Reason="", readiness=false. Elapsed: 3.55405ms
Sep 27 17:06:19.772: INFO: Pod "downwardapi-volume-91674c88-44c0-453c-8bfb-6f5bdc7da3da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009927677s
STEP: Saw pod success
Sep 27 17:06:19.772: INFO: Pod "downwardapi-volume-91674c88-44c0-453c-8bfb-6f5bdc7da3da" satisfied condition "Succeeded or Failed"
Sep 27 17:06:19.776: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod downwardapi-volume-91674c88-44c0-453c-8bfb-6f5bdc7da3da container client-container: <nil>
STEP: delete the pod
Sep 27 17:06:19.807: INFO: Waiting for pod downwardapi-volume-91674c88-44c0-453c-8bfb-6f5bdc7da3da to disappear
Sep 27 17:06:19.811: INFO: Pod downwardapi-volume-91674c88-44c0-453c-8bfb-6f5bdc7da3da no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:06:19.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3216" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":194,"skipped":2825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:06:19.823: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:06:19.868: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b88ed7d5-02f9-484d-ac27-760c85273658" in namespace "security-context-test-4435" to be "Succeeded or Failed"
Sep 27 17:06:19.872: INFO: Pod "busybox-privileged-false-b88ed7d5-02f9-484d-ac27-760c85273658": Phase="Pending", Reason="", readiness=false. Elapsed: 3.467715ms
Sep 27 17:06:21.879: INFO: Pod "busybox-privileged-false-b88ed7d5-02f9-484d-ac27-760c85273658": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010919707s
Sep 27 17:06:21.879: INFO: Pod "busybox-privileged-false-b88ed7d5-02f9-484d-ac27-760c85273658" satisfied condition "Succeeded or Failed"
Sep 27 17:06:21.888: INFO: Got logs for pod "busybox-privileged-false-b88ed7d5-02f9-484d-ac27-760c85273658": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:06:21.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4435" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":195,"skipped":2877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:06:21.903: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:06:23.961: INFO: Deleting pod "var-expansion-de9b684a-004c-4315-9ad3-2a3a3d2cf2aa" in namespace "var-expansion-368"
Sep 27 17:06:23.972: INFO: Wait up to 5m0s for pod "var-expansion-de9b684a-004c-4315-9ad3-2a3a3d2cf2aa" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:06:31.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-368" for this suite.

• [SLOW TEST:10.102 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":339,"completed":196,"skipped":2909,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:06:32.005: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 27 17:06:32.064: INFO: Waiting up to 5m0s for pod "pod-9cc05a37-ec17-4bd3-ae31-905b317c8c18" in namespace "emptydir-1216" to be "Succeeded or Failed"
Sep 27 17:06:32.068: INFO: Pod "pod-9cc05a37-ec17-4bd3-ae31-905b317c8c18": Phase="Pending", Reason="", readiness=false. Elapsed: 3.865624ms
Sep 27 17:06:34.073: INFO: Pod "pod-9cc05a37-ec17-4bd3-ae31-905b317c8c18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008362851s
STEP: Saw pod success
Sep 27 17:06:34.073: INFO: Pod "pod-9cc05a37-ec17-4bd3-ae31-905b317c8c18" satisfied condition "Succeeded or Failed"
Sep 27 17:06:34.076: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-9cc05a37-ec17-4bd3-ae31-905b317c8c18 container test-container: <nil>
STEP: delete the pod
Sep 27 17:06:34.100: INFO: Waiting for pod pod-9cc05a37-ec17-4bd3-ae31-905b317c8c18 to disappear
Sep 27 17:06:34.103: INFO: Pod pod-9cc05a37-ec17-4bd3-ae31-905b317c8c18 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:06:34.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1216" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":197,"skipped":2911,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:06:34.115: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:06:34.147: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Creating first CR 
Sep 27 17:06:36.709: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-27T17:06:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-27T17:06:36Z]] name:name1 resourceVersion:1721100 uid:f21f2b92-e556-49d7-b5db-d382a0141c59] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep 27 17:06:46.717: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-27T17:06:46Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-27T17:06:46Z]] name:name2 resourceVersion:1721170 uid:51976338-cd1a-4b5c-95d7-b5fb6b26603d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep 27 17:06:56.727: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-27T17:06:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-27T17:06:56Z]] name:name1 resourceVersion:1721221 uid:f21f2b92-e556-49d7-b5db-d382a0141c59] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep 27 17:07:06.736: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-27T17:06:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-27T17:07:06Z]] name:name2 resourceVersion:1721272 uid:51976338-cd1a-4b5c-95d7-b5fb6b26603d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep 27 17:07:16.748: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-27T17:06:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-27T17:06:56Z]] name:name1 resourceVersion:1721326 uid:f21f2b92-e556-49d7-b5db-d382a0141c59] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep 27 17:07:26.758: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-27T17:06:46Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-27T17:07:06Z]] name:name2 resourceVersion:1721381 uid:51976338-cd1a-4b5c-95d7-b5fb6b26603d] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:07:37.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7828" for this suite.

• [SLOW TEST:63.173 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":339,"completed":198,"skipped":2982,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:07:37.288: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:07:37.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9285" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":339,"completed":199,"skipped":2983,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:07:37.344: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:07:53.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-554" for this suite.

• [SLOW TEST:16.152 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":339,"completed":200,"skipped":2990,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:07:53.496: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep 27 17:07:53.539: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3850  527a3c25-5340-4a2e-8332-46aed2f6fc53 1721570 0 2021-09-27 17:07:53 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-09-27 17:07:53 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ljsgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ljsgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:07:53.545: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:07:55.549: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Sep 27 17:07:55.549: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3850 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:07:55.549: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Verifying customized DNS server is configured on pod...
Sep 27 17:07:55.662: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3850 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:07:55.662: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:07:55.748: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:07:55.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3850" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":339,"completed":201,"skipped":3004,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:07:55.774: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Sep 27 17:07:55.811: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:07:58.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9649" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":339,"completed":202,"skipped":3013,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:07:58.363: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-5691
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5691 to expose endpoints map[]
Sep 27 17:07:58.416: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Sep 27 17:07:59.426: INFO: successfully validated that service multi-endpoint-test in namespace services-5691 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5691
Sep 27 17:07:59.440: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:08:01.446: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5691 to expose endpoints map[pod1:[100]]
Sep 27 17:08:01.461: INFO: successfully validated that service multi-endpoint-test in namespace services-5691 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5691
Sep 27 17:08:01.471: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:08:03.475: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5691 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 27 17:08:03.497: INFO: successfully validated that service multi-endpoint-test in namespace services-5691 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-5691
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5691 to expose endpoints map[pod2:[101]]
Sep 27 17:08:03.533: INFO: successfully validated that service multi-endpoint-test in namespace services-5691 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5691
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5691 to expose endpoints map[]
Sep 27 17:08:03.560: INFO: successfully validated that service multi-endpoint-test in namespace services-5691 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:08:03.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5691" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:5.240 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":339,"completed":203,"skipped":3019,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:08:03.602: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 17:08:04.110: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 17:08:07.138: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:08:07.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5004" for this suite.
STEP: Destroying namespace "webhook-5004-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":339,"completed":204,"skipped":3022,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:08:07.306: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Sep 27 17:08:07.348: INFO: Waiting up to 5m0s for pod "test-pod-e17bf4f1-2d41-4d92-ab60-856510dee944" in namespace "svcaccounts-1507" to be "Succeeded or Failed"
Sep 27 17:08:07.351: INFO: Pod "test-pod-e17bf4f1-2d41-4d92-ab60-856510dee944": Phase="Pending", Reason="", readiness=false. Elapsed: 3.62191ms
Sep 27 17:08:09.357: INFO: Pod "test-pod-e17bf4f1-2d41-4d92-ab60-856510dee944": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00909414s
STEP: Saw pod success
Sep 27 17:08:09.357: INFO: Pod "test-pod-e17bf4f1-2d41-4d92-ab60-856510dee944" satisfied condition "Succeeded or Failed"
Sep 27 17:08:09.361: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod test-pod-e17bf4f1-2d41-4d92-ab60-856510dee944 container agnhost-container: <nil>
STEP: delete the pod
Sep 27 17:08:09.392: INFO: Waiting for pod test-pod-e17bf4f1-2d41-4d92-ab60-856510dee944 to disappear
Sep 27 17:08:09.396: INFO: Pod test-pod-e17bf4f1-2d41-4d92-ab60-856510dee944 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:08:09.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1507" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":339,"completed":205,"skipped":3034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:08:09.408: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:08:09.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3266 version'
Sep 27 17:08:09.500: INFO: stderr: ""
Sep 27 17:08:09.500: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.2\", GitCommit:\"092fbfbf53427de67cac1e9fa54aaa09a28371d7\", GitTreeState:\"clean\", BuildDate:\"2021-06-16T12:59:11Z\", GoVersion:\"go1.16.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.2+vmware.1\", GitCommit:\"54e7e68e30dd3f9f7bb4f814c9d112f54f0fb273\", GitTreeState:\"clean\", BuildDate:\"2021-06-28T22:12:04Z\", GoVersion:\"go1.16.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:08:09.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3266" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":339,"completed":206,"skipped":3106,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:08:09.514: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 17:08:09.562: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa1948d3-29a9-4dd1-9c7f-5b45171aabcb" in namespace "downward-api-8939" to be "Succeeded or Failed"
Sep 27 17:08:09.565: INFO: Pod "downwardapi-volume-fa1948d3-29a9-4dd1-9c7f-5b45171aabcb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.699622ms
Sep 27 17:08:11.571: INFO: Pod "downwardapi-volume-fa1948d3-29a9-4dd1-9c7f-5b45171aabcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009025346s
STEP: Saw pod success
Sep 27 17:08:11.571: INFO: Pod "downwardapi-volume-fa1948d3-29a9-4dd1-9c7f-5b45171aabcb" satisfied condition "Succeeded or Failed"
Sep 27 17:08:11.575: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downwardapi-volume-fa1948d3-29a9-4dd1-9c7f-5b45171aabcb container client-container: <nil>
STEP: delete the pod
Sep 27 17:08:11.599: INFO: Waiting for pod downwardapi-volume-fa1948d3-29a9-4dd1-9c7f-5b45171aabcb to disappear
Sep 27 17:08:11.603: INFO: Pod downwardapi-volume-fa1948d3-29a9-4dd1-9c7f-5b45171aabcb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:08:11.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8939" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":207,"skipped":3116,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:08:11.616: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:08:22.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5226" for this suite.

• [SLOW TEST:11.235 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":339,"completed":208,"skipped":3123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:08:22.852: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 27 17:08:22.890: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 17:08:22.899: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 17:08:22.902: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-31-225.us-east-2.compute.internal before test
Sep 27 17:08:22.909: INFO: antrea-agent-cv4j2 from kube-system started at 2021-09-23 17:40:20 +0000 UTC (2 container statuses recorded)
Sep 27 17:08:22.909: INFO: 	Container antrea-agent ready: true, restart count 0
Sep 27 17:08:22.909: INFO: 	Container antrea-ovs ready: true, restart count 0
Sep 27 17:08:22.909: INFO: kube-proxy-sjbtj from kube-system started at 2021-09-23 17:30:51 +0000 UTC (1 container statuses recorded)
Sep 27 17:08:22.909: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 17:08:22.909: INFO: metrics-server-6c47b7847-kd7fj from kube-system started at 2021-09-23 22:19:30 +0000 UTC (1 container statuses recorded)
Sep 27 17:08:22.909: INFO: 	Container metrics-server ready: true, restart count 0
Sep 27 17:08:22.909: INFO: sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-p4r29 from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 17:08:22.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 17:08:22.909: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 17:08:22.909: INFO: tkr-controller-manager-6bc455b5d4-m5rjt from tkr-system started at 2021-09-23 22:19:30 +0000 UTC (1 container statuses recorded)
Sep 27 17:08:22.909: INFO: 	Container manager ready: true, restart count 0
Sep 27 17:08:22.909: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-62-6.us-east-2.compute.internal before test
Sep 27 17:08:22.916: INFO: antrea-agent-85fx9 from kube-system started at 2021-09-27 15:39:42 +0000 UTC (2 container statuses recorded)
Sep 27 17:08:22.916: INFO: 	Container antrea-agent ready: true, restart count 0
Sep 27 17:08:22.916: INFO: 	Container antrea-ovs ready: true, restart count 0
Sep 27 17:08:22.916: INFO: kube-proxy-882cf from kube-system started at 2021-09-27 15:39:42 +0000 UTC (1 container statuses recorded)
Sep 27 17:08:22.916: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 17:08:22.916: INFO: sonobuoy-e2e-job-87b0b9424e96441f from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 17:08:22.916: INFO: 	Container e2e ready: true, restart count 0
Sep 27 17:08:22.916: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 17:08:22.916: INFO: sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-88tn9 from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 17:08:22.916: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 17:08:22.916: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 17:08:22.916: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-95-24.us-east-2.compute.internal before test
Sep 27 17:08:22.922: INFO: antrea-agent-jwpnj from kube-system started at 2021-09-27 15:39:42 +0000 UTC (2 container statuses recorded)
Sep 27 17:08:22.922: INFO: 	Container antrea-agent ready: true, restart count 0
Sep 27 17:08:22.922: INFO: 	Container antrea-ovs ready: true, restart count 0
Sep 27 17:08:22.922: INFO: kube-proxy-2bm5v from kube-system started at 2021-09-27 15:39:42 +0000 UTC (1 container statuses recorded)
Sep 27 17:08:22.922: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 17:08:22.922: INFO: sonobuoy from sonobuoy started at 2021-09-27 15:43:02 +0000 UTC (1 container statuses recorded)
Sep 27 17:08:22.922: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 17:08:22.922: INFO: sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-nv46v from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 17:08:22.922: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 17:08:22.922: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16a8bcd87eb20f38], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:08:23.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-604" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":339,"completed":209,"skipped":3161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:08:23.966: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 27 17:08:24.008: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 27 17:08:29.014: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:08:30.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7882" for this suite.

• [SLOW TEST:6.082 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":339,"completed":210,"skipped":3189,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:08:30.048: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 17:08:30.875: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 17:08:33.906: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:08:34.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3125" for this suite.
STEP: Destroying namespace "webhook-3125-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":339,"completed":211,"skipped":3193,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:08:34.176: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:08:34.212: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: creating the pod
STEP: submitting the pod to kubernetes
Sep 27 17:08:34.226: INFO: The status of Pod pod-logs-websocket-f3621e61-0449-4848-864b-9817957b22eb is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:08:36.233: INFO: The status of Pod pod-logs-websocket-f3621e61-0449-4848-864b-9817957b22eb is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:08:36.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5335" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":339,"completed":212,"skipped":3238,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:08:36.263: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:08:36.298: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:08:42.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6036" for this suite.

• [SLOW TEST:6.560 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":339,"completed":213,"skipped":3239,"failed":0}
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:08:42.823: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2361
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 27 17:08:42.859: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 27 17:08:42.896: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:08:44.901: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:08:46.900: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:08:48.901: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:08:50.901: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:08:52.901: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:08:54.902: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:08:56.902: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:08:58.904: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:09:00.903: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:09:02.907: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:09:04.901: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 27 17:09:04.909: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 27 17:09:04.916: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 27 17:09:06.943: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep 27 17:09:06.943: INFO: Breadth first check of 100.96.1.253 on host 10.0.31.225...
Sep 27 17:09:06.947: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.4.158:9080/dial?request=hostname&protocol=http&host=100.96.1.253&port=8080&tries=1'] Namespace:pod-network-test-2361 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:06.947: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:07.010: INFO: Waiting for responses: map[]
Sep 27 17:09:07.010: INFO: reached 100.96.1.253 after 0/1 tries
Sep 27 17:09:07.010: INFO: Breadth first check of 100.96.5.54 on host 10.0.62.6...
Sep 27 17:09:07.014: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.4.158:9080/dial?request=hostname&protocol=http&host=100.96.5.54&port=8080&tries=1'] Namespace:pod-network-test-2361 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:07.014: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:07.067: INFO: Waiting for responses: map[]
Sep 27 17:09:07.068: INFO: reached 100.96.5.54 after 0/1 tries
Sep 27 17:09:07.068: INFO: Breadth first check of 100.96.4.157 on host 10.0.95.24...
Sep 27 17:09:07.072: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.4.158:9080/dial?request=hostname&protocol=http&host=100.96.4.157&port=8080&tries=1'] Namespace:pod-network-test-2361 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:07.072: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:07.157: INFO: Waiting for responses: map[]
Sep 27 17:09:07.157: INFO: reached 100.96.4.157 after 0/1 tries
Sep 27 17:09:07.157: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:09:07.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2361" for this suite.

• [SLOW TEST:24.347 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":339,"completed":214,"skipped":3243,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:09:07.170: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Sep 27 17:09:07.218: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:09:09.222: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Sep 27 17:09:09.237: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:09:11.242: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 27 17:09:11.246: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5631 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:11.246: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:11.327: INFO: Exec stderr: ""
Sep 27 17:09:11.327: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5631 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:11.327: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:11.371: INFO: Exec stderr: ""
Sep 27 17:09:11.371: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5631 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:11.371: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:11.446: INFO: Exec stderr: ""
Sep 27 17:09:11.446: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5631 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:11.446: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:11.514: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 27 17:09:11.514: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5631 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:11.514: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:11.560: INFO: Exec stderr: ""
Sep 27 17:09:11.560: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5631 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:11.560: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:11.603: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 27 17:09:11.603: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5631 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:11.603: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:11.696: INFO: Exec stderr: ""
Sep 27 17:09:11.696: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5631 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:11.696: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:11.784: INFO: Exec stderr: ""
Sep 27 17:09:11.784: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5631 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:11.784: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:11.826: INFO: Exec stderr: ""
Sep 27 17:09:11.826: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5631 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:09:11.826: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:09:11.895: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:09:11.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5631" for this suite.
•{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":215,"skipped":3259,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:09:11.919: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 17:09:11.966: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff9ad0f0-3d52-47ad-af78-4c98cf6c3f71" in namespace "projected-8462" to be "Succeeded or Failed"
Sep 27 17:09:11.969: INFO: Pod "downwardapi-volume-ff9ad0f0-3d52-47ad-af78-4c98cf6c3f71": Phase="Pending", Reason="", readiness=false. Elapsed: 3.525162ms
Sep 27 17:09:13.975: INFO: Pod "downwardapi-volume-ff9ad0f0-3d52-47ad-af78-4c98cf6c3f71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009236306s
STEP: Saw pod success
Sep 27 17:09:13.975: INFO: Pod "downwardapi-volume-ff9ad0f0-3d52-47ad-af78-4c98cf6c3f71" satisfied condition "Succeeded or Failed"
Sep 27 17:09:13.979: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downwardapi-volume-ff9ad0f0-3d52-47ad-af78-4c98cf6c3f71 container client-container: <nil>
STEP: delete the pod
Sep 27 17:09:14.006: INFO: Waiting for pod downwardapi-volume-ff9ad0f0-3d52-47ad-af78-4c98cf6c3f71 to disappear
Sep 27 17:09:14.010: INFO: Pod downwardapi-volume-ff9ad0f0-3d52-47ad-af78-4c98cf6c3f71 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:09:14.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8462" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":216,"skipped":3265,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:09:14.021: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-560c967f-c978-4bb2-b23a-70b9e6e06f72
STEP: Creating a pod to test consume configMaps
Sep 27 17:09:14.070: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d1b0e7c6-2630-4446-8cc7-536c65329ad0" in namespace "projected-8934" to be "Succeeded or Failed"
Sep 27 17:09:14.073: INFO: Pod "pod-projected-configmaps-d1b0e7c6-2630-4446-8cc7-536c65329ad0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.409093ms
Sep 27 17:09:16.080: INFO: Pod "pod-projected-configmaps-d1b0e7c6-2630-4446-8cc7-536c65329ad0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009905979s
STEP: Saw pod success
Sep 27 17:09:16.080: INFO: Pod "pod-projected-configmaps-d1b0e7c6-2630-4446-8cc7-536c65329ad0" satisfied condition "Succeeded or Failed"
Sep 27 17:09:16.083: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-projected-configmaps-d1b0e7c6-2630-4446-8cc7-536c65329ad0 container agnhost-container: <nil>
STEP: delete the pod
Sep 27 17:09:16.107: INFO: Waiting for pod pod-projected-configmaps-d1b0e7c6-2630-4446-8cc7-536c65329ad0 to disappear
Sep 27 17:09:16.110: INFO: Pod pod-projected-configmaps-d1b0e7c6-2630-4446-8cc7-536c65329ad0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:09:16.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8934" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":217,"skipped":3286,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:09:16.121: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 17:09:16.366: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 17:09:19.402: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:09:19.406: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4224-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:09:22.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4646" for this suite.
STEP: Destroying namespace "webhook-4646-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.612 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":339,"completed":218,"skipped":3331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:09:22.733: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 17:09:22.786: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b473b96-0b9a-43a9-8826-5008316311a1" in namespace "downward-api-5822" to be "Succeeded or Failed"
Sep 27 17:09:22.790: INFO: Pod "downwardapi-volume-1b473b96-0b9a-43a9-8826-5008316311a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.1198ms
Sep 27 17:09:24.798: INFO: Pod "downwardapi-volume-1b473b96-0b9a-43a9-8826-5008316311a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011982228s
STEP: Saw pod success
Sep 27 17:09:24.798: INFO: Pod "downwardapi-volume-1b473b96-0b9a-43a9-8826-5008316311a1" satisfied condition "Succeeded or Failed"
Sep 27 17:09:24.802: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downwardapi-volume-1b473b96-0b9a-43a9-8826-5008316311a1 container client-container: <nil>
STEP: delete the pod
Sep 27 17:09:24.851: INFO: Waiting for pod downwardapi-volume-1b473b96-0b9a-43a9-8826-5008316311a1 to disappear
Sep 27 17:09:24.856: INFO: Pod downwardapi-volume-1b473b96-0b9a-43a9-8826-5008316311a1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:09:24.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5822" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":219,"skipped":3381,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:09:24.868: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-3923
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3923 to expose endpoints map[]
Sep 27 17:09:24.951: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Sep 27 17:09:25.964: INFO: successfully validated that service endpoint-test2 in namespace services-3923 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3923
Sep 27 17:09:25.979: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:09:27.985: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3923 to expose endpoints map[pod1:[80]]
Sep 27 17:09:28.005: INFO: successfully validated that service endpoint-test2 in namespace services-3923 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-3923
Sep 27 17:09:28.017: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:09:30.024: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3923 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 27 17:09:30.045: INFO: successfully validated that service endpoint-test2 in namespace services-3923 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-3923
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3923 to expose endpoints map[pod2:[80]]
Sep 27 17:09:30.075: INFO: successfully validated that service endpoint-test2 in namespace services-3923 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-3923
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3923 to expose endpoints map[]
Sep 27 17:09:30.098: INFO: successfully validated that service endpoint-test2 in namespace services-3923 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:09:30.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3923" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:5.269 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":339,"completed":220,"skipped":3389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:09:30.137: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-wxsk
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 17:09:30.194: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-wxsk" in namespace "subpath-1650" to be "Succeeded or Failed"
Sep 27 17:09:30.197: INFO: Pod "pod-subpath-test-projected-wxsk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.459023ms
Sep 27 17:09:32.205: INFO: Pod "pod-subpath-test-projected-wxsk": Phase="Running", Reason="", readiness=true. Elapsed: 2.010669558s
Sep 27 17:09:34.212: INFO: Pod "pod-subpath-test-projected-wxsk": Phase="Running", Reason="", readiness=true. Elapsed: 4.018216124s
Sep 27 17:09:36.219: INFO: Pod "pod-subpath-test-projected-wxsk": Phase="Running", Reason="", readiness=true. Elapsed: 6.025025701s
Sep 27 17:09:38.226: INFO: Pod "pod-subpath-test-projected-wxsk": Phase="Running", Reason="", readiness=true. Elapsed: 8.031906594s
Sep 27 17:09:40.232: INFO: Pod "pod-subpath-test-projected-wxsk": Phase="Running", Reason="", readiness=true. Elapsed: 10.038478647s
Sep 27 17:09:42.239: INFO: Pod "pod-subpath-test-projected-wxsk": Phase="Running", Reason="", readiness=true. Elapsed: 12.04456205s
Sep 27 17:09:44.245: INFO: Pod "pod-subpath-test-projected-wxsk": Phase="Running", Reason="", readiness=true. Elapsed: 14.050691047s
Sep 27 17:09:46.252: INFO: Pod "pod-subpath-test-projected-wxsk": Phase="Running", Reason="", readiness=true. Elapsed: 16.058525498s
Sep 27 17:09:48.259: INFO: Pod "pod-subpath-test-projected-wxsk": Phase="Running", Reason="", readiness=true. Elapsed: 18.065315015s
Sep 27 17:09:50.267: INFO: Pod "pod-subpath-test-projected-wxsk": Phase="Running", Reason="", readiness=true. Elapsed: 20.072778696s
Sep 27 17:09:52.274: INFO: Pod "pod-subpath-test-projected-wxsk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.079744554s
STEP: Saw pod success
Sep 27 17:09:52.274: INFO: Pod "pod-subpath-test-projected-wxsk" satisfied condition "Succeeded or Failed"
Sep 27 17:09:52.278: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-subpath-test-projected-wxsk container test-container-subpath-projected-wxsk: <nil>
STEP: delete the pod
Sep 27 17:09:52.305: INFO: Waiting for pod pod-subpath-test-projected-wxsk to disappear
Sep 27 17:09:52.309: INFO: Pod pod-subpath-test-projected-wxsk no longer exists
STEP: Deleting pod pod-subpath-test-projected-wxsk
Sep 27 17:09:52.309: INFO: Deleting pod "pod-subpath-test-projected-wxsk" in namespace "subpath-1650"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:09:52.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1650" for this suite.

• [SLOW TEST:22.187 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":339,"completed":221,"skipped":3444,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:09:52.324: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 17:09:52.699: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 17:09:55.730: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:10:05.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5614" for this suite.
STEP: Destroying namespace "webhook-5614-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.613 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":339,"completed":222,"skipped":3449,"failed":0}
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:10:05.937: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 27 17:10:05.976: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 17:10:05.985: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 17:10:05.989: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-31-225.us-east-2.compute.internal before test
Sep 27 17:10:05.995: INFO: antrea-agent-cv4j2 from kube-system started at 2021-09-23 17:40:20 +0000 UTC (2 container statuses recorded)
Sep 27 17:10:05.995: INFO: 	Container antrea-agent ready: true, restart count 0
Sep 27 17:10:05.995: INFO: 	Container antrea-ovs ready: true, restart count 0
Sep 27 17:10:05.995: INFO: kube-proxy-sjbtj from kube-system started at 2021-09-23 17:30:51 +0000 UTC (1 container statuses recorded)
Sep 27 17:10:05.995: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 17:10:05.995: INFO: metrics-server-6c47b7847-kd7fj from kube-system started at 2021-09-23 22:19:30 +0000 UTC (1 container statuses recorded)
Sep 27 17:10:05.995: INFO: 	Container metrics-server ready: true, restart count 0
Sep 27 17:10:05.995: INFO: sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-p4r29 from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 17:10:05.995: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 17:10:05.995: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 17:10:05.995: INFO: tkr-controller-manager-6bc455b5d4-m5rjt from tkr-system started at 2021-09-23 22:19:30 +0000 UTC (1 container statuses recorded)
Sep 27 17:10:05.995: INFO: 	Container manager ready: true, restart count 0
Sep 27 17:10:05.995: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-62-6.us-east-2.compute.internal before test
Sep 27 17:10:06.002: INFO: antrea-agent-85fx9 from kube-system started at 2021-09-27 15:39:42 +0000 UTC (2 container statuses recorded)
Sep 27 17:10:06.002: INFO: 	Container antrea-agent ready: true, restart count 0
Sep 27 17:10:06.002: INFO: 	Container antrea-ovs ready: true, restart count 0
Sep 27 17:10:06.002: INFO: kube-proxy-882cf from kube-system started at 2021-09-27 15:39:42 +0000 UTC (1 container statuses recorded)
Sep 27 17:10:06.002: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 17:10:06.002: INFO: sonobuoy-e2e-job-87b0b9424e96441f from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 17:10:06.002: INFO: 	Container e2e ready: true, restart count 0
Sep 27 17:10:06.002: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 17:10:06.002: INFO: sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-88tn9 from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 17:10:06.002: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 17:10:06.002: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 17:10:06.002: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-95-24.us-east-2.compute.internal before test
Sep 27 17:10:06.009: INFO: antrea-agent-jwpnj from kube-system started at 2021-09-27 15:39:42 +0000 UTC (2 container statuses recorded)
Sep 27 17:10:06.009: INFO: 	Container antrea-agent ready: true, restart count 0
Sep 27 17:10:06.009: INFO: 	Container antrea-ovs ready: true, restart count 0
Sep 27 17:10:06.009: INFO: kube-proxy-2bm5v from kube-system started at 2021-09-27 15:39:42 +0000 UTC (1 container statuses recorded)
Sep 27 17:10:06.009: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 17:10:06.009: INFO: sonobuoy from sonobuoy started at 2021-09-27 15:43:02 +0000 UTC (1 container statuses recorded)
Sep 27 17:10:06.009: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 17:10:06.009: INFO: sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-nv46v from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 17:10:06.009: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 17:10:06.009: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node ip-10-0-31-225.us-east-2.compute.internal
STEP: verifying the node has the label node ip-10-0-62-6.us-east-2.compute.internal
STEP: verifying the node has the label node ip-10-0-95-24.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod antrea-agent-85fx9 requesting resource cpu=400m on Node ip-10-0-62-6.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod antrea-agent-cv4j2 requesting resource cpu=400m on Node ip-10-0-31-225.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod antrea-agent-jwpnj requesting resource cpu=400m on Node ip-10-0-95-24.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod kube-proxy-2bm5v requesting resource cpu=0m on Node ip-10-0-95-24.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod kube-proxy-882cf requesting resource cpu=0m on Node ip-10-0-62-6.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod kube-proxy-sjbtj requesting resource cpu=0m on Node ip-10-0-31-225.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod metrics-server-6c47b7847-kd7fj requesting resource cpu=0m on Node ip-10-0-31-225.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-95-24.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod sonobuoy-e2e-job-87b0b9424e96441f requesting resource cpu=0m on Node ip-10-0-62-6.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-88tn9 requesting resource cpu=0m on Node ip-10-0-62-6.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-nv46v requesting resource cpu=0m on Node ip-10-0-95-24.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-p4r29 requesting resource cpu=0m on Node ip-10-0-31-225.us-east-2.compute.internal
Sep 27 17:10:06.072: INFO: Pod tkr-controller-manager-6bc455b5d4-m5rjt requesting resource cpu=100m on Node ip-10-0-31-225.us-east-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Sep 27 17:10:06.072: INFO: Creating a pod which consumes cpu=5320m on Node ip-10-0-62-6.us-east-2.compute.internal
Sep 27 17:10:06.082: INFO: Creating a pod which consumes cpu=5320m on Node ip-10-0-95-24.us-east-2.compute.internal
Sep 27 17:10:06.088: INFO: Creating a pod which consumes cpu=5250m on Node ip-10-0-31-225.us-east-2.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-89542b71-9048-4332-a765-791327a94ce4.16a8bcf0834a1fa4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-613/filler-pod-89542b71-9048-4332-a765-791327a94ce4 to ip-10-0-31-225.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-89542b71-9048-4332-a765-791327a94ce4.16a8bcf0a00b70ef], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-89542b71-9048-4332-a765-791327a94ce4.16a8bcf0a75ae8d4], Reason = [Created], Message = [Created container filler-pod-89542b71-9048-4332-a765-791327a94ce4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-89542b71-9048-4332-a765-791327a94ce4.16a8bcf0ac2acc60], Reason = [Started], Message = [Started container filler-pod-89542b71-9048-4332-a765-791327a94ce4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-986eb8e2-98d0-4a5f-b66f-bfb0d62bd10f.16a8bcf082ba92e9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-613/filler-pod-986eb8e2-98d0-4a5f-b66f-bfb0d62bd10f to ip-10-0-95-24.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-986eb8e2-98d0-4a5f-b66f-bfb0d62bd10f.16a8bcf09fc1baec], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-986eb8e2-98d0-4a5f-b66f-bfb0d62bd10f.16a8bcf0a4e24e91], Reason = [Created], Message = [Created container filler-pod-986eb8e2-98d0-4a5f-b66f-bfb0d62bd10f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-986eb8e2-98d0-4a5f-b66f-bfb0d62bd10f.16a8bcf0a90b3ee8], Reason = [Started], Message = [Started container filler-pod-986eb8e2-98d0-4a5f-b66f-bfb0d62bd10f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f63b7b4a-0c41-49f6-a3ec-23bfa7ccb07e.16a8bcf082877d57], Reason = [Scheduled], Message = [Successfully assigned sched-pred-613/filler-pod-f63b7b4a-0c41-49f6-a3ec-23bfa7ccb07e to ip-10-0-62-6.us-east-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f63b7b4a-0c41-49f6-a3ec-23bfa7ccb07e.16a8bcf0a03a0014], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f63b7b4a-0c41-49f6-a3ec-23bfa7ccb07e.16a8bcf0a54ba7c8], Reason = [Created], Message = [Created container filler-pod-f63b7b4a-0c41-49f6-a3ec-23bfa7ccb07e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f63b7b4a-0c41-49f6-a3ec-23bfa7ccb07e.16a8bcf0a9f7149b], Reason = [Started], Message = [Started container filler-pod-f63b7b4a-0c41-49f6-a3ec-23bfa7ccb07e]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16a8bcf0fbf8f5d7], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: removing the label node off the node ip-10-0-31-225.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-62-6.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-95-24.us-east-2.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:10:09.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-613" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":339,"completed":223,"skipped":3450,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:10:09.194: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Sep 27 17:10:09.232: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Sep 27 17:10:09.239: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 27 17:10:09.239: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Sep 27 17:10:09.250: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep 27 17:10:09.250: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Sep 27 17:10:09.262: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep 27 17:10:09.262: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Sep 27 17:10:16.303: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:10:16.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9594" for this suite.

• [SLOW TEST:7.131 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":339,"completed":224,"skipped":3492,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:10:16.326: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Sep 27 17:10:16.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 create -f -'
Sep 27 17:10:16.699: INFO: stderr: ""
Sep 27 17:10:16.699: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 27 17:10:16.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 27 17:10:16.761: INFO: stderr: ""
Sep 27 17:10:16.761: INFO: stdout: "update-demo-nautilus-hnl44 update-demo-nautilus-z56q6 "
Sep 27 17:10:16.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods update-demo-nautilus-hnl44 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 27 17:10:16.817: INFO: stderr: ""
Sep 27 17:10:16.817: INFO: stdout: ""
Sep 27 17:10:16.817: INFO: update-demo-nautilus-hnl44 is created but not running
Sep 27 17:10:21.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 27 17:10:21.885: INFO: stderr: ""
Sep 27 17:10:21.885: INFO: stdout: "update-demo-nautilus-hnl44 update-demo-nautilus-z56q6 "
Sep 27 17:10:21.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods update-demo-nautilus-hnl44 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 27 17:10:21.947: INFO: stderr: ""
Sep 27 17:10:21.947: INFO: stdout: "true"
Sep 27 17:10:21.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods update-demo-nautilus-hnl44 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 27 17:10:22.003: INFO: stderr: ""
Sep 27 17:10:22.003: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Sep 27 17:10:22.003: INFO: validating pod update-demo-nautilus-hnl44
Sep 27 17:10:22.009: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 17:10:22.009: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 17:10:22.009: INFO: update-demo-nautilus-hnl44 is verified up and running
Sep 27 17:10:22.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods update-demo-nautilus-z56q6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 27 17:10:22.068: INFO: stderr: ""
Sep 27 17:10:22.068: INFO: stdout: "true"
Sep 27 17:10:22.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods update-demo-nautilus-z56q6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 27 17:10:22.123: INFO: stderr: ""
Sep 27 17:10:22.123: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Sep 27 17:10:22.123: INFO: validating pod update-demo-nautilus-z56q6
Sep 27 17:10:22.131: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 17:10:22.131: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 17:10:22.131: INFO: update-demo-nautilus-z56q6 is verified up and running
STEP: scaling down the replication controller
Sep 27 17:10:22.133: INFO: scanned /root for discovery docs: <nil>
Sep 27 17:10:22.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep 27 17:10:23.209: INFO: stderr: ""
Sep 27 17:10:23.209: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 27 17:10:23.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 27 17:10:23.271: INFO: stderr: ""
Sep 27 17:10:23.271: INFO: stdout: "update-demo-nautilus-hnl44 update-demo-nautilus-z56q6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 27 17:10:28.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 27 17:10:28.336: INFO: stderr: ""
Sep 27 17:10:28.336: INFO: stdout: "update-demo-nautilus-hnl44 update-demo-nautilus-z56q6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 27 17:10:33.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 27 17:10:33.400: INFO: stderr: ""
Sep 27 17:10:33.400: INFO: stdout: "update-demo-nautilus-hnl44 "
Sep 27 17:10:33.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods update-demo-nautilus-hnl44 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 27 17:10:33.461: INFO: stderr: ""
Sep 27 17:10:33.461: INFO: stdout: "true"
Sep 27 17:10:33.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods update-demo-nautilus-hnl44 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 27 17:10:33.519: INFO: stderr: ""
Sep 27 17:10:33.519: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Sep 27 17:10:33.519: INFO: validating pod update-demo-nautilus-hnl44
Sep 27 17:10:33.523: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 17:10:33.523: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 17:10:33.523: INFO: update-demo-nautilus-hnl44 is verified up and running
STEP: scaling up the replication controller
Sep 27 17:10:33.526: INFO: scanned /root for discovery docs: <nil>
Sep 27 17:10:33.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep 27 17:10:34.612: INFO: stderr: ""
Sep 27 17:10:34.613: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 27 17:10:34.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep 27 17:10:34.674: INFO: stderr: ""
Sep 27 17:10:34.674: INFO: stdout: "update-demo-nautilus-5d6jb update-demo-nautilus-hnl44 "
Sep 27 17:10:34.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods update-demo-nautilus-5d6jb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 27 17:10:34.731: INFO: stderr: ""
Sep 27 17:10:34.731: INFO: stdout: "true"
Sep 27 17:10:34.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods update-demo-nautilus-5d6jb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 27 17:10:34.792: INFO: stderr: ""
Sep 27 17:10:34.792: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Sep 27 17:10:34.792: INFO: validating pod update-demo-nautilus-5d6jb
Sep 27 17:10:34.800: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 17:10:34.800: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 17:10:34.800: INFO: update-demo-nautilus-5d6jb is verified up and running
Sep 27 17:10:34.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods update-demo-nautilus-hnl44 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep 27 17:10:34.857: INFO: stderr: ""
Sep 27 17:10:34.857: INFO: stdout: "true"
Sep 27 17:10:34.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods update-demo-nautilus-hnl44 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep 27 17:10:34.916: INFO: stderr: ""
Sep 27 17:10:34.916: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Sep 27 17:10:34.916: INFO: validating pod update-demo-nautilus-hnl44
Sep 27 17:10:34.920: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 27 17:10:34.920: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 27 17:10:34.920: INFO: update-demo-nautilus-hnl44 is verified up and running
STEP: using delete to clean up resources
Sep 27 17:10:34.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 delete --grace-period=0 --force -f -'
Sep 27 17:10:34.985: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 17:10:34.985: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 27 17:10:34.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get rc,svc -l name=update-demo --no-headers'
Sep 27 17:10:35.047: INFO: stderr: "No resources found in kubectl-3474 namespace.\n"
Sep 27 17:10:35.048: INFO: stdout: ""
Sep 27 17:10:35.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-3474 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 27 17:10:35.109: INFO: stderr: ""
Sep 27 17:10:35.109: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:10:35.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3474" for this suite.

• [SLOW TEST:18.796 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":339,"completed":225,"skipped":3505,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:10:35.122: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:10:35.181: INFO: Create a RollingUpdate DaemonSet
Sep 27 17:10:35.188: INFO: Check that daemon pods launch on every node of the cluster
Sep 27 17:10:35.194: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:35.194: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:35.194: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:35.198: INFO: Number of nodes with available pods: 0
Sep 27 17:10:35.198: INFO: Node ip-10-0-31-225.us-east-2.compute.internal is running more than one daemon pod
Sep 27 17:10:36.205: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:36.205: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:36.205: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:36.209: INFO: Number of nodes with available pods: 2
Sep 27 17:10:36.209: INFO: Node ip-10-0-95-24.us-east-2.compute.internal is running more than one daemon pod
Sep 27 17:10:37.205: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:37.205: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:37.205: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:37.209: INFO: Number of nodes with available pods: 3
Sep 27 17:10:37.209: INFO: Number of running nodes: 3, number of available pods: 3
Sep 27 17:10:37.209: INFO: Update the DaemonSet to trigger a rollout
Sep 27 17:10:37.219: INFO: Updating DaemonSet daemon-set
Sep 27 17:10:52.239: INFO: Roll back the DaemonSet before rollout is complete
Sep 27 17:10:52.249: INFO: Updating DaemonSet daemon-set
Sep 27 17:10:52.249: INFO: Make sure DaemonSet rollback is complete
Sep 27 17:10:52.253: INFO: Wrong image for pod: daemon-set-bwqzl. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Sep 27 17:10:52.253: INFO: Pod daemon-set-bwqzl is not available
Sep 27 17:10:52.257: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:52.258: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:52.258: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:53.270: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:53.270: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:53.270: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:54.268: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:54.268: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:54.268: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:55.267: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:55.267: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:55.267: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:56.269: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:56.269: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:56.269: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:57.269: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:57.269: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:57.269: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:58.269: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:58.269: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:58.269: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:59.269: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:59.269: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:10:59.269: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:11:00.269: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:11:00.269: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:11:00.269: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:11:01.268: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:11:01.269: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:11:01.269: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:11:02.264: INFO: Pod daemon-set-vw867 is not available
Sep 27 17:11:02.269: INFO: DaemonSet pods can't tolerate node ip-10-0-23-154.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:11:02.269: INFO: DaemonSet pods can't tolerate node ip-10-0-62-20.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep 27 17:11:02.269: INFO: DaemonSet pods can't tolerate node ip-10-0-87-99.us-east-2.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4948, will wait for the garbage collector to delete the pods
Sep 27 17:11:02.338: INFO: Deleting DaemonSet.extensions daemon-set took: 7.065213ms
Sep 27 17:11:02.439: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.266198ms
Sep 27 17:11:12.144: INFO: Number of nodes with available pods: 0
Sep 27 17:11:12.144: INFO: Number of running nodes: 0, number of available pods: 0
Sep 27 17:11:12.147: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1723908"},"items":null}

Sep 27 17:11:12.151: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1723908"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:11:12.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4948" for this suite.

• [SLOW TEST:37.054 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":339,"completed":226,"skipped":3529,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:11:12.177: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-7d5a23a3-bf5a-4de9-9f36-7fa89d07925c
STEP: Creating a pod to test consume configMaps
Sep 27 17:11:12.222: INFO: Waiting up to 5m0s for pod "pod-configmaps-346db035-c50a-40a1-9e93-b94e3f986b06" in namespace "configmap-1720" to be "Succeeded or Failed"
Sep 27 17:11:12.225: INFO: Pod "pod-configmaps-346db035-c50a-40a1-9e93-b94e3f986b06": Phase="Pending", Reason="", readiness=false. Elapsed: 3.531281ms
Sep 27 17:11:14.231: INFO: Pod "pod-configmaps-346db035-c50a-40a1-9e93-b94e3f986b06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009175864s
STEP: Saw pod success
Sep 27 17:11:14.231: INFO: Pod "pod-configmaps-346db035-c50a-40a1-9e93-b94e3f986b06" satisfied condition "Succeeded or Failed"
Sep 27 17:11:14.235: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-configmaps-346db035-c50a-40a1-9e93-b94e3f986b06 container agnhost-container: <nil>
STEP: delete the pod
Sep 27 17:11:14.257: INFO: Waiting for pod pod-configmaps-346db035-c50a-40a1-9e93-b94e3f986b06 to disappear
Sep 27 17:11:14.260: INFO: Pod pod-configmaps-346db035-c50a-40a1-9e93-b94e3f986b06 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:11:14.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1720" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":227,"skipped":3536,"failed":0}
SSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:11:14.277: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Sep 27 17:11:14.317: INFO: Waiting up to 5m0s for pod "client-containers-0acd94cc-256a-479b-8e94-f8fd283395d0" in namespace "containers-2208" to be "Succeeded or Failed"
Sep 27 17:11:14.320: INFO: Pod "client-containers-0acd94cc-256a-479b-8e94-f8fd283395d0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.418442ms
Sep 27 17:11:16.326: INFO: Pod "client-containers-0acd94cc-256a-479b-8e94-f8fd283395d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009012707s
STEP: Saw pod success
Sep 27 17:11:16.326: INFO: Pod "client-containers-0acd94cc-256a-479b-8e94-f8fd283395d0" satisfied condition "Succeeded or Failed"
Sep 27 17:11:16.329: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod client-containers-0acd94cc-256a-479b-8e94-f8fd283395d0 container agnhost-container: <nil>
STEP: delete the pod
Sep 27 17:11:16.353: INFO: Waiting for pod client-containers-0acd94cc-256a-479b-8e94-f8fd283395d0 to disappear
Sep 27 17:11:16.356: INFO: Pod client-containers-0acd94cc-256a-479b-8e94-f8fd283395d0 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:11:16.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2208" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":339,"completed":228,"skipped":3540,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:11:16.367: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:11:16.414: INFO: created pod
Sep 27 17:11:16.414: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7675" to be "Succeeded or Failed"
Sep 27 17:11:16.419: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.766883ms
Sep 27 17:11:18.424: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010299683s
STEP: Saw pod success
Sep 27 17:11:18.424: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Sep 27 17:11:48.425: INFO: polling logs
Sep 27 17:11:48.434: INFO: Pod logs: 
2021/09/27 17:11:17 OK: Got token
2021/09/27 17:11:17 validating with in-cluster discovery
2021/09/27 17:11:17 OK: got issuer https://kubernetes.default.svc.cluster.local
2021/09/27 17:11:17 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7675:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1632763276, NotBefore:1632762676, IssuedAt:1632762676, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7675", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b5a2ea49-b57e-41b1-8e53-b5c9f2d014f9"}}}
2021/09/27 17:11:17 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2021/09/27 17:11:17 OK: Validated signature on JWT
2021/09/27 17:11:17 OK: Got valid claims from token!
2021/09/27 17:11:17 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7675:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1632763276, NotBefore:1632762676, IssuedAt:1632762676, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7675", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b5a2ea49-b57e-41b1-8e53-b5c9f2d014f9"}}}

Sep 27 17:11:48.434: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:11:48.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7675" for this suite.

• [SLOW TEST:32.087 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":339,"completed":229,"skipped":3557,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:11:48.454: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:11:51.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-444" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":339,"completed":230,"skipped":3565,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:11:51.146: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:12:04.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4439" for this suite.
STEP: Destroying namespace "nsdeletetest-9351" for this suite.
Sep 27 17:12:04.293: INFO: Namespace nsdeletetest-9351 was already deleted
STEP: Destroying namespace "nsdeletetest-3510" for this suite.

• [SLOW TEST:13.154 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":339,"completed":231,"skipped":3582,"failed":0}
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:12:04.300: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:12:04.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4100" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":339,"completed":232,"skipped":3582,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:12:04.349: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Sep 27 17:12:06.406: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7993 PodName:var-expansion-e89a54be-de69-4623-87f5-fdbd5f5045b7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:12:06.406: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: test for file in mounted path
Sep 27 17:12:06.478: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7993 PodName:var-expansion-e89a54be-de69-4623-87f5-fdbd5f5045b7 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:12:06.478: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: updating the annotation value
Sep 27 17:12:07.031: INFO: Successfully updated pod "var-expansion-e89a54be-de69-4623-87f5-fdbd5f5045b7"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Sep 27 17:12:07.035: INFO: Deleting pod "var-expansion-e89a54be-de69-4623-87f5-fdbd5f5045b7" in namespace "var-expansion-7993"
Sep 27 17:12:07.043: INFO: Wait up to 5m0s for pod "var-expansion-e89a54be-de69-4623-87f5-fdbd5f5045b7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:12:53.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7993" for this suite.

• [SLOW TEST:48.718 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":339,"completed":233,"skipped":3587,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:12:53.067: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Sep 27 17:12:53.125: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Sep 27 17:12:55.144: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Sep 27 17:12:57.170: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:12:59.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-4055" for this suite.

• [SLOW TEST:6.121 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":339,"completed":234,"skipped":3613,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:12:59.189: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2984, will wait for the garbage collector to delete the pods
Sep 27 17:13:01.298: INFO: Deleting Job.batch foo took: 8.362613ms
Sep 27 17:13:01.399: INFO: Terminating Job.batch foo pods took: 101.022983ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:13:42.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2984" for this suite.

• [SLOW TEST:42.929 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":339,"completed":235,"skipped":3620,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:13:42.117: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Sep 27 17:13:42.161: INFO: Waiting up to 5m0s for pod "downward-api-a832a949-27b2-4fcc-85f5-e7833346d34c" in namespace "downward-api-8673" to be "Succeeded or Failed"
Sep 27 17:13:42.164: INFO: Pod "downward-api-a832a949-27b2-4fcc-85f5-e7833346d34c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.729963ms
Sep 27 17:13:44.169: INFO: Pod "downward-api-a832a949-27b2-4fcc-85f5-e7833346d34c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008257419s
STEP: Saw pod success
Sep 27 17:13:44.169: INFO: Pod "downward-api-a832a949-27b2-4fcc-85f5-e7833346d34c" satisfied condition "Succeeded or Failed"
Sep 27 17:13:44.173: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downward-api-a832a949-27b2-4fcc-85f5-e7833346d34c container dapi-container: <nil>
STEP: delete the pod
Sep 27 17:13:44.203: INFO: Waiting for pod downward-api-a832a949-27b2-4fcc-85f5-e7833346d34c to disappear
Sep 27 17:13:44.207: INFO: Pod downward-api-a832a949-27b2-4fcc-85f5-e7833346d34c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:13:44.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8673" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":339,"completed":236,"skipped":3627,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:13:44.218: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:13:44.678: INFO: Checking APIGroup: apiregistration.k8s.io
Sep 27 17:13:44.680: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep 27 17:13:44.680: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.680: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep 27 17:13:44.680: INFO: Checking APIGroup: apps
Sep 27 17:13:44.681: INFO: PreferredVersion.GroupVersion: apps/v1
Sep 27 17:13:44.681: INFO: Versions found [{apps/v1 v1}]
Sep 27 17:13:44.681: INFO: apps/v1 matches apps/v1
Sep 27 17:13:44.681: INFO: Checking APIGroup: events.k8s.io
Sep 27 17:13:44.682: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep 27 17:13:44.682: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.682: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep 27 17:13:44.682: INFO: Checking APIGroup: authentication.k8s.io
Sep 27 17:13:44.683: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep 27 17:13:44.683: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.683: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep 27 17:13:44.683: INFO: Checking APIGroup: authorization.k8s.io
Sep 27 17:13:44.684: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep 27 17:13:44.684: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.684: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep 27 17:13:44.684: INFO: Checking APIGroup: autoscaling
Sep 27 17:13:44.686: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Sep 27 17:13:44.686: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Sep 27 17:13:44.686: INFO: autoscaling/v1 matches autoscaling/v1
Sep 27 17:13:44.686: INFO: Checking APIGroup: batch
Sep 27 17:13:44.687: INFO: PreferredVersion.GroupVersion: batch/v1
Sep 27 17:13:44.687: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Sep 27 17:13:44.687: INFO: batch/v1 matches batch/v1
Sep 27 17:13:44.687: INFO: Checking APIGroup: certificates.k8s.io
Sep 27 17:13:44.688: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep 27 17:13:44.688: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.688: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep 27 17:13:44.688: INFO: Checking APIGroup: networking.k8s.io
Sep 27 17:13:44.689: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep 27 17:13:44.689: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.689: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep 27 17:13:44.689: INFO: Checking APIGroup: extensions
Sep 27 17:13:44.691: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Sep 27 17:13:44.691: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Sep 27 17:13:44.691: INFO: extensions/v1beta1 matches extensions/v1beta1
Sep 27 17:13:44.691: INFO: Checking APIGroup: policy
Sep 27 17:13:44.692: INFO: PreferredVersion.GroupVersion: policy/v1
Sep 27 17:13:44.692: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Sep 27 17:13:44.692: INFO: policy/v1 matches policy/v1
Sep 27 17:13:44.692: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep 27 17:13:44.693: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep 27 17:13:44.693: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.693: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep 27 17:13:44.693: INFO: Checking APIGroup: storage.k8s.io
Sep 27 17:13:44.694: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep 27 17:13:44.694: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.694: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep 27 17:13:44.694: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep 27 17:13:44.695: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep 27 17:13:44.695: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.695: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep 27 17:13:44.695: INFO: Checking APIGroup: apiextensions.k8s.io
Sep 27 17:13:44.696: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep 27 17:13:44.696: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.696: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep 27 17:13:44.696: INFO: Checking APIGroup: scheduling.k8s.io
Sep 27 17:13:44.698: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep 27 17:13:44.698: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.698: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep 27 17:13:44.698: INFO: Checking APIGroup: coordination.k8s.io
Sep 27 17:13:44.699: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep 27 17:13:44.699: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.699: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep 27 17:13:44.699: INFO: Checking APIGroup: node.k8s.io
Sep 27 17:13:44.700: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep 27 17:13:44.700: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.700: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep 27 17:13:44.700: INFO: Checking APIGroup: discovery.k8s.io
Sep 27 17:13:44.701: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Sep 27 17:13:44.701: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.701: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Sep 27 17:13:44.701: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep 27 17:13:44.702: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Sep 27 17:13:44.702: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.702: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Sep 27 17:13:44.702: INFO: Checking APIGroup: acme.cert-manager.io
Sep 27 17:13:44.703: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Sep 27 17:13:44.703: INFO: Versions found [{acme.cert-manager.io/v1 v1} {acme.cert-manager.io/v1beta1 v1beta1} {acme.cert-manager.io/v1alpha3 v1alpha3} {acme.cert-manager.io/v1alpha2 v1alpha2}]
Sep 27 17:13:44.703: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Sep 27 17:13:44.703: INFO: Checking APIGroup: cert-manager.io
Sep 27 17:13:44.704: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Sep 27 17:13:44.704: INFO: Versions found [{cert-manager.io/v1 v1} {cert-manager.io/v1beta1 v1beta1} {cert-manager.io/v1alpha3 v1alpha3} {cert-manager.io/v1alpha2 v1alpha2}]
Sep 27 17:13:44.705: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Sep 27 17:13:44.705: INFO: Checking APIGroup: crd.antrea.tanzu.vmware.com
Sep 27 17:13:44.706: INFO: PreferredVersion.GroupVersion: crd.antrea.tanzu.vmware.com/v1alpha1
Sep 27 17:13:44.706: INFO: Versions found [{crd.antrea.tanzu.vmware.com/v1alpha1 v1alpha1}]
Sep 27 17:13:44.706: INFO: crd.antrea.tanzu.vmware.com/v1alpha1 matches crd.antrea.tanzu.vmware.com/v1alpha1
Sep 27 17:13:44.706: INFO: Checking APIGroup: internal.packaging.carvel.dev
Sep 27 17:13:44.707: INFO: PreferredVersion.GroupVersion: internal.packaging.carvel.dev/v1alpha1
Sep 27 17:13:44.707: INFO: Versions found [{internal.packaging.carvel.dev/v1alpha1 v1alpha1}]
Sep 27 17:13:44.707: INFO: internal.packaging.carvel.dev/v1alpha1 matches internal.packaging.carvel.dev/v1alpha1
Sep 27 17:13:44.707: INFO: Checking APIGroup: kappctrl.k14s.io
Sep 27 17:13:44.708: INFO: PreferredVersion.GroupVersion: kappctrl.k14s.io/v1alpha1
Sep 27 17:13:44.708: INFO: Versions found [{kappctrl.k14s.io/v1alpha1 v1alpha1}]
Sep 27 17:13:44.708: INFO: kappctrl.k14s.io/v1alpha1 matches kappctrl.k14s.io/v1alpha1
Sep 27 17:13:44.708: INFO: Checking APIGroup: ops.antrea.tanzu.vmware.com
Sep 27 17:13:44.709: INFO: PreferredVersion.GroupVersion: ops.antrea.tanzu.vmware.com/v1alpha1
Sep 27 17:13:44.709: INFO: Versions found [{ops.antrea.tanzu.vmware.com/v1alpha1 v1alpha1}]
Sep 27 17:13:44.709: INFO: ops.antrea.tanzu.vmware.com/v1alpha1 matches ops.antrea.tanzu.vmware.com/v1alpha1
Sep 27 17:13:44.709: INFO: Checking APIGroup: packaging.carvel.dev
Sep 27 17:13:44.710: INFO: PreferredVersion.GroupVersion: packaging.carvel.dev/v1alpha1
Sep 27 17:13:44.710: INFO: Versions found [{packaging.carvel.dev/v1alpha1 v1alpha1}]
Sep 27 17:13:44.710: INFO: packaging.carvel.dev/v1alpha1 matches packaging.carvel.dev/v1alpha1
Sep 27 17:13:44.710: INFO: Checking APIGroup: run.tanzu.vmware.com
Sep 27 17:13:44.711: INFO: PreferredVersion.GroupVersion: run.tanzu.vmware.com/v1alpha1
Sep 27 17:13:44.711: INFO: Versions found [{run.tanzu.vmware.com/v1alpha1 v1alpha1}]
Sep 27 17:13:44.711: INFO: run.tanzu.vmware.com/v1alpha1 matches run.tanzu.vmware.com/v1alpha1
Sep 27 17:13:44.711: INFO: Checking APIGroup: security.antrea.tanzu.vmware.com
Sep 27 17:13:44.713: INFO: PreferredVersion.GroupVersion: security.antrea.tanzu.vmware.com/v1alpha1
Sep 27 17:13:44.713: INFO: Versions found [{security.antrea.tanzu.vmware.com/v1alpha1 v1alpha1}]
Sep 27 17:13:44.713: INFO: security.antrea.tanzu.vmware.com/v1alpha1 matches security.antrea.tanzu.vmware.com/v1alpha1
Sep 27 17:13:44.713: INFO: Checking APIGroup: bootstrap.cluster.x-k8s.io
Sep 27 17:13:44.714: INFO: PreferredVersion.GroupVersion: bootstrap.cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.714: INFO: Versions found [{bootstrap.cluster.x-k8s.io/v1alpha3 v1alpha3} {bootstrap.cluster.x-k8s.io/v1alpha2 v1alpha2}]
Sep 27 17:13:44.714: INFO: bootstrap.cluster.x-k8s.io/v1alpha3 matches bootstrap.cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.714: INFO: Checking APIGroup: cluster.x-k8s.io
Sep 27 17:13:44.715: INFO: PreferredVersion.GroupVersion: cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.715: INFO: Versions found [{cluster.x-k8s.io/v1alpha3 v1alpha3} {cluster.x-k8s.io/v1alpha2 v1alpha2}]
Sep 27 17:13:44.715: INFO: cluster.x-k8s.io/v1alpha3 matches cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.715: INFO: Checking APIGroup: core.antrea.tanzu.vmware.com
Sep 27 17:13:44.716: INFO: PreferredVersion.GroupVersion: core.antrea.tanzu.vmware.com/v1alpha2
Sep 27 17:13:44.716: INFO: Versions found [{core.antrea.tanzu.vmware.com/v1alpha2 v1alpha2}]
Sep 27 17:13:44.716: INFO: core.antrea.tanzu.vmware.com/v1alpha2 matches core.antrea.tanzu.vmware.com/v1alpha2
Sep 27 17:13:44.716: INFO: Checking APIGroup: infrastructure.cluster.x-k8s.io
Sep 27 17:13:44.717: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.717: INFO: Versions found [{infrastructure.cluster.x-k8s.io/v1alpha3 v1alpha3} {infrastructure.cluster.x-k8s.io/v1alpha2 v1alpha2}]
Sep 27 17:13:44.717: INFO: infrastructure.cluster.x-k8s.io/v1alpha3 matches infrastructure.cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.717: INFO: Checking APIGroup: addons.cluster.x-k8s.io
Sep 27 17:13:44.718: INFO: PreferredVersion.GroupVersion: addons.cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.718: INFO: Versions found [{addons.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Sep 27 17:13:44.718: INFO: addons.cluster.x-k8s.io/v1alpha3 matches addons.cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.718: INFO: Checking APIGroup: clusterctl.cluster.x-k8s.io
Sep 27 17:13:44.719: INFO: PreferredVersion.GroupVersion: clusterctl.cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.719: INFO: Versions found [{clusterctl.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Sep 27 17:13:44.719: INFO: clusterctl.cluster.x-k8s.io/v1alpha3 matches clusterctl.cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.719: INFO: Checking APIGroup: controlplane.cluster.x-k8s.io
Sep 27 17:13:44.720: INFO: PreferredVersion.GroupVersion: controlplane.cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.720: INFO: Versions found [{controlplane.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Sep 27 17:13:44.720: INFO: controlplane.cluster.x-k8s.io/v1alpha3 matches controlplane.cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.720: INFO: Checking APIGroup: exp.cluster.x-k8s.io
Sep 27 17:13:44.722: INFO: PreferredVersion.GroupVersion: exp.cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.722: INFO: Versions found [{exp.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Sep 27 17:13:44.722: INFO: exp.cluster.x-k8s.io/v1alpha3 matches exp.cluster.x-k8s.io/v1alpha3
Sep 27 17:13:44.722: INFO: Checking APIGroup: clusterinformation.antrea.tanzu.vmware.com
Sep 27 17:13:44.723: INFO: PreferredVersion.GroupVersion: clusterinformation.antrea.tanzu.vmware.com/v1beta1
Sep 27 17:13:44.723: INFO: Versions found [{clusterinformation.antrea.tanzu.vmware.com/v1beta1 v1beta1}]
Sep 27 17:13:44.723: INFO: clusterinformation.antrea.tanzu.vmware.com/v1beta1 matches clusterinformation.antrea.tanzu.vmware.com/v1beta1
Sep 27 17:13:44.723: INFO: Checking APIGroup: data.packaging.carvel.dev
Sep 27 17:13:44.724: INFO: PreferredVersion.GroupVersion: data.packaging.carvel.dev/v1alpha1
Sep 27 17:13:44.724: INFO: Versions found [{data.packaging.carvel.dev/v1alpha1 v1alpha1}]
Sep 27 17:13:44.724: INFO: data.packaging.carvel.dev/v1alpha1 matches data.packaging.carvel.dev/v1alpha1
Sep 27 17:13:44.724: INFO: Checking APIGroup: stats.antrea.tanzu.vmware.com
Sep 27 17:13:44.725: INFO: PreferredVersion.GroupVersion: stats.antrea.tanzu.vmware.com/v1alpha1
Sep 27 17:13:44.725: INFO: Versions found [{stats.antrea.tanzu.vmware.com/v1alpha1 v1alpha1}]
Sep 27 17:13:44.725: INFO: stats.antrea.tanzu.vmware.com/v1alpha1 matches stats.antrea.tanzu.vmware.com/v1alpha1
Sep 27 17:13:44.725: INFO: Checking APIGroup: controlplane.antrea.tanzu.vmware.com
Sep 27 17:13:44.726: INFO: PreferredVersion.GroupVersion: controlplane.antrea.tanzu.vmware.com/v1beta2
Sep 27 17:13:44.726: INFO: Versions found [{controlplane.antrea.tanzu.vmware.com/v1beta2 v1beta2} {controlplane.antrea.tanzu.vmware.com/v1beta1 v1beta1}]
Sep 27 17:13:44.726: INFO: controlplane.antrea.tanzu.vmware.com/v1beta2 matches controlplane.antrea.tanzu.vmware.com/v1beta2
Sep 27 17:13:44.726: INFO: Checking APIGroup: metrics.k8s.io
Sep 27 17:13:44.727: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Sep 27 17:13:44.727: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Sep 27 17:13:44.727: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
Sep 27 17:13:44.727: INFO: Checking APIGroup: networking.antrea.tanzu.vmware.com
Sep 27 17:13:44.728: INFO: PreferredVersion.GroupVersion: networking.antrea.tanzu.vmware.com/v1beta1
Sep 27 17:13:44.728: INFO: Versions found [{networking.antrea.tanzu.vmware.com/v1beta1 v1beta1}]
Sep 27 17:13:44.728: INFO: networking.antrea.tanzu.vmware.com/v1beta1 matches networking.antrea.tanzu.vmware.com/v1beta1
Sep 27 17:13:44.728: INFO: Checking APIGroup: system.antrea.tanzu.vmware.com
Sep 27 17:13:44.729: INFO: PreferredVersion.GroupVersion: system.antrea.tanzu.vmware.com/v1beta1
Sep 27 17:13:44.729: INFO: Versions found [{system.antrea.tanzu.vmware.com/v1beta1 v1beta1}]
Sep 27 17:13:44.729: INFO: system.antrea.tanzu.vmware.com/v1beta1 matches system.antrea.tanzu.vmware.com/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:13:44.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5267" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":339,"completed":237,"skipped":3642,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:13:44.742: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-a67ee931-f759-4ab4-91e3-95daa8522827
STEP: Creating a pod to test consume configMaps
Sep 27 17:13:44.787: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-47ef2d18-2051-4c8f-b936-7d39bd8ea528" in namespace "projected-4360" to be "Succeeded or Failed"
Sep 27 17:13:44.790: INFO: Pod "pod-projected-configmaps-47ef2d18-2051-4c8f-b936-7d39bd8ea528": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490605ms
Sep 27 17:13:46.795: INFO: Pod "pod-projected-configmaps-47ef2d18-2051-4c8f-b936-7d39bd8ea528": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008170202s
STEP: Saw pod success
Sep 27 17:13:46.795: INFO: Pod "pod-projected-configmaps-47ef2d18-2051-4c8f-b936-7d39bd8ea528" satisfied condition "Succeeded or Failed"
Sep 27 17:13:46.799: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-projected-configmaps-47ef2d18-2051-4c8f-b936-7d39bd8ea528 container agnhost-container: <nil>
STEP: delete the pod
Sep 27 17:13:46.819: INFO: Waiting for pod pod-projected-configmaps-47ef2d18-2051-4c8f-b936-7d39bd8ea528 to disappear
Sep 27 17:13:46.823: INFO: Pod pod-projected-configmaps-47ef2d18-2051-4c8f-b936-7d39bd8ea528 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:13:46.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4360" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":238,"skipped":3680,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:13:46.834: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:13:57.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6274" for this suite.

• [SLOW TEST:11.091 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":339,"completed":239,"skipped":3685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:13:57.926: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 27 17:13:57.976: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1046  54c0d7b6-e7dc-4c3b-a2a7-e1d4007a8f29 1725195 0 2021-09-27 17:13:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-27 17:13:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 17:13:57.976: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1046  54c0d7b6-e7dc-4c3b-a2a7-e1d4007a8f29 1725196 0 2021-09-27 17:13:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-27 17:13:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 27 17:13:57.993: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1046  54c0d7b6-e7dc-4c3b-a2a7-e1d4007a8f29 1725197 0 2021-09-27 17:13:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-27 17:13:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 17:13:57.993: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1046  54c0d7b6-e7dc-4c3b-a2a7-e1d4007a8f29 1725198 0 2021-09-27 17:13:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-27 17:13:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:13:57.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1046" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":339,"completed":240,"skipped":3738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:13:58.005: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 27 17:13:58.070: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7189  ad39614a-5311-4d2b-b161-878c57673907 1725207 0 2021-09-27 17:13:58 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-09-27 17:13:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep 27 17:13:58.070: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7189  ad39614a-5311-4d2b-b161-878c57673907 1725208 0 2021-09-27 17:13:58 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-09-27 17:13:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:13:58.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7189" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":339,"completed":241,"skipped":3765,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:13:58.082: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 17:13:58.120: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f1479af-b4ce-4b71-a780-b7c451ecab18" in namespace "projected-3871" to be "Succeeded or Failed"
Sep 27 17:13:58.123: INFO: Pod "downwardapi-volume-1f1479af-b4ce-4b71-a780-b7c451ecab18": Phase="Pending", Reason="", readiness=false. Elapsed: 3.507389ms
Sep 27 17:14:00.128: INFO: Pod "downwardapi-volume-1f1479af-b4ce-4b71-a780-b7c451ecab18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007842178s
STEP: Saw pod success
Sep 27 17:14:00.128: INFO: Pod "downwardapi-volume-1f1479af-b4ce-4b71-a780-b7c451ecab18" satisfied condition "Succeeded or Failed"
Sep 27 17:14:00.131: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downwardapi-volume-1f1479af-b4ce-4b71-a780-b7c451ecab18 container client-container: <nil>
STEP: delete the pod
Sep 27 17:14:00.181: INFO: Waiting for pod downwardapi-volume-1f1479af-b4ce-4b71-a780-b7c451ecab18 to disappear
Sep 27 17:14:00.185: INFO: Pod downwardapi-volume-1f1479af-b4ce-4b71-a780-b7c451ecab18 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:14:00.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3871" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":339,"completed":242,"skipped":3783,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:14:00.197: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:14:00.236: INFO: Creating simple deployment test-new-deployment
Sep 27 17:14:00.249: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Sep 27 17:14:02.299: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-189  f285ce8e-3890-46ca-b6bd-6e5cecd445c0 1725275 3 2021-09-27 17:14:00 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-09-27 17:14:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 17:14:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032ba8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-27 17:14:01 +0000 UTC,LastTransitionTime:2021-09-27 17:14:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-09-27 17:14:01 +0000 UTC,LastTransitionTime:2021-09-27 17:14:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 27 17:14:02.313: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-189  7dc6325f-53bf-4e55-9f6e-d111ffd2eac6 1725281 2 2021-09-27 17:14:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment f285ce8e-3890-46ca-b6bd-6e5cecd445c0 0xc0032bacd7 0xc0032bacd8}] []  [{kube-controller-manager Update apps/v1 2021-09-27 17:14:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f285ce8e-3890-46ca-b6bd-6e5cecd445c0\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032bad48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 27 17:14:02.317: INFO: Pod "test-new-deployment-847dcfb7fb-lgxwt" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-lgxwt test-new-deployment-847dcfb7fb- deployment-189  a3d52126-0c33-4784-80c5-eb38a15eb79f 1725280 0 2021-09-27 17:14:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 7dc6325f-53bf-4e55-9f6e-d111ffd2eac6 0xc0032bb177 0xc0032bb178}] []  [{kube-controller-manager Update v1 2021-09-27 17:14:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7dc6325f-53bf-4e55-9f6e-d111ffd2eac6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zv47g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zv47g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:14:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:14:02.317: INFO: Pod "test-new-deployment-847dcfb7fb-mj45l" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-mj45l test-new-deployment-847dcfb7fb- deployment-189  9669182b-a192-4d66-a5b0-24766c0d76c4 1725270 0 2021-09-27 17:14:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb 7dc6325f-53bf-4e55-9f6e-d111ffd2eac6 0xc0032bb4d7 0xc0032bb4d8}] []  [{kube-controller-manager Update v1 2021-09-27 17:14:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7dc6325f-53bf-4e55-9f6e-d111ffd2eac6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:14:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dgjnv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dgjnv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:14:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:14:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:14:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:14:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.95.24,PodIP:100.96.4.180,StartTime:2021-09-27 17:14:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 17:14:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://182f7a86014055dd3d69a813489245410045a2c1b1b09ea99a7a2c08d087a3e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.180,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:14:02.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-189" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":339,"completed":243,"skipped":3813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:14:02.335: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Sep 27 17:14:02.383: INFO: Waiting up to 5m0s for pod "var-expansion-a127cdcf-6dd9-4148-9408-79f228781601" in namespace "var-expansion-8153" to be "Succeeded or Failed"
Sep 27 17:14:02.388: INFO: Pod "var-expansion-a127cdcf-6dd9-4148-9408-79f228781601": Phase="Pending", Reason="", readiness=false. Elapsed: 4.798278ms
Sep 27 17:14:04.393: INFO: Pod "var-expansion-a127cdcf-6dd9-4148-9408-79f228781601": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009578728s
STEP: Saw pod success
Sep 27 17:14:04.393: INFO: Pod "var-expansion-a127cdcf-6dd9-4148-9408-79f228781601" satisfied condition "Succeeded or Failed"
Sep 27 17:14:04.397: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod var-expansion-a127cdcf-6dd9-4148-9408-79f228781601 container dapi-container: <nil>
STEP: delete the pod
Sep 27 17:14:04.422: INFO: Waiting for pod var-expansion-a127cdcf-6dd9-4148-9408-79f228781601 to disappear
Sep 27 17:14:04.425: INFO: Pod var-expansion-a127cdcf-6dd9-4148-9408-79f228781601 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:14:04.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8153" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":339,"completed":244,"skipped":3851,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:14:04.438: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 27 17:14:04.485: INFO: Waiting up to 5m0s for pod "pod-a3d88919-6d9e-4103-8384-0f433049adbf" in namespace "emptydir-7146" to be "Succeeded or Failed"
Sep 27 17:14:04.491: INFO: Pod "pod-a3d88919-6d9e-4103-8384-0f433049adbf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.642635ms
Sep 27 17:14:06.497: INFO: Pod "pod-a3d88919-6d9e-4103-8384-0f433049adbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011724288s
STEP: Saw pod success
Sep 27 17:14:06.497: INFO: Pod "pod-a3d88919-6d9e-4103-8384-0f433049adbf" satisfied condition "Succeeded or Failed"
Sep 27 17:14:06.501: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-a3d88919-6d9e-4103-8384-0f433049adbf container test-container: <nil>
STEP: delete the pod
Sep 27 17:14:06.538: INFO: Waiting for pod pod-a3d88919-6d9e-4103-8384-0f433049adbf to disappear
Sep 27 17:14:06.546: INFO: Pod pod-a3d88919-6d9e-4103-8384-0f433049adbf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:14:06.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7146" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":245,"skipped":3864,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:14:06.569: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-fz9td in namespace proxy-9850
I0927 17:14:06.632728      23 runners.go:190] Created replication controller with name: proxy-service-fz9td, namespace: proxy-9850, replica count: 1
I0927 17:14:07.683889      23 runners.go:190] proxy-service-fz9td Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0927 17:14:08.684258      23 runners.go:190] proxy-service-fz9td Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 17:14:09.684370      23 runners.go:190] proxy-service-fz9td Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 17:14:10.685163      23 runners.go:190] proxy-service-fz9td Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 17:14:11.685262      23 runners.go:190] proxy-service-fz9td Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 17:14:12.685341      23 runners.go:190] proxy-service-fz9td Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 17:14:13.685571      23 runners.go:190] proxy-service-fz9td Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 17:14:14.685678      23 runners.go:190] proxy-service-fz9td Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 17:14:15.685817      23 runners.go:190] proxy-service-fz9td Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0927 17:14:16.686442      23 runners.go:190] proxy-service-fz9td Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 17:14:16.690: INFO: setup took 10.080638462s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 27 17:14:16.698: INFO: (0) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 7.153858ms)
Sep 27 17:14:16.699: INFO: (0) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 8.410458ms)
Sep 27 17:14:16.700: INFO: (0) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 8.990391ms)
Sep 27 17:14:16.700: INFO: (0) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 8.998088ms)
Sep 27 17:14:16.700: INFO: (0) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 9.141481ms)
Sep 27 17:14:16.700: INFO: (0) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 9.125297ms)
Sep 27 17:14:16.700: INFO: (0) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 9.202791ms)
Sep 27 17:14:16.703: INFO: (0) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 12.382ms)
Sep 27 17:14:16.703: INFO: (0) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 12.422809ms)
Sep 27 17:14:16.703: INFO: (0) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 12.37674ms)
Sep 27 17:14:16.703: INFO: (0) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 12.338494ms)
Sep 27 17:14:16.705: INFO: (0) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 14.12966ms)
Sep 27 17:14:16.705: INFO: (0) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 14.036896ms)
Sep 27 17:14:16.705: INFO: (0) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 14.002754ms)
Sep 27 17:14:16.705: INFO: (0) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 14.128753ms)
Sep 27 17:14:16.706: INFO: (0) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 15.845391ms)
Sep 27 17:14:16.713: INFO: (1) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 6.823334ms)
Sep 27 17:14:16.713: INFO: (1) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.833593ms)
Sep 27 17:14:16.713: INFO: (1) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.971934ms)
Sep 27 17:14:16.714: INFO: (1) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 7.034601ms)
Sep 27 17:14:16.714: INFO: (1) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 7.069098ms)
Sep 27 17:14:16.714: INFO: (1) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 7.021948ms)
Sep 27 17:14:16.714: INFO: (1) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 7.08425ms)
Sep 27 17:14:16.714: INFO: (1) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 7.130753ms)
Sep 27 17:14:16.714: INFO: (1) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 7.17567ms)
Sep 27 17:14:16.714: INFO: (1) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 7.159446ms)
Sep 27 17:14:16.715: INFO: (1) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 8.517939ms)
Sep 27 17:14:16.717: INFO: (1) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.081472ms)
Sep 27 17:14:16.717: INFO: (1) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 10.384797ms)
Sep 27 17:14:16.717: INFO: (1) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 10.24501ms)
Sep 27 17:14:16.717: INFO: (1) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 10.351594ms)
Sep 27 17:14:16.717: INFO: (1) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 10.353835ms)
Sep 27 17:14:16.722: INFO: (2) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 4.806862ms)
Sep 27 17:14:16.725: INFO: (2) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 7.4953ms)
Sep 27 17:14:16.725: INFO: (2) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 7.643188ms)
Sep 27 17:14:16.725: INFO: (2) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 7.648698ms)
Sep 27 17:14:16.725: INFO: (2) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 7.645894ms)
Sep 27 17:14:16.725: INFO: (2) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 7.747404ms)
Sep 27 17:14:16.725: INFO: (2) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 7.799132ms)
Sep 27 17:14:16.725: INFO: (2) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 7.773373ms)
Sep 27 17:14:16.725: INFO: (2) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 7.84308ms)
Sep 27 17:14:16.725: INFO: (2) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 7.87787ms)
Sep 27 17:14:16.727: INFO: (2) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 10.075621ms)
Sep 27 17:14:16.729: INFO: (2) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 12.058861ms)
Sep 27 17:14:16.729: INFO: (2) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 12.033582ms)
Sep 27 17:14:16.729: INFO: (2) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 12.043462ms)
Sep 27 17:14:16.729: INFO: (2) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 12.093714ms)
Sep 27 17:14:16.729: INFO: (2) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 12.106258ms)
Sep 27 17:14:16.736: INFO: (3) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 6.76086ms)
Sep 27 17:14:16.736: INFO: (3) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 6.790358ms)
Sep 27 17:14:16.736: INFO: (3) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.700849ms)
Sep 27 17:14:16.736: INFO: (3) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.740602ms)
Sep 27 17:14:16.736: INFO: (3) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.722273ms)
Sep 27 17:14:16.736: INFO: (3) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 6.85851ms)
Sep 27 17:14:16.736: INFO: (3) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 6.773853ms)
Sep 27 17:14:16.736: INFO: (3) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.96878ms)
Sep 27 17:14:16.736: INFO: (3) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 7.323304ms)
Sep 27 17:14:16.736: INFO: (3) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 7.241564ms)
Sep 27 17:14:16.737: INFO: (3) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 8.231677ms)
Sep 27 17:14:16.739: INFO: (3) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 9.966903ms)
Sep 27 17:14:16.739: INFO: (3) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 10.185334ms)
Sep 27 17:14:16.739: INFO: (3) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 10.142293ms)
Sep 27 17:14:16.739: INFO: (3) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.205513ms)
Sep 27 17:14:16.739: INFO: (3) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 10.274013ms)
Sep 27 17:14:16.744: INFO: (4) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 4.692176ms)
Sep 27 17:14:16.746: INFO: (4) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 6.537065ms)
Sep 27 17:14:16.746: INFO: (4) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 6.541137ms)
Sep 27 17:14:16.746: INFO: (4) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 6.687937ms)
Sep 27 17:14:16.746: INFO: (4) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.83726ms)
Sep 27 17:14:16.746: INFO: (4) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 6.760032ms)
Sep 27 17:14:16.746: INFO: (4) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.924019ms)
Sep 27 17:14:16.746: INFO: (4) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.799983ms)
Sep 27 17:14:16.747: INFO: (4) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 6.993548ms)
Sep 27 17:14:16.747: INFO: (4) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 6.947405ms)
Sep 27 17:14:16.748: INFO: (4) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 8.118511ms)
Sep 27 17:14:16.750: INFO: (4) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 10.092383ms)
Sep 27 17:14:16.750: INFO: (4) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 10.036833ms)
Sep 27 17:14:16.750: INFO: (4) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.203782ms)
Sep 27 17:14:16.750: INFO: (4) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 10.108839ms)
Sep 27 17:14:16.750: INFO: (4) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 10.243585ms)
Sep 27 17:14:16.755: INFO: (5) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 4.661988ms)
Sep 27 17:14:16.756: INFO: (5) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.296469ms)
Sep 27 17:14:16.756: INFO: (5) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 6.44012ms)
Sep 27 17:14:16.757: INFO: (5) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 6.773538ms)
Sep 27 17:14:16.757: INFO: (5) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 6.785243ms)
Sep 27 17:14:16.757: INFO: (5) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 6.801659ms)
Sep 27 17:14:16.757: INFO: (5) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 7.022903ms)
Sep 27 17:14:16.757: INFO: (5) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.993878ms)
Sep 27 17:14:16.757: INFO: (5) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 7.105015ms)
Sep 27 17:14:16.757: INFO: (5) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 7.142529ms)
Sep 27 17:14:16.758: INFO: (5) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 8.479384ms)
Sep 27 17:14:16.760: INFO: (5) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.315679ms)
Sep 27 17:14:16.761: INFO: (5) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 10.537998ms)
Sep 27 17:14:16.761: INFO: (5) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 10.607342ms)
Sep 27 17:14:16.761: INFO: (5) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 10.562397ms)
Sep 27 17:14:16.761: INFO: (5) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 10.658026ms)
Sep 27 17:14:16.765: INFO: (6) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 4.619997ms)
Sep 27 17:14:16.768: INFO: (6) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.847417ms)
Sep 27 17:14:16.768: INFO: (6) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 6.888179ms)
Sep 27 17:14:16.768: INFO: (6) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 6.904702ms)
Sep 27 17:14:16.768: INFO: (6) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 6.908128ms)
Sep 27 17:14:16.768: INFO: (6) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.995637ms)
Sep 27 17:14:16.768: INFO: (6) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 6.928903ms)
Sep 27 17:14:16.768: INFO: (6) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.952336ms)
Sep 27 17:14:16.768: INFO: (6) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 6.951556ms)
Sep 27 17:14:16.768: INFO: (6) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 7.007675ms)
Sep 27 17:14:16.770: INFO: (6) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 8.955098ms)
Sep 27 17:14:16.772: INFO: (6) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 10.890678ms)
Sep 27 17:14:16.772: INFO: (6) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.903474ms)
Sep 27 17:14:16.772: INFO: (6) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 10.961496ms)
Sep 27 17:14:16.772: INFO: (6) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 10.989997ms)
Sep 27 17:14:16.772: INFO: (6) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 11.088214ms)
Sep 27 17:14:16.776: INFO: (7) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 4.697079ms)
Sep 27 17:14:16.778: INFO: (7) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.291081ms)
Sep 27 17:14:16.778: INFO: (7) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.475955ms)
Sep 27 17:14:16.778: INFO: (7) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.550057ms)
Sep 27 17:14:16.779: INFO: (7) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 6.679325ms)
Sep 27 17:14:16.779: INFO: (7) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 6.618984ms)
Sep 27 17:14:16.779: INFO: (7) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.783518ms)
Sep 27 17:14:16.779: INFO: (7) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 6.749157ms)
Sep 27 17:14:16.779: INFO: (7) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 6.754201ms)
Sep 27 17:14:16.779: INFO: (7) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 6.794656ms)
Sep 27 17:14:16.781: INFO: (7) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 8.792135ms)
Sep 27 17:14:16.783: INFO: (7) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.705862ms)
Sep 27 17:14:16.783: INFO: (7) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 10.677601ms)
Sep 27 17:14:16.783: INFO: (7) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 10.765625ms)
Sep 27 17:14:16.783: INFO: (7) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 10.723892ms)
Sep 27 17:14:16.783: INFO: (7) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 10.778236ms)
Sep 27 17:14:16.787: INFO: (8) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 4.677688ms)
Sep 27 17:14:16.789: INFO: (8) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.557496ms)
Sep 27 17:14:16.789: INFO: (8) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.586903ms)
Sep 27 17:14:16.789: INFO: (8) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 6.544322ms)
Sep 27 17:14:16.789: INFO: (8) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 6.56821ms)
Sep 27 17:14:16.790: INFO: (8) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 6.772182ms)
Sep 27 17:14:16.790: INFO: (8) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 6.761421ms)
Sep 27 17:14:16.790: INFO: (8) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 6.810643ms)
Sep 27 17:14:16.790: INFO: (8) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.774399ms)
Sep 27 17:14:16.790: INFO: (8) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.802407ms)
Sep 27 17:14:16.791: INFO: (8) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 8.571478ms)
Sep 27 17:14:16.793: INFO: (8) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.658859ms)
Sep 27 17:14:16.793: INFO: (8) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 10.711169ms)
Sep 27 17:14:16.793: INFO: (8) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 10.68117ms)
Sep 27 17:14:16.793: INFO: (8) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 10.642237ms)
Sep 27 17:14:16.793: INFO: (8) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 10.623426ms)
Sep 27 17:14:16.798: INFO: (9) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 4.609785ms)
Sep 27 17:14:16.800: INFO: (9) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 6.485555ms)
Sep 27 17:14:16.800: INFO: (9) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.434851ms)
Sep 27 17:14:16.800: INFO: (9) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.49733ms)
Sep 27 17:14:16.800: INFO: (9) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 6.564407ms)
Sep 27 17:14:16.800: INFO: (9) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 6.626406ms)
Sep 27 17:14:16.800: INFO: (9) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 6.639834ms)
Sep 27 17:14:16.800: INFO: (9) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.723989ms)
Sep 27 17:14:16.800: INFO: (9) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 6.680385ms)
Sep 27 17:14:16.800: INFO: (9) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.66093ms)
Sep 27 17:14:16.802: INFO: (9) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 8.77605ms)
Sep 27 17:14:16.804: INFO: (9) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 10.568646ms)
Sep 27 17:14:16.804: INFO: (9) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 10.607094ms)
Sep 27 17:14:16.804: INFO: (9) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.552336ms)
Sep 27 17:14:16.804: INFO: (9) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 10.905622ms)
Sep 27 17:14:16.805: INFO: (9) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 11.238557ms)
Sep 27 17:14:16.811: INFO: (10) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.463819ms)
Sep 27 17:14:16.811: INFO: (10) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.49755ms)
Sep 27 17:14:16.811: INFO: (10) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 6.612741ms)
Sep 27 17:14:16.811: INFO: (10) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 6.563341ms)
Sep 27 17:14:16.811: INFO: (10) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 6.574302ms)
Sep 27 17:14:16.811: INFO: (10) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 6.655735ms)
Sep 27 17:14:16.812: INFO: (10) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.812552ms)
Sep 27 17:14:16.812: INFO: (10) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.847704ms)
Sep 27 17:14:16.812: INFO: (10) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 6.844585ms)
Sep 27 17:14:16.812: INFO: (10) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 6.947035ms)
Sep 27 17:14:16.813: INFO: (10) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 8.108475ms)
Sep 27 17:14:16.813: INFO: (10) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 8.141053ms)
Sep 27 17:14:16.815: INFO: (10) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 9.860904ms)
Sep 27 17:14:16.815: INFO: (10) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 9.905628ms)
Sep 27 17:14:16.815: INFO: (10) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 9.810131ms)
Sep 27 17:14:16.815: INFO: (10) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 9.875465ms)
Sep 27 17:14:16.820: INFO: (11) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 4.780154ms)
Sep 27 17:14:16.820: INFO: (11) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 4.897819ms)
Sep 27 17:14:16.822: INFO: (11) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.793603ms)
Sep 27 17:14:16.822: INFO: (11) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 6.77886ms)
Sep 27 17:14:16.822: INFO: (11) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 6.778776ms)
Sep 27 17:14:16.822: INFO: (11) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.79029ms)
Sep 27 17:14:16.822: INFO: (11) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.840319ms)
Sep 27 17:14:16.822: INFO: (11) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 6.845158ms)
Sep 27 17:14:16.822: INFO: (11) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 6.81778ms)
Sep 27 17:14:16.822: INFO: (11) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 6.892224ms)
Sep 27 17:14:16.823: INFO: (11) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 8.198637ms)
Sep 27 17:14:16.825: INFO: (11) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 9.819057ms)
Sep 27 17:14:16.825: INFO: (11) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 9.873572ms)
Sep 27 17:14:16.825: INFO: (11) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 9.991016ms)
Sep 27 17:14:16.825: INFO: (11) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.058012ms)
Sep 27 17:14:16.825: INFO: (11) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 10.102264ms)
Sep 27 17:14:16.830: INFO: (12) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 4.633996ms)
Sep 27 17:14:16.831: INFO: (12) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 6.414086ms)
Sep 27 17:14:16.832: INFO: (12) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.681635ms)
Sep 27 17:14:16.832: INFO: (12) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 6.788335ms)
Sep 27 17:14:16.832: INFO: (12) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.771244ms)
Sep 27 17:14:16.832: INFO: (12) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 6.830892ms)
Sep 27 17:14:16.832: INFO: (12) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.824961ms)
Sep 27 17:14:16.832: INFO: (12) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 7.012913ms)
Sep 27 17:14:16.832: INFO: (12) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 6.955188ms)
Sep 27 17:14:16.832: INFO: (12) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 7.119962ms)
Sep 27 17:14:16.833: INFO: (12) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 8.264342ms)
Sep 27 17:14:16.835: INFO: (12) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 10.114266ms)
Sep 27 17:14:16.835: INFO: (12) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.08744ms)
Sep 27 17:14:16.835: INFO: (12) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 10.119699ms)
Sep 27 17:14:16.835: INFO: (12) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 10.075094ms)
Sep 27 17:14:16.835: INFO: (12) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 10.12418ms)
Sep 27 17:14:16.840: INFO: (13) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 4.661453ms)
Sep 27 17:14:16.842: INFO: (13) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 6.687072ms)
Sep 27 17:14:16.842: INFO: (13) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.846292ms)
Sep 27 17:14:16.842: INFO: (13) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.827901ms)
Sep 27 17:14:16.842: INFO: (13) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.845469ms)
Sep 27 17:14:16.842: INFO: (13) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 6.895977ms)
Sep 27 17:14:16.842: INFO: (13) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 7.063688ms)
Sep 27 17:14:16.842: INFO: (13) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 7.115765ms)
Sep 27 17:14:16.842: INFO: (13) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 7.039837ms)
Sep 27 17:14:16.842: INFO: (13) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 7.266225ms)
Sep 27 17:14:16.844: INFO: (13) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 8.73738ms)
Sep 27 17:14:16.846: INFO: (13) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 10.563437ms)
Sep 27 17:14:16.846: INFO: (13) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 10.684481ms)
Sep 27 17:14:16.846: INFO: (13) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.674261ms)
Sep 27 17:14:16.846: INFO: (13) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 10.664087ms)
Sep 27 17:14:16.846: INFO: (13) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 10.735951ms)
Sep 27 17:14:16.851: INFO: (14) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 4.650629ms)
Sep 27 17:14:16.852: INFO: (14) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.290341ms)
Sep 27 17:14:16.852: INFO: (14) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 6.46627ms)
Sep 27 17:14:16.852: INFO: (14) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 6.585556ms)
Sep 27 17:14:16.853: INFO: (14) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 6.555646ms)
Sep 27 17:14:16.853: INFO: (14) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.938752ms)
Sep 27 17:14:16.853: INFO: (14) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.939003ms)
Sep 27 17:14:16.853: INFO: (14) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 7.085169ms)
Sep 27 17:14:16.853: INFO: (14) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 7.120385ms)
Sep 27 17:14:16.853: INFO: (14) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 7.137106ms)
Sep 27 17:14:16.854: INFO: (14) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 8.146227ms)
Sep 27 17:14:16.856: INFO: (14) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 9.699013ms)
Sep 27 17:14:16.856: INFO: (14) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 9.970752ms)
Sep 27 17:14:16.856: INFO: (14) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 9.961848ms)
Sep 27 17:14:16.856: INFO: (14) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 9.920413ms)
Sep 27 17:14:16.856: INFO: (14) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 9.961573ms)
Sep 27 17:14:16.861: INFO: (15) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 4.746002ms)
Sep 27 17:14:16.863: INFO: (15) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.705408ms)
Sep 27 17:14:16.863: INFO: (15) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 6.808667ms)
Sep 27 17:14:16.863: INFO: (15) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.74449ms)
Sep 27 17:14:16.863: INFO: (15) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 6.879134ms)
Sep 27 17:14:16.863: INFO: (15) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 6.917296ms)
Sep 27 17:14:16.863: INFO: (15) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 6.901703ms)
Sep 27 17:14:16.863: INFO: (15) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.946754ms)
Sep 27 17:14:16.863: INFO: (15) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 7.015499ms)
Sep 27 17:14:16.863: INFO: (15) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 7.079186ms)
Sep 27 17:14:16.865: INFO: (15) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 8.633088ms)
Sep 27 17:14:16.866: INFO: (15) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 10.455481ms)
Sep 27 17:14:16.867: INFO: (15) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 10.555597ms)
Sep 27 17:14:16.867: INFO: (15) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 10.565404ms)
Sep 27 17:14:16.867: INFO: (15) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.586953ms)
Sep 27 17:14:16.867: INFO: (15) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 10.579566ms)
Sep 27 17:14:16.871: INFO: (16) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 4.634828ms)
Sep 27 17:14:16.874: INFO: (16) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 7.337963ms)
Sep 27 17:14:16.875: INFO: (16) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 8.023649ms)
Sep 27 17:14:16.875: INFO: (16) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 8.721217ms)
Sep 27 17:14:16.876: INFO: (16) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 9.500365ms)
Sep 27 17:14:16.876: INFO: (16) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 9.629336ms)
Sep 27 17:14:16.876: INFO: (16) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 9.657254ms)
Sep 27 17:14:16.877: INFO: (16) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 10.388645ms)
Sep 27 17:14:16.878: INFO: (16) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 11.606721ms)
Sep 27 17:14:16.881: INFO: (16) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 14.246783ms)
Sep 27 17:14:16.881: INFO: (16) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 14.290362ms)
Sep 27 17:14:16.886: INFO: (16) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 19.376091ms)
Sep 27 17:14:16.886: INFO: (16) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 19.509322ms)
Sep 27 17:14:16.886: INFO: (16) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 19.517412ms)
Sep 27 17:14:16.886: INFO: (16) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 19.423454ms)
Sep 27 17:14:16.886: INFO: (16) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 19.581524ms)
Sep 27 17:14:16.891: INFO: (17) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 4.614295ms)
Sep 27 17:14:16.893: INFO: (17) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 6.524963ms)
Sep 27 17:14:16.893: INFO: (17) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.616767ms)
Sep 27 17:14:16.893: INFO: (17) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 6.58556ms)
Sep 27 17:14:16.893: INFO: (17) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.604591ms)
Sep 27 17:14:16.893: INFO: (17) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 6.798824ms)
Sep 27 17:14:16.893: INFO: (17) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 6.810032ms)
Sep 27 17:14:16.893: INFO: (17) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 6.912027ms)
Sep 27 17:14:16.893: INFO: (17) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 6.938248ms)
Sep 27 17:14:16.893: INFO: (17) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 6.891151ms)
Sep 27 17:14:16.895: INFO: (17) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 8.926634ms)
Sep 27 17:14:16.898: INFO: (17) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 11.343246ms)
Sep 27 17:14:16.898: INFO: (17) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 11.532274ms)
Sep 27 17:14:16.898: INFO: (17) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 11.503405ms)
Sep 27 17:14:16.898: INFO: (17) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 11.569778ms)
Sep 27 17:14:16.898: INFO: (17) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 11.555421ms)
Sep 27 17:14:16.905: INFO: (18) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 7.065619ms)
Sep 27 17:14:16.906: INFO: (18) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 7.497308ms)
Sep 27 17:14:16.906: INFO: (18) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 7.522919ms)
Sep 27 17:14:16.906: INFO: (18) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 7.497798ms)
Sep 27 17:14:16.906: INFO: (18) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 7.493895ms)
Sep 27 17:14:16.906: INFO: (18) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 7.468887ms)
Sep 27 17:14:16.906: INFO: (18) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 7.607265ms)
Sep 27 17:14:16.906: INFO: (18) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 7.563329ms)
Sep 27 17:14:16.906: INFO: (18) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 7.480398ms)
Sep 27 17:14:16.906: INFO: (18) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 7.492221ms)
Sep 27 17:14:16.907: INFO: (18) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 8.71385ms)
Sep 27 17:14:16.908: INFO: (18) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 10.267322ms)
Sep 27 17:14:16.908: INFO: (18) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.208133ms)
Sep 27 17:14:16.908: INFO: (18) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 10.327043ms)
Sep 27 17:14:16.908: INFO: (18) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 10.30375ms)
Sep 27 17:14:16.909: INFO: (18) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 10.477948ms)
Sep 27 17:14:16.914: INFO: (19) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:162/proxy/: bar (200; 5.169602ms)
Sep 27 17:14:16.916: INFO: (19) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:1080/proxy/rewriteme">... (200; 7.197501ms)
Sep 27 17:14:16.916: INFO: (19) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:162/proxy/: bar (200; 7.339024ms)
Sep 27 17:14:16.916: INFO: (19) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:462/proxy/: tls qux (200; 7.331095ms)
Sep 27 17:14:16.916: INFO: (19) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:160/proxy/: foo (200; 7.372878ms)
Sep 27 17:14:16.916: INFO: (19) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86:1080/proxy/rewriteme">test<... (200; 7.429067ms)
Sep 27 17:14:16.916: INFO: (19) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:460/proxy/: tls baz (200; 7.409892ms)
Sep 27 17:14:16.916: INFO: (19) /api/v1/namespaces/proxy-9850/pods/http:proxy-service-fz9td-6nj86:160/proxy/: foo (200; 7.540533ms)
Sep 27 17:14:16.916: INFO: (19) /api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/proxy-service-fz9td-6nj86/proxy/rewriteme">test</a> (200; 7.551088ms)
Sep 27 17:14:16.916: INFO: (19) /api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/: <a href="/api/v1/namespaces/proxy-9850/pods/https:proxy-service-fz9td-6nj86:443/proxy/tlsrewritem... (200; 7.597642ms)
Sep 27 17:14:16.917: INFO: (19) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname1/proxy/: tls baz (200; 8.501413ms)
Sep 27 17:14:16.919: INFO: (19) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname2/proxy/: bar (200; 10.174136ms)
Sep 27 17:14:16.919: INFO: (19) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname2/proxy/: bar (200; 10.195065ms)
Sep 27 17:14:16.919: INFO: (19) /api/v1/namespaces/proxy-9850/services/http:proxy-service-fz9td:portname1/proxy/: foo (200; 10.307767ms)
Sep 27 17:14:16.919: INFO: (19) /api/v1/namespaces/proxy-9850/services/proxy-service-fz9td:portname1/proxy/: foo (200; 10.315396ms)
Sep 27 17:14:16.919: INFO: (19) /api/v1/namespaces/proxy-9850/services/https:proxy-service-fz9td:tlsportname2/proxy/: tls qux (200; 10.406308ms)
STEP: deleting ReplicationController proxy-service-fz9td in namespace proxy-9850, will wait for the garbage collector to delete the pods
Sep 27 17:14:16.981: INFO: Deleting ReplicationController proxy-service-fz9td took: 7.129164ms
Sep 27 17:14:17.081: INFO: Terminating ReplicationController proxy-service-fz9td pods took: 100.557311ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:14:18.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9850" for this suite.

• [SLOW TEST:12.424 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":339,"completed":246,"skipped":3885,"failed":0}
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:14:18.994: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2565
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2565
STEP: creating replication controller externalsvc in namespace services-2565
I0927 17:14:19.069970      23 runners.go:190] Created replication controller with name: externalsvc, namespace: services-2565, replica count: 2
I0927 17:14:22.122079      23 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Sep 27 17:14:22.155: INFO: Creating new exec pod
Sep 27 17:14:24.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-2565 exec execpodvfccd -- /bin/sh -x -c nslookup nodeport-service.services-2565.svc.cluster.local'
Sep 27 17:14:24.763: INFO: stderr: "+ nslookup nodeport-service.services-2565.svc.cluster.local\n"
Sep 27 17:14:24.763: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nnodeport-service.services-2565.svc.cluster.local\tcanonical name = externalsvc.services-2565.svc.cluster.local.\nName:\texternalsvc.services-2565.svc.cluster.local\nAddress: 100.71.185.186\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2565, will wait for the garbage collector to delete the pods
Sep 27 17:14:24.826: INFO: Deleting ReplicationController externalsvc took: 7.618944ms
Sep 27 17:14:24.927: INFO: Terminating ReplicationController externalsvc pods took: 101.009781ms
Sep 27 17:14:32.152: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:14:32.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2565" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:13.188 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":339,"completed":247,"skipped":3885,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:14:32.182: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Sep 27 17:14:32.217: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Sep 27 17:14:32.518: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 27 17:14:34.578: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 17:14:36.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 17:14:38.583: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 17:14:40.583: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768359672, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 17:14:45.805: INFO: Waited 3.213633134s for the sample-apiserver to be ready to handle requests.
I0927 17:14:46.841146      23 request.go:668] Waited for 1.011000699s due to client-side throttling, not priority and fairness, request: GET:https://100.64.0.1:443/apis/apiregistration.k8s.io/v1?timeout=32s
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Sep 27 17:14:47.605: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:14:48.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8053" for this suite.

• [SLOW TEST:16.322 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":339,"completed":248,"skipped":3899,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:14:48.504: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Sep 27 17:14:48.557: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 17:15:48.589: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:15:48.593: INFO: Starting informer...
STEP: Starting pod...
Sep 27 17:15:48.808: INFO: Pod is running on ip-10-0-95-24.us-east-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep 27 17:15:48.822: INFO: Pod wasn't evicted. Proceeding
Sep 27 17:15:48.822: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep 27 17:17:03.840: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:17:03.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-1141" for this suite.

• [SLOW TEST:135.353 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":339,"completed":249,"skipped":3910,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:17:03.857: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Sep 27 17:17:03.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-2598 cluster-info'
Sep 27 17:17:03.954: INFO: stderr: ""
Sep 27 17:17:03.954: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:17:03.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2598" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":339,"completed":250,"skipped":3920,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:17:03.967: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 17:17:04.417: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 17:17:07.458: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep 27 17:17:09.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=webhook-3983 attach --namespace=webhook-3983 to-be-attached-pod -i -c=container1'
Sep 27 17:17:09.571: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:17:09.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3983" for this suite.
STEP: Destroying namespace "webhook-3983-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.681 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":339,"completed":251,"skipped":3941,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:17:09.648: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0927 17:17:09.688891      23 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:22:09.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9725" for this suite.

• [SLOW TEST:300.081 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":339,"completed":252,"skipped":3983,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:22:09.729: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:22:09.772: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep 27 17:22:14.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-995 --namespace=crd-publish-openapi-995 create -f -'
Sep 27 17:22:15.655: INFO: stderr: ""
Sep 27 17:22:15.655: INFO: stdout: "e2e-test-crd-publish-openapi-4299-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 27 17:22:15.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-995 --namespace=crd-publish-openapi-995 delete e2e-test-crd-publish-openapi-4299-crds test-cr'
Sep 27 17:22:15.752: INFO: stderr: ""
Sep 27 17:22:15.752: INFO: stdout: "e2e-test-crd-publish-openapi-4299-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep 27 17:22:15.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-995 --namespace=crd-publish-openapi-995 apply -f -'
Sep 27 17:22:16.103: INFO: stderr: ""
Sep 27 17:22:16.103: INFO: stdout: "e2e-test-crd-publish-openapi-4299-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep 27 17:22:16.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-995 --namespace=crd-publish-openapi-995 delete e2e-test-crd-publish-openapi-4299-crds test-cr'
Sep 27 17:22:16.167: INFO: stderr: ""
Sep 27 17:22:16.167: INFO: stdout: "e2e-test-crd-publish-openapi-4299-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep 27 17:22:16.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=crd-publish-openapi-995 explain e2e-test-crd-publish-openapi-4299-crds'
Sep 27 17:22:16.501: INFO: stderr: ""
Sep 27 17:22:16.501: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4299-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:22:22.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-995" for this suite.

• [SLOW TEST:12.382 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":339,"completed":253,"skipped":3986,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:22:22.112: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep 27 17:22:22.416: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 17:22:25.444: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:22:25.450: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:22:28.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4443" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.568 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":339,"completed":254,"skipped":3992,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:22:28.680: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 17:22:28.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3438b93f-0418-46c5-8a67-0edc69875849" in namespace "projected-5596" to be "Succeeded or Failed"
Sep 27 17:22:28.736: INFO: Pod "downwardapi-volume-3438b93f-0418-46c5-8a67-0edc69875849": Phase="Pending", Reason="", readiness=false. Elapsed: 4.23404ms
Sep 27 17:22:30.742: INFO: Pod "downwardapi-volume-3438b93f-0418-46c5-8a67-0edc69875849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010986906s
STEP: Saw pod success
Sep 27 17:22:30.742: INFO: Pod "downwardapi-volume-3438b93f-0418-46c5-8a67-0edc69875849" satisfied condition "Succeeded or Failed"
Sep 27 17:22:30.746: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downwardapi-volume-3438b93f-0418-46c5-8a67-0edc69875849 container client-container: <nil>
STEP: delete the pod
Sep 27 17:22:30.780: INFO: Waiting for pod downwardapi-volume-3438b93f-0418-46c5-8a67-0edc69875849 to disappear
Sep 27 17:22:30.784: INFO: Pod downwardapi-volume-3438b93f-0418-46c5-8a67-0edc69875849 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:22:30.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5596" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":255,"skipped":4004,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:22:30.796: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 27 17:22:30.842: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 17:23:30.883: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:23:30.888: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:23:30.940: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Sep 27 17:23:30.944: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:23:30.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2347" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:23:30.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2114" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.248 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":339,"completed":256,"skipped":4023,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:23:31.044: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:23:31.088: INFO: The status of Pod busybox-host-aliasesb964e40b-5f66-450b-9389-7df131d83ed5 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:23:33.094: INFO: The status of Pod busybox-host-aliasesb964e40b-5f66-450b-9389-7df131d83ed5 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:23:33.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4401" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":257,"skipped":4034,"failed":0}
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:23:33.118: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:23:35.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5188" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":258,"skipped":4035,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:23:35.202: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Sep 27 17:23:35.240: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-2708 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:23:35.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2708" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":339,"completed":259,"skipped":4039,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:23:35.304: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-a14f93e7-d6c6-4c9f-a412-d9a601a74472
STEP: Creating a pod to test consume secrets
Sep 27 17:23:35.384: INFO: Waiting up to 5m0s for pod "pod-secrets-c825219b-273c-4cf5-9b16-b969336e1978" in namespace "secrets-363" to be "Succeeded or Failed"
Sep 27 17:23:35.388: INFO: Pod "pod-secrets-c825219b-273c-4cf5-9b16-b969336e1978": Phase="Pending", Reason="", readiness=false. Elapsed: 3.647412ms
Sep 27 17:23:37.395: INFO: Pod "pod-secrets-c825219b-273c-4cf5-9b16-b969336e1978": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010716816s
STEP: Saw pod success
Sep 27 17:23:37.395: INFO: Pod "pod-secrets-c825219b-273c-4cf5-9b16-b969336e1978" satisfied condition "Succeeded or Failed"
Sep 27 17:23:37.398: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-secrets-c825219b-273c-4cf5-9b16-b969336e1978 container secret-volume-test: <nil>
STEP: delete the pod
Sep 27 17:23:37.422: INFO: Waiting for pod pod-secrets-c825219b-273c-4cf5-9b16-b969336e1978 to disappear
Sep 27 17:23:37.425: INFO: Pod pod-secrets-c825219b-273c-4cf5-9b16-b969336e1978 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:23:37.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-363" for this suite.
STEP: Destroying namespace "secret-namespace-3512" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":339,"completed":260,"skipped":4086,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:23:37.444: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 17:23:37.875: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 17:23:40.907: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep 27 17:23:40.926: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:23:40.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4097" for this suite.
STEP: Destroying namespace "webhook-4097-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":339,"completed":261,"skipped":4122,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:23:41.013: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 27 17:23:41.063: INFO: Waiting up to 5m0s for pod "pod-115c95c7-9744-44f5-a5e8-704234f1bb93" in namespace "emptydir-1571" to be "Succeeded or Failed"
Sep 27 17:23:41.066: INFO: Pod "pod-115c95c7-9744-44f5-a5e8-704234f1bb93": Phase="Pending", Reason="", readiness=false. Elapsed: 3.497666ms
Sep 27 17:23:43.072: INFO: Pod "pod-115c95c7-9744-44f5-a5e8-704234f1bb93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009410714s
STEP: Saw pod success
Sep 27 17:23:43.072: INFO: Pod "pod-115c95c7-9744-44f5-a5e8-704234f1bb93" satisfied condition "Succeeded or Failed"
Sep 27 17:23:43.076: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-115c95c7-9744-44f5-a5e8-704234f1bb93 container test-container: <nil>
STEP: delete the pod
Sep 27 17:23:43.098: INFO: Waiting for pod pod-115c95c7-9744-44f5-a5e8-704234f1bb93 to disappear
Sep 27 17:23:43.102: INFO: Pod pod-115c95c7-9744-44f5-a5e8-704234f1bb93 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:23:43.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1571" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":262,"skipped":4148,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:23:43.114: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:23:43.161: INFO: The status of Pod busybox-scheduling-d0cf4508-5466-4907-8e0f-c286def017f9 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:23:45.166: INFO: The status of Pod busybox-scheduling-d0cf4508-5466-4907-8e0f-c286def017f9 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:23:45.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1513" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":339,"completed":263,"skipped":4151,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:23:45.190: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-8a57e27e-ee0a-40e3-a90a-7597ec4aba88
STEP: Creating a pod to test consume configMaps
Sep 27 17:23:45.235: INFO: Waiting up to 5m0s for pod "pod-configmaps-bfcfb47a-6d34-4415-bbbb-cad8d39123c1" in namespace "configmap-7071" to be "Succeeded or Failed"
Sep 27 17:23:45.238: INFO: Pod "pod-configmaps-bfcfb47a-6d34-4415-bbbb-cad8d39123c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.476393ms
Sep 27 17:23:47.244: INFO: Pod "pod-configmaps-bfcfb47a-6d34-4415-bbbb-cad8d39123c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009172419s
STEP: Saw pod success
Sep 27 17:23:47.244: INFO: Pod "pod-configmaps-bfcfb47a-6d34-4415-bbbb-cad8d39123c1" satisfied condition "Succeeded or Failed"
Sep 27 17:23:47.248: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-configmaps-bfcfb47a-6d34-4415-bbbb-cad8d39123c1 container agnhost-container: <nil>
STEP: delete the pod
Sep 27 17:23:47.271: INFO: Waiting for pod pod-configmaps-bfcfb47a-6d34-4415-bbbb-cad8d39123c1 to disappear
Sep 27 17:23:47.276: INFO: Pod pod-configmaps-bfcfb47a-6d34-4415-bbbb-cad8d39123c1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:23:47.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7071" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":264,"skipped":4173,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:23:47.287: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2574
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 27 17:23:47.321: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep 27 17:23:47.360: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:23:49.365: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:23:51.366: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:23:53.366: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:23:55.365: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:23:57.365: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:23:59.367: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:24:01.366: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:24:03.366: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:24:05.365: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep 27 17:24:07.365: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep 27 17:24:07.381: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep 27 17:24:07.390: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep 27 17:24:09.431: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep 27 17:24:09.431: INFO: Going to poll 100.96.1.15 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Sep 27 17:24:09.435: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.15:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2574 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:24:09.435: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:24:09.516: INFO: Found all 1 expected endpoints: [netserver-0]
Sep 27 17:24:09.516: INFO: Going to poll 100.96.5.60 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Sep 27 17:24:09.520: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.5.60:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2574 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:24:09.520: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:24:09.564: INFO: Found all 1 expected endpoints: [netserver-1]
Sep 27 17:24:09.564: INFO: Going to poll 100.96.4.192 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Sep 27 17:24:09.568: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.4.192:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2574 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep 27 17:24:09.568: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:24:09.611: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:24:09.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2574" for this suite.

• [SLOW TEST:22.338 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":265,"skipped":4179,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:24:09.625: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-e5bfe464-d29c-41f5-97f2-8c650e992606
STEP: Creating a pod to test consume configMaps
Sep 27 17:24:09.677: INFO: Waiting up to 5m0s for pod "pod-configmaps-a43fc143-2e12-412b-ac9c-15f0b7437fcd" in namespace "configmap-7772" to be "Succeeded or Failed"
Sep 27 17:24:09.680: INFO: Pod "pod-configmaps-a43fc143-2e12-412b-ac9c-15f0b7437fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.714361ms
Sep 27 17:24:11.686: INFO: Pod "pod-configmaps-a43fc143-2e12-412b-ac9c-15f0b7437fcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009175352s
STEP: Saw pod success
Sep 27 17:24:11.686: INFO: Pod "pod-configmaps-a43fc143-2e12-412b-ac9c-15f0b7437fcd" satisfied condition "Succeeded or Failed"
Sep 27 17:24:11.689: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-configmaps-a43fc143-2e12-412b-ac9c-15f0b7437fcd container agnhost-container: <nil>
STEP: delete the pod
Sep 27 17:24:11.711: INFO: Waiting for pod pod-configmaps-a43fc143-2e12-412b-ac9c-15f0b7437fcd to disappear
Sep 27 17:24:11.715: INFO: Pod pod-configmaps-a43fc143-2e12-412b-ac9c-15f0b7437fcd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:24:11.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7772" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":266,"skipped":4206,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:24:11.729: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-1441
STEP: creating replication controller nodeport-test in namespace services-1441
I0927 17:24:11.788411      23 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-1441, replica count: 2
Sep 27 17:24:14.839: INFO: Creating new exec pod
I0927 17:24:14.839663      23 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 17:24:17.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-1441 exec execpodjjv2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Sep 27 17:24:18.015: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 27 17:24:18.015: INFO: stdout: ""
Sep 27 17:24:19.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-1441 exec execpodjjv2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Sep 27 17:24:19.161: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Sep 27 17:24:19.161: INFO: stdout: "nodeport-test-xnn4s"
Sep 27 17:24:19.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-1441 exec execpodjjv2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.70.145.13 80'
Sep 27 17:24:19.305: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.70.145.13 80\nConnection to 100.70.145.13 80 port [tcp/http] succeeded!\n"
Sep 27 17:24:19.305: INFO: stdout: "nodeport-test-xnn4s"
Sep 27 17:24:19.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-1441 exec execpodjjv2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.95.24 30885'
Sep 27 17:24:19.433: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.95.24 30885\nConnection to 10.0.95.24 30885 port [tcp/*] succeeded!\n"
Sep 27 17:24:19.433: INFO: stdout: "nodeport-test-b8pr7"
Sep 27 17:24:19.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-1441 exec execpodjjv2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.62.6 30885'
Sep 27 17:24:19.590: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.62.6 30885\nConnection to 10.0.62.6 30885 port [tcp/*] succeeded!\n"
Sep 27 17:24:19.590: INFO: stdout: ""
Sep 27 17:24:20.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-1441 exec execpodjjv2h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.62.6 30885'
Sep 27 17:24:20.750: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.62.6 30885\nConnection to 10.0.62.6 30885 port [tcp/*] succeeded!\n"
Sep 27 17:24:20.750: INFO: stdout: "nodeport-test-b8pr7"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:24:20.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1441" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:9.037 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":339,"completed":267,"skipped":4238,"failed":0}
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:24:20.766: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Sep 27 17:24:40.945: INFO: EndpointSlice for Service endpointslice-3924/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:24:50.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3924" for this suite.

• [SLOW TEST:30.208 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":339,"completed":268,"skipped":4238,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:24:50.974: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Sep 27 17:24:51.024: INFO: The status of Pod pod-update-38bc6b35-073e-4a0d-bd90-e4943bb72eae is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:24:53.031: INFO: The status of Pod pod-update-38bc6b35-073e-4a0d-bd90-e4943bb72eae is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 27 17:24:53.555: INFO: Successfully updated pod "pod-update-38bc6b35-073e-4a0d-bd90-e4943bb72eae"
STEP: verifying the updated pod is in kubernetes
Sep 27 17:24:53.565: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:24:53.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9632" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":339,"completed":269,"skipped":4242,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:24:53.577: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:24:53.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2950" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":339,"completed":270,"skipped":4255,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:24:53.622: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Sep 27 17:24:53.657: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep 27 17:24:53.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8451 create -f -'
Sep 27 17:24:54.009: INFO: stderr: ""
Sep 27 17:24:54.009: INFO: stdout: "service/agnhost-replica created\n"
Sep 27 17:24:54.009: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep 27 17:24:54.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8451 create -f -'
Sep 27 17:24:54.357: INFO: stderr: ""
Sep 27 17:24:54.357: INFO: stdout: "service/agnhost-primary created\n"
Sep 27 17:24:54.357: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 27 17:24:54.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8451 create -f -'
Sep 27 17:24:54.695: INFO: stderr: ""
Sep 27 17:24:54.695: INFO: stdout: "service/frontend created\n"
Sep 27 17:24:54.695: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep 27 17:24:54.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8451 create -f -'
Sep 27 17:24:55.035: INFO: stderr: ""
Sep 27 17:24:55.035: INFO: stdout: "deployment.apps/frontend created\n"
Sep 27 17:24:55.035: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 27 17:24:55.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8451 create -f -'
Sep 27 17:24:55.381: INFO: stderr: ""
Sep 27 17:24:55.381: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep 27 17:24:55.382: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 27 17:24:55.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8451 create -f -'
Sep 27 17:24:55.738: INFO: stderr: ""
Sep 27 17:24:55.738: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Sep 27 17:24:55.738: INFO: Waiting for all frontend pods to be Running.
Sep 27 17:25:00.790: INFO: Waiting for frontend to serve content.
Sep 27 17:25:00.805: INFO: Trying to add a new entry to the guestbook.
Sep 27 17:25:00.821: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep 27 17:25:00.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8451 delete --grace-period=0 --force -f -'
Sep 27 17:25:00.916: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 17:25:00.916: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 17:25:00.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8451 delete --grace-period=0 --force -f -'
Sep 27 17:25:00.990: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 17:25:00.990: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 17:25:00.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8451 delete --grace-period=0 --force -f -'
Sep 27 17:25:01.070: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 17:25:01.070: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 17:25:01.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8451 delete --grace-period=0 --force -f -'
Sep 27 17:25:01.135: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 17:25:01.135: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 17:25:01.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8451 delete --grace-period=0 --force -f -'
Sep 27 17:25:01.195: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 17:25:01.195: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep 27 17:25:01.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8451 delete --grace-period=0 --force -f -'
Sep 27 17:25:01.262: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 17:25:01.262: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:25:01.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8451" for this suite.

• [SLOW TEST:7.652 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:336
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":339,"completed":271,"skipped":4272,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:25:01.275: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Sep 27 17:25:01.318: INFO: Waiting up to 5m0s for pod "downward-api-b8c922b7-edc5-4752-9b5a-69564f6e1fbb" in namespace "downward-api-1499" to be "Succeeded or Failed"
Sep 27 17:25:01.322: INFO: Pod "downward-api-b8c922b7-edc5-4752-9b5a-69564f6e1fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.675287ms
Sep 27 17:25:03.327: INFO: Pod "downward-api-b8c922b7-edc5-4752-9b5a-69564f6e1fbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008675523s
STEP: Saw pod success
Sep 27 17:25:03.327: INFO: Pod "downward-api-b8c922b7-edc5-4752-9b5a-69564f6e1fbb" satisfied condition "Succeeded or Failed"
Sep 27 17:25:03.330: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod downward-api-b8c922b7-edc5-4752-9b5a-69564f6e1fbb container dapi-container: <nil>
STEP: delete the pod
Sep 27 17:25:03.354: INFO: Waiting for pod downward-api-b8c922b7-edc5-4752-9b5a-69564f6e1fbb to disappear
Sep 27 17:25:03.358: INFO: Pod downward-api-b8c922b7-edc5-4752-9b5a-69564f6e1fbb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:25:03.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1499" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":339,"completed":272,"skipped":4275,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:25:03.376: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:26:03.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3265" for this suite.

• [SLOW TEST:60.071 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":339,"completed":273,"skipped":4281,"failed":0}
SSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:26:03.447: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0927 17:26:03.489868      23 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep 27 17:26:03.502: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep 27 17:26:03.507: INFO: starting watch
STEP: patching
STEP: updating
Sep 27 17:26:03.527: INFO: waiting for watch events with expected annotations
Sep 27 17:26:03.527: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:26:03.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7542" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":339,"completed":274,"skipped":4288,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:26:03.586: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-6xpp
STEP: Creating a pod to test atomic-volume-subpath
Sep 27 17:26:03.636: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6xpp" in namespace "subpath-8573" to be "Succeeded or Failed"
Sep 27 17:26:03.639: INFO: Pod "pod-subpath-test-configmap-6xpp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.502999ms
Sep 27 17:26:05.645: INFO: Pod "pod-subpath-test-configmap-6xpp": Phase="Running", Reason="", readiness=true. Elapsed: 2.009109442s
Sep 27 17:26:07.651: INFO: Pod "pod-subpath-test-configmap-6xpp": Phase="Running", Reason="", readiness=true. Elapsed: 4.014959211s
Sep 27 17:26:09.657: INFO: Pod "pod-subpath-test-configmap-6xpp": Phase="Running", Reason="", readiness=true. Elapsed: 6.020638688s
Sep 27 17:26:11.663: INFO: Pod "pod-subpath-test-configmap-6xpp": Phase="Running", Reason="", readiness=true. Elapsed: 8.026689256s
Sep 27 17:26:13.668: INFO: Pod "pod-subpath-test-configmap-6xpp": Phase="Running", Reason="", readiness=true. Elapsed: 10.031918888s
Sep 27 17:26:15.673: INFO: Pod "pod-subpath-test-configmap-6xpp": Phase="Running", Reason="", readiness=true. Elapsed: 12.037008605s
Sep 27 17:26:17.679: INFO: Pod "pod-subpath-test-configmap-6xpp": Phase="Running", Reason="", readiness=true. Elapsed: 14.042601399s
Sep 27 17:26:19.684: INFO: Pod "pod-subpath-test-configmap-6xpp": Phase="Running", Reason="", readiness=true. Elapsed: 16.047827203s
Sep 27 17:26:21.689: INFO: Pod "pod-subpath-test-configmap-6xpp": Phase="Running", Reason="", readiness=true. Elapsed: 18.053131578s
Sep 27 17:26:23.695: INFO: Pod "pod-subpath-test-configmap-6xpp": Phase="Running", Reason="", readiness=true. Elapsed: 20.0585416s
Sep 27 17:26:25.700: INFO: Pod "pod-subpath-test-configmap-6xpp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.063636656s
STEP: Saw pod success
Sep 27 17:26:25.700: INFO: Pod "pod-subpath-test-configmap-6xpp" satisfied condition "Succeeded or Failed"
Sep 27 17:26:25.703: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-subpath-test-configmap-6xpp container test-container-subpath-configmap-6xpp: <nil>
STEP: delete the pod
Sep 27 17:26:25.735: INFO: Waiting for pod pod-subpath-test-configmap-6xpp to disappear
Sep 27 17:26:25.738: INFO: Pod pod-subpath-test-configmap-6xpp no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6xpp
Sep 27 17:26:25.738: INFO: Deleting pod "pod-subpath-test-configmap-6xpp" in namespace "subpath-8573"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:26:25.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8573" for this suite.

• [SLOW TEST:22.167 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":339,"completed":275,"skipped":4298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:26:25.753: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Sep 27 17:26:25.796: INFO: The status of Pod annotationupdate19fb595f-6cc0-4764-bd6f-c5db41361296 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:26:27.801: INFO: The status of Pod annotationupdate19fb595f-6cc0-4764-bd6f-c5db41361296 is Running (Ready = true)
Sep 27 17:26:28.326: INFO: Successfully updated pod "annotationupdate19fb595f-6cc0-4764-bd6f-c5db41361296"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:26:32.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3492" for this suite.

• [SLOW TEST:6.612 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":276,"skipped":4329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:26:32.365: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:26:32.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3767" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":339,"completed":277,"skipped":4360,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:26:32.450: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0927 17:26:33.541681      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 27 17:31:33.549: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:31:33.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7720" for this suite.

• [SLOW TEST:301.115 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":339,"completed":278,"skipped":4363,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:31:33.565: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-bad037a5-37a3-47ed-8515-09897232d5b7
Sep 27 17:31:33.611: INFO: Pod name my-hostname-basic-bad037a5-37a3-47ed-8515-09897232d5b7: Found 0 pods out of 1
Sep 27 17:31:38.616: INFO: Pod name my-hostname-basic-bad037a5-37a3-47ed-8515-09897232d5b7: Found 1 pods out of 1
Sep 27 17:31:38.616: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-bad037a5-37a3-47ed-8515-09897232d5b7" are running
Sep 27 17:31:38.620: INFO: Pod "my-hostname-basic-bad037a5-37a3-47ed-8515-09897232d5b7-2c9cz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 17:31:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 17:31:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 17:31:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-27 17:31:33 +0000 UTC Reason: Message:}])
Sep 27 17:31:38.620: INFO: Trying to dial the pod
Sep 27 17:31:43.640: INFO: Controller my-hostname-basic-bad037a5-37a3-47ed-8515-09897232d5b7: Got expected result from replica 1 [my-hostname-basic-bad037a5-37a3-47ed-8515-09897232d5b7-2c9cz]: "my-hostname-basic-bad037a5-37a3-47ed-8515-09897232d5b7-2c9cz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:31:43.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9699" for this suite.

• [SLOW TEST:10.087 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":279,"skipped":4371,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:31:43.652: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-a4eeb7b6-0d3d-4a5b-97ae-bb681f4fbd62 in namespace container-probe-2985
Sep 27 17:31:45.703: INFO: Started pod liveness-a4eeb7b6-0d3d-4a5b-97ae-bb681f4fbd62 in namespace container-probe-2985
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 17:31:45.707: INFO: Initial restart count of pod liveness-a4eeb7b6-0d3d-4a5b-97ae-bb681f4fbd62 is 0
Sep 27 17:32:05.766: INFO: Restart count of pod container-probe-2985/liveness-a4eeb7b6-0d3d-4a5b-97ae-bb681f4fbd62 is now 1 (20.05941189s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:32:05.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2985" for this suite.

• [SLOW TEST:22.148 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":280,"skipped":4379,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:32:05.800: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0927 17:32:15.928274      23 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Sep 27 17:37:15.931: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Sep 27 17:37:15.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wxjv" in namespace "gc-2905"
Sep 27 17:37:15.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-cftq6" in namespace "gc-2905"
Sep 27 17:37:15.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7vxc" in namespace "gc-2905"
Sep 27 17:37:15.982: INFO: Deleting pod "simpletest-rc-to-be-deleted-fm27w" in namespace "gc-2905"
Sep 27 17:37:15.997: INFO: Deleting pod "simpletest-rc-to-be-deleted-fq5j2" in namespace "gc-2905"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:37:16.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2905" for this suite.

• [SLOW TEST:310.229 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":339,"completed":281,"skipped":4384,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:37:16.030: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-abe00ee6-8120-403f-a22f-048460aade7b
STEP: Creating a pod to test consume configMaps
Sep 27 17:37:16.081: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c7fcc12-6c50-454f-aacf-d2ea304eafda" in namespace "projected-6907" to be "Succeeded or Failed"
Sep 27 17:37:16.084: INFO: Pod "pod-projected-configmaps-9c7fcc12-6c50-454f-aacf-d2ea304eafda": Phase="Pending", Reason="", readiness=false. Elapsed: 3.499013ms
Sep 27 17:37:18.089: INFO: Pod "pod-projected-configmaps-9c7fcc12-6c50-454f-aacf-d2ea304eafda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008480687s
STEP: Saw pod success
Sep 27 17:37:18.089: INFO: Pod "pod-projected-configmaps-9c7fcc12-6c50-454f-aacf-d2ea304eafda" satisfied condition "Succeeded or Failed"
Sep 27 17:37:18.093: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-projected-configmaps-9c7fcc12-6c50-454f-aacf-d2ea304eafda container agnhost-container: <nil>
STEP: delete the pod
Sep 27 17:37:18.157: INFO: Waiting for pod pod-projected-configmaps-9c7fcc12-6c50-454f-aacf-d2ea304eafda to disappear
Sep 27 17:37:18.161: INFO: Pod pod-projected-configmaps-9c7fcc12-6c50-454f-aacf-d2ea304eafda no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:37:18.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6907" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":282,"skipped":4435,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:37:18.189: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1514
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Sep 27 17:37:18.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-163 run e2e-test-httpd-pod --restart=Never --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Sep 27 17:37:18.752: INFO: stderr: ""
Sep 27 17:37:18.752: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1518
Sep 27 17:37:18.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-163 delete pods e2e-test-httpd-pod'
Sep 27 17:37:22.080: INFO: stderr: ""
Sep 27 17:37:22.080: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:37:22.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-163" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":339,"completed":283,"skipped":4450,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:37:22.095: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-02284c0b-408f-4281-b119-874176a8fd34 in namespace container-probe-8287
Sep 27 17:37:24.150: INFO: Started pod liveness-02284c0b-408f-4281-b119-874176a8fd34 in namespace container-probe-8287
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 17:37:24.154: INFO: Initial restart count of pod liveness-02284c0b-408f-4281-b119-874176a8fd34 is 0
Sep 27 17:37:44.215: INFO: Restart count of pod container-probe-8287/liveness-02284c0b-408f-4281-b119-874176a8fd34 is now 1 (20.060349505s elapsed)
Sep 27 17:38:04.272: INFO: Restart count of pod container-probe-8287/liveness-02284c0b-408f-4281-b119-874176a8fd34 is now 2 (40.117584581s elapsed)
Sep 27 17:38:24.326: INFO: Restart count of pod container-probe-8287/liveness-02284c0b-408f-4281-b119-874176a8fd34 is now 3 (1m0.171778809s elapsed)
Sep 27 17:38:44.411: INFO: Restart count of pod container-probe-8287/liveness-02284c0b-408f-4281-b119-874176a8fd34 is now 4 (1m20.257097083s elapsed)
Sep 27 17:39:44.667: INFO: Restart count of pod container-probe-8287/liveness-02284c0b-408f-4281-b119-874176a8fd34 is now 5 (2m20.512705545s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:39:44.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8287" for this suite.

• [SLOW TEST:142.599 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":339,"completed":284,"skipped":4465,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:39:44.694: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-ff85d44a-5c68-41c1-a3cf-76cbfdead12a
STEP: Creating a pod to test consume configMaps
Sep 27 17:39:44.741: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d989fe79-e93c-498e-adcc-3a8616aafe75" in namespace "projected-8746" to be "Succeeded or Failed"
Sep 27 17:39:44.745: INFO: Pod "pod-projected-configmaps-d989fe79-e93c-498e-adcc-3a8616aafe75": Phase="Pending", Reason="", readiness=false. Elapsed: 3.449507ms
Sep 27 17:39:46.751: INFO: Pod "pod-projected-configmaps-d989fe79-e93c-498e-adcc-3a8616aafe75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00985542s
STEP: Saw pod success
Sep 27 17:39:46.751: INFO: Pod "pod-projected-configmaps-d989fe79-e93c-498e-adcc-3a8616aafe75" satisfied condition "Succeeded or Failed"
Sep 27 17:39:46.755: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-projected-configmaps-d989fe79-e93c-498e-adcc-3a8616aafe75 container agnhost-container: <nil>
STEP: delete the pod
Sep 27 17:39:46.783: INFO: Waiting for pod pod-projected-configmaps-d989fe79-e93c-498e-adcc-3a8616aafe75 to disappear
Sep 27 17:39:46.787: INFO: Pod pod-projected-configmaps-d989fe79-e93c-498e-adcc-3a8616aafe75 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:39:46.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8746" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":285,"skipped":4476,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:39:46.799: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-92462a04-fc4f-43fc-a841-c64f011b7f08 in namespace container-probe-4661
Sep 27 17:39:48.863: INFO: Started pod liveness-92462a04-fc4f-43fc-a841-c64f011b7f08 in namespace container-probe-4661
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 17:39:48.868: INFO: Initial restart count of pod liveness-92462a04-fc4f-43fc-a841-c64f011b7f08 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:43:49.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4661" for this suite.

• [SLOW TEST:242.973 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":339,"completed":286,"skipped":4488,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:43:49.773: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Sep 27 17:43:49.809: INFO: Major version: 1
STEP: Confirm minor version
Sep 27 17:43:49.809: INFO: cleanMinorVersion: 21
Sep 27 17:43:49.809: INFO: Minor version: 21
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:43:49.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8299" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":339,"completed":287,"skipped":4493,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:43:49.820: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:43:55.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6596" for this suite.
STEP: Destroying namespace "nsdeletetest-6654" for this suite.
Sep 27 17:43:55.965: INFO: Namespace nsdeletetest-6654 was already deleted
STEP: Destroying namespace "nsdeletetest-4107" for this suite.

• [SLOW TEST:6.152 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":339,"completed":288,"skipped":4495,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:43:55.972: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Sep 27 17:43:56.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-2726 create -f -'
Sep 27 17:43:56.360: INFO: stderr: ""
Sep 27 17:43:56.360: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep 27 17:43:57.365: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 17:43:57.365: INFO: Found 0 / 1
Sep 27 17:43:58.365: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 17:43:58.365: INFO: Found 1 / 1
Sep 27 17:43:58.365: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 27 17:43:58.369: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 17:43:58.369: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 27 17:43:58.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-2726 patch pod agnhost-primary-dmctd -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 27 17:43:58.437: INFO: stderr: ""
Sep 27 17:43:58.437: INFO: stdout: "pod/agnhost-primary-dmctd patched\n"
STEP: checking annotations
Sep 27 17:43:58.441: INFO: Selector matched 1 pods for map[app:agnhost]
Sep 27 17:43:58.441: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:43:58.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2726" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":339,"completed":289,"skipped":4497,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:43:58.453: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Sep 27 17:43:58.508: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Sep 27 17:43:58.531: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:43:58.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7734" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":339,"completed":290,"skipped":4525,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:43:58.574: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1308
STEP: creating the pod
Sep 27 17:43:58.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8590 create -f -'
Sep 27 17:43:58.944: INFO: stderr: ""
Sep 27 17:43:58.944: INFO: stdout: "pod/pause created\n"
Sep 27 17:43:58.944: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 27 17:43:58.944: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8590" to be "running and ready"
Sep 27 17:43:58.948: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.859696ms
Sep 27 17:44:00.954: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009087489s
Sep 27 17:44:00.954: INFO: Pod "pause" satisfied condition "running and ready"
Sep 27 17:44:00.954: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 27 17:44:00.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8590 label pods pause testing-label=testing-label-value'
Sep 27 17:44:01.022: INFO: stderr: ""
Sep 27 17:44:01.022: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 27 17:44:01.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8590 get pod pause -L testing-label'
Sep 27 17:44:01.081: INFO: stderr: ""
Sep 27 17:44:01.081: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 27 17:44:01.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8590 label pods pause testing-label-'
Sep 27 17:44:01.144: INFO: stderr: ""
Sep 27 17:44:01.144: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 27 17:44:01.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8590 get pod pause -L testing-label'
Sep 27 17:44:01.205: INFO: stderr: ""
Sep 27 17:44:01.205: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: using delete to clean up resources
Sep 27 17:44:01.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8590 delete --grace-period=0 --force -f -'
Sep 27 17:44:01.280: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 27 17:44:01.280: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 27 17:44:01.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8590 get rc,svc -l name=pause --no-headers'
Sep 27 17:44:01.346: INFO: stderr: "No resources found in kubectl-8590 namespace.\n"
Sep 27 17:44:01.346: INFO: stdout: ""
Sep 27 17:44:01.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-8590 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 27 17:44:01.402: INFO: stderr: ""
Sep 27 17:44:01.402: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:44:01.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8590" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":339,"completed":291,"skipped":4534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:44:01.415: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep 27 17:44:01.467: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 17:45:01.504: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:45:01.508: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Sep 27 17:45:03.585: INFO: found a healthy node: ip-10-0-95-24.us-east-2.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:45:17.670: INFO: pods created so far: [1 1 1]
Sep 27 17:45:17.670: INFO: length of pods created so far: 3
Sep 27 17:45:27.683: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:45:34.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5077" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:45:34.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5646" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:93.378 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":339,"completed":292,"skipped":4585,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:45:34.793: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-9990
Sep 27 17:45:34.841: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:45:36.849: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Sep 27 17:45:36.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-9990 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Sep 27 17:45:37.010: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Sep 27 17:45:37.010: INFO: stdout: "iptables"
Sep 27 17:45:37.010: INFO: proxyMode: iptables
Sep 27 17:45:37.021: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Sep 27 17:45:37.024: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-9990
STEP: creating replication controller affinity-clusterip-timeout in namespace services-9990
I0927 17:45:37.045882      23 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-9990, replica count: 3
I0927 17:45:40.098042      23 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 27 17:45:40.108: INFO: Creating new exec pod
Sep 27 17:45:43.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-9990 exec execpod-affinitymvb2j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Sep 27 17:45:43.324: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Sep 27 17:45:43.324: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 17:45:43.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-9990 exec execpod-affinitymvb2j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 100.69.227.180 80'
Sep 27 17:45:43.469: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 100.69.227.180 80\nConnection to 100.69.227.180 80 port [tcp/http] succeeded!\n"
Sep 27 17:45:43.469: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Sep 27 17:45:43.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-9990 exec execpod-affinitymvb2j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.69.227.180:80/ ; done'
Sep 27 17:45:43.656: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n"
Sep 27 17:45:43.656: INFO: stdout: "\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6\naffinity-clusterip-timeout-v4rh6"
Sep 27 17:45:43.656: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.656: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.656: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.656: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.656: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.657: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.657: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.657: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.657: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.657: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.657: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.657: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.657: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.657: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.657: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.657: INFO: Received response from host: affinity-clusterip-timeout-v4rh6
Sep 27 17:45:43.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-9990 exec execpod-affinitymvb2j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.69.227.180:80/'
Sep 27 17:45:43.783: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n"
Sep 27 17:45:43.783: INFO: stdout: "affinity-clusterip-timeout-v4rh6"
Sep 27 17:46:03.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=services-9990 exec execpod-affinitymvb2j -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://100.69.227.180:80/'
Sep 27 17:46:03.933: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://100.69.227.180:80/\n"
Sep 27 17:46:03.933: INFO: stdout: "affinity-clusterip-timeout-7wjtx"
Sep 27 17:46:03.933: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-9990, will wait for the garbage collector to delete the pods
Sep 27 17:46:04.012: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.66109ms
Sep 27 17:46:04.112: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.120386ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:46:12.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9990" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:37.356 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":293,"skipped":4598,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:46:12.149: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 17:46:12.522: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 17:46:15.549: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:46:15.553: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:46:18.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6258" for this suite.
STEP: Destroying namespace "webhook-6258-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.624 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":339,"completed":294,"skipped":4600,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:46:18.773: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Sep 27 17:46:18.827: INFO: The status of Pod pod-hostip-e13940c1-b5ff-43ef-ace9-bc2e11ab68a2 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:46:20.837: INFO: The status of Pod pod-hostip-e13940c1-b5ff-43ef-ace9-bc2e11ab68a2 is Running (Ready = true)
Sep 27 17:46:20.848: INFO: Pod pod-hostip-e13940c1-b5ff-43ef-ace9-bc2e11ab68a2 has hostIP: 10.0.95.24
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:46:20.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9562" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":339,"completed":295,"skipped":4604,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:46:20.870: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Sep 27 17:46:20.905: INFO: Waiting up to 1m0s for all nodes to be ready
Sep 27 17:47:20.943: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:47:20.948: INFO: Starting informer...
STEP: Starting pods...
Sep 27 17:47:21.168: INFO: Pod1 is running on ip-10-0-95-24.us-east-2.compute.internal. Tainting Node
Sep 27 17:47:23.392: INFO: Pod2 is running on ip-10-0-95-24.us-east-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep 27 17:47:42.090: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep 27 17:47:52.080: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:47:52.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-3435" for this suite.

• [SLOW TEST:91.237 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":339,"completed":296,"skipped":4621,"failed":0}
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:47:52.107: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:47:54.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9724" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":297,"skipped":4624,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:47:54.194: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:47:54.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1196" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":339,"completed":298,"skipped":4648,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:47:54.273: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7239.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7239.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7239.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7239.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7239.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7239.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 17:47:56.343: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:47:56.347: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:47:56.352: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:47:56.356: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:47:56.370: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:47:56.374: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:47:56.378: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:47:56.383: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:47:56.391: INFO: Lookups using dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local]

Sep 27 17:48:01.398: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:01.403: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:01.407: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:01.412: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:01.425: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:01.430: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:01.434: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:01.438: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:01.447: INFO: Lookups using dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local]

Sep 27 17:48:06.397: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:06.402: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:06.406: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:06.411: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:06.424: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:06.428: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:06.433: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:06.437: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:06.446: INFO: Lookups using dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local]

Sep 27 17:48:11.397: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:11.402: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:11.407: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:11.411: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:11.425: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:11.429: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:11.434: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:11.438: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:11.447: INFO: Lookups using dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local]

Sep 27 17:48:16.398: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:16.402: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:16.407: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:16.411: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:16.424: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:16.431: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:16.435: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:16.439: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:16.448: INFO: Lookups using dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local]

Sep 27 17:48:21.397: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:21.402: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:21.406: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:21.411: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:21.424: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:21.429: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:21.433: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:21.437: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local from pod dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3: the server could not find the requested resource (get pods dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3)
Sep 27 17:48:21.446: INFO: Lookups using dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7239.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7239.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7239.svc.cluster.local jessie_udp@dns-test-service-2.dns-7239.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7239.svc.cluster.local]

Sep 27 17:48:26.446: INFO: DNS probes using dns-7239/dns-test-c4557395-8c9f-41b1-a512-3c6666ddfac3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:48:26.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7239" for this suite.

• [SLOW TEST:32.223 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":339,"completed":299,"skipped":4654,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:48:26.496: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:48:28.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-366" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":339,"completed":300,"skipped":4725,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:48:28.610: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 27 17:48:28.654: INFO: Waiting up to 5m0s for pod "pod-8e43f234-0295-47e0-9bdf-10c4810cb59d" in namespace "emptydir-3811" to be "Succeeded or Failed"
Sep 27 17:48:28.657: INFO: Pod "pod-8e43f234-0295-47e0-9bdf-10c4810cb59d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.468022ms
Sep 27 17:48:30.663: INFO: Pod "pod-8e43f234-0295-47e0-9bdf-10c4810cb59d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00944784s
STEP: Saw pod success
Sep 27 17:48:30.663: INFO: Pod "pod-8e43f234-0295-47e0-9bdf-10c4810cb59d" satisfied condition "Succeeded or Failed"
Sep 27 17:48:30.667: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-8e43f234-0295-47e0-9bdf-10c4810cb59d container test-container: <nil>
STEP: delete the pod
Sep 27 17:48:30.691: INFO: Waiting for pod pod-8e43f234-0295-47e0-9bdf-10c4810cb59d to disappear
Sep 27 17:48:30.695: INFO: Pod pod-8e43f234-0295-47e0-9bdf-10c4810cb59d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:48:30.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3811" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":301,"skipped":4756,"failed":0}
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:48:30.706: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:48:30.750: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 27 17:48:35.758: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 27 17:48:35.758: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 27 17:48:37.766: INFO: Creating deployment "test-rollover-deployment"
Sep 27 17:48:37.778: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 27 17:48:39.788: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 27 17:48:39.796: INFO: Ensure that both replica sets have 1 created replica
Sep 27 17:48:39.803: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 27 17:48:39.813: INFO: Updating deployment test-rollover-deployment
Sep 27 17:48:39.813: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 27 17:48:41.829: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 27 17:48:41.836: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 27 17:48:41.844: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 17:48:41.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361721, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 17:48:43.855: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 17:48:43.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361721, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 17:48:45.856: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 17:48:45.856: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361721, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 17:48:47.857: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 17:48:47.857: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361721, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 17:48:49.857: INFO: all replica sets need to contain the pod-template-hash label
Sep 27 17:48:49.857: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361721, loc:(*time.Location)(0x9dde5a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361717, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 27 17:48:51.855: INFO: 
Sep 27 17:48:51.855: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Sep 27 17:48:51.866: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9898  49345638-2718-4bbc-9479-f276131f2b58 1738922 2 2021-09-27 17:48:37 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-27 17:48:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 17:48:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006de96a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-27 17:48:37 +0000 UTC,LastTransitionTime:2021-09-27 17:48:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-09-27 17:48:51 +0000 UTC,LastTransitionTime:2021-09-27 17:48:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep 27 17:48:51.870: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-9898  dbcc2858-b7b1-4191-8a3f-77c2fd5ba7ed 1738911 2 2021-09-27 17:48:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 49345638-2718-4bbc-9479-f276131f2b58 0xc006de9c10 0xc006de9c11}] []  [{kube-controller-manager Update apps/v1 2021-09-27 17:48:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49345638-2718-4bbc-9479-f276131f2b58\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006de9c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep 27 17:48:51.870: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 27 17:48:51.870: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9898  7369c59b-ad4c-4312-9136-fce979cdee41 1738920 2 2021-09-27 17:48:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 49345638-2718-4bbc-9479-f276131f2b58 0xc006de99ff 0xc006de9a10}] []  [{e2e.test Update apps/v1 2021-09-27 17:48:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 17:48:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49345638-2718-4bbc-9479-f276131f2b58\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006de9aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 17:48:51.870: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-9898  48e18a15-8283-4eef-a879-7199fc531f6c 1738843 2 2021-09-27 17:48:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 49345638-2718-4bbc-9479-f276131f2b58 0xc006de9b17 0xc006de9b18}] []  [{kube-controller-manager Update apps/v1 2021-09-27 17:48:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"49345638-2718-4bbc-9479-f276131f2b58\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006de9ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 17:48:51.874: INFO: Pod "test-rollover-deployment-98c5f4599-w85hp" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-w85hp test-rollover-deployment-98c5f4599- deployment-9898  c3d1d69f-b02f-4afc-9030-ad9a51548b3c 1738858 0 2021-09-27 17:48:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 dbcc2858-b7b1-4191-8a3f-77c2fd5ba7ed 0xc007dac1b0 0xc007dac1b1}] []  [{kube-controller-manager Update v1 2021-09-27 17:48:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dbcc2858-b7b1-4191-8a3f-77c2fd5ba7ed\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:48:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b67gf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b67gf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:48:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:48:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:48:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:48:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.31.225,PodIP:100.96.1.29,StartTime:2021-09-27 17:48:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 17:48:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://b3a8ffabafd9331cba80178e7ba2a70c88c5ae5b502187bd7c5e14d27b0c38f3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:48:51.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9898" for this suite.

• [SLOW TEST:21.180 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":339,"completed":302,"skipped":4761,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:48:51.887: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-7464/configmap-test-2052b185-385d-4ef0-9699-ac39e5908bfd
STEP: Creating a pod to test consume configMaps
Sep 27 17:48:51.933: INFO: Waiting up to 5m0s for pod "pod-configmaps-4c1680a8-f433-4666-ba2d-7041cb8ad5f3" in namespace "configmap-7464" to be "Succeeded or Failed"
Sep 27 17:48:51.937: INFO: Pod "pod-configmaps-4c1680a8-f433-4666-ba2d-7041cb8ad5f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.502517ms
Sep 27 17:48:53.945: INFO: Pod "pod-configmaps-4c1680a8-f433-4666-ba2d-7041cb8ad5f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011224656s
STEP: Saw pod success
Sep 27 17:48:53.945: INFO: Pod "pod-configmaps-4c1680a8-f433-4666-ba2d-7041cb8ad5f3" satisfied condition "Succeeded or Failed"
Sep 27 17:48:53.948: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-configmaps-4c1680a8-f433-4666-ba2d-7041cb8ad5f3 container env-test: <nil>
STEP: delete the pod
Sep 27 17:48:53.973: INFO: Waiting for pod pod-configmaps-4c1680a8-f433-4666-ba2d-7041cb8ad5f3 to disappear
Sep 27 17:48:53.976: INFO: Pod pod-configmaps-4c1680a8-f433-4666-ba2d-7041cb8ad5f3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:48:53.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7464" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":339,"completed":303,"skipped":4769,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:48:53.988: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Sep 27 17:48:54.036: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:48:56.043: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Sep 27 17:48:56.059: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:48:58.067: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Sep 27 17:48:58.080: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 17:48:58.083: INFO: Pod pod-with-prestop-http-hook still exists
Sep 27 17:49:00.084: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 17:49:00.088: INFO: Pod pod-with-prestop-http-hook still exists
Sep 27 17:49:02.084: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 27 17:49:02.091: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:49:02.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8175" for this suite.

• [SLOW TEST:8.123 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":339,"completed":304,"skipped":4775,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:49:02.112: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:49:13.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4016" for this suite.

• [SLOW TEST:11.112 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":339,"completed":305,"skipped":4778,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:49:13.224: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Sep 27 17:49:13.265: INFO: Waiting up to 5m0s for pod "downward-api-d7610478-3a6f-4b8e-93aa-f300b6f1bd74" in namespace "downward-api-8737" to be "Succeeded or Failed"
Sep 27 17:49:13.279: INFO: Pod "downward-api-d7610478-3a6f-4b8e-93aa-f300b6f1bd74": Phase="Pending", Reason="", readiness=false. Elapsed: 13.635047ms
Sep 27 17:49:15.283: INFO: Pod "downward-api-d7610478-3a6f-4b8e-93aa-f300b6f1bd74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017772484s
STEP: Saw pod success
Sep 27 17:49:15.283: INFO: Pod "downward-api-d7610478-3a6f-4b8e-93aa-f300b6f1bd74" satisfied condition "Succeeded or Failed"
Sep 27 17:49:15.287: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downward-api-d7610478-3a6f-4b8e-93aa-f300b6f1bd74 container dapi-container: <nil>
STEP: delete the pod
Sep 27 17:49:15.313: INFO: Waiting for pod downward-api-d7610478-3a6f-4b8e-93aa-f300b6f1bd74 to disappear
Sep 27 17:49:15.317: INFO: Pod downward-api-d7610478-3a6f-4b8e-93aa-f300b6f1bd74 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:49:15.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8737" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":339,"completed":306,"skipped":4786,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:49:15.329: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2923 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2923;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2923 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2923;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2923.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2923.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2923.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2923.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2923.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2923.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2923.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2923.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2923.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2923.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2923.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 93.232.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.232.93_udp@PTR;check="$$(dig +tcp +noall +answer +search 93.232.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.232.93_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2923 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2923;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2923 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2923;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2923.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2923.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2923.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2923.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2923.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2923.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2923.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2923.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2923.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2923.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2923.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2923.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 93.232.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.232.93_udp@PTR;check="$$(dig +tcp +noall +answer +search 93.232.66.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.66.232.93_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 17:49:17.419: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.424: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.428: INFO: Unable to read wheezy_udp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.432: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.437: INFO: Unable to read wheezy_udp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.441: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.446: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.450: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.482: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.486: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.490: INFO: Unable to read jessie_udp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.495: INFO: Unable to read jessie_tcp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.499: INFO: Unable to read jessie_udp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.503: INFO: Unable to read jessie_tcp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.508: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.512: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:17.539: INFO: Lookups using dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2923 wheezy_tcp@dns-test-service.dns-2923 wheezy_udp@dns-test-service.dns-2923.svc wheezy_tcp@dns-test-service.dns-2923.svc wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2923 jessie_tcp@dns-test-service.dns-2923 jessie_udp@dns-test-service.dns-2923.svc jessie_tcp@dns-test-service.dns-2923.svc jessie_udp@_http._tcp.dns-test-service.dns-2923.svc jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc]

Sep 27 17:49:22.545: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.549: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.554: INFO: Unable to read wheezy_udp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.558: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.563: INFO: Unable to read wheezy_udp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.567: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.571: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.576: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.606: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.611: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.615: INFO: Unable to read jessie_udp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.619: INFO: Unable to read jessie_tcp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.624: INFO: Unable to read jessie_udp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.628: INFO: Unable to read jessie_tcp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.633: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.637: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:22.663: INFO: Lookups using dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2923 wheezy_tcp@dns-test-service.dns-2923 wheezy_udp@dns-test-service.dns-2923.svc wheezy_tcp@dns-test-service.dns-2923.svc wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2923 jessie_tcp@dns-test-service.dns-2923 jessie_udp@dns-test-service.dns-2923.svc jessie_tcp@dns-test-service.dns-2923.svc jessie_udp@_http._tcp.dns-test-service.dns-2923.svc jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc]

Sep 27 17:49:27.546: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.551: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.556: INFO: Unable to read wheezy_udp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.560: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.565: INFO: Unable to read wheezy_udp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.569: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.574: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.578: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.609: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.613: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.617: INFO: Unable to read jessie_udp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.622: INFO: Unable to read jessie_tcp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.626: INFO: Unable to read jessie_udp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.630: INFO: Unable to read jessie_tcp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.640: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.645: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:27.671: INFO: Lookups using dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2923 wheezy_tcp@dns-test-service.dns-2923 wheezy_udp@dns-test-service.dns-2923.svc wheezy_tcp@dns-test-service.dns-2923.svc wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2923 jessie_tcp@dns-test-service.dns-2923 jessie_udp@dns-test-service.dns-2923.svc jessie_tcp@dns-test-service.dns-2923.svc jessie_udp@_http._tcp.dns-test-service.dns-2923.svc jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc]

Sep 27 17:49:32.546: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.550: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.555: INFO: Unable to read wheezy_udp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.559: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.564: INFO: Unable to read wheezy_udp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.568: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.572: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.580: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.610: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.615: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.619: INFO: Unable to read jessie_udp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.624: INFO: Unable to read jessie_tcp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.628: INFO: Unable to read jessie_udp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.632: INFO: Unable to read jessie_tcp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.636: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.641: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:32.667: INFO: Lookups using dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2923 wheezy_tcp@dns-test-service.dns-2923 wheezy_udp@dns-test-service.dns-2923.svc wheezy_tcp@dns-test-service.dns-2923.svc wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2923 jessie_tcp@dns-test-service.dns-2923 jessie_udp@dns-test-service.dns-2923.svc jessie_tcp@dns-test-service.dns-2923.svc jessie_udp@_http._tcp.dns-test-service.dns-2923.svc jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc]

Sep 27 17:49:37.547: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.551: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.556: INFO: Unable to read wheezy_udp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.560: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.565: INFO: Unable to read wheezy_udp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.569: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.573: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.578: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.608: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.613: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.617: INFO: Unable to read jessie_udp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.621: INFO: Unable to read jessie_tcp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.626: INFO: Unable to read jessie_udp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.630: INFO: Unable to read jessie_tcp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.634: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.638: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:37.664: INFO: Lookups using dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2923 wheezy_tcp@dns-test-service.dns-2923 wheezy_udp@dns-test-service.dns-2923.svc wheezy_tcp@dns-test-service.dns-2923.svc wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2923 jessie_tcp@dns-test-service.dns-2923 jessie_udp@dns-test-service.dns-2923.svc jessie_tcp@dns-test-service.dns-2923.svc jessie_udp@_http._tcp.dns-test-service.dns-2923.svc jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc]

Sep 27 17:49:42.547: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.551: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.556: INFO: Unable to read wheezy_udp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.560: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.564: INFO: Unable to read wheezy_udp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.569: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.573: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.577: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.608: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.612: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.617: INFO: Unable to read jessie_udp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.621: INFO: Unable to read jessie_tcp@dns-test-service.dns-2923 from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.625: INFO: Unable to read jessie_udp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.629: INFO: Unable to read jessie_tcp@dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.634: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.639: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc from pod dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c: the server could not find the requested resource (get pods dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c)
Sep 27 17:49:42.664: INFO: Lookups using dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2923 wheezy_tcp@dns-test-service.dns-2923 wheezy_udp@dns-test-service.dns-2923.svc wheezy_tcp@dns-test-service.dns-2923.svc wheezy_udp@_http._tcp.dns-test-service.dns-2923.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2923.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2923 jessie_tcp@dns-test-service.dns-2923 jessie_udp@dns-test-service.dns-2923.svc jessie_tcp@dns-test-service.dns-2923.svc jessie_udp@_http._tcp.dns-test-service.dns-2923.svc jessie_tcp@_http._tcp.dns-test-service.dns-2923.svc]

Sep 27 17:49:47.665: INFO: DNS probes using dns-2923/dns-test-a95cec34-022e-4897-b18a-01042c8e9f1c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:49:47.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2923" for this suite.

• [SLOW TEST:32.421 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":339,"completed":307,"skipped":4796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:49:47.751: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-7d26e665-62d8-48ce-a052-82f79773f5e9
STEP: Creating a pod to test consume configMaps
Sep 27 17:49:47.798: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-799b8aa2-7bdd-4279-92cf-32a4c6be52a4" in namespace "projected-6935" to be "Succeeded or Failed"
Sep 27 17:49:47.802: INFO: Pod "pod-projected-configmaps-799b8aa2-7bdd-4279-92cf-32a4c6be52a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.592476ms
Sep 27 17:49:49.809: INFO: Pod "pod-projected-configmaps-799b8aa2-7bdd-4279-92cf-32a4c6be52a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010841083s
STEP: Saw pod success
Sep 27 17:49:49.809: INFO: Pod "pod-projected-configmaps-799b8aa2-7bdd-4279-92cf-32a4c6be52a4" satisfied condition "Succeeded or Failed"
Sep 27 17:49:49.813: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-projected-configmaps-799b8aa2-7bdd-4279-92cf-32a4c6be52a4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 27 17:49:49.838: INFO: Waiting for pod pod-projected-configmaps-799b8aa2-7bdd-4279-92cf-32a4c6be52a4 to disappear
Sep 27 17:49:49.842: INFO: Pod pod-projected-configmaps-799b8aa2-7bdd-4279-92cf-32a4c6be52a4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:49:49.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6935" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":308,"skipped":4828,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:49:49.853: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:49:49.895: INFO: Creating deployment "webserver-deployment"
Sep 27 17:49:49.901: INFO: Waiting for observed generation 1
Sep 27 17:49:51.912: INFO: Waiting for all required pods to come up
Sep 27 17:49:51.916: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 27 17:49:53.926: INFO: Waiting for deployment "webserver-deployment" to complete
Sep 27 17:49:53.934: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep 27 17:49:53.946: INFO: Updating deployment webserver-deployment
Sep 27 17:49:53.946: INFO: Waiting for observed generation 2
Sep 27 17:49:55.955: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 27 17:49:55.958: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 27 17:49:55.962: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 27 17:49:55.972: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 27 17:49:55.972: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 27 17:49:55.975: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep 27 17:49:55.982: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep 27 17:49:55.982: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep 27 17:49:55.993: INFO: Updating deployment webserver-deployment
Sep 27 17:49:55.993: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep 27 17:49:56.000: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 27 17:49:56.007: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Sep 27 17:49:56.024: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2445  229a5724-65f0-447b-9861-aba4d1a4ab8f 1739724 3 2021-09-27 17:49:49 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-27 17:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-27 17:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00768cb28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-09-27 17:49:54 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-09-27 17:49:56 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep 27 17:49:56.046: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-2445  7b1683b0-cf80-4696-881d-a00dbc071c72 1739719 3 2021-09-27 17:49:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 229a5724-65f0-447b-9861-aba4d1a4ab8f 0xc00768cf37 0xc00768cf38}] []  [{kube-controller-manager Update apps/v1 2021-09-27 17:49:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"229a5724-65f0-447b-9861-aba4d1a4ab8f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00768cfb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep 27 17:49:56.046: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep 27 17:49:56.046: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-2445  84de47c0-c212-4a53-b124-af6bc9b57700 1739716 3 2021-09-27 17:49:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 229a5724-65f0-447b-9861-aba4d1a4ab8f 0xc00768d017 0xc00768d018}] []  [{kube-controller-manager Update apps/v1 2021-09-27 17:49:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"229a5724-65f0-447b-9861-aba4d1a4ab8f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00768d088 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep 27 17:49:56.060: INFO: Pod "webserver-deployment-795d758f88-5b5zw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5b5zw webserver-deployment-795d758f88- deployment-2445  9905bcbf-8f77-4437-a8f0-2a6db5cb399b 1739762 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc0067047e7 0xc0067047e8}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nb5kg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nb5kg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.060: INFO: Pod "webserver-deployment-795d758f88-5svmc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5svmc webserver-deployment-795d758f88- deployment-2445  49ce8981-f083-4c35-b07a-bd171e7fa5ec 1739742 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc006704957 0xc006704958}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gjq9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gjq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.060: INFO: Pod "webserver-deployment-795d758f88-7qr99" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7qr99 webserver-deployment-795d758f88- deployment-2445  a5248b5e-4cda-4664-931f-c9609c1389e3 1739740 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc006704ac7 0xc006704ac8}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r2f49,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r2f49,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-62-6.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.060: INFO: Pod "webserver-deployment-795d758f88-cxd5h" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cxd5h webserver-deployment-795d758f88- deployment-2445  ddafd49c-c1c7-439b-a849-47e353aaeb23 1739709 0 2021-09-27 17:49:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc006704c37 0xc006704c38}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.5.77\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7n57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7n57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-62-6.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.62.6,PodIP:100.96.5.77,StartTime:2021-09-27 17:49:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.5.77,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.060: INFO: Pod "webserver-deployment-795d758f88-dk862" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-dk862 webserver-deployment-795d758f88- deployment-2445  35a03be4-35e0-4d5c-8d28-cabc145e9f54 1739764 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc006704e37 0xc006704e38}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kt78t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kt78t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.061: INFO: Pod "webserver-deployment-795d758f88-fsxrd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fsxrd webserver-deployment-795d758f88- deployment-2445  b4ce78b2-e1c5-45ad-a495-33ca18026c1b 1739765 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc006704fa7 0xc006704fa8}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2r6q8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2r6q8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-62-6.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.061: INFO: Pod "webserver-deployment-795d758f88-hbjjw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hbjjw webserver-deployment-795d758f88- deployment-2445  4763c36f-81a0-4426-9866-48a15206ecb2 1739761 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc006705117 0xc006705118}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vcv9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vcv9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.061: INFO: Pod "webserver-deployment-795d758f88-nbvtx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nbvtx webserver-deployment-795d758f88- deployment-2445  de16f84e-71b1-44df-a530-e73f6c20e7d6 1739706 0 2021-09-27 17:49:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc006705297 0xc006705298}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.5.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c6c46,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c6c46,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-62-6.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.62.6,PodIP:100.96.5.76,StartTime:2021-09-27 17:49:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.5.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.061: INFO: Pod "webserver-deployment-795d758f88-nprdr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nprdr webserver-deployment-795d758f88- deployment-2445  9276c95d-d8b7-4eac-b58a-27a7f548ab24 1739713 0 2021-09-27 17:49:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc006705497 0xc006705498}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.235\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zdzk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zdzk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.95.24,PodIP:100.96.4.235,StartTime:2021-09-27 17:49:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.235,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.061: INFO: Pod "webserver-deployment-795d758f88-nqjb5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nqjb5 webserver-deployment-795d758f88- deployment-2445  6f17d76f-378f-4fea-928e-4c7d0ed71928 1739702 0 2021-09-27 17:49:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc006705697 0xc006705698}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pzzjc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pzzjc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.31.225,PodIP:100.96.1.33,StartTime:2021-09-27 17:49:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.061: INFO: Pod "webserver-deployment-795d758f88-p85bt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-p85bt webserver-deployment-795d758f88- deployment-2445  ddd97666-573c-46d4-96f8-54d0ca3a057e 1739688 0 2021-09-27 17:49:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc006705897 0xc006705898}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sf5kc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sf5kc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.95.24,PodIP:100.96.4.236,StartTime:2021-09-27 17:49:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.236,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.061: INFO: Pod "webserver-deployment-795d758f88-s9t5j" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-s9t5j webserver-deployment-795d758f88- deployment-2445  04368934-817b-4e50-8bd5-8a77fac5e2ba 1739727 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc006705a97 0xc006705a98}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7swfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7swfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.062: INFO: Pod "webserver-deployment-795d758f88-vknvg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vknvg webserver-deployment-795d758f88- deployment-2445  c4f23a8c-7981-4095-8c1a-95ba73a2e32e 1739768 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 7b1683b0-cf80-4696-881d-a00dbc071c72 0xc006705c07 0xc006705c08}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b1683b0-cf80-4696-881d-a00dbc071c72\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s4j8l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s4j8l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.062: INFO: Pod "webserver-deployment-847dcfb7fb-2s5lv" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2s5lv webserver-deployment-847dcfb7fb- deployment-2445  a5ba8844-100a-4fab-b680-a8c1becba3c3 1739767 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc006705d60 0xc006705d61}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lx6b7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lx6b7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.062: INFO: Pod "webserver-deployment-847dcfb7fb-5qq24" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-5qq24 webserver-deployment-847dcfb7fb- deployment-2445  06eed139-8c1d-41fa-a28f-552e40052d5e 1739728 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc006705eb7 0xc006705eb8}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fdgwj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fdgwj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.062: INFO: Pod "webserver-deployment-847dcfb7fb-67pv5" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-67pv5 webserver-deployment-847dcfb7fb- deployment-2445  69c13c89-0deb-4be5-865e-de191c28a40e 1739766 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008842017 0xc008842018}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7s4ss,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7s4ss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.95.24,PodIP:,StartTime:2021-09-27 17:49:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.062: INFO: Pod "webserver-deployment-847dcfb7fb-6c8kf" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-6c8kf webserver-deployment-847dcfb7fb- deployment-2445  e2b02353-9c59-480a-b902-d64acb752735 1739744 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc0088421c7 0xc0088421c8}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fpqtj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fpqtj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.062: INFO: Pod "webserver-deployment-847dcfb7fb-8kmts" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8kmts webserver-deployment-847dcfb7fb- deployment-2445  08b7812f-8340-4aa1-a634-7c64c9734745 1739759 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008842327 0xc008842328}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2rw8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2rw8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.062: INFO: Pod "webserver-deployment-847dcfb7fb-bvf6x" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-bvf6x webserver-deployment-847dcfb7fb- deployment-2445  df9e2e36-68be-42ab-bced-7581b9a156f1 1739563 0 2021-09-27 17:49:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008842470 0xc008842471}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.5.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmr8l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmr8l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-62-6.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.62.6,PodIP:100.96.5.75,StartTime:2021-09-27 17:49:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 17:49:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://676059be50d40eb3df82995ac0a697ace1f273c3414d3b38ca9e04cab8831c66,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.5.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.062: INFO: Pod "webserver-deployment-847dcfb7fb-ch8rk" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-ch8rk webserver-deployment-847dcfb7fb- deployment-2445  e0c74e1c-9b9e-4750-8972-b146e9c6b092 1739741 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008842647 0xc008842648}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h8tpg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h8tpg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-62-6.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.063: INFO: Pod "webserver-deployment-847dcfb7fb-djgsx" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-djgsx webserver-deployment-847dcfb7fb- deployment-2445  4227509f-ff7e-41e0-a95e-fbeb0f3c0f75 1739556 0 2021-09-27 17:49:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc0088427a7 0xc0088427a8}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r4skd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4skd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.31.225,PodIP:100.96.1.31,StartTime:2021-09-27 17:49:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 17:49:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://cb8555ce98c6ed0110e7ee6ca992bbe2b6ec81ff9978f0d1195c58d0c81f6d5f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.063: INFO: Pod "webserver-deployment-847dcfb7fb-dwbqw" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-dwbqw webserver-deployment-847dcfb7fb- deployment-2445  8062a356-a69d-4408-aee7-bdfd7559260a 1739569 0 2021-09-27 17:49:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008842977 0xc008842978}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.5.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9dhsr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9dhsr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-62-6.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.62.6,PodIP:100.96.5.74,StartTime:2021-09-27 17:49:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 17:49:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://ae2f6b5c6aba7f367fa3d6b235e3ab6683e3e8a9d76655d9d927eeb92eeea038,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.5.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.063: INFO: Pod "webserver-deployment-847dcfb7fb-gh97n" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-gh97n webserver-deployment-847dcfb7fb- deployment-2445  c54501ef-1647-4be2-9312-8b571b208945 1739726 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008842b47 0xc008842b48}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqknd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqknd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.063: INFO: Pod "webserver-deployment-847dcfb7fb-lqkvn" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-lqkvn webserver-deployment-847dcfb7fb- deployment-2445  b0496a17-70d3-4fa9-956b-e079c62235b2 1739566 0 2021-09-27 17:49:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008842ca7 0xc008842ca8}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.5.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kxlx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kxlx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-62-6.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.62.6,PodIP:100.96.5.73,StartTime:2021-09-27 17:49:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 17:49:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://3c2460096878f5ee2e4f249967feb5bec95ef0870d804c6b2a764f69de14130a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.5.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.063: INFO: Pod "webserver-deployment-847dcfb7fb-mcs7z" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mcs7z webserver-deployment-847dcfb7fb- deployment-2445  9c21c2fa-69b1-405c-9043-3621343fa301 1739760 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008842e77 0xc008842e78}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6sklm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6sklm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.063: INFO: Pod "webserver-deployment-847dcfb7fb-mltx4" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-mltx4 webserver-deployment-847dcfb7fb- deployment-2445  a932ea88-04ec-444b-9ee2-755fb8e450be 1739572 0 2021-09-27 17:49:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008842fc0 0xc008842fc1}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j4jpp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j4jpp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.95.24,PodIP:100.96.4.233,StartTime:2021-09-27 17:49:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 17:49:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://02d66bca4539e7d0649b3ce8898b02495cd737c53c2d2bf61ff309f204b4b435,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.063: INFO: Pod "webserver-deployment-847dcfb7fb-pmkp8" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-pmkp8 webserver-deployment-847dcfb7fb- deployment-2445  8f220a16-9479-4f45-9338-70b83e56a257 1739550 0 2021-09-27 17:49:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008843197 0xc008843198}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6d2mm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6d2mm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.95.24,PodIP:100.96.4.231,StartTime:2021-09-27 17:49:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 17:49:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://bae64eb8ccfd0fabc3bac8c188735a72110de46fa1be1ad46651bd78318cee19,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.063: INFO: Pod "webserver-deployment-847dcfb7fb-rndxq" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-rndxq webserver-deployment-847dcfb7fb- deployment-2445  412b1d3a-f6b2-4098-ae65-4ee0265e8f54 1739745 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008843367 0xc008843368}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ffmbw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ffmbw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-62-6.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.064: INFO: Pod "webserver-deployment-847dcfb7fb-sbrdf" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-sbrdf webserver-deployment-847dcfb7fb- deployment-2445  7208fb01-0836-4758-b3b0-9862a6db649a 1739743 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc0088434c7 0xc0088434c8}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2gglx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2gglx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.064: INFO: Pod "webserver-deployment-847dcfb7fb-tmgrc" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-tmgrc webserver-deployment-847dcfb7fb- deployment-2445  8b288ed2-d090-42e8-bdd6-5181d407da99 1739559 0 2021-09-27 17:49:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008843627 0xc008843628}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bs9sc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bs9sc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.31.225,PodIP:100.96.1.32,StartTime:2021-09-27 17:49:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 17:49:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://937928853e6caff2967c83aef88b373dd8d1cb8993ad855f7f08cec8ba598a37,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.064: INFO: Pod "webserver-deployment-847dcfb7fb-w7qpr" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-w7qpr webserver-deployment-847dcfb7fb- deployment-2445  b73d660e-2fbb-4d91-98cb-d75157177414 1739757 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008843807 0xc008843808}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjqcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjqcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.064: INFO: Pod "webserver-deployment-847dcfb7fb-wbbvq" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wbbvq webserver-deployment-847dcfb7fb- deployment-2445  adfed3e2-11b7-4337-925c-85450064b2a8 1739554 0 2021-09-27 17:49:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008843950 0xc008843951}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:49:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfm25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfm25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:49:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.31.225,PodIP:100.96.1.30,StartTime:2021-09-27 17:49:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 17:49:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://0f563dbf3e572b348936fdbe0d1023446dce44ff0039444e0fb69462c194924a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep 27 17:49:56.064: INFO: Pod "webserver-deployment-847dcfb7fb-wpzpr" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wpzpr webserver-deployment-847dcfb7fb- deployment-2445  8bd38f69-9143-4fa8-a880-f07146a0b43f 1739758 0 2021-09-27 17:49:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 84de47c0-c212-4a53-b124-af6bc9b57700 0xc008843b27 0xc008843b28}] []  [{kube-controller-manager Update v1 2021-09-27 17:49:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84de47c0-c212-4a53-b124-af6bc9b57700\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bpvz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bpvz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:49:56.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2445" for this suite.

• [SLOW TEST:6.238 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":339,"completed":309,"skipped":4831,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:49:56.091: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-a192046a-1ad8-478b-8422-b5b6692b96a8
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:50:04.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6641" for this suite.

• [SLOW TEST:8.099 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":310,"skipped":4850,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:50:04.190: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 27 17:50:04.238: INFO: Waiting up to 5m0s for pod "pod-c1380190-40a6-498a-b87d-69a3749e98b4" in namespace "emptydir-8952" to be "Succeeded or Failed"
Sep 27 17:50:04.242: INFO: Pod "pod-c1380190-40a6-498a-b87d-69a3749e98b4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.68213ms
Sep 27 17:50:06.249: INFO: Pod "pod-c1380190-40a6-498a-b87d-69a3749e98b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01126356s
STEP: Saw pod success
Sep 27 17:50:06.249: INFO: Pod "pod-c1380190-40a6-498a-b87d-69a3749e98b4" satisfied condition "Succeeded or Failed"
Sep 27 17:50:06.253: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-c1380190-40a6-498a-b87d-69a3749e98b4 container test-container: <nil>
STEP: delete the pod
Sep 27 17:50:06.311: INFO: Waiting for pod pod-c1380190-40a6-498a-b87d-69a3749e98b4 to disappear
Sep 27 17:50:06.315: INFO: Pod pod-c1380190-40a6-498a-b87d-69a3749e98b4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:50:06.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8952" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":311,"skipped":4861,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:50:06.326: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 17:50:06.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31e3c32a-515e-49cc-a8b7-9bba3408f3b7" in namespace "projected-5776" to be "Succeeded or Failed"
Sep 27 17:50:06.375: INFO: Pod "downwardapi-volume-31e3c32a-515e-49cc-a8b7-9bba3408f3b7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.596844ms
Sep 27 17:50:08.384: INFO: Pod "downwardapi-volume-31e3c32a-515e-49cc-a8b7-9bba3408f3b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012181169s
STEP: Saw pod success
Sep 27 17:50:08.384: INFO: Pod "downwardapi-volume-31e3c32a-515e-49cc-a8b7-9bba3408f3b7" satisfied condition "Succeeded or Failed"
Sep 27 17:50:08.388: INFO: Trying to get logs from node ip-10-0-62-6.us-east-2.compute.internal pod downwardapi-volume-31e3c32a-515e-49cc-a8b7-9bba3408f3b7 container client-container: <nil>
STEP: delete the pod
Sep 27 17:50:08.424: INFO: Waiting for pod downwardapi-volume-31e3c32a-515e-49cc-a8b7-9bba3408f3b7 to disappear
Sep 27 17:50:08.428: INFO: Pod downwardapi-volume-31e3c32a-515e-49cc-a8b7-9bba3408f3b7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:50:08.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5776" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":312,"skipped":4867,"failed":0}
SSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:50:08.440: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:50:08.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-916" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":339,"completed":313,"skipped":4872,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:50:08.554: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep 27 17:50:08.588: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep 27 17:50:27.173: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
Sep 27 17:50:32.277: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:50:51.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5730" for this suite.

• [SLOW TEST:42.769 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":339,"completed":314,"skipped":4891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:50:51.323: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 27 17:50:53.383: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:50:53.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6991" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":315,"skipped":4936,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:50:53.414: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:50:53.466: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 27 17:50:58.487: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Sep 27 17:50:58.505: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Sep 27 17:50:58.516: INFO: observed ReplicaSet test-rs in namespace replicaset-3718 with ReadyReplicas 1, AvailableReplicas 1
Sep 27 17:50:58.564: INFO: observed ReplicaSet test-rs in namespace replicaset-3718 with ReadyReplicas 1, AvailableReplicas 1
Sep 27 17:50:58.588: INFO: observed ReplicaSet test-rs in namespace replicaset-3718 with ReadyReplicas 1, AvailableReplicas 1
Sep 27 17:50:58.606: INFO: observed ReplicaSet test-rs in namespace replicaset-3718 with ReadyReplicas 1, AvailableReplicas 1
Sep 27 17:50:59.239: INFO: observed ReplicaSet test-rs in namespace replicaset-3718 with ReadyReplicas 2, AvailableReplicas 2
Sep 27 17:51:00.353: INFO: observed Replicaset test-rs in namespace replicaset-3718 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:51:00.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3718" for this suite.

• [SLOW TEST:6.953 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":339,"completed":316,"skipped":4965,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:51:00.368: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Sep 27 17:51:00.415: INFO: The status of Pod labelsupdatedb7dc3c8-eb7c-4ff7-bcfd-1fb6facd37a5 is Pending, waiting for it to be Running (with Ready = true)
Sep 27 17:51:02.422: INFO: The status of Pod labelsupdatedb7dc3c8-eb7c-4ff7-bcfd-1fb6facd37a5 is Running (Ready = true)
Sep 27 17:51:02.948: INFO: Successfully updated pod "labelsupdatedb7dc3c8-eb7c-4ff7-bcfd-1fb6facd37a5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:51:06.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1998" for this suite.

• [SLOW TEST:6.624 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":317,"skipped":4974,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:51:06.992: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3141.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3141.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3141.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3141.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3141.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3141.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 17:51:09.094: INFO: DNS probes using dns-3141/dns-test-91f0613f-0603-4f77-9b08-23720e7011f1 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:51:09.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3141" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":339,"completed":318,"skipped":4979,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:51:09.145: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Sep 27 17:51:09.175: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep 27 17:51:11.215: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:51:12.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1442" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":339,"completed":319,"skipped":4984,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:51:12.238: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Sep 27 17:51:12.283: INFO: Waiting up to 5m0s for pod "client-containers-2e40d0e3-e46b-453d-8a9a-a852685d51f4" in namespace "containers-7180" to be "Succeeded or Failed"
Sep 27 17:51:12.286: INFO: Pod "client-containers-2e40d0e3-e46b-453d-8a9a-a852685d51f4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.434744ms
Sep 27 17:51:14.292: INFO: Pod "client-containers-2e40d0e3-e46b-453d-8a9a-a852685d51f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009699466s
STEP: Saw pod success
Sep 27 17:51:14.292: INFO: Pod "client-containers-2e40d0e3-e46b-453d-8a9a-a852685d51f4" satisfied condition "Succeeded or Failed"
Sep 27 17:51:14.296: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod client-containers-2e40d0e3-e46b-453d-8a9a-a852685d51f4 container agnhost-container: <nil>
STEP: delete the pod
Sep 27 17:51:14.319: INFO: Waiting for pod client-containers-2e40d0e3-e46b-453d-8a9a-a852685d51f4 to disappear
Sep 27 17:51:14.323: INFO: Pod client-containers-2e40d0e3-e46b-453d-8a9a-a852685d51f4 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:51:14.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7180" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":339,"completed":320,"skipped":5068,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:51:14.335: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:51:14.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4606" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":339,"completed":321,"skipped":5069,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:51:14.447: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 27 17:51:14.488: INFO: Waiting up to 5m0s for pod "pod-e8e7a681-aa43-4d37-bcce-71829d5c499e" in namespace "emptydir-2657" to be "Succeeded or Failed"
Sep 27 17:51:14.492: INFO: Pod "pod-e8e7a681-aa43-4d37-bcce-71829d5c499e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.927204ms
Sep 27 17:51:16.499: INFO: Pod "pod-e8e7a681-aa43-4d37-bcce-71829d5c499e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010856866s
STEP: Saw pod success
Sep 27 17:51:16.499: INFO: Pod "pod-e8e7a681-aa43-4d37-bcce-71829d5c499e" satisfied condition "Succeeded or Failed"
Sep 27 17:51:16.503: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-e8e7a681-aa43-4d37-bcce-71829d5c499e container test-container: <nil>
STEP: delete the pod
Sep 27 17:51:16.526: INFO: Waiting for pod pod-e8e7a681-aa43-4d37-bcce-71829d5c499e to disappear
Sep 27 17:51:16.529: INFO: Pod pod-e8e7a681-aa43-4d37-bcce-71829d5c499e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:51:16.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2657" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":322,"skipped":5126,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:51:16.542: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep 27 17:51:16.573: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 27 17:51:16.584: INFO: Waiting for terminating namespaces to be deleted...
Sep 27 17:51:16.588: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-31-225.us-east-2.compute.internal before test
Sep 27 17:51:16.595: INFO: antrea-agent-cv4j2 from kube-system started at 2021-09-23 17:40:20 +0000 UTC (2 container statuses recorded)
Sep 27 17:51:16.595: INFO: 	Container antrea-agent ready: true, restart count 0
Sep 27 17:51:16.595: INFO: 	Container antrea-ovs ready: true, restart count 0
Sep 27 17:51:16.595: INFO: kube-proxy-sjbtj from kube-system started at 2021-09-23 17:30:51 +0000 UTC (1 container statuses recorded)
Sep 27 17:51:16.595: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 17:51:16.595: INFO: metrics-server-6c47b7847-kd7fj from kube-system started at 2021-09-23 22:19:30 +0000 UTC (1 container statuses recorded)
Sep 27 17:51:16.595: INFO: 	Container metrics-server ready: true, restart count 0
Sep 27 17:51:16.595: INFO: condition-test-m4skh from replication-controller-1442 started at 2021-09-27 17:51:10 +0000 UTC (1 container statuses recorded)
Sep 27 17:51:16.595: INFO: 	Container httpd ready: true, restart count 0
Sep 27 17:51:16.595: INFO: sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-p4r29 from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 17:51:16.595: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 17:51:16.595: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 17:51:16.595: INFO: tkr-controller-manager-6bc455b5d4-m5rjt from tkr-system started at 2021-09-23 22:19:30 +0000 UTC (1 container statuses recorded)
Sep 27 17:51:16.595: INFO: 	Container manager ready: true, restart count 0
Sep 27 17:51:16.595: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-62-6.us-east-2.compute.internal before test
Sep 27 17:51:16.602: INFO: antrea-agent-85fx9 from kube-system started at 2021-09-27 15:39:42 +0000 UTC (2 container statuses recorded)
Sep 27 17:51:16.602: INFO: 	Container antrea-agent ready: true, restart count 0
Sep 27 17:51:16.602: INFO: 	Container antrea-ovs ready: true, restart count 0
Sep 27 17:51:16.602: INFO: kube-proxy-882cf from kube-system started at 2021-09-27 15:39:42 +0000 UTC (1 container statuses recorded)
Sep 27 17:51:16.602: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 17:51:16.602: INFO: condition-test-mzj2g from replication-controller-1442 started at 2021-09-27 17:51:10 +0000 UTC (1 container statuses recorded)
Sep 27 17:51:16.602: INFO: 	Container httpd ready: true, restart count 0
Sep 27 17:51:16.602: INFO: sonobuoy-e2e-job-87b0b9424e96441f from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 17:51:16.602: INFO: 	Container e2e ready: true, restart count 0
Sep 27 17:51:16.602: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 17:51:16.602: INFO: sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-88tn9 from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 17:51:16.602: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 17:51:16.602: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 27 17:51:16.602: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-95-24.us-east-2.compute.internal before test
Sep 27 17:51:16.609: INFO: antrea-agent-jwpnj from kube-system started at 2021-09-27 15:39:42 +0000 UTC (2 container statuses recorded)
Sep 27 17:51:16.609: INFO: 	Container antrea-agent ready: true, restart count 0
Sep 27 17:51:16.609: INFO: 	Container antrea-ovs ready: true, restart count 0
Sep 27 17:51:16.609: INFO: kube-proxy-2bm5v from kube-system started at 2021-09-27 15:39:42 +0000 UTC (1 container statuses recorded)
Sep 27 17:51:16.609: INFO: 	Container kube-proxy ready: true, restart count 0
Sep 27 17:51:16.609: INFO: labelsupdatedb7dc3c8-eb7c-4ff7-bcfd-1fb6facd37a5 from projected-1998 started at 2021-09-27 17:51:00 +0000 UTC (1 container statuses recorded)
Sep 27 17:51:16.609: INFO: 	Container client-container ready: false, restart count 0
Sep 27 17:51:16.609: INFO: sonobuoy from sonobuoy started at 2021-09-27 15:43:02 +0000 UTC (1 container statuses recorded)
Sep 27 17:51:16.609: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 27 17:51:16.609: INFO: sonobuoy-systemd-logs-daemon-set-d3c8508d0dee41c4-nv46v from sonobuoy started at 2021-09-27 15:43:07 +0000 UTC (2 container statuses recorded)
Sep 27 17:51:16.609: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 27 17:51:16.609: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2f0bb5a2-69d0-484d-b747-617c8453df47 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-2f0bb5a2-69d0-484d-b747-617c8453df47 off the node ip-10-0-95-24.us-east-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2f0bb5a2-69d0-484d-b747-617c8453df47
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:51:20.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6871" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":339,"completed":323,"skipped":5157,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:51:20.716: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-0f09f9f3-b9b8-4d1f-a1c2-fc2dcc5551a4
STEP: Creating a pod to test consume secrets
Sep 27 17:51:20.765: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-432fb0af-1764-4f6d-bf9d-ef3d3adb630a" in namespace "projected-6346" to be "Succeeded or Failed"
Sep 27 17:51:20.769: INFO: Pod "pod-projected-secrets-432fb0af-1764-4f6d-bf9d-ef3d3adb630a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.442848ms
Sep 27 17:51:22.776: INFO: Pod "pod-projected-secrets-432fb0af-1764-4f6d-bf9d-ef3d3adb630a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010988156s
STEP: Saw pod success
Sep 27 17:51:22.776: INFO: Pod "pod-projected-secrets-432fb0af-1764-4f6d-bf9d-ef3d3adb630a" satisfied condition "Succeeded or Failed"
Sep 27 17:51:22.780: INFO: Trying to get logs from node ip-10-0-31-225.us-east-2.compute.internal pod pod-projected-secrets-432fb0af-1764-4f6d-bf9d-ef3d3adb630a container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 17:51:22.802: INFO: Waiting for pod pod-projected-secrets-432fb0af-1764-4f6d-bf9d-ef3d3adb630a to disappear
Sep 27 17:51:22.806: INFO: Pod pod-projected-secrets-432fb0af-1764-4f6d-bf9d-ef3d3adb630a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:51:22.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6346" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":324,"skipped":5177,"failed":0}
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:51:22.818: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Sep 27 17:51:22.852: INFO: PodSpec: initContainers in spec.initContainers
Sep 27 17:52:03.352: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3a2acc20-2390-45f6-af10-b5b13470b4ef", GenerateName:"", Namespace:"init-container-7052", SelfLink:"", UID:"fba10993-dc8d-4545-804f-60876433a60f", ResourceVersion:"1741244", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63768361882, loc:(*time.Location)(0x9dde5a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"852635715"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc009da7e00), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc009da7e18)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc009da7e30), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc009da7e48)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-g2snr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00c052460), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-g2snr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-g2snr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.4.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-g2snr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0109111e0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-31-225.us-east-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000caa230), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc010911290)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0109112c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0109112c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0109112cc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00a410c40), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361882, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361882, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361882, loc:(*time.Location)(0x9dde5a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63768361882, loc:(*time.Location)(0x9dde5a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.31.225", PodIP:"100.96.1.38", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.96.1.38"}}, StartTime:(*v1.Time)(0xc009da7e78), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000caa380)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000caa3f0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"containerd://fcc4882ad501c0a524df24d41f3ee14c5fa95ca43877eacf80f50b64b972b43b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00c0524e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00c0524c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.4.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc01091142f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:52:03.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7052" for this suite.

• [SLOW TEST:40.552 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":339,"completed":325,"skipped":5182,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:52:03.370: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-51c64989-8c01-49d2-9a51-da7264556ad7 in namespace container-probe-5732
Sep 27 17:52:05.421: INFO: Started pod busybox-51c64989-8c01-49d2-9a51-da7264556ad7 in namespace container-probe-5732
STEP: checking the pod's current state and verifying that restartCount is present
Sep 27 17:52:05.426: INFO: Initial restart count of pod busybox-51c64989-8c01-49d2-9a51-da7264556ad7 is 0
Sep 27 17:52:55.595: INFO: Restart count of pod container-probe-5732/busybox-51c64989-8c01-49d2-9a51-da7264556ad7 is now 1 (50.168721621s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:52:55.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5732" for this suite.

• [SLOW TEST:52.251 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":326,"skipped":5184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:52:55.621: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 17:52:55.665: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18ad44ca-273a-4fd9-b0e2-a1439cde1602" in namespace "downward-api-4844" to be "Succeeded or Failed"
Sep 27 17:52:55.669: INFO: Pod "downwardapi-volume-18ad44ca-273a-4fd9-b0e2-a1439cde1602": Phase="Pending", Reason="", readiness=false. Elapsed: 3.951152ms
Sep 27 17:52:57.676: INFO: Pod "downwardapi-volume-18ad44ca-273a-4fd9-b0e2-a1439cde1602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010208615s
STEP: Saw pod success
Sep 27 17:52:57.676: INFO: Pod "downwardapi-volume-18ad44ca-273a-4fd9-b0e2-a1439cde1602" satisfied condition "Succeeded or Failed"
Sep 27 17:52:57.682: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downwardapi-volume-18ad44ca-273a-4fd9-b0e2-a1439cde1602 container client-container: <nil>
STEP: delete the pod
Sep 27 17:52:57.716: INFO: Waiting for pod downwardapi-volume-18ad44ca-273a-4fd9-b0e2-a1439cde1602 to disappear
Sep 27 17:52:57.720: INFO: Pod downwardapi-volume-18ad44ca-273a-4fd9-b0e2-a1439cde1602 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:52:57.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4844" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":327,"skipped":5211,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:52:57.733: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1548
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Sep 27 17:52:57.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-2142 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Sep 27 17:52:58.246: INFO: stderr: ""
Sep 27 17:52:58.246: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep 27 17:53:03.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-2142 get pod e2e-test-httpd-pod -o json'
Sep 27 17:53:03.356: INFO: stderr: ""
Sep 27 17:53:03.356: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-09-27T17:52:58Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2142\",\n        \"resourceVersion\": \"1741601\",\n        \"uid\": \"feafebf8-6a28-453a-8e48-ede423a80cb3\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-fc4pb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-95-24.us-east-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-fc4pb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-27T17:52:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-27T17:52:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-27T17:52:59Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-27T17:52:58Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://7002bc9feba26466d323609f8eb2740da345e5d90b81923bd7d3dec9b7d24a55\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-09-27T17:52:58Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.95.24\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.4.247\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.96.4.247\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-09-27T17:52:58Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 27 17:53:03.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-2142 replace -f -'
Sep 27 17:53:03.781: INFO: stderr: ""
Sep 27 17:53:03.781: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1552
Sep 27 17:53:03.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-348526495 --namespace=kubectl-2142 delete pods e2e-test-httpd-pod'
Sep 27 17:53:12.085: INFO: stderr: ""
Sep 27 17:53:12.085: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:53:12.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2142" for this suite.

• [SLOW TEST:14.364 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":339,"completed":328,"skipped":5231,"failed":0}
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:53:12.097: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Sep 27 17:53:12.146: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 27 17:53:12.146: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 27 17:53:12.155: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 27 17:53:12.155: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 27 17:53:12.168: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 27 17:53:12.168: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 27 17:53:12.189: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 27 17:53:12.189: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep 27 17:53:13.223: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep 27 17:53:13.223: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep 27 17:53:13.438: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Sep 27 17:53:13.448: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 0
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:13.449: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:13.459: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:13.459: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:13.472: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:13.472: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:13.496: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1
Sep 27 17:53:13.496: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1
Sep 27 17:53:14.550: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:14.550: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:14.578: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1
STEP: listing Deployments
Sep 27 17:53:14.584: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Sep 27 17:53:14.595: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Sep 27 17:53:14.604: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 27 17:53:14.607: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 27 17:53:14.625: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 27 17:53:14.639: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 27 17:53:14.650: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep 27 17:53:15.451: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 27 17:53:15.464: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 27 17:53:15.473: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 27 17:53:15.485: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Sep 27 17:53:16.240: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Sep 27 17:53:16.280: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1
Sep 27 17:53:16.280: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1
Sep 27 17:53:16.280: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1
Sep 27 17:53:16.280: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1
Sep 27 17:53:16.280: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 1
Sep 27 17:53:16.281: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:16.281: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:16.281: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:16.281: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 2
Sep 27 17:53:16.281: INFO: observed Deployment test-deployment in namespace deployment-4364 with ReadyReplicas 3
STEP: deleting the Deployment
Sep 27 17:53:16.292: INFO: observed event type MODIFIED
Sep 27 17:53:16.292: INFO: observed event type MODIFIED
Sep 27 17:53:16.292: INFO: observed event type MODIFIED
Sep 27 17:53:16.292: INFO: observed event type MODIFIED
Sep 27 17:53:16.292: INFO: observed event type MODIFIED
Sep 27 17:53:16.292: INFO: observed event type MODIFIED
Sep 27 17:53:16.292: INFO: observed event type MODIFIED
Sep 27 17:53:16.292: INFO: observed event type MODIFIED
Sep 27 17:53:16.293: INFO: observed event type MODIFIED
Sep 27 17:53:16.293: INFO: observed event type MODIFIED
Sep 27 17:53:16.293: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Sep 27 17:53:16.301: INFO: Log out all the ReplicaSets if there is no deployment created
Sep 27 17:53:16.304: INFO: ReplicaSet "test-deployment-748588b7cd":
&ReplicaSet{ObjectMeta:{test-deployment-748588b7cd  deployment-4364  4ed3a275-5b67-484a-923f-be51e5d84800 1741863 4 2021-09-27 17:53:13 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 023e3d25-fea2-427a-a246-dbbfdaedeaba 0xc00947b667 0xc00947b668}] []  [{kube-controller-manager Update apps/v1 2021-09-27 17:53:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"023e3d25-fea2-427a-a246-dbbfdaedeaba\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 748588b7cd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:748588b7cd test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.4.1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00947b6d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Sep 27 17:53:16.308: INFO: ReplicaSet "test-deployment-7b4c744884":
&ReplicaSet{ObjectMeta:{test-deployment-7b4c744884  deployment-4364  26e0877b-5460-44d6-9c56-2e640637230f 1741786 3 2021-09-27 17:53:12 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 023e3d25-fea2-427a-a246-dbbfdaedeaba 0xc00947b737 0xc00947b738}] []  [{kube-controller-manager Update apps/v1 2021-09-27 17:53:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"023e3d25-fea2-427a-a246-dbbfdaedeaba\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b4c744884,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00947b7a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Sep 27 17:53:16.312: INFO: ReplicaSet "test-deployment-85d87c6f4b":
&ReplicaSet{ObjectMeta:{test-deployment-85d87c6f4b  deployment-4364  6e6d1582-023d-4db2-a45a-2bbbf4278b3f 1741855 2 2021-09-27 17:53:14 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 023e3d25-fea2-427a-a246-dbbfdaedeaba 0xc00947b807 0xc00947b808}] []  [{kube-controller-manager Update apps/v1 2021-09-27 17:53:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"023e3d25-fea2-427a-a246-dbbfdaedeaba\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 85d87c6f4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00947b870 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Sep 27 17:53:16.316: INFO: pod: "test-deployment-85d87c6f4b-c9c9f":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-c9c9f test-deployment-85d87c6f4b- deployment-4364  82869649-8d23-43df-9e97-b9061245dcc5 1741819 0 2021-09-27 17:53:14 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b 6e6d1582-023d-4db2-a45a-2bbbf4278b3f 0xc00947bf17 0xc00947bf18}] []  [{kube-controller-manager Update v1 2021-09-27 17:53:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e6d1582-023d-4db2-a45a-2bbbf4278b3f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:53:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.1.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nnx6j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nnx6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-31-225.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:53:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:53:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:53:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:53:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.31.225,PodIP:100.96.1.40,StartTime:2021-09-27 17:53:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 17:53:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://5fe7378326ad94fcbc7174d024dba2e4ba620182bc671fa0c7e516492298eb13,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.1.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep 27 17:53:16.316: INFO: pod: "test-deployment-85d87c6f4b-f746c":
&Pod{ObjectMeta:{test-deployment-85d87c6f4b-f746c test-deployment-85d87c6f4b- deployment-4364  ecd087ed-ac51-4ac7-9563-de5b9974d3e8 1741854 0 2021-09-27 17:53:15 +0000 UTC <nil> <nil> map[pod-template-hash:85d87c6f4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-85d87c6f4b 6e6d1582-023d-4db2-a45a-2bbbf4278b3f 0xc00a61a107 0xc00a61a108}] []  [{kube-controller-manager Update v1 2021-09-27 17:53:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e6d1582-023d-4db2-a45a-2bbbf4278b3f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-09-27 17:53:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-djlmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-djlmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-95-24.us-east-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:53:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:53:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:53:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-27 17:53:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.95.24,PodIP:100.96.4.251,StartTime:2021-09-27 17:53:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-27 17:53:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://711a791ec26dd15faf1c4b8590ab05b24c75d3d5f56dcc681091e53bf1db88ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:53:16.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4364" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":339,"completed":329,"skipped":5231,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:53:16.331: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 17:53:16.386: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5b33d931-b30e-4582-90f0-f5bbdd469143" in namespace "downward-api-3199" to be "Succeeded or Failed"
Sep 27 17:53:16.390: INFO: Pod "downwardapi-volume-5b33d931-b30e-4582-90f0-f5bbdd469143": Phase="Pending", Reason="", readiness=false. Elapsed: 3.662174ms
Sep 27 17:53:18.395: INFO: Pod "downwardapi-volume-5b33d931-b30e-4582-90f0-f5bbdd469143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008954804s
STEP: Saw pod success
Sep 27 17:53:18.395: INFO: Pod "downwardapi-volume-5b33d931-b30e-4582-90f0-f5bbdd469143" satisfied condition "Succeeded or Failed"
Sep 27 17:53:18.399: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downwardapi-volume-5b33d931-b30e-4582-90f0-f5bbdd469143 container client-container: <nil>
STEP: delete the pod
Sep 27 17:53:18.422: INFO: Waiting for pod downwardapi-volume-5b33d931-b30e-4582-90f0-f5bbdd469143 to disappear
Sep 27 17:53:18.425: INFO: Pod downwardapi-volume-5b33d931-b30e-4582-90f0-f5bbdd469143 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:53:18.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3199" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":330,"skipped":5264,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:53:18.440: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Sep 27 17:53:18.481: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9250b9f-eea1-4c1a-b1ad-f46af3284d6a" in namespace "downward-api-8006" to be "Succeeded or Failed"
Sep 27 17:53:18.485: INFO: Pod "downwardapi-volume-f9250b9f-eea1-4c1a-b1ad-f46af3284d6a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.539004ms
Sep 27 17:53:20.491: INFO: Pod "downwardapi-volume-f9250b9f-eea1-4c1a-b1ad-f46af3284d6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009794352s
STEP: Saw pod success
Sep 27 17:53:20.491: INFO: Pod "downwardapi-volume-f9250b9f-eea1-4c1a-b1ad-f46af3284d6a" satisfied condition "Succeeded or Failed"
Sep 27 17:53:20.495: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod downwardapi-volume-f9250b9f-eea1-4c1a-b1ad-f46af3284d6a container client-container: <nil>
STEP: delete the pod
Sep 27 17:53:20.528: INFO: Waiting for pod downwardapi-volume-f9250b9f-eea1-4c1a-b1ad-f46af3284d6a to disappear
Sep 27 17:53:20.531: INFO: Pod downwardapi-volume-f9250b9f-eea1-4c1a-b1ad-f46af3284d6a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:53:20.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8006" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":331,"skipped":5266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:53:20.545: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-805e5882-457a-4da7-afda-81df64bfe1b0
STEP: Creating a pod to test consume secrets
Sep 27 17:53:20.595: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06e7cda6-8c52-4308-a1fa-bdcfbda98dcc" in namespace "projected-4602" to be "Succeeded or Failed"
Sep 27 17:53:20.598: INFO: Pod "pod-projected-secrets-06e7cda6-8c52-4308-a1fa-bdcfbda98dcc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.433096ms
Sep 27 17:53:22.605: INFO: Pod "pod-projected-secrets-06e7cda6-8c52-4308-a1fa-bdcfbda98dcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010200893s
STEP: Saw pod success
Sep 27 17:53:22.605: INFO: Pod "pod-projected-secrets-06e7cda6-8c52-4308-a1fa-bdcfbda98dcc" satisfied condition "Succeeded or Failed"
Sep 27 17:53:22.609: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-projected-secrets-06e7cda6-8c52-4308-a1fa-bdcfbda98dcc container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 17:53:22.636: INFO: Waiting for pod pod-projected-secrets-06e7cda6-8c52-4308-a1fa-bdcfbda98dcc to disappear
Sep 27 17:53:22.640: INFO: Pod pod-projected-secrets-06e7cda6-8c52-4308-a1fa-bdcfbda98dcc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:53:22.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4602" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":332,"skipped":5322,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:53:22.656: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 27 17:53:22.697: INFO: Waiting up to 5m0s for pod "pod-b26fbb2c-9205-4740-95e0-83747940d634" in namespace "emptydir-9900" to be "Succeeded or Failed"
Sep 27 17:53:22.701: INFO: Pod "pod-b26fbb2c-9205-4740-95e0-83747940d634": Phase="Pending", Reason="", readiness=false. Elapsed: 3.763908ms
Sep 27 17:53:24.707: INFO: Pod "pod-b26fbb2c-9205-4740-95e0-83747940d634": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009995881s
STEP: Saw pod success
Sep 27 17:53:24.707: INFO: Pod "pod-b26fbb2c-9205-4740-95e0-83747940d634" satisfied condition "Succeeded or Failed"
Sep 27 17:53:24.711: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-b26fbb2c-9205-4740-95e0-83747940d634 container test-container: <nil>
STEP: delete the pod
Sep 27 17:53:24.735: INFO: Waiting for pod pod-b26fbb2c-9205-4740-95e0-83747940d634 to disappear
Sep 27 17:53:24.738: INFO: Pod pod-b26fbb2c-9205-4740-95e0-83747940d634 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:53:24.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9900" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":333,"skipped":5325,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:53:24.749: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-2a88bafc-34ac-4392-98c1-5c444e24690b
STEP: Creating a pod to test consume secrets
Sep 27 17:53:24.795: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d367b355-50f7-40cb-91ad-236f6967e6dd" in namespace "projected-8330" to be "Succeeded or Failed"
Sep 27 17:53:24.799: INFO: Pod "pod-projected-secrets-d367b355-50f7-40cb-91ad-236f6967e6dd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.429443ms
Sep 27 17:53:26.805: INFO: Pod "pod-projected-secrets-d367b355-50f7-40cb-91ad-236f6967e6dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009896838s
STEP: Saw pod success
Sep 27 17:53:26.805: INFO: Pod "pod-projected-secrets-d367b355-50f7-40cb-91ad-236f6967e6dd" satisfied condition "Succeeded or Failed"
Sep 27 17:53:26.809: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-projected-secrets-d367b355-50f7-40cb-91ad-236f6967e6dd container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 27 17:53:26.833: INFO: Waiting for pod pod-projected-secrets-d367b355-50f7-40cb-91ad-236f6967e6dd to disappear
Sep 27 17:53:26.836: INFO: Pod pod-projected-secrets-d367b355-50f7-40cb-91ad-236f6967e6dd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:53:26.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8330" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":334,"skipped":5329,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:53:26.849: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1842.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1842.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 17:53:28.928: INFO: DNS probes using dns-test-78604d9f-a5b2-4af4-9c7f-bf795d69ae0e succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1842.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1842.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 17:53:30.987: INFO: File wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local from pod  dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 17:53:30.992: INFO: File jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local from pod  dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 17:53:30.992: INFO: Lookups using dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 failed for: [wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local]

Sep 27 17:53:35.998: INFO: File wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local from pod  dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 17:53:36.003: INFO: File jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local from pod  dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 17:53:36.003: INFO: Lookups using dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 failed for: [wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local]

Sep 27 17:53:40.997: INFO: File wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local from pod  dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 17:53:41.002: INFO: File jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local from pod  dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 17:53:41.002: INFO: Lookups using dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 failed for: [wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local]

Sep 27 17:53:45.997: INFO: File wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local from pod  dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 17:53:46.001: INFO: File jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local from pod  dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 17:53:46.001: INFO: Lookups using dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 failed for: [wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local]

Sep 27 17:53:50.998: INFO: File wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local from pod  dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 17:53:51.002: INFO: File jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local from pod  dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 17:53:51.002: INFO: Lookups using dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 failed for: [wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local]

Sep 27 17:53:55.998: INFO: File wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local from pod  dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 17:53:56.003: INFO: File jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local from pod  dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 contains 'foo.example.com.
' instead of 'bar.example.com.'
Sep 27 17:53:56.003: INFO: Lookups using dns-1842/dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 failed for: [wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local]

Sep 27 17:54:01.003: INFO: DNS probes using dns-test-380c0915-60d0-4524-8647-c6b0ceca40d0 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1842.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1842.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1842.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1842.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 27 17:54:03.091: INFO: DNS probes using dns-test-00dcc1b5-c96a-4b66-b248-52700ff30e14 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:54:03.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1842" for this suite.

• [SLOW TEST:36.291 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":339,"completed":335,"skipped":5383,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:54:03.140: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep 27 17:54:05.204: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:54:05.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3353" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":336,"skipped":5409,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:54:05.237: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 27 17:54:05.281: INFO: Waiting up to 5m0s for pod "pod-551b1ee7-d10a-4f7c-a4f0-97a5e4cb4a03" in namespace "emptydir-1632" to be "Succeeded or Failed"
Sep 27 17:54:05.284: INFO: Pod "pod-551b1ee7-d10a-4f7c-a4f0-97a5e4cb4a03": Phase="Pending", Reason="", readiness=false. Elapsed: 3.871254ms
Sep 27 17:54:07.294: INFO: Pod "pod-551b1ee7-d10a-4f7c-a4f0-97a5e4cb4a03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013225967s
STEP: Saw pod success
Sep 27 17:54:07.294: INFO: Pod "pod-551b1ee7-d10a-4f7c-a4f0-97a5e4cb4a03" satisfied condition "Succeeded or Failed"
Sep 27 17:54:07.298: INFO: Trying to get logs from node ip-10-0-95-24.us-east-2.compute.internal pod pod-551b1ee7-d10a-4f7c-a4f0-97a5e4cb4a03 container test-container: <nil>
STEP: delete the pod
Sep 27 17:54:07.321: INFO: Waiting for pod pod-551b1ee7-d10a-4f7c-a4f0-97a5e4cb4a03 to disappear
Sep 27 17:54:07.325: INFO: Pod pod-551b1ee7-d10a-4f7c-a4f0-97a5e4cb4a03 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:54:07.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1632" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":337,"skipped":5409,"failed":0}
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:54:07.338: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Sep 27 17:54:07.372: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:54:10.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8947" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":339,"completed":338,"skipped":5413,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Sep 27 17:54:10.347: INFO: >>> kubeConfig: /tmp/kubeconfig-348526495
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep 27 17:54:10.732: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep 27 17:54:13.760: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Sep 27 17:54:13.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1263" for this suite.
STEP: Destroying namespace "webhook-1263-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":339,"completed":339,"skipped":5432,"failed":0}
Sep 27 17:54:13.890: INFO: Running AfterSuite actions on all nodes
Sep 27 17:54:13.890: INFO: Running AfterSuite actions on node 1
Sep 27 17:54:13.890: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":339,"completed":339,"skipped":5432,"failed":0}

Ran 339 of 5771 Specs in 7839.786 seconds
SUCCESS! -- 339 Passed | 0 Failed | 0 Pending | 5432 Skipped
PASS

Ginkgo ran 1 suite in 2h10m41.026452255s
Test Suite Passed
