I0311 15:56:00.596453      19 e2e.go:129] Starting e2e run "84b524fd-3b1e-4ba7-8d3e-349d0681232f" on Ginkgo node 1
{"msg":"Test Suite starting","total":339,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1647014158 - Will randomize all specs
Will run 339 of 5770 specs

Mar 11 15:56:00.612: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 15:56:00.614: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0311 15:56:00.613136      19 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar 11 15:56:00.685: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 11 15:56:00.758: INFO: 29 / 29 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 11 15:56:00.758: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Mar 11 15:56:00.758: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 11 15:56:00.771: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
Mar 11 15:56:00.771: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar 11 15:56:00.771: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'vsphere-cloud-controller-manager' (0 seconds elapsed)
Mar 11 15:56:00.771: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'vsphere-csi-node' (0 seconds elapsed)
Mar 11 15:56:00.771: INFO: e2e test version: v1.21.5
Mar 11 15:56:00.772: INFO: kube-apiserver version: v1.21.5-eks-1-21
Mar 11 15:56:00.772: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 15:56:00.779: INFO: Cluster IP family: ipv4
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:56:00.779: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename resourcequota
Mar 11 15:56:00.843: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
W0311 15:56:00.843718      19 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:56:11.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4744" for this suite.

• [SLOW TEST:11.160 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":339,"completed":1,"skipped":8,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:56:11.943: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 11 15:56:12.043: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:12.043: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:12.049: INFO: Number of nodes with available pods: 0
Mar 11 15:56:12.049: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 15:56:13.058: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:13.058: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:13.068: INFO: Number of nodes with available pods: 0
Mar 11 15:56:13.068: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 15:56:14.058: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:14.058: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:14.062: INFO: Number of nodes with available pods: 0
Mar 11 15:56:14.062: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 15:56:15.059: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:15.059: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:15.063: INFO: Number of nodes with available pods: 0
Mar 11 15:56:15.063: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 15:56:16.059: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:16.059: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:16.063: INFO: Number of nodes with available pods: 0
Mar 11 15:56:16.063: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 15:56:17.056: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:17.056: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:17.061: INFO: Number of nodes with available pods: 0
Mar 11 15:56:17.062: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 15:56:18.064: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:18.064: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:18.071: INFO: Number of nodes with available pods: 0
Mar 11 15:56:18.071: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 15:56:19.057: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:19.057: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:19.061: INFO: Number of nodes with available pods: 1
Mar 11 15:56:19.061: INFO: Node 198.18.167.130 is running more than one daemon pod
Mar 11 15:56:20.059: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:20.060: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:20.064: INFO: Number of nodes with available pods: 2
Mar 11 15:56:20.064: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 11 15:56:20.088: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:20.088: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:20.096: INFO: Number of nodes with available pods: 1
Mar 11 15:56:20.096: INFO: Node 198.18.167.130 is running more than one daemon pod
Mar 11 15:56:21.101: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:21.101: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:21.105: INFO: Number of nodes with available pods: 1
Mar 11 15:56:21.105: INFO: Node 198.18.167.130 is running more than one daemon pod
Mar 11 15:56:22.106: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:22.106: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 15:56:22.111: INFO: Number of nodes with available pods: 2
Mar 11 15:56:22.111: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-953, will wait for the garbage collector to delete the pods
Mar 11 15:56:22.182: INFO: Deleting DaemonSet.extensions daemon-set took: 10.165136ms
Mar 11 15:56:22.283: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.987111ms
Mar 11 15:56:31.187: INFO: Number of nodes with available pods: 0
Mar 11 15:56:31.187: INFO: Number of running nodes: 0, number of available pods: 0
Mar 11 15:56:31.193: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10372"},"items":null}

Mar 11 15:56:31.197: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10372"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:56:31.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-953" for this suite.

• [SLOW TEST:19.277 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":339,"completed":2,"skipped":21,"failed":0}
SSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:56:31.220: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:56:31.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5136" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":3,"skipped":24,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:56:31.331: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 15:56:31.814: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 11 15:56:31.815: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 11 15:56:31.815: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.815: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 11 15:56:31.815: INFO: Checking APIGroup: apps
Mar 11 15:56:31.816: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 11 15:56:31.816: INFO: Versions found [{apps/v1 v1}]
Mar 11 15:56:31.816: INFO: apps/v1 matches apps/v1
Mar 11 15:56:31.816: INFO: Checking APIGroup: events.k8s.io
Mar 11 15:56:31.817: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 11 15:56:31.817: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.817: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 11 15:56:31.817: INFO: Checking APIGroup: authentication.k8s.io
Mar 11 15:56:31.820: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 11 15:56:31.820: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.820: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 11 15:56:31.820: INFO: Checking APIGroup: authorization.k8s.io
Mar 11 15:56:31.821: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 11 15:56:31.821: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.821: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 11 15:56:31.821: INFO: Checking APIGroup: autoscaling
Mar 11 15:56:31.822: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Mar 11 15:56:31.822: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Mar 11 15:56:31.822: INFO: autoscaling/v1 matches autoscaling/v1
Mar 11 15:56:31.822: INFO: Checking APIGroup: batch
Mar 11 15:56:31.823: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 11 15:56:31.823: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Mar 11 15:56:31.823: INFO: batch/v1 matches batch/v1
Mar 11 15:56:31.823: INFO: Checking APIGroup: certificates.k8s.io
Mar 11 15:56:31.824: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 11 15:56:31.824: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.824: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 11 15:56:31.824: INFO: Checking APIGroup: networking.k8s.io
Mar 11 15:56:31.825: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 11 15:56:31.825: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.825: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 11 15:56:31.825: INFO: Checking APIGroup: extensions
Mar 11 15:56:31.826: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Mar 11 15:56:31.826: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Mar 11 15:56:31.826: INFO: extensions/v1beta1 matches extensions/v1beta1
Mar 11 15:56:31.826: INFO: Checking APIGroup: policy
Mar 11 15:56:31.827: INFO: PreferredVersion.GroupVersion: policy/v1
Mar 11 15:56:31.827: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Mar 11 15:56:31.827: INFO: policy/v1 matches policy/v1
Mar 11 15:56:31.827: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 11 15:56:31.828: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 11 15:56:31.828: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.828: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 11 15:56:31.828: INFO: Checking APIGroup: storage.k8s.io
Mar 11 15:56:31.829: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 11 15:56:31.829: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.829: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 11 15:56:31.829: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 11 15:56:31.830: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 11 15:56:31.830: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.830: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 11 15:56:31.830: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 11 15:56:31.831: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 11 15:56:31.831: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.831: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 11 15:56:31.831: INFO: Checking APIGroup: scheduling.k8s.io
Mar 11 15:56:31.832: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 11 15:56:31.832: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.832: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 11 15:56:31.832: INFO: Checking APIGroup: coordination.k8s.io
Mar 11 15:56:31.833: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 11 15:56:31.833: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.833: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 11 15:56:31.833: INFO: Checking APIGroup: node.k8s.io
Mar 11 15:56:31.833: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 11 15:56:31.834: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.834: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 11 15:56:31.834: INFO: Checking APIGroup: discovery.k8s.io
Mar 11 15:56:31.834: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar 11 15:56:31.834: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.834: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar 11 15:56:31.834: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 11 15:56:31.835: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Mar 11 15:56:31.835: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar 11 15:56:31.835: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Mar 11 15:56:31.835: INFO: Checking APIGroup: acme.cert-manager.io
Mar 11 15:56:31.836: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Mar 11 15:56:31.836: INFO: Versions found [{acme.cert-manager.io/v1 v1} {acme.cert-manager.io/v1beta1 v1beta1} {acme.cert-manager.io/v1alpha3 v1alpha3} {acme.cert-manager.io/v1alpha2 v1alpha2}]
Mar 11 15:56:31.836: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Mar 11 15:56:31.836: INFO: Checking APIGroup: cert-manager.io
Mar 11 15:56:31.837: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Mar 11 15:56:31.837: INFO: Versions found [{cert-manager.io/v1 v1} {cert-manager.io/v1beta1 v1beta1} {cert-manager.io/v1alpha3 v1alpha3} {cert-manager.io/v1alpha2 v1alpha2}]
Mar 11 15:56:31.837: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Mar 11 15:56:31.837: INFO: Checking APIGroup: anywhere.eks.amazonaws.com
Mar 11 15:56:31.838: INFO: PreferredVersion.GroupVersion: anywhere.eks.amazonaws.com/v1alpha1
Mar 11 15:56:31.838: INFO: Versions found [{anywhere.eks.amazonaws.com/v1alpha1 v1alpha1}]
Mar 11 15:56:31.838: INFO: anywhere.eks.amazonaws.com/v1alpha1 matches anywhere.eks.amazonaws.com/v1alpha1
Mar 11 15:56:31.838: INFO: Checking APIGroup: addons.cluster.x-k8s.io
Mar 11 15:56:31.839: INFO: PreferredVersion.GroupVersion: addons.cluster.x-k8s.io/v1beta1
Mar 11 15:56:31.839: INFO: Versions found [{addons.cluster.x-k8s.io/v1beta1 v1beta1} {addons.cluster.x-k8s.io/v1alpha4 v1alpha4} {addons.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Mar 11 15:56:31.839: INFO: addons.cluster.x-k8s.io/v1beta1 matches addons.cluster.x-k8s.io/v1beta1
Mar 11 15:56:31.839: INFO: Checking APIGroup: bootstrap.cluster.x-k8s.io
Mar 11 15:56:31.840: INFO: PreferredVersion.GroupVersion: bootstrap.cluster.x-k8s.io/v1beta1
Mar 11 15:56:31.840: INFO: Versions found [{bootstrap.cluster.x-k8s.io/v1beta1 v1beta1} {bootstrap.cluster.x-k8s.io/v1alpha4 v1alpha4} {bootstrap.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Mar 11 15:56:31.840: INFO: bootstrap.cluster.x-k8s.io/v1beta1 matches bootstrap.cluster.x-k8s.io/v1beta1
Mar 11 15:56:31.840: INFO: Checking APIGroup: cluster.x-k8s.io
Mar 11 15:56:31.841: INFO: PreferredVersion.GroupVersion: cluster.x-k8s.io/v1beta1
Mar 11 15:56:31.841: INFO: Versions found [{cluster.x-k8s.io/v1beta1 v1beta1} {cluster.x-k8s.io/v1alpha4 v1alpha4} {cluster.x-k8s.io/v1alpha3 v1alpha3}]
Mar 11 15:56:31.841: INFO: cluster.x-k8s.io/v1beta1 matches cluster.x-k8s.io/v1beta1
Mar 11 15:56:31.841: INFO: Checking APIGroup: clusterctl.cluster.x-k8s.io
Mar 11 15:56:31.842: INFO: PreferredVersion.GroupVersion: clusterctl.cluster.x-k8s.io/v1alpha3
Mar 11 15:56:31.842: INFO: Versions found [{clusterctl.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Mar 11 15:56:31.842: INFO: clusterctl.cluster.x-k8s.io/v1alpha3 matches clusterctl.cluster.x-k8s.io/v1alpha3
Mar 11 15:56:31.842: INFO: Checking APIGroup: controlplane.cluster.x-k8s.io
Mar 11 15:56:31.843: INFO: PreferredVersion.GroupVersion: controlplane.cluster.x-k8s.io/v1beta1
Mar 11 15:56:31.843: INFO: Versions found [{controlplane.cluster.x-k8s.io/v1beta1 v1beta1} {controlplane.cluster.x-k8s.io/v1alpha4 v1alpha4} {controlplane.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Mar 11 15:56:31.843: INFO: controlplane.cluster.x-k8s.io/v1beta1 matches controlplane.cluster.x-k8s.io/v1beta1
Mar 11 15:56:31.843: INFO: Checking APIGroup: etcdcluster.cluster.x-k8s.io
Mar 11 15:56:31.844: INFO: PreferredVersion.GroupVersion: etcdcluster.cluster.x-k8s.io/v1beta1
Mar 11 15:56:31.844: INFO: Versions found [{etcdcluster.cluster.x-k8s.io/v1beta1 v1beta1} {etcdcluster.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Mar 11 15:56:31.844: INFO: etcdcluster.cluster.x-k8s.io/v1beta1 matches etcdcluster.cluster.x-k8s.io/v1beta1
Mar 11 15:56:31.844: INFO: Checking APIGroup: infrastructure.cluster.x-k8s.io
Mar 11 15:56:31.845: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.x-k8s.io/v1beta1
Mar 11 15:56:31.845: INFO: Versions found [{infrastructure.cluster.x-k8s.io/v1beta1 v1beta1} {infrastructure.cluster.x-k8s.io/v1alpha4 v1alpha4} {infrastructure.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Mar 11 15:56:31.845: INFO: infrastructure.cluster.x-k8s.io/v1beta1 matches infrastructure.cluster.x-k8s.io/v1beta1
Mar 11 15:56:31.845: INFO: Checking APIGroup: cilium.io
Mar 11 15:56:31.846: INFO: PreferredVersion.GroupVersion: cilium.io/v2
Mar 11 15:56:31.846: INFO: Versions found [{cilium.io/v2 v2}]
Mar 11 15:56:31.846: INFO: cilium.io/v2 matches cilium.io/v2
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:56:31.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1111" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":339,"completed":4,"skipped":79,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:56:31.860: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-942.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-942.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-942.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-942.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-942.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-942.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 11 15:56:45.989: INFO: DNS probes using dns-942/dns-test-b0743bd6-7ab8-4516-ae2a-bb0fd6c3cbdd succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:56:46.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-942" for this suite.

• [SLOW TEST:14.189 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":339,"completed":5,"skipped":123,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:56:46.051: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-22d2e450-ff09-4cd4-87e2-017c813ade25
STEP: Creating a pod to test consume secrets
Mar 11 15:56:46.119: INFO: Waiting up to 5m0s for pod "pod-secrets-0cfd0cbe-311d-4de5-8eea-77ced64b6fe7" in namespace "secrets-1605" to be "Succeeded or Failed"
Mar 11 15:56:46.122: INFO: Pod "pod-secrets-0cfd0cbe-311d-4de5-8eea-77ced64b6fe7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.640246ms
Mar 11 15:56:48.134: INFO: Pod "pod-secrets-0cfd0cbe-311d-4de5-8eea-77ced64b6fe7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015664608s
Mar 11 15:56:50.143: INFO: Pod "pod-secrets-0cfd0cbe-311d-4de5-8eea-77ced64b6fe7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024132586s
Mar 11 15:56:52.153: INFO: Pod "pod-secrets-0cfd0cbe-311d-4de5-8eea-77ced64b6fe7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033820325s
STEP: Saw pod success
Mar 11 15:56:52.153: INFO: Pod "pod-secrets-0cfd0cbe-311d-4de5-8eea-77ced64b6fe7" satisfied condition "Succeeded or Failed"
Mar 11 15:56:52.157: INFO: Trying to get logs from node 198.18.167.130 pod pod-secrets-0cfd0cbe-311d-4de5-8eea-77ced64b6fe7 container secret-volume-test: <nil>
STEP: delete the pod
Mar 11 15:56:52.198: INFO: Waiting for pod pod-secrets-0cfd0cbe-311d-4de5-8eea-77ced64b6fe7 to disappear
Mar 11 15:56:52.202: INFO: Pod pod-secrets-0cfd0cbe-311d-4de5-8eea-77ced64b6fe7 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:56:52.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1605" for this suite.

• [SLOW TEST:6.163 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":6,"skipped":139,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:56:52.214: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:56:52.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3221" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":339,"completed":7,"skipped":164,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:56:52.280: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 11 15:56:52.335: INFO: Waiting up to 5m0s for pod "pod-fe9bf757-711c-4453-9cb5-4b1aecdcb4d4" in namespace "emptydir-4971" to be "Succeeded or Failed"
Mar 11 15:56:52.340: INFO: Pod "pod-fe9bf757-711c-4453-9cb5-4b1aecdcb4d4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.541236ms
Mar 11 15:56:54.348: INFO: Pod "pod-fe9bf757-711c-4453-9cb5-4b1aecdcb4d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013267474s
Mar 11 15:56:56.356: INFO: Pod "pod-fe9bf757-711c-4453-9cb5-4b1aecdcb4d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021405526s
STEP: Saw pod success
Mar 11 15:56:56.356: INFO: Pod "pod-fe9bf757-711c-4453-9cb5-4b1aecdcb4d4" satisfied condition "Succeeded or Failed"
Mar 11 15:56:56.363: INFO: Trying to get logs from node 198.18.167.130 pod pod-fe9bf757-711c-4453-9cb5-4b1aecdcb4d4 container test-container: <nil>
STEP: delete the pod
Mar 11 15:56:56.387: INFO: Waiting for pod pod-fe9bf757-711c-4453-9cb5-4b1aecdcb4d4 to disappear
Mar 11 15:56:56.390: INFO: Pod pod-fe9bf757-711c-4453-9cb5-4b1aecdcb4d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:56:56.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4971" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":8,"skipped":173,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:56:56.406: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 11 15:56:57.404: INFO: starting watch
STEP: patching
STEP: updating
Mar 11 15:56:57.423: INFO: waiting for watch events with expected annotations
Mar 11 15:56:57.423: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:56:57.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7212" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":339,"completed":9,"skipped":205,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:56:57.527: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:57:13.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-633" for this suite.

• [SLOW TEST:16.246 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":339,"completed":10,"skipped":214,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:57:13.774: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 15:57:13.864: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 11 15:57:13.879: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 11 15:57:18.887: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 11 15:57:18.887: INFO: Creating deployment "test-rolling-update-deployment"
Mar 11 15:57:18.895: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 11 15:57:18.902: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 11 15:57:20.915: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 11 15:57:20.917: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Mar 11 15:57:20.929: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9794  9ce71cab-a084-4e82-bec5-e94b7b2bd4cf 11179 1 2022-03-11 15:57:18 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-03-11 15:57:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-11 15:57:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00467a7d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-11 15:57:18 +0000 UTC,LastTransitionTime:2022-03-11 15:57:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2022-03-11 15:57:20 +0000 UTC,LastTransitionTime:2022-03-11 15:57:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 11 15:57:20.933: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-9794  b7e71e64-5039-4718-869f-8e748344dc54 11168 1 2022-03-11 15:57:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 9ce71cab-a084-4e82-bec5-e94b7b2bd4cf 0xc00467aca7 0xc00467aca8}] []  [{kube-controller-manager Update apps/v1 2022-03-11 15:57:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ce71cab-a084-4e82-bec5-e94b7b2bd4cf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00467ad38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 11 15:57:20.933: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 11 15:57:20.933: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9794  e505fffe-85ff-41d9-aa63-a192b28cb699 11178 2 2022-03-11 15:57:13 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 9ce71cab-a084-4e82-bec5-e94b7b2bd4cf 0xc00467ab97 0xc00467ab98}] []  [{e2e.test Update apps/v1 2022-03-11 15:57:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-11 15:57:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ce71cab-a084-4e82-bec5-e94b7b2bd4cf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00467ac38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 11 15:57:20.937: INFO: Pod "test-rolling-update-deployment-585b757574-6cp47" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-6cp47 test-rolling-update-deployment-585b757574- deployment-9794  808a322f-3f41-4c7e-b15a-6d04ee32dbf0 11167 0 2022-03-11 15:57:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 b7e71e64-5039-4718-869f-8e748344dc54 0xc00467b177 0xc00467b178}] []  [{kube-controller-manager Update v1 2022-03-11 15:57:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7e71e64-5039-4718-869f-8e748344dc54\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 15:57:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8llz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8llz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 15:57:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 15:57:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 15:57:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 15:57:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.16.160,PodIP:192.168.1.44,StartTime:2022-03-11 15:57:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 15:57:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://0873d155799ec03c5cf57147c44e3135943edb0cf937867a8848550ce1353285,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.1.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:57:20.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9794" for this suite.

• [SLOW TEST:7.178 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":11,"skipped":240,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:57:20.956: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 15:57:21.034: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6cbacaba-9a23-4f74-970b-32d3124517bf" in namespace "projected-2773" to be "Succeeded or Failed"
Mar 11 15:57:21.042: INFO: Pod "downwardapi-volume-6cbacaba-9a23-4f74-970b-32d3124517bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.376816ms
Mar 11 15:57:23.049: INFO: Pod "downwardapi-volume-6cbacaba-9a23-4f74-970b-32d3124517bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015203095s
STEP: Saw pod success
Mar 11 15:57:23.049: INFO: Pod "downwardapi-volume-6cbacaba-9a23-4f74-970b-32d3124517bf" satisfied condition "Succeeded or Failed"
Mar 11 15:57:23.053: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-6cbacaba-9a23-4f74-970b-32d3124517bf container client-container: <nil>
STEP: delete the pod
Mar 11 15:57:23.095: INFO: Waiting for pod downwardapi-volume-6cbacaba-9a23-4f74-970b-32d3124517bf to disappear
Mar 11 15:57:23.099: INFO: Pod downwardapi-volume-6cbacaba-9a23-4f74-970b-32d3124517bf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:57:23.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2773" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":12,"skipped":265,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:57:23.117: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:57:55.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7609" for this suite.
STEP: Destroying namespace "nsdeletetest-4788" for this suite.
Mar 11 15:57:55.340: INFO: Namespace nsdeletetest-4788 was already deleted
STEP: Destroying namespace "nsdeletetest-2856" for this suite.

• [SLOW TEST:32.229 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":339,"completed":13,"skipped":270,"failed":0}
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:57:55.349: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Mar 11 15:57:55.389: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 15:58:00.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3904" for this suite.

• [SLOW TEST:5.202 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":339,"completed":14,"skipped":278,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 15:58:00.554: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
W0311 15:58:00.619029      19 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:03:00.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4565" for this suite.

• [SLOW TEST:300.118 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":339,"completed":15,"skipped":286,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:03:00.673: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-3675
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 11 16:03:00.729: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 11 16:03:00.773: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:03:02.781: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:03:04.781: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:03:06.778: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:03:08.781: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:03:10.781: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:03:12.781: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:03:14.781: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:03:16.778: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:03:18.780: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:03:20.782: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 11 16:03:20.791: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Mar 11 16:03:22.850: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar 11 16:03:22.850: INFO: Going to poll 192.168.1.80 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Mar 11 16:03:22.853: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.1.80:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3675 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 16:03:22.853: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 16:03:22.953: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 11 16:03:22.953: INFO: Going to poll 192.168.2.109 on port 8080 at least 0 times, with a maximum of 34 tries before failing
Mar 11 16:03:22.958: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.2.109:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3675 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 16:03:22.958: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 16:03:23.048: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:03:23.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3675" for this suite.

• [SLOW TEST:22.391 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":16,"skipped":287,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:03:23.064: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-d7e2268f-8f4e-4462-a2ba-41852e6362b9
STEP: Creating a pod to test consume secrets
Mar 11 16:03:23.137: INFO: Waiting up to 5m0s for pod "pod-secrets-f484c01b-dba4-400d-a937-e23d9c97d10c" in namespace "secrets-5823" to be "Succeeded or Failed"
Mar 11 16:03:23.146: INFO: Pod "pod-secrets-f484c01b-dba4-400d-a937-e23d9c97d10c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.521339ms
Mar 11 16:03:25.150: INFO: Pod "pod-secrets-f484c01b-dba4-400d-a937-e23d9c97d10c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011587319s
Mar 11 16:03:27.162: INFO: Pod "pod-secrets-f484c01b-dba4-400d-a937-e23d9c97d10c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022977451s
STEP: Saw pod success
Mar 11 16:03:27.162: INFO: Pod "pod-secrets-f484c01b-dba4-400d-a937-e23d9c97d10c" satisfied condition "Succeeded or Failed"
Mar 11 16:03:27.167: INFO: Trying to get logs from node 198.18.16.160 pod pod-secrets-f484c01b-dba4-400d-a937-e23d9c97d10c container secret-volume-test: <nil>
STEP: delete the pod
Mar 11 16:03:27.203: INFO: Waiting for pod pod-secrets-f484c01b-dba4-400d-a937-e23d9c97d10c to disappear
Mar 11 16:03:27.214: INFO: Pod pod-secrets-f484c01b-dba4-400d-a937-e23d9c97d10c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:03:27.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5823" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":17,"skipped":306,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:03:27.241: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:03:27.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2691" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":339,"completed":18,"skipped":327,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:03:27.374: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Mar 11 16:03:27.443: INFO: The status of Pod labelsupdatef22b4f9a-b482-46fc-bb6b-42833ff0a2d8 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:03:29.450: INFO: The status of Pod labelsupdatef22b4f9a-b482-46fc-bb6b-42833ff0a2d8 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:03:31.450: INFO: The status of Pod labelsupdatef22b4f9a-b482-46fc-bb6b-42833ff0a2d8 is Running (Ready = true)
Mar 11 16:03:31.984: INFO: Successfully updated pod "labelsupdatef22b4f9a-b482-46fc-bb6b-42833ff0a2d8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:03:34.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5909" for this suite.

• [SLOW TEST:6.646 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":19,"skipped":341,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:03:34.020: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-2981
STEP: creating service affinity-clusterip in namespace services-2981
STEP: creating replication controller affinity-clusterip in namespace services-2981
I0311 16:03:34.109199      19 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-2981, replica count: 3
I0311 16:03:37.175570      19 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 11 16:03:37.185: INFO: Creating new exec pod
Mar 11 16:03:40.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-2981 exec execpod-affinitygbnk8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Mar 11 16:03:40.898: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 11 16:03:40.898: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:03:40.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-2981 exec execpod-affinitygbnk8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.229.104 80'
Mar 11 16:03:41.078: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.229.104 80\nConnection to 10.100.229.104 80 port [tcp/http] succeeded!\n"
Mar 11 16:03:41.078: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:03:41.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-2981 exec execpod-affinitygbnk8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.229.104:80/ ; done'
Mar 11 16:03:41.347: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.229.104:80/\n"
Mar 11 16:03:41.347: INFO: stdout: "\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s\naffinity-clusterip-cv54s"
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Received response from host: affinity-clusterip-cv54s
Mar 11 16:03:41.347: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2981, will wait for the garbage collector to delete the pods
Mar 11 16:03:41.441: INFO: Deleting ReplicationController affinity-clusterip took: 13.053829ms
Mar 11 16:03:41.542: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.507914ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:03:45.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2981" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:11.370 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":20,"skipped":344,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:03:45.393: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Mar 11 16:03:45.459: INFO: The status of Pod labelsupdateac6c188f-f860-4d28-8ba5-1555c895afc3 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:03:47.463: INFO: The status of Pod labelsupdateac6c188f-f860-4d28-8ba5-1555c895afc3 is Running (Ready = true)
Mar 11 16:03:48.009: INFO: Successfully updated pod "labelsupdateac6c188f-f860-4d28-8ba5-1555c895afc3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:03:50.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3474" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":21,"skipped":360,"failed":0}
S
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:03:50.048: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:03:50.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9302" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":22,"skipped":361,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:03:50.235: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Mar 11 16:03:50.301: INFO: Waiting up to 5m0s for pod "var-expansion-7b17f585-1189-43f4-8a1d-e29bdaa4ccef" in namespace "var-expansion-8114" to be "Succeeded or Failed"
Mar 11 16:03:50.309: INFO: Pod "var-expansion-7b17f585-1189-43f4-8a1d-e29bdaa4ccef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.975116ms
Mar 11 16:03:52.315: INFO: Pod "var-expansion-7b17f585-1189-43f4-8a1d-e29bdaa4ccef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013894331s
Mar 11 16:03:54.323: INFO: Pod "var-expansion-7b17f585-1189-43f4-8a1d-e29bdaa4ccef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021229683s
STEP: Saw pod success
Mar 11 16:03:54.323: INFO: Pod "var-expansion-7b17f585-1189-43f4-8a1d-e29bdaa4ccef" satisfied condition "Succeeded or Failed"
Mar 11 16:03:54.327: INFO: Trying to get logs from node 198.18.16.160 pod var-expansion-7b17f585-1189-43f4-8a1d-e29bdaa4ccef container dapi-container: <nil>
STEP: delete the pod
Mar 11 16:03:54.349: INFO: Waiting for pod var-expansion-7b17f585-1189-43f4-8a1d-e29bdaa4ccef to disappear
Mar 11 16:03:54.352: INFO: Pod var-expansion-7b17f585-1189-43f4-8a1d-e29bdaa4ccef no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:03:54.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8114" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":339,"completed":23,"skipped":389,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:03:54.365: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4167, will wait for the garbage collector to delete the pods
Mar 11 16:03:56.542: INFO: Deleting Job.batch foo took: 11.10045ms
Mar 11 16:03:56.643: INFO: Terminating Job.batch foo pods took: 101.145681ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:04:39.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4167" for this suite.

• [SLOW TEST:44.806 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":339,"completed":24,"skipped":401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:04:39.171: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 16:04:39.626: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 16:04:42.662: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:04:42.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2118" for this suite.
STEP: Destroying namespace "webhook-2118-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":339,"completed":25,"skipped":426,"failed":0}
SSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:04:43.002: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0311 16:04:43.062981      19 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:06:01.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5363" for this suite.

• [SLOW TEST:78.125 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":339,"completed":26,"skipped":431,"failed":0}
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:06:01.132: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:06:05.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7568" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":339,"completed":27,"skipped":431,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:06:05.238: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 11 16:06:05.309: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 11 16:07:05.355: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:07:05.359: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:07:05.422: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Mar 11 16:07:05.426: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:07:05.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4195" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:07:05.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7985" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.305 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":339,"completed":28,"skipped":460,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:07:05.544: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-7890
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 11 16:07:05.593: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 11 16:07:05.635: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:07:07.666: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:07:09.649: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:07:11.644: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:07:13.642: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:07:15.642: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:07:17.645: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:07:19.645: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:07:21.646: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:07:23.644: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 16:07:25.651: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 11 16:07:25.659: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Mar 11 16:07:27.698: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar 11 16:07:27.698: INFO: Breadth first check of 192.168.1.177 on host 198.18.16.160...
Mar 11 16:07:27.701: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.1.82:9080/dial?request=hostname&protocol=udp&host=192.168.1.177&port=8081&tries=1'] Namespace:pod-network-test-7890 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 16:07:27.701: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 16:07:27.788: INFO: Waiting for responses: map[]
Mar 11 16:07:27.788: INFO: reached 192.168.1.177 after 0/1 tries
Mar 11 16:07:27.788: INFO: Breadth first check of 192.168.2.130 on host 198.18.167.130...
Mar 11 16:07:27.792: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.1.82:9080/dial?request=hostname&protocol=udp&host=192.168.2.130&port=8081&tries=1'] Namespace:pod-network-test-7890 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 16:07:27.792: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 16:07:27.877: INFO: Waiting for responses: map[]
Mar 11 16:07:27.878: INFO: reached 192.168.2.130 after 0/1 tries
Mar 11 16:07:27.878: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:07:27.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7890" for this suite.

• [SLOW TEST:22.348 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":339,"completed":29,"skipped":475,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:07:27.892: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:07:27.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2544" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":339,"completed":30,"skipped":501,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:07:27.965: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 16:07:28.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61203a51-7bf1-49c5-a097-df3eae40fae2" in namespace "downward-api-900" to be "Succeeded or Failed"
Mar 11 16:07:28.044: INFO: Pod "downwardapi-volume-61203a51-7bf1-49c5-a097-df3eae40fae2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.964431ms
Mar 11 16:07:30.053: INFO: Pod "downwardapi-volume-61203a51-7bf1-49c5-a097-df3eae40fae2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013299999s
STEP: Saw pod success
Mar 11 16:07:30.053: INFO: Pod "downwardapi-volume-61203a51-7bf1-49c5-a097-df3eae40fae2" satisfied condition "Succeeded or Failed"
Mar 11 16:07:30.059: INFO: Trying to get logs from node 198.18.16.160 pod downwardapi-volume-61203a51-7bf1-49c5-a097-df3eae40fae2 container client-container: <nil>
STEP: delete the pod
Mar 11 16:07:30.101: INFO: Waiting for pod downwardapi-volume-61203a51-7bf1-49c5-a097-df3eae40fae2 to disappear
Mar 11 16:07:30.104: INFO: Pod downwardapi-volume-61203a51-7bf1-49c5-a097-df3eae40fae2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:07:30.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-900" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":31,"skipped":505,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:07:30.121: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 16:07:30.644: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 16:07:33.691: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:07:33.698: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:07:37.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4816" for this suite.
STEP: Destroying namespace "webhook-4816-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.733 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":339,"completed":32,"skipped":514,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:07:38.862: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-b7ae33f3-3589-446d-a0c0-38de23e65b9e
STEP: Creating a pod to test consume secrets
Mar 11 16:07:39.044: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2e77249b-3126-46ae-9a99-537db1b5bb2c" in namespace "projected-3700" to be "Succeeded or Failed"
Mar 11 16:07:39.059: INFO: Pod "pod-projected-secrets-2e77249b-3126-46ae-9a99-537db1b5bb2c": Phase="Pending", Reason="", readiness=false. Elapsed: 15.347994ms
Mar 11 16:07:41.069: INFO: Pod "pod-projected-secrets-2e77249b-3126-46ae-9a99-537db1b5bb2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025284959s
STEP: Saw pod success
Mar 11 16:07:41.069: INFO: Pod "pod-projected-secrets-2e77249b-3126-46ae-9a99-537db1b5bb2c" satisfied condition "Succeeded or Failed"
Mar 11 16:07:41.072: INFO: Trying to get logs from node 198.18.16.160 pod pod-projected-secrets-2e77249b-3126-46ae-9a99-537db1b5bb2c container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 11 16:07:41.101: INFO: Waiting for pod pod-projected-secrets-2e77249b-3126-46ae-9a99-537db1b5bb2c to disappear
Mar 11 16:07:41.104: INFO: Pod pod-projected-secrets-2e77249b-3126-46ae-9a99-537db1b5bb2c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:07:41.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3700" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":33,"skipped":514,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:07:41.115: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 16:07:41.180: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b799a94c-eab1-404d-9759-2dbf4420c929" in namespace "downward-api-3382" to be "Succeeded or Failed"
Mar 11 16:07:41.183: INFO: Pod "downwardapi-volume-b799a94c-eab1-404d-9759-2dbf4420c929": Phase="Pending", Reason="", readiness=false. Elapsed: 2.962614ms
Mar 11 16:07:43.196: INFO: Pod "downwardapi-volume-b799a94c-eab1-404d-9759-2dbf4420c929": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015335069s
STEP: Saw pod success
Mar 11 16:07:43.196: INFO: Pod "downwardapi-volume-b799a94c-eab1-404d-9759-2dbf4420c929" satisfied condition "Succeeded or Failed"
Mar 11 16:07:43.200: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-b799a94c-eab1-404d-9759-2dbf4420c929 container client-container: <nil>
STEP: delete the pod
Mar 11 16:07:43.254: INFO: Waiting for pod downwardapi-volume-b799a94c-eab1-404d-9759-2dbf4420c929 to disappear
Mar 11 16:07:43.257: INFO: Pod downwardapi-volume-b799a94c-eab1-404d-9759-2dbf4420c929 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:07:43.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3382" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":34,"skipped":515,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:07:43.283: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 11 16:07:43.430: INFO: Waiting up to 5m0s for pod "pod-2cbba685-48e4-476d-8534-abba410d111b" in namespace "emptydir-4151" to be "Succeeded or Failed"
Mar 11 16:07:43.475: INFO: Pod "pod-2cbba685-48e4-476d-8534-abba410d111b": Phase="Pending", Reason="", readiness=false. Elapsed: 45.230923ms
Mar 11 16:07:45.484: INFO: Pod "pod-2cbba685-48e4-476d-8534-abba410d111b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054049258s
STEP: Saw pod success
Mar 11 16:07:45.484: INFO: Pod "pod-2cbba685-48e4-476d-8534-abba410d111b" satisfied condition "Succeeded or Failed"
Mar 11 16:07:45.489: INFO: Trying to get logs from node 198.18.167.130 pod pod-2cbba685-48e4-476d-8534-abba410d111b container test-container: <nil>
STEP: delete the pod
Mar 11 16:07:45.532: INFO: Waiting for pod pod-2cbba685-48e4-476d-8534-abba410d111b to disappear
Mar 11 16:07:45.542: INFO: Pod pod-2cbba685-48e4-476d-8534-abba410d111b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:07:45.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4151" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":35,"skipped":516,"failed":0}

------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:07:45.559: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:07:47.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1019" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":339,"completed":36,"skipped":516,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:07:47.713: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar 11 16:07:47.790: INFO: The status of Pod pod-update-activedeadlineseconds-138b9b71-b4d0-4bc3-8e70-d1c0299716ea is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:07:49.800: INFO: The status of Pod pod-update-activedeadlineseconds-138b9b71-b4d0-4bc3-8e70-d1c0299716ea is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:07:51.799: INFO: The status of Pod pod-update-activedeadlineseconds-138b9b71-b4d0-4bc3-8e70-d1c0299716ea is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 11 16:07:52.322: INFO: Successfully updated pod "pod-update-activedeadlineseconds-138b9b71-b4d0-4bc3-8e70-d1c0299716ea"
Mar 11 16:07:52.323: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-138b9b71-b4d0-4bc3-8e70-d1c0299716ea" in namespace "pods-4893" to be "terminated due to deadline exceeded"
Mar 11 16:07:52.328: INFO: Pod "pod-update-activedeadlineseconds-138b9b71-b4d0-4bc3-8e70-d1c0299716ea": Phase="Running", Reason="", readiness=true. Elapsed: 5.070703ms
Mar 11 16:07:54.334: INFO: Pod "pod-update-activedeadlineseconds-138b9b71-b4d0-4bc3-8e70-d1c0299716ea": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.011733768s
Mar 11 16:07:54.334: INFO: Pod "pod-update-activedeadlineseconds-138b9b71-b4d0-4bc3-8e70-d1c0299716ea" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:07:54.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4893" for this suite.

• [SLOW TEST:6.637 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":339,"completed":37,"skipped":541,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:07:54.350: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 16:07:54.413: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7db17872-69e1-448d-bc0d-6a8539602f45" in namespace "projected-3839" to be "Succeeded or Failed"
Mar 11 16:07:54.422: INFO: Pod "downwardapi-volume-7db17872-69e1-448d-bc0d-6a8539602f45": Phase="Pending", Reason="", readiness=false. Elapsed: 9.390752ms
Mar 11 16:07:56.429: INFO: Pod "downwardapi-volume-7db17872-69e1-448d-bc0d-6a8539602f45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015930624s
STEP: Saw pod success
Mar 11 16:07:56.429: INFO: Pod "downwardapi-volume-7db17872-69e1-448d-bc0d-6a8539602f45" satisfied condition "Succeeded or Failed"
Mar 11 16:07:56.433: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-7db17872-69e1-448d-bc0d-6a8539602f45 container client-container: <nil>
STEP: delete the pod
Mar 11 16:07:56.460: INFO: Waiting for pod downwardapi-volume-7db17872-69e1-448d-bc0d-6a8539602f45 to disappear
Mar 11 16:07:56.463: INFO: Pod downwardapi-volume-7db17872-69e1-448d-bc0d-6a8539602f45 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:07:56.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3839" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":38,"skipped":544,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:07:56.476: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:08:01.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-258" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":339,"completed":39,"skipped":550,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:08:01.118: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 16:08:01.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4b1dcdb-5a9c-4604-8709-4d3aa2a3c941" in namespace "projected-986" to be "Succeeded or Failed"
Mar 11 16:08:01.186: INFO: Pod "downwardapi-volume-d4b1dcdb-5a9c-4604-8709-4d3aa2a3c941": Phase="Pending", Reason="", readiness=false. Elapsed: 3.398514ms
Mar 11 16:08:03.197: INFO: Pod "downwardapi-volume-d4b1dcdb-5a9c-4604-8709-4d3aa2a3c941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01366013s
STEP: Saw pod success
Mar 11 16:08:03.197: INFO: Pod "downwardapi-volume-d4b1dcdb-5a9c-4604-8709-4d3aa2a3c941" satisfied condition "Succeeded or Failed"
Mar 11 16:08:03.202: INFO: Trying to get logs from node 198.18.16.160 pod downwardapi-volume-d4b1dcdb-5a9c-4604-8709-4d3aa2a3c941 container client-container: <nil>
STEP: delete the pod
Mar 11 16:08:03.228: INFO: Waiting for pod downwardapi-volume-d4b1dcdb-5a9c-4604-8709-4d3aa2a3c941 to disappear
Mar 11 16:08:03.231: INFO: Pod downwardapi-volume-d4b1dcdb-5a9c-4604-8709-4d3aa2a3c941 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:08:03.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-986" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":40,"skipped":553,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:08:03.248: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Mar 11 16:08:03.314: INFO: The status of Pod annotationupdatef259edfa-c7fa-4d50-a3da-02bfa25d2b14 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:08:05.326: INFO: The status of Pod annotationupdatef259edfa-c7fa-4d50-a3da-02bfa25d2b14 is Running (Ready = true)
Mar 11 16:08:05.853: INFO: Successfully updated pod "annotationupdatef259edfa-c7fa-4d50-a3da-02bfa25d2b14"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:08:07.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9722" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":41,"skipped":571,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:08:07.895: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Mar 11 16:08:07.942: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Mar 11 16:08:07.951: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 11 16:08:07.951: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Mar 11 16:08:08.017: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 11 16:08:08.017: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Mar 11 16:08:08.038: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 11 16:08:08.039: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Mar 11 16:08:15.106: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:08:15.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5432" for this suite.

• [SLOW TEST:7.245 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":339,"completed":42,"skipped":581,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:08:15.141: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-7571
STEP: creating service affinity-nodeport in namespace services-7571
STEP: creating replication controller affinity-nodeport in namespace services-7571
I0311 16:08:15.283869      19 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-7571, replica count: 3
I0311 16:08:18.340165      19 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 11 16:08:18.353: INFO: Creating new exec pod
Mar 11 16:08:21.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-7571 exec execpod-affinitycwvx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Mar 11 16:08:21.573: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 11 16:08:21.573: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:08:21.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-7571 exec execpod-affinitycwvx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.193.45 80'
Mar 11 16:08:21.775: INFO: stderr: "+ nc -v -t -w 2 10.107.193.45 80\n+ echo hostName\nConnection to 10.107.193.45 80 port [tcp/http] succeeded!\n"
Mar 11 16:08:21.775: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:08:21.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-7571 exec execpod-affinitycwvx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 198.18.16.160 31542'
Mar 11 16:08:21.954: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 198.18.16.160 31542\nConnection to 198.18.16.160 31542 port [tcp/*] succeeded!\n"
Mar 11 16:08:21.954: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:08:21.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-7571 exec execpod-affinitycwvx6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 198.18.167.130 31542'
Mar 11 16:08:22.134: INFO: stderr: "+ + nc -v -t -w 2 198.18.167.130echo 31542 hostName\n\nConnection to 198.18.167.130 31542 port [tcp/*] succeeded!\n"
Mar 11 16:08:22.135: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:08:22.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-7571 exec execpod-affinitycwvx6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://198.18.16.160:31542/ ; done'
Mar 11 16:08:22.370: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31542/\n"
Mar 11 16:08:22.370: INFO: stdout: "\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd\naffinity-nodeport-9dkrd"
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Received response from host: affinity-nodeport-9dkrd
Mar 11 16:08:22.370: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7571, will wait for the garbage collector to delete the pods
Mar 11 16:08:22.470: INFO: Deleting ReplicationController affinity-nodeport took: 21.369282ms
Mar 11 16:08:22.571: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.143099ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:08:31.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7571" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:16.091 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":43,"skipped":587,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:08:31.233: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:08:42.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-177" for this suite.

• [SLOW TEST:11.271 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":339,"completed":44,"skipped":650,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:08:42.507: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 16:08:43.099: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 16:08:46.145: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:08:46.153: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7360-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:08:49.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3526" for this suite.
STEP: Destroying namespace "webhook-3526-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.102 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":339,"completed":45,"skipped":667,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:08:51.613: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 11 16:08:51.689: INFO: Waiting up to 5m0s for pod "pod-25d8c956-2e13-4283-9cdb-219ffe4fbf05" in namespace "emptydir-4229" to be "Succeeded or Failed"
Mar 11 16:08:51.693: INFO: Pod "pod-25d8c956-2e13-4283-9cdb-219ffe4fbf05": Phase="Pending", Reason="", readiness=false. Elapsed: 3.076674ms
Mar 11 16:08:53.707: INFO: Pod "pod-25d8c956-2e13-4283-9cdb-219ffe4fbf05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018036095s
STEP: Saw pod success
Mar 11 16:08:53.708: INFO: Pod "pod-25d8c956-2e13-4283-9cdb-219ffe4fbf05" satisfied condition "Succeeded or Failed"
Mar 11 16:08:53.712: INFO: Trying to get logs from node 198.18.167.130 pod pod-25d8c956-2e13-4283-9cdb-219ffe4fbf05 container test-container: <nil>
STEP: delete the pod
Mar 11 16:08:53.778: INFO: Waiting for pod pod-25d8c956-2e13-4283-9cdb-219ffe4fbf05 to disappear
Mar 11 16:08:53.781: INFO: Pod pod-25d8c956-2e13-4283-9cdb-219ffe4fbf05 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:08:53.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4229" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":46,"skipped":670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:08:53.807: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-3ff847b2-05a0-4f5c-9bec-53908802ce55 in namespace container-probe-1420
Mar 11 16:08:56.147: INFO: Started pod test-webserver-3ff847b2-05a0-4f5c-9bec-53908802ce55 in namespace container-probe-1420
STEP: checking the pod's current state and verifying that restartCount is present
Mar 11 16:08:56.155: INFO: Initial restart count of pod test-webserver-3ff847b2-05a0-4f5c-9bec-53908802ce55 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:12:57.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1420" for this suite.

• [SLOW TEST:243.388 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":47,"skipped":702,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:12:57.195: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar 11 16:12:57.277: INFO: The status of Pod pod-update-faf66e67-4703-481e-bf97-4ca390f193a7 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:12:59.283: INFO: The status of Pod pod-update-faf66e67-4703-481e-bf97-4ca390f193a7 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 11 16:12:59.807: INFO: Successfully updated pod "pod-update-faf66e67-4703-481e-bf97-4ca390f193a7"
STEP: verifying the updated pod is in kubernetes
Mar 11 16:12:59.814: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:12:59.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4852" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":339,"completed":48,"skipped":705,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:12:59.835: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-0bf73e10-7f48-4dcf-b0d6-dcfcedb1589f in namespace container-probe-5738
Mar 11 16:13:01.925: INFO: Started pod liveness-0bf73e10-7f48-4dcf-b0d6-dcfcedb1589f in namespace container-probe-5738
STEP: checking the pod's current state and verifying that restartCount is present
Mar 11 16:13:01.928: INFO: Initial restart count of pod liveness-0bf73e10-7f48-4dcf-b0d6-dcfcedb1589f is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:17:03.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5738" for this suite.

• [SLOW TEST:243.220 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":339,"completed":49,"skipped":731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:17:03.080: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Mar 11 16:17:03.134: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:17:44.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4238" for this suite.

• [SLOW TEST:41.191 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":339,"completed":50,"skipped":763,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:17:44.272: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-jgrj
STEP: Creating a pod to test atomic-volume-subpath
Mar 11 16:17:44.349: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jgrj" in namespace "subpath-5639" to be "Succeeded or Failed"
Mar 11 16:17:44.356: INFO: Pod "pod-subpath-test-configmap-jgrj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299462ms
Mar 11 16:17:46.363: INFO: Pod "pod-subpath-test-configmap-jgrj": Phase="Running", Reason="", readiness=true. Elapsed: 2.013465705s
Mar 11 16:17:48.371: INFO: Pod "pod-subpath-test-configmap-jgrj": Phase="Running", Reason="", readiness=true. Elapsed: 4.021759645s
Mar 11 16:17:50.378: INFO: Pod "pod-subpath-test-configmap-jgrj": Phase="Running", Reason="", readiness=true. Elapsed: 6.028848846s
Mar 11 16:17:52.384: INFO: Pod "pod-subpath-test-configmap-jgrj": Phase="Running", Reason="", readiness=true. Elapsed: 8.03514154s
Mar 11 16:17:54.395: INFO: Pod "pod-subpath-test-configmap-jgrj": Phase="Running", Reason="", readiness=true. Elapsed: 10.045460176s
Mar 11 16:17:56.403: INFO: Pod "pod-subpath-test-configmap-jgrj": Phase="Running", Reason="", readiness=true. Elapsed: 12.053351741s
Mar 11 16:17:58.411: INFO: Pod "pod-subpath-test-configmap-jgrj": Phase="Running", Reason="", readiness=true. Elapsed: 14.062102494s
Mar 11 16:18:00.419: INFO: Pod "pod-subpath-test-configmap-jgrj": Phase="Running", Reason="", readiness=true. Elapsed: 16.069611816s
Mar 11 16:18:02.427: INFO: Pod "pod-subpath-test-configmap-jgrj": Phase="Running", Reason="", readiness=true. Elapsed: 18.077252778s
Mar 11 16:18:04.436: INFO: Pod "pod-subpath-test-configmap-jgrj": Phase="Running", Reason="", readiness=true. Elapsed: 20.086740736s
Mar 11 16:18:06.542: INFO: Pod "pod-subpath-test-configmap-jgrj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.192344257s
STEP: Saw pod success
Mar 11 16:18:06.542: INFO: Pod "pod-subpath-test-configmap-jgrj" satisfied condition "Succeeded or Failed"
Mar 11 16:18:06.549: INFO: Trying to get logs from node 198.18.167.130 pod pod-subpath-test-configmap-jgrj container test-container-subpath-configmap-jgrj: <nil>
STEP: delete the pod
Mar 11 16:18:06.591: INFO: Waiting for pod pod-subpath-test-configmap-jgrj to disappear
Mar 11 16:18:06.594: INFO: Pod pod-subpath-test-configmap-jgrj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jgrj
Mar 11 16:18:06.594: INFO: Deleting pod "pod-subpath-test-configmap-jgrj" in namespace "subpath-5639"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:18:06.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5639" for this suite.

• [SLOW TEST:22.340 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":339,"completed":51,"skipped":769,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:18:06.612: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 11 16:18:06.669: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 11 16:18:36.440: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 16:18:45.340: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:19:17.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3643" for this suite.

• [SLOW TEST:71.093 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":339,"completed":52,"skipped":772,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:19:17.706: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:19:17.749: INFO: Creating simple deployment test-new-deployment
Mar 11 16:19:17.767: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Mar 11 16:19:19.833: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2758  3705ebe7-ac4d-4d3b-9605-fa1fbaba1b09 26296 3 2022-03-11 16:19:17 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-03-11 16:19:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-11 16:19:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b8fc778 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-11 16:19:19 +0000 UTC,LastTransitionTime:2022-03-11 16:19:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2022-03-11 16:19:19 +0000 UTC,LastTransitionTime:2022-03-11 16:19:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 11 16:19:19.839: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-2758  f76d0d51-de19-44bd-a9ad-cfeef842efe1 26303 3 2022-03-11 16:19:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 3705ebe7-ac4d-4d3b-9605-fa1fbaba1b09 0xc00bb183f7 0xc00bb183f8}] []  [{kube-controller-manager Update apps/v1 2022-03-11 16:19:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3705ebe7-ac4d-4d3b-9605-fa1fbaba1b09\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bb18468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 11 16:19:19.843: INFO: Pod "test-new-deployment-847dcfb7fb-ck2sr" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-ck2sr test-new-deployment-847dcfb7fb- deployment-2758  9d935dc6-6218-4caf-a0ed-d52700bf0c88 26299 0 2022-03-11 16:19:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb f76d0d51-de19-44bd-a9ad-cfeef842efe1 0xc00b8fcb87 0xc00b8fcb88}] []  [{kube-controller-manager Update v1 2022-03-11 16:19:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f76d0d51-de19-44bd-a9ad-cfeef842efe1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dl46v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dl46v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:19:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 16:19:19.844: INFO: Pod "test-new-deployment-847dcfb7fb-z2ppg" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-z2ppg test-new-deployment-847dcfb7fb- deployment-2758  1fdbbcc3-f59b-47d0-b274-12672cb86ace 26286 0 2022-03-11 16:19:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb f76d0d51-de19-44bd-a9ad-cfeef842efe1 0xc00b8fccf0 0xc00b8fccf1}] []  [{kube-controller-manager Update v1 2022-03-11 16:19:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f76d0d51-de19-44bd-a9ad-cfeef842efe1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 16:19:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vk4cj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vk4cj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.167.130,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:19:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:19:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:19:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:19:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.167.130,PodIP:192.168.2.70,StartTime:2022-03-11 16:19:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 16:19:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://ba60a3370045b65b843f54cbaedf30370ca1d50c7ba054273a4c5b8b1d00be9c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:19:19.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2758" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":339,"completed":53,"skipped":786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:19:19.868: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 11 16:19:19.933: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 11 16:19:24.940: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:19:25.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4847" for this suite.

• [SLOW TEST:6.135 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":339,"completed":54,"skipped":819,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:19:26.004: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Mar 11 16:19:26.061: INFO: Waiting up to 5m0s for pod "client-containers-d35ac170-1870-43cb-a7d9-5c1322294b8b" in namespace "containers-5973" to be "Succeeded or Failed"
Mar 11 16:19:26.064: INFO: Pod "client-containers-d35ac170-1870-43cb-a7d9-5c1322294b8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.414902ms
Mar 11 16:19:28.073: INFO: Pod "client-containers-d35ac170-1870-43cb-a7d9-5c1322294b8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011703654s
STEP: Saw pod success
Mar 11 16:19:28.073: INFO: Pod "client-containers-d35ac170-1870-43cb-a7d9-5c1322294b8b" satisfied condition "Succeeded or Failed"
Mar 11 16:19:28.079: INFO: Trying to get logs from node 198.18.167.130 pod client-containers-d35ac170-1870-43cb-a7d9-5c1322294b8b container agnhost-container: <nil>
STEP: delete the pod
Mar 11 16:19:28.104: INFO: Waiting for pod client-containers-d35ac170-1870-43cb-a7d9-5c1322294b8b to disappear
Mar 11 16:19:28.108: INFO: Pod client-containers-d35ac170-1870-43cb-a7d9-5c1322294b8b no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:19:28.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5973" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":339,"completed":55,"skipped":827,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:19:28.121: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 11 16:19:28.167: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 11 16:19:28.180: INFO: Waiting for terminating namespaces to be deleted...
Mar 11 16:19:28.184: INFO: 
Logging pods the apiserver thinks is on node 198.18.16.160 before test
Mar 11 16:19:28.196: INFO: capi-kubeadm-bootstrap-controller-manager-694cc79bb7-m8mq5 from capi-kubeadm-bootstrap-system started at 2022-03-11 15:43:57 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.196: INFO: 	Container manager ready: true, restart count 0
Mar 11 16:19:28.196: INFO: capi-controller-manager-689cd9b4fd-pkbpt from capi-system started at 2022-03-11 15:43:50 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.196: INFO: 	Container manager ready: true, restart count 0
Mar 11 16:19:28.196: INFO: capv-controller-manager-6b467446b9-4qlm4 from capv-system started at 2022-03-11 15:44:30 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.196: INFO: 	Container manager ready: true, restart count 0
Mar 11 16:19:28.196: INFO: cert-manager-7988d4fb6c-knnwq from cert-manager started at 2022-03-11 15:43:27 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.196: INFO: 	Container cert-manager ready: true, restart count 0
Mar 11 16:19:28.196: INFO: cert-manager-cainjector-6bc8dcdb64-8vk7b from cert-manager started at 2022-03-11 15:43:26 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.196: INFO: 	Container cert-manager ready: true, restart count 0
Mar 11 16:19:28.196: INFO: cert-manager-webhook-68979bfb95-ksn6x from cert-manager started at 2022-03-11 15:43:27 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.196: INFO: 	Container cert-manager ready: true, restart count 0
Mar 11 16:19:28.196: INFO: etcdadm-bootstrap-provider-controller-manager-74c86ffb56-l6l9z from etcdadm-bootstrap-provider-system started at 2022-03-11 15:44:04 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.196: INFO: 	Container manager ready: true, restart count 0
Mar 11 16:19:28.196: INFO: cilium-operator-86d59d5c88-fvvbn from kube-system started at 2022-03-11 15:42:57 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.196: INFO: 	Container cilium-operator ready: true, restart count 0
Mar 11 16:19:28.196: INFO: cilium-qm8kg from kube-system started at 2022-03-11 15:42:57 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.196: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 11 16:19:28.196: INFO: kube-proxy-8jj6k from kube-system started at 2022-03-11 15:42:19 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.196: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 11 16:19:28.196: INFO: vsphere-cloud-controller-manager-l6kzs from kube-system started at 2022-03-11 15:42:19 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.196: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 1
Mar 11 16:19:28.196: INFO: vsphere-csi-node-zmmbm from kube-system started at 2022-03-11 15:42:19 +0000 UTC (3 container statuses recorded)
Mar 11 16:19:28.197: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 11 16:19:28.197: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 11 16:19:28.197: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Mar 11 16:19:28.197: INFO: pod-release-22p75 from replication-controller-4847 started at 2022-03-11 16:19:25 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.197: INFO: 	Container pod-release ready: true, restart count 0
Mar 11 16:19:28.197: INFO: pod-release-zpsgh from replication-controller-4847 started at 2022-03-11 16:19:19 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.197: INFO: 	Container pod-release ready: true, restart count 0
Mar 11 16:19:28.197: INFO: sonobuoy from sonobuoy started at 2022-03-11 15:55:43 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.197: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 11 16:19:28.197: INFO: sonobuoy-systemd-logs-daemon-set-5218357723e74163-g6x4l from sonobuoy started at 2022-03-11 15:55:47 +0000 UTC (2 container statuses recorded)
Mar 11 16:19:28.197: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 11 16:19:28.197: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 11 16:19:28.197: INFO: 
Logging pods the apiserver thinks is on node 198.18.167.130 before test
Mar 11 16:19:28.206: INFO: etcdadm-controller-controller-manager-7894945688-ps7xp from etcdadm-controller-system started at 2022-03-11 15:44:10 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.206: INFO: 	Container manager ready: true, restart count 0
Mar 11 16:19:28.206: INFO: cilium-69rlw from kube-system started at 2022-03-11 15:42:57 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.206: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 11 16:19:28.206: INFO: coredns-745c7986c7-5w7kj from kube-system started at 2022-03-11 15:43:12 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.206: INFO: 	Container coredns ready: true, restart count 0
Mar 11 16:19:28.206: INFO: coredns-745c7986c7-6ntkg from kube-system started at 2022-03-11 15:43:12 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.206: INFO: 	Container coredns ready: true, restart count 0
Mar 11 16:19:28.206: INFO: kube-proxy-6trtj from kube-system started at 2022-03-11 15:42:21 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.206: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 11 16:19:28.206: INFO: vsphere-cloud-controller-manager-5xrtz from kube-system started at 2022-03-11 15:42:21 +0000 UTC (1 container statuses recorded)
Mar 11 16:19:28.206: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 1
Mar 11 16:19:28.206: INFO: vsphere-csi-controller-576c9c8dc8-79zxc from kube-system started at 2022-03-11 15:43:13 +0000 UTC (5 container statuses recorded)
Mar 11 16:19:28.206: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 11 16:19:28.206: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 11 16:19:28.206: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 11 16:19:28.206: INFO: 	Container vsphere-csi-controller ready: true, restart count 0
Mar 11 16:19:28.206: INFO: 	Container vsphere-syncer ready: true, restart count 0
Mar 11 16:19:28.206: INFO: vsphere-csi-node-xgb98 from kube-system started at 2022-03-11 15:42:21 +0000 UTC (3 container statuses recorded)
Mar 11 16:19:28.206: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 11 16:19:28.206: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 11 16:19:28.206: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Mar 11 16:19:28.206: INFO: sonobuoy-systemd-logs-daemon-set-5218357723e74163-tgcrn from sonobuoy started at 2022-03-11 15:55:47 +0000 UTC (2 container statuses recorded)
Mar 11 16:19:28.206: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 11 16:19:28.206: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9fd48d3f-394a-481c-ba83-675f6d6d4f77 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-9fd48d3f-394a-481c-ba83-675f6d6d4f77 off the node 198.18.167.130
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9fd48d3f-394a-481c-ba83-675f6d6d4f77
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:19:32.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5774" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":339,"completed":56,"skipped":842,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:19:32.332: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-c76c7c8d-0505-4b94-8448-8b575afa4542
STEP: Creating a pod to test consume configMaps
Mar 11 16:19:32.401: INFO: Waiting up to 5m0s for pod "pod-configmaps-9f567c9f-eff7-48f5-b452-c757e3b38c1a" in namespace "configmap-9276" to be "Succeeded or Failed"
Mar 11 16:19:32.408: INFO: Pod "pod-configmaps-9f567c9f-eff7-48f5-b452-c757e3b38c1a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.726969ms
Mar 11 16:19:34.418: INFO: Pod "pod-configmaps-9f567c9f-eff7-48f5-b452-c757e3b38c1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017159895s
STEP: Saw pod success
Mar 11 16:19:34.418: INFO: Pod "pod-configmaps-9f567c9f-eff7-48f5-b452-c757e3b38c1a" satisfied condition "Succeeded or Failed"
Mar 11 16:19:34.422: INFO: Trying to get logs from node 198.18.16.160 pod pod-configmaps-9f567c9f-eff7-48f5-b452-c757e3b38c1a container agnhost-container: <nil>
STEP: delete the pod
Mar 11 16:19:34.475: INFO: Waiting for pod pod-configmaps-9f567c9f-eff7-48f5-b452-c757e3b38c1a to disappear
Mar 11 16:19:34.479: INFO: Pod pod-configmaps-9f567c9f-eff7-48f5-b452-c757e3b38c1a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:19:34.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9276" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":57,"skipped":861,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:19:34.491: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:19:34.557: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 11 16:19:34.567: INFO: Number of nodes with available pods: 0
Mar 11 16:19:34.567: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 11 16:19:34.591: INFO: Number of nodes with available pods: 0
Mar 11 16:19:34.591: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:35.601: INFO: Number of nodes with available pods: 0
Mar 11 16:19:35.601: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:36.599: INFO: Number of nodes with available pods: 0
Mar 11 16:19:36.599: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:37.597: INFO: Number of nodes with available pods: 1
Mar 11 16:19:37.597: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 11 16:19:37.624: INFO: Number of nodes with available pods: 1
Mar 11 16:19:37.625: INFO: Number of running nodes: 0, number of available pods: 1
Mar 11 16:19:38.632: INFO: Number of nodes with available pods: 0
Mar 11 16:19:38.632: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 11 16:19:38.655: INFO: Number of nodes with available pods: 0
Mar 11 16:19:38.655: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:39.664: INFO: Number of nodes with available pods: 0
Mar 11 16:19:39.664: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:40.663: INFO: Number of nodes with available pods: 0
Mar 11 16:19:40.663: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:41.661: INFO: Number of nodes with available pods: 0
Mar 11 16:19:41.661: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:42.664: INFO: Number of nodes with available pods: 0
Mar 11 16:19:42.664: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:43.661: INFO: Number of nodes with available pods: 0
Mar 11 16:19:43.661: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:44.663: INFO: Number of nodes with available pods: 0
Mar 11 16:19:44.663: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:45.663: INFO: Number of nodes with available pods: 0
Mar 11 16:19:45.663: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:46.663: INFO: Number of nodes with available pods: 0
Mar 11 16:19:46.663: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:47.661: INFO: Number of nodes with available pods: 0
Mar 11 16:19:47.661: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:48.664: INFO: Number of nodes with available pods: 0
Mar 11 16:19:48.664: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:49.663: INFO: Number of nodes with available pods: 0
Mar 11 16:19:49.663: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:50.663: INFO: Number of nodes with available pods: 0
Mar 11 16:19:50.663: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 16:19:51.661: INFO: Number of nodes with available pods: 1
Mar 11 16:19:51.661: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9165, will wait for the garbage collector to delete the pods
Mar 11 16:19:51.738: INFO: Deleting DaemonSet.extensions daemon-set took: 15.611076ms
Mar 11 16:19:51.845: INFO: Terminating DaemonSet.extensions daemon-set pods took: 107.570408ms
Mar 11 16:19:59.151: INFO: Number of nodes with available pods: 0
Mar 11 16:19:59.151: INFO: Number of running nodes: 0, number of available pods: 0
Mar 11 16:19:59.155: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26981"},"items":null}

Mar 11 16:19:59.158: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26981"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:19:59.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9165" for this suite.

• [SLOW TEST:24.704 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":339,"completed":58,"skipped":868,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:19:59.199: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:19:59.261: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:20:09.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7772" for this suite.

• [SLOW TEST:10.640 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":339,"completed":59,"skipped":909,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:20:09.839: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
W0311 16:20:11.404071      19 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:26:01.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6245" for this suite.

• [SLOW TEST:351.644 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":339,"completed":60,"skipped":929,"failed":0}
S
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:26:01.484: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Mar 11 16:26:01.526: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:26:04.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5986" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":339,"completed":61,"skipped":930,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:26:04.648: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Mar 11 16:26:04.743: INFO: The status of Pod pod-hostip-25c9a47e-7eb2-49fc-89c5-bd11e0c07457 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:26:06.810: INFO: The status of Pod pod-hostip-25c9a47e-7eb2-49fc-89c5-bd11e0c07457 is Running (Ready = true)
Mar 11 16:26:06.816: INFO: Pod pod-hostip-25c9a47e-7eb2-49fc-89c5-bd11e0c07457 has hostIP: 198.18.167.130
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:26:06.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2747" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":339,"completed":62,"skipped":936,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:26:06.833: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5413
Mar 11 16:26:06.919: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:26:08.926: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar 11 16:26:08.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5413 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 11 16:26:09.432: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 11 16:26:09.432: INFO: stdout: "iptables"
Mar 11 16:26:09.432: INFO: proxyMode: iptables
Mar 11 16:26:09.445: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 11 16:26:09.449: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5413
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5413
I0311 16:26:09.483910      19 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5413, replica count: 3
I0311 16:26:12.535147      19 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 11 16:26:12.550: INFO: Creating new exec pod
Mar 11 16:26:17.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5413 exec execpod-affinityms9pf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Mar 11 16:26:17.788: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar 11 16:26:17.788: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:26:17.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5413 exec execpod-affinityms9pf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.98.159.69 80'
Mar 11 16:26:17.971: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.98.159.69 80\nConnection to 10.98.159.69 80 port [tcp/http] succeeded!\n"
Mar 11 16:26:17.972: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:26:17.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5413 exec execpod-affinityms9pf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 198.18.16.160 31605'
Mar 11 16:26:18.147: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 198.18.16.160 31605\nConnection to 198.18.16.160 31605 port [tcp/*] succeeded!\n"
Mar 11 16:26:18.147: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:26:18.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5413 exec execpod-affinityms9pf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 198.18.167.130 31605'
Mar 11 16:26:18.330: INFO: stderr: "+ nc -v -t -w 2 198.18.167.130 31605\n+ echo hostName\nConnection to 198.18.167.130 31605 port [tcp/*] succeeded!\n"
Mar 11 16:26:18.330: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:26:18.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5413 exec execpod-affinityms9pf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://198.18.16.160:31605/ ; done'
Mar 11 16:26:18.582: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n"
Mar 11 16:26:18.582: INFO: stdout: "\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s\naffinity-nodeport-timeout-rlr7s"
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.582: INFO: Received response from host: affinity-nodeport-timeout-rlr7s
Mar 11 16:26:18.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5413 exec execpod-affinityms9pf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://198.18.16.160:31605/'
Mar 11 16:26:18.762: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n"
Mar 11 16:26:18.762: INFO: stdout: "affinity-nodeport-timeout-rlr7s"
Mar 11 16:26:38.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5413 exec execpod-affinityms9pf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://198.18.16.160:31605/'
Mar 11 16:26:38.935: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n"
Mar 11 16:26:38.935: INFO: stdout: "affinity-nodeport-timeout-rlr7s"
Mar 11 16:26:58.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5413 exec execpod-affinityms9pf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://198.18.16.160:31605/'
Mar 11 16:26:59.131: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n"
Mar 11 16:26:59.131: INFO: stdout: "affinity-nodeport-timeout-rlr7s"
Mar 11 16:27:19.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5413 exec execpod-affinityms9pf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://198.18.16.160:31605/'
Mar 11 16:27:19.313: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n"
Mar 11 16:27:19.313: INFO: stdout: "affinity-nodeport-timeout-rlr7s"
Mar 11 16:27:39.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5413 exec execpod-affinityms9pf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://198.18.16.160:31605/'
Mar 11 16:27:39.493: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://198.18.16.160:31605/\n"
Mar 11 16:27:39.493: INFO: stdout: "affinity-nodeport-timeout-8prqk"
Mar 11 16:27:39.493: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5413, will wait for the garbage collector to delete the pods
Mar 11 16:27:39.579: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 12.062234ms
Mar 11 16:27:39.680: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.982663ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:27:49.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5413" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:102.397 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":63,"skipped":942,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:27:49.234: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:27:49.282: INFO: Got root ca configmap in namespace "svcaccounts-2371"
Mar 11 16:27:49.289: INFO: Deleted root ca configmap in namespace "svcaccounts-2371"
STEP: waiting for a new root ca configmap created
Mar 11 16:27:49.796: INFO: Recreated root ca configmap in namespace "svcaccounts-2371"
Mar 11 16:27:49.801: INFO: Updated root ca configmap in namespace "svcaccounts-2371"
STEP: waiting for the root ca configmap reconciled
Mar 11 16:27:50.308: INFO: Reconciled root ca configmap in namespace "svcaccounts-2371"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:27:50.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2371" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":339,"completed":64,"skipped":961,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:27:50.324: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5439
Mar 11 16:27:50.384: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:27:52.390: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar 11 16:27:52.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5439 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 11 16:27:52.580: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 11 16:27:52.580: INFO: stdout: "iptables"
Mar 11 16:27:52.580: INFO: proxyMode: iptables
Mar 11 16:27:52.603: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 11 16:27:52.606: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-5439
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5439
I0311 16:27:52.639672      19 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5439, replica count: 3
I0311 16:27:55.690589      19 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 11 16:27:55.697: INFO: Creating new exec pod
Mar 11 16:27:58.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5439 exec execpod-affinityqkdrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Mar 11 16:27:58.913: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar 11 16:27:58.913: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:27:58.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5439 exec execpod-affinityqkdrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.17.59 80'
Mar 11 16:27:59.100: INFO: stderr: "+ nc -v -t -w 2 10.111.17.59 80\n+ echo hostName\nConnection to 10.111.17.59 80 port [tcp/http] succeeded!\n"
Mar 11 16:27:59.100: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:27:59.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5439 exec execpod-affinityqkdrx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.17.59:80/ ; done'
Mar 11 16:27:59.354: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n"
Mar 11 16:27:59.354: INFO: stdout: "\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl\naffinity-clusterip-timeout-bw9jl"
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Received response from host: affinity-clusterip-timeout-bw9jl
Mar 11 16:27:59.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5439 exec execpod-affinityqkdrx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.111.17.59:80/'
Mar 11 16:27:59.518: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n"
Mar 11 16:27:59.518: INFO: stdout: "affinity-clusterip-timeout-bw9jl"
Mar 11 16:28:19.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-5439 exec execpod-affinityqkdrx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.111.17.59:80/'
Mar 11 16:28:19.702: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.111.17.59:80/\n"
Mar 11 16:28:19.702: INFO: stdout: "affinity-clusterip-timeout-kgjjb"
Mar 11 16:28:19.702: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5439, will wait for the garbage collector to delete the pods
Mar 11 16:28:19.798: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.639449ms
Mar 11 16:28:19.900: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.699465ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:28:31.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5439" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:40.828 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":65,"skipped":979,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:28:31.155: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 11 16:28:31.219: INFO: Waiting up to 5m0s for pod "pod-becd12da-f2c4-4977-bcb2-88db1f9f112b" in namespace "emptydir-232" to be "Succeeded or Failed"
Mar 11 16:28:31.224: INFO: Pod "pod-becd12da-f2c4-4977-bcb2-88db1f9f112b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.86696ms
Mar 11 16:28:33.234: INFO: Pod "pod-becd12da-f2c4-4977-bcb2-88db1f9f112b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015096877s
STEP: Saw pod success
Mar 11 16:28:33.234: INFO: Pod "pod-becd12da-f2c4-4977-bcb2-88db1f9f112b" satisfied condition "Succeeded or Failed"
Mar 11 16:28:33.238: INFO: Trying to get logs from node 198.18.167.130 pod pod-becd12da-f2c4-4977-bcb2-88db1f9f112b container test-container: <nil>
STEP: delete the pod
Mar 11 16:28:33.278: INFO: Waiting for pod pod-becd12da-f2c4-4977-bcb2-88db1f9f112b to disappear
Mar 11 16:28:33.281: INFO: Pod pod-becd12da-f2c4-4977-bcb2-88db1f9f112b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:28:33.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-232" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":66,"skipped":987,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:28:33.295: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar 11 16:28:33.359: INFO: Waiting up to 5m0s for pod "downward-api-0c5a9333-3891-47c8-a673-184bdc57a34b" in namespace "downward-api-5551" to be "Succeeded or Failed"
Mar 11 16:28:33.371: INFO: Pod "downward-api-0c5a9333-3891-47c8-a673-184bdc57a34b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.816381ms
Mar 11 16:28:35.379: INFO: Pod "downward-api-0c5a9333-3891-47c8-a673-184bdc57a34b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019330015s
STEP: Saw pod success
Mar 11 16:28:35.379: INFO: Pod "downward-api-0c5a9333-3891-47c8-a673-184bdc57a34b" satisfied condition "Succeeded or Failed"
Mar 11 16:28:35.383: INFO: Trying to get logs from node 198.18.167.130 pod downward-api-0c5a9333-3891-47c8-a673-184bdc57a34b container dapi-container: <nil>
STEP: delete the pod
Mar 11 16:28:35.408: INFO: Waiting for pod downward-api-0c5a9333-3891-47c8-a673-184bdc57a34b to disappear
Mar 11 16:28:35.411: INFO: Pod downward-api-0c5a9333-3891-47c8-a673-184bdc57a34b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:28:35.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5551" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":339,"completed":67,"skipped":1000,"failed":0}
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:28:35.425: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:28:35.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9544" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":339,"completed":68,"skipped":1007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:28:35.515: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 16:28:35.575: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01a9a1f4-6701-43b4-bbb8-755f688cb845" in namespace "projected-3710" to be "Succeeded or Failed"
Mar 11 16:28:35.582: INFO: Pod "downwardapi-volume-01a9a1f4-6701-43b4-bbb8-755f688cb845": Phase="Pending", Reason="", readiness=false. Elapsed: 7.113535ms
Mar 11 16:28:37.590: INFO: Pod "downwardapi-volume-01a9a1f4-6701-43b4-bbb8-755f688cb845": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014323559s
STEP: Saw pod success
Mar 11 16:28:37.590: INFO: Pod "downwardapi-volume-01a9a1f4-6701-43b4-bbb8-755f688cb845" satisfied condition "Succeeded or Failed"
Mar 11 16:28:37.593: INFO: Trying to get logs from node 198.18.16.160 pod downwardapi-volume-01a9a1f4-6701-43b4-bbb8-755f688cb845 container client-container: <nil>
STEP: delete the pod
Mar 11 16:28:37.635: INFO: Waiting for pod downwardapi-volume-01a9a1f4-6701-43b4-bbb8-755f688cb845 to disappear
Mar 11 16:28:37.639: INFO: Pod downwardapi-volume-01a9a1f4-6701-43b4-bbb8-755f688cb845 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:28:37.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3710" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":69,"skipped":1034,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:28:37.652: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-7758
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Mar 11 16:28:37.728: INFO: Found 0 stateful pods, waiting for 3
Mar 11 16:28:47.739: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 11 16:28:47.739: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 11 16:28:47.739: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 11 16:28:47.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-7758 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 11 16:28:47.934: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 11 16:28:47.934: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 11 16:28:47.934: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Mar 11 16:28:57.980: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 11 16:29:08.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-7758 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 11 16:29:08.178: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 11 16:29:08.178: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 11 16:29:08.178: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 11 16:29:18.207: INFO: Waiting for StatefulSet statefulset-7758/ss2 to complete update
Mar 11 16:29:18.207: INFO: Waiting for Pod statefulset-7758/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Mar 11 16:29:18.207: INFO: Waiting for Pod statefulset-7758/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Mar 11 16:29:18.207: INFO: Waiting for Pod statefulset-7758/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Mar 11 16:29:28.218: INFO: Waiting for StatefulSet statefulset-7758/ss2 to complete update
Mar 11 16:29:28.218: INFO: Waiting for Pod statefulset-7758/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Mar 11 16:29:28.218: INFO: Waiting for Pod statefulset-7758/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Mar 11 16:29:38.217: INFO: Waiting for StatefulSet statefulset-7758/ss2 to complete update
Mar 11 16:29:38.217: INFO: Waiting for Pod statefulset-7758/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Mar 11 16:29:38.217: INFO: Waiting for Pod statefulset-7758/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Rolling back to a previous revision
Mar 11 16:29:48.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-7758 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 11 16:29:48.410: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 11 16:29:48.410: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 11 16:29:48.410: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 11 16:29:58.450: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 11 16:30:08.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-7758 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 11 16:30:08.646: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 11 16:30:08.646: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 11 16:30:08.646: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 11 16:30:18.675: INFO: Waiting for StatefulSet statefulset-7758/ss2 to complete update
Mar 11 16:30:18.675: INFO: Waiting for Pod statefulset-7758/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Mar 11 16:30:28.689: INFO: Deleting all statefulset in ns statefulset-7758
Mar 11 16:30:28.692: INFO: Scaling statefulset ss2 to 0
Mar 11 16:31:08.712: INFO: Waiting for statefulset status.replicas updated to 0
Mar 11 16:31:08.718: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:31:08.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7758" for this suite.

• [SLOW TEST:151.115 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":339,"completed":70,"skipped":1044,"failed":0}
SSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:31:08.767: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:31:08.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-7360" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":339,"completed":71,"skipped":1048,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:31:08.883: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 11 16:31:08.959: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar 11 16:31:08.969: INFO: starting watch
STEP: patching
STEP: updating
Mar 11 16:31:08.987: INFO: waiting for watch events with expected annotations
Mar 11 16:31:08.987: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:31:09.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-7650" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":339,"completed":72,"skipped":1056,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:31:09.047: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Mar 11 16:31:09.094: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:31:53.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2917" for this suite.

• [SLOW TEST:44.903 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":339,"completed":73,"skipped":1069,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:31:53.950: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-59446608-0c2d-4e34-aae2-10b10deebdc4
STEP: Creating the pod
Mar 11 16:31:54.021: INFO: The status of Pod pod-configmaps-f7c7a161-99e0-4646-872f-87e27b0f9db0 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:31:56.030: INFO: The status of Pod pod-configmaps-f7c7a161-99e0-4646-872f-87e27b0f9db0 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:31:58.029: INFO: The status of Pod pod-configmaps-f7c7a161-99e0-4646-872f-87e27b0f9db0 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-59446608-0c2d-4e34-aae2-10b10deebdc4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:33:08.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4716" for this suite.

• [SLOW TEST:74.445 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":74,"skipped":1076,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:33:08.396: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:33:08.433: INFO: Creating deployment "test-recreate-deployment"
Mar 11 16:33:08.441: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 11 16:33:08.448: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Mar 11 16:33:10.456: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 11 16:33:10.459: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 11 16:33:10.468: INFO: Updating deployment test-recreate-deployment
Mar 11 16:33:10.469: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Mar 11 16:33:10.609: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1984  fdad2a34-860f-4631-9a22-8c7becf1c435 36061 2 2022-03-11 16:33:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-03-11 16:33:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-11 16:33:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006f5ada8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-03-11 16:33:10 +0000 UTC,LastTransitionTime:2022-03-11 16:33:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2022-03-11 16:33:10 +0000 UTC,LastTransitionTime:2022-03-11 16:33:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 11 16:33:10.612: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-1984  aa206fdd-2bba-47e6-a232-ce83eadfefa7 36057 1 2022-03-11 16:33:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment fdad2a34-860f-4631-9a22-8c7becf1c435 0xc006f5b240 0xc006f5b241}] []  [{kube-controller-manager Update apps/v1 2022-03-11 16:33:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdad2a34-860f-4631-9a22-8c7becf1c435\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006f5b2b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 11 16:33:10.613: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 11 16:33:10.613: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-1984  dd095066-53ee-4d11-8a32-a1a474ef1c3e 36048 2 2022-03-11 16:33:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment fdad2a34-860f-4631-9a22-8c7becf1c435 0xc006f5b147 0xc006f5b148}] []  [{kube-controller-manager Update apps/v1 2022-03-11 16:33:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fdad2a34-860f-4631-9a22-8c7becf1c435\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006f5b1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 11 16:33:10.616: INFO: Pod "test-recreate-deployment-85d47dcb4-h5bh5" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-h5bh5 test-recreate-deployment-85d47dcb4- deployment-1984  673cc0b0-3c4b-445e-a4a6-cfe38542ce9e 36060 0 2022-03-11 16:33:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 aa206fdd-2bba-47e6-a232-ce83eadfefa7 0xc006f5b710 0xc006f5b711}] []  [{kube-controller-manager Update v1 2022-03-11 16:33:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa206fdd-2bba-47e6-a232-ce83eadfefa7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 16:33:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tztc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tztc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:33:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:33:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:33:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:33:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.16.160,PodIP:,StartTime:2022-03-11 16:33:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:33:10.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1984" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":75,"skipped":1084,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:33:10.631: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Mar 11 16:33:10.687: INFO: created test-podtemplate-1
Mar 11 16:33:10.693: INFO: created test-podtemplate-2
Mar 11 16:33:10.699: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Mar 11 16:33:10.702: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Mar 11 16:33:10.725: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:33:10.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2507" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":339,"completed":76,"skipped":1096,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:33:10.743: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-bf18e44c-72a0-42a8-b89f-0f21ddec3ef4
STEP: Creating a pod to test consume configMaps
Mar 11 16:33:10.809: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48e32691-18cb-4e97-aad0-5de6be8388cc" in namespace "projected-4484" to be "Succeeded or Failed"
Mar 11 16:33:10.816: INFO: Pod "pod-projected-configmaps-48e32691-18cb-4e97-aad0-5de6be8388cc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.615495ms
Mar 11 16:33:12.823: INFO: Pod "pod-projected-configmaps-48e32691-18cb-4e97-aad0-5de6be8388cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013273819s
STEP: Saw pod success
Mar 11 16:33:12.823: INFO: Pod "pod-projected-configmaps-48e32691-18cb-4e97-aad0-5de6be8388cc" satisfied condition "Succeeded or Failed"
Mar 11 16:33:12.828: INFO: Trying to get logs from node 198.18.16.160 pod pod-projected-configmaps-48e32691-18cb-4e97-aad0-5de6be8388cc container agnhost-container: <nil>
STEP: delete the pod
Mar 11 16:33:12.853: INFO: Waiting for pod pod-projected-configmaps-48e32691-18cb-4e97-aad0-5de6be8388cc to disappear
Mar 11 16:33:12.855: INFO: Pod pod-projected-configmaps-48e32691-18cb-4e97-aad0-5de6be8388cc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:33:12.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4484" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":77,"skipped":1135,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:33:12.873: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Mar 11 16:33:12.940: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:33:14.948: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Mar 11 16:33:14.965: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:33:16.981: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Mar 11 16:33:16.996: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 11 16:33:17.007: INFO: Pod pod-with-prestop-http-hook still exists
Mar 11 16:33:19.007: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 11 16:33:19.013: INFO: Pod pod-with-prestop-http-hook still exists
Mar 11 16:33:21.007: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 11 16:33:21.027: INFO: Pod pod-with-prestop-http-hook still exists
Mar 11 16:33:23.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 11 16:33:23.014: INFO: Pod pod-with-prestop-http-hook still exists
Mar 11 16:33:25.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 11 16:33:25.015: INFO: Pod pod-with-prestop-http-hook still exists
Mar 11 16:33:27.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 11 16:33:27.015: INFO: Pod pod-with-prestop-http-hook still exists
Mar 11 16:33:29.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 11 16:33:29.014: INFO: Pod pod-with-prestop-http-hook still exists
Mar 11 16:33:31.008: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 11 16:33:31.014: INFO: Pod pod-with-prestop-http-hook still exists
Mar 11 16:33:33.009: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 11 16:33:33.016: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:33:33.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6948" for this suite.

• [SLOW TEST:20.175 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":339,"completed":78,"skipped":1156,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:33:33.049: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-9j97
STEP: Creating a pod to test atomic-volume-subpath
Mar 11 16:33:33.124: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-9j97" in namespace "subpath-5510" to be "Succeeded or Failed"
Mar 11 16:33:33.129: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Pending", Reason="", readiness=false. Elapsed: 3.656642ms
Mar 11 16:33:35.135: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Running", Reason="", readiness=true. Elapsed: 2.009870632s
Mar 11 16:33:37.141: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Running", Reason="", readiness=true. Elapsed: 4.016307349s
Mar 11 16:33:39.147: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Running", Reason="", readiness=true. Elapsed: 6.02255942s
Mar 11 16:33:41.154: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Running", Reason="", readiness=true. Elapsed: 8.029128402s
Mar 11 16:33:43.160: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Running", Reason="", readiness=true. Elapsed: 10.03545941s
Mar 11 16:33:45.167: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Running", Reason="", readiness=true. Elapsed: 12.042442073s
Mar 11 16:33:47.173: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Running", Reason="", readiness=true. Elapsed: 14.047904465s
Mar 11 16:33:49.178: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Running", Reason="", readiness=true. Elapsed: 16.053584387s
Mar 11 16:33:51.185: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Running", Reason="", readiness=true. Elapsed: 18.059912736s
Mar 11 16:33:53.193: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Running", Reason="", readiness=true. Elapsed: 20.068504359s
Mar 11 16:33:55.202: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Running", Reason="", readiness=true. Elapsed: 22.077383182s
Mar 11 16:33:57.209: INFO: Pod "pod-subpath-test-downwardapi-9j97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.084113587s
STEP: Saw pod success
Mar 11 16:33:57.209: INFO: Pod "pod-subpath-test-downwardapi-9j97" satisfied condition "Succeeded or Failed"
Mar 11 16:33:57.212: INFO: Trying to get logs from node 198.18.167.130 pod pod-subpath-test-downwardapi-9j97 container test-container-subpath-downwardapi-9j97: <nil>
STEP: delete the pod
Mar 11 16:33:57.249: INFO: Waiting for pod pod-subpath-test-downwardapi-9j97 to disappear
Mar 11 16:33:57.252: INFO: Pod pod-subpath-test-downwardapi-9j97 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-9j97
Mar 11 16:33:57.252: INFO: Deleting pod "pod-subpath-test-downwardapi-9j97" in namespace "subpath-5510"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:33:57.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5510" for this suite.

• [SLOW TEST:24.218 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":339,"completed":79,"skipped":1157,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:33:57.267: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0311 16:34:07.370400      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 11 16:35:09.390: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:35:09.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2926" for this suite.

• [SLOW TEST:72.134 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":339,"completed":80,"skipped":1170,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:35:09.403: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:35:09.450: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-dd0db152-6e17-45a7-92ca-fb040e055ee4" in namespace "security-context-test-5446" to be "Succeeded or Failed"
Mar 11 16:35:09.456: INFO: Pod "busybox-readonly-false-dd0db152-6e17-45a7-92ca-fb040e055ee4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013818ms
Mar 11 16:35:11.466: INFO: Pod "busybox-readonly-false-dd0db152-6e17-45a7-92ca-fb040e055ee4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016216875s
Mar 11 16:35:11.466: INFO: Pod "busybox-readonly-false-dd0db152-6e17-45a7-92ca-fb040e055ee4" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:35:11.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5446" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":339,"completed":81,"skipped":1196,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:35:11.484: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:35:11.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9332" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":339,"completed":82,"skipped":1225,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:35:11.625: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-9414
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9414
STEP: Creating statefulset with conflicting port in namespace statefulset-9414
STEP: Waiting until pod test-pod will start running in namespace statefulset-9414
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9414
Mar 11 16:35:15.718: INFO: Observed stateful pod in namespace: statefulset-9414, name: ss-0, uid: 7c0fa952-1aeb-4156-994a-96e7acfd82fc, status phase: Pending. Waiting for statefulset controller to delete.
Mar 11 16:35:16.049: INFO: Observed stateful pod in namespace: statefulset-9414, name: ss-0, uid: 7c0fa952-1aeb-4156-994a-96e7acfd82fc, status phase: Failed. Waiting for statefulset controller to delete.
Mar 11 16:35:16.063: INFO: Observed stateful pod in namespace: statefulset-9414, name: ss-0, uid: 7c0fa952-1aeb-4156-994a-96e7acfd82fc, status phase: Failed. Waiting for statefulset controller to delete.
Mar 11 16:35:16.070: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9414
STEP: Removing pod with conflicting port in namespace statefulset-9414
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9414 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Mar 11 16:35:20.113: INFO: Deleting all statefulset in ns statefulset-9414
Mar 11 16:35:20.117: INFO: Scaling statefulset ss to 0
Mar 11 16:35:40.138: INFO: Waiting for statefulset status.replicas updated to 0
Mar 11 16:35:40.142: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:35:40.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9414" for this suite.

• [SLOW TEST:28.564 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":339,"completed":83,"skipped":1269,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:35:40.191: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 11 16:35:40.253: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 11 16:36:40.296: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Mar 11 16:36:40.326: INFO: Created pod: pod0-sched-preemption-low-priority
Mar 11 16:36:40.350: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:37:00.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-905" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:80.259 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":339,"completed":84,"skipped":1271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:37:00.450: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-5229
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Mar 11 16:37:00.536: INFO: Found 0 stateful pods, waiting for 3
Mar 11 16:37:10.546: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 11 16:37:10.546: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 11 16:37:10.546: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Mar 11 16:37:10.592: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 11 16:37:20.642: INFO: Updating stateful set ss2
Mar 11 16:37:20.649: INFO: Waiting for Pod statefulset-5229/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Mar 11 16:37:30.754: INFO: Found 1 stateful pods, waiting for 3
Mar 11 16:37:40.763: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 11 16:37:40.763: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 11 16:37:40.763: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 11 16:37:40.797: INFO: Updating stateful set ss2
Mar 11 16:37:40.807: INFO: Waiting for Pod statefulset-5229/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Mar 11 16:37:50.839: INFO: Updating stateful set ss2
Mar 11 16:37:50.847: INFO: Waiting for StatefulSet statefulset-5229/ss2 to complete update
Mar 11 16:37:50.847: INFO: Waiting for Pod statefulset-5229/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Mar 11 16:38:00.864: INFO: Waiting for StatefulSet statefulset-5229/ss2 to complete update
Mar 11 16:38:00.864: INFO: Waiting for Pod statefulset-5229/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Mar 11 16:38:10.858: INFO: Deleting all statefulset in ns statefulset-5229
Mar 11 16:38:10.861: INFO: Scaling statefulset ss2 to 0
Mar 11 16:38:50.888: INFO: Waiting for statefulset status.replicas updated to 0
Mar 11 16:38:50.892: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:38:50.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5229" for this suite.

• [SLOW TEST:110.473 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":339,"completed":85,"skipped":1298,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:38:50.932: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:38:50.977: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 11 16:39:00.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-4473 --namespace=crd-publish-openapi-4473 create -f -'
Mar 11 16:39:01.117: INFO: stderr: ""
Mar 11 16:39:01.117: INFO: stdout: "e2e-test-crd-publish-openapi-2574-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 11 16:39:01.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-4473 --namespace=crd-publish-openapi-4473 delete e2e-test-crd-publish-openapi-2574-crds test-cr'
Mar 11 16:39:01.267: INFO: stderr: ""
Mar 11 16:39:01.267: INFO: stdout: "e2e-test-crd-publish-openapi-2574-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 11 16:39:01.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-4473 --namespace=crd-publish-openapi-4473 apply -f -'
Mar 11 16:39:01.682: INFO: stderr: ""
Mar 11 16:39:01.682: INFO: stdout: "e2e-test-crd-publish-openapi-2574-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 11 16:39:01.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-4473 --namespace=crd-publish-openapi-4473 delete e2e-test-crd-publish-openapi-2574-crds test-cr'
Mar 11 16:39:01.782: INFO: stderr: ""
Mar 11 16:39:01.782: INFO: stdout: "e2e-test-crd-publish-openapi-2574-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 11 16:39:01.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-4473 explain e2e-test-crd-publish-openapi-2574-crds'
Mar 11 16:39:02.312: INFO: stderr: ""
Mar 11 16:39:02.312: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2574-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:39:10.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4473" for this suite.

• [SLOW TEST:20.045 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":339,"completed":86,"skipped":1301,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:39:10.978: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 16:39:11.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf87e973-1947-4a50-98eb-14568e59d2fd" in namespace "projected-9981" to be "Succeeded or Failed"
Mar 11 16:39:11.052: INFO: Pod "downwardapi-volume-cf87e973-1947-4a50-98eb-14568e59d2fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.670287ms
Mar 11 16:39:13.063: INFO: Pod "downwardapi-volume-cf87e973-1947-4a50-98eb-14568e59d2fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015794581s
STEP: Saw pod success
Mar 11 16:39:13.063: INFO: Pod "downwardapi-volume-cf87e973-1947-4a50-98eb-14568e59d2fd" satisfied condition "Succeeded or Failed"
Mar 11 16:39:13.066: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-cf87e973-1947-4a50-98eb-14568e59d2fd container client-container: <nil>
STEP: delete the pod
Mar 11 16:39:13.108: INFO: Waiting for pod downwardapi-volume-cf87e973-1947-4a50-98eb-14568e59d2fd to disappear
Mar 11 16:39:13.111: INFO: Pod downwardapi-volume-cf87e973-1947-4a50-98eb-14568e59d2fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:39:13.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9981" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":339,"completed":87,"skipped":1330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:39:13.125: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 16:39:13.576: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 16:39:16.619: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:39:16.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1762" for this suite.
STEP: Destroying namespace "webhook-1762-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":339,"completed":88,"skipped":1361,"failed":0}
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:39:16.789: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0311 16:39:17.916933      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 11 16:40:19.940: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:40:19.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2014" for this suite.

• [SLOW TEST:63.164 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":339,"completed":89,"skipped":1363,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:40:19.955: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Mar 11 16:40:20.012: INFO: created test-event-1
Mar 11 16:40:20.019: INFO: created test-event-2
Mar 11 16:40:20.024: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Mar 11 16:40:20.027: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Mar 11 16:40:20.052: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:40:20.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1510" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":339,"completed":90,"skipped":1380,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:40:20.073: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-2539965e-d1b7-42db-9ebc-0eda66d220ca
STEP: Creating a pod to test consume secrets
Mar 11 16:40:20.194: INFO: Waiting up to 5m0s for pod "pod-secrets-eb288688-92ec-4d32-b970-c73ef75f4a9f" in namespace "secrets-6673" to be "Succeeded or Failed"
Mar 11 16:40:20.200: INFO: Pod "pod-secrets-eb288688-92ec-4d32-b970-c73ef75f4a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.985235ms
Mar 11 16:40:22.212: INFO: Pod "pod-secrets-eb288688-92ec-4d32-b970-c73ef75f4a9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018389142s
STEP: Saw pod success
Mar 11 16:40:22.213: INFO: Pod "pod-secrets-eb288688-92ec-4d32-b970-c73ef75f4a9f" satisfied condition "Succeeded or Failed"
Mar 11 16:40:22.217: INFO: Trying to get logs from node 198.18.16.160 pod pod-secrets-eb288688-92ec-4d32-b970-c73ef75f4a9f container secret-volume-test: <nil>
STEP: delete the pod
Mar 11 16:40:22.260: INFO: Waiting for pod pod-secrets-eb288688-92ec-4d32-b970-c73ef75f4a9f to disappear
Mar 11 16:40:22.263: INFO: Pod pod-secrets-eb288688-92ec-4d32-b970-c73ef75f4a9f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:40:22.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6673" for this suite.
STEP: Destroying namespace "secret-namespace-8091" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":339,"completed":91,"skipped":1403,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:40:22.282: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 11 16:40:26.860: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6447 pod-service-account-002e4617-ebfe-434b-9122-1940bb2db953 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 11 16:40:27.051: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6447 pod-service-account-002e4617-ebfe-434b-9122-1940bb2db953 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 11 16:40:27.237: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6447 pod-service-account-002e4617-ebfe-434b-9122-1940bb2db953 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:40:27.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6447" for this suite.

• [SLOW TEST:5.226 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":339,"completed":92,"skipped":1412,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:40:27.509: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Mar 11 16:40:29.598: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4641 PodName:pod-sharedvolume-3db26d06-0417-4354-8f4f-b9315fc9826f ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 16:40:29.598: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 16:40:29.682: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:40:29.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4641" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":339,"completed":93,"skipped":1413,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:40:29.701: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 16:40:30.260: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 16:40:33.298: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:40:33.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-36" for this suite.
STEP: Destroying namespace "webhook-36-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":339,"completed":94,"skipped":1427,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:40:33.452: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:40:44.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2147" for this suite.

• [SLOW TEST:11.146 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":339,"completed":95,"skipped":1497,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:40:44.599: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 16:40:44.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e36a3fd7-3d00-4edf-a183-7971ef0b7e14" in namespace "downward-api-9357" to be "Succeeded or Failed"
Mar 11 16:40:44.676: INFO: Pod "downwardapi-volume-e36a3fd7-3d00-4edf-a183-7971ef0b7e14": Phase="Pending", Reason="", readiness=false. Elapsed: 3.066043ms
Mar 11 16:40:46.689: INFO: Pod "downwardapi-volume-e36a3fd7-3d00-4edf-a183-7971ef0b7e14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016054101s
STEP: Saw pod success
Mar 11 16:40:46.689: INFO: Pod "downwardapi-volume-e36a3fd7-3d00-4edf-a183-7971ef0b7e14" satisfied condition "Succeeded or Failed"
Mar 11 16:40:46.693: INFO: Trying to get logs from node 198.18.16.160 pod downwardapi-volume-e36a3fd7-3d00-4edf-a183-7971ef0b7e14 container client-container: <nil>
STEP: delete the pod
Mar 11 16:40:46.723: INFO: Waiting for pod downwardapi-volume-e36a3fd7-3d00-4edf-a183-7971ef0b7e14 to disappear
Mar 11 16:40:46.728: INFO: Pod downwardapi-volume-e36a3fd7-3d00-4edf-a183-7971ef0b7e14 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:40:46.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9357" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":96,"skipped":1540,"failed":0}
SSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:40:46.739: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:40:46.860: INFO: The status of Pod test-webserver-d533f9d8-128b-4949-b177-51a86db461f4 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:40:48.864: INFO: The status of Pod test-webserver-d533f9d8-128b-4949-b177-51a86db461f4 is Running (Ready = false)
Mar 11 16:40:50.867: INFO: The status of Pod test-webserver-d533f9d8-128b-4949-b177-51a86db461f4 is Running (Ready = false)
Mar 11 16:40:52.864: INFO: The status of Pod test-webserver-d533f9d8-128b-4949-b177-51a86db461f4 is Running (Ready = false)
Mar 11 16:40:54.864: INFO: The status of Pod test-webserver-d533f9d8-128b-4949-b177-51a86db461f4 is Running (Ready = false)
Mar 11 16:40:56.865: INFO: The status of Pod test-webserver-d533f9d8-128b-4949-b177-51a86db461f4 is Running (Ready = false)
Mar 11 16:40:58.868: INFO: The status of Pod test-webserver-d533f9d8-128b-4949-b177-51a86db461f4 is Running (Ready = false)
Mar 11 16:41:00.869: INFO: The status of Pod test-webserver-d533f9d8-128b-4949-b177-51a86db461f4 is Running (Ready = false)
Mar 11 16:41:02.870: INFO: The status of Pod test-webserver-d533f9d8-128b-4949-b177-51a86db461f4 is Running (Ready = false)
Mar 11 16:41:04.868: INFO: The status of Pod test-webserver-d533f9d8-128b-4949-b177-51a86db461f4 is Running (Ready = false)
Mar 11 16:41:06.869: INFO: The status of Pod test-webserver-d533f9d8-128b-4949-b177-51a86db461f4 is Running (Ready = false)
Mar 11 16:41:08.875: INFO: The status of Pod test-webserver-d533f9d8-128b-4949-b177-51a86db461f4 is Running (Ready = true)
Mar 11 16:41:08.879: INFO: Container started at 2022-03-11 16:40:47 +0000 UTC, pod became ready at 2022-03-11 16:41:06 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:41:08.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5523" for this suite.

• [SLOW TEST:22.160 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":339,"completed":97,"skipped":1544,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:41:08.915: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-69c095db-2643-4c88-a4c5-98dc8b40bdf7
STEP: Creating configMap with name cm-test-opt-upd-21f80720-af5b-4684-88d9-3408c656271f
STEP: Creating the pod
Mar 11 16:41:08.996: INFO: The status of Pod pod-projected-configmaps-57411a2e-2a8b-4919-8277-f8e76720abf1 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:41:11.006: INFO: The status of Pod pod-projected-configmaps-57411a2e-2a8b-4919-8277-f8e76720abf1 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:41:13.007: INFO: The status of Pod pod-projected-configmaps-57411a2e-2a8b-4919-8277-f8e76720abf1 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-69c095db-2643-4c88-a4c5-98dc8b40bdf7
STEP: Updating configmap cm-test-opt-upd-21f80720-af5b-4684-88d9-3408c656271f
STEP: Creating configMap with name cm-test-opt-create-884630b4-8a85-496c-9d77-5eb0863ba336
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:42:21.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5450" for this suite.

• [SLOW TEST:72.496 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":98,"skipped":1563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:42:21.412: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-be40a234-4936-4588-a685-3429c23424ed
STEP: Creating secret with name s-test-opt-upd-3cae3c6d-52a1-4603-a198-31dc62639008
STEP: Creating the pod
Mar 11 16:42:21.511: INFO: The status of Pod pod-secrets-b1ffb493-cdf3-4bec-b7c8-fae4b8fda261 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:42:23.522: INFO: The status of Pod pod-secrets-b1ffb493-cdf3-4bec-b7c8-fae4b8fda261 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-be40a234-4936-4588-a685-3429c23424ed
STEP: Updating secret s-test-opt-upd-3cae3c6d-52a1-4603-a198-31dc62639008
STEP: Creating secret with name s-test-opt-create-072fd2e1-bd83-41ab-b620-b62f8bd305ca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:42:25.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4367" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":99,"skipped":1602,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:42:25.609: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-8037/configmap-test-27476feb-27b9-46b1-8cea-914fc8952157
STEP: Creating a pod to test consume configMaps
Mar 11 16:42:25.668: INFO: Waiting up to 5m0s for pod "pod-configmaps-9163c312-039c-4163-998e-86d4a2a497a8" in namespace "configmap-8037" to be "Succeeded or Failed"
Mar 11 16:42:25.673: INFO: Pod "pod-configmaps-9163c312-039c-4163-998e-86d4a2a497a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.818658ms
Mar 11 16:42:27.680: INFO: Pod "pod-configmaps-9163c312-039c-4163-998e-86d4a2a497a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012153804s
STEP: Saw pod success
Mar 11 16:42:27.680: INFO: Pod "pod-configmaps-9163c312-039c-4163-998e-86d4a2a497a8" satisfied condition "Succeeded or Failed"
Mar 11 16:42:27.683: INFO: Trying to get logs from node 198.18.16.160 pod pod-configmaps-9163c312-039c-4163-998e-86d4a2a497a8 container env-test: <nil>
STEP: delete the pod
Mar 11 16:42:27.718: INFO: Waiting for pod pod-configmaps-9163c312-039c-4163-998e-86d4a2a497a8 to disappear
Mar 11 16:42:27.723: INFO: Pod pod-configmaps-9163c312-039c-4163-998e-86d4a2a497a8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:42:27.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8037" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":100,"skipped":1611,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:42:27.737: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Mar 11 16:42:27.792: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 11 16:43:27.834: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:43:27.838: INFO: Starting informer...
STEP: Starting pod...
Mar 11 16:43:28.058: INFO: Pod is running on 198.18.167.130. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar 11 16:43:28.082: INFO: Pod wasn't evicted. Proceeding
Mar 11 16:43:28.082: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar 11 16:44:43.118: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:44:43.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8097" for this suite.

• [SLOW TEST:135.401 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":339,"completed":101,"skipped":1635,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:44:43.138: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:44:43.204: INFO: The status of Pod server-envvars-6034ed6a-7b70-4b1e-9a87-8b30ab2c435a is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:44:45.213: INFO: The status of Pod server-envvars-6034ed6a-7b70-4b1e-9a87-8b30ab2c435a is Running (Ready = true)
Mar 11 16:44:45.258: INFO: Waiting up to 5m0s for pod "client-envvars-ffa8d046-ef7c-432d-b9df-b39a8755f2e4" in namespace "pods-3556" to be "Succeeded or Failed"
Mar 11 16:44:45.270: INFO: Pod "client-envvars-ffa8d046-ef7c-432d-b9df-b39a8755f2e4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.000551ms
Mar 11 16:44:47.277: INFO: Pod "client-envvars-ffa8d046-ef7c-432d-b9df-b39a8755f2e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019301463s
STEP: Saw pod success
Mar 11 16:44:47.278: INFO: Pod "client-envvars-ffa8d046-ef7c-432d-b9df-b39a8755f2e4" satisfied condition "Succeeded or Failed"
Mar 11 16:44:47.281: INFO: Trying to get logs from node 198.18.167.130 pod client-envvars-ffa8d046-ef7c-432d-b9df-b39a8755f2e4 container env3cont: <nil>
STEP: delete the pod
Mar 11 16:44:47.324: INFO: Waiting for pod client-envvars-ffa8d046-ef7c-432d-b9df-b39a8755f2e4 to disappear
Mar 11 16:44:47.329: INFO: Pod client-envvars-ffa8d046-ef7c-432d-b9df-b39a8755f2e4 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:44:47.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3556" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":339,"completed":102,"skipped":1643,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:44:47.343: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:45:04.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3487" for this suite.

• [SLOW TEST:17.159 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":339,"completed":103,"skipped":1655,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:45:04.504: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:45:04.613: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"02c3b6df-ddb7-44ac-8682-60be58033983", Controller:(*bool)(0xc007082e26), BlockOwnerDeletion:(*bool)(0xc007082e27)}}
Mar 11 16:45:04.625: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"09d0c1fb-821f-4195-ba7c-03370cd75125", Controller:(*bool)(0xc002e9111e), BlockOwnerDeletion:(*bool)(0xc002e9111f)}}
Mar 11 16:45:04.633: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d89d8a13-6c95-451f-b795-25ce74ccc188", Controller:(*bool)(0xc0070830de), BlockOwnerDeletion:(*bool)(0xc0070830df)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:45:09.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6201" for this suite.

• [SLOW TEST:5.163 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":339,"completed":104,"skipped":1662,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:45:09.667: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:45:25.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9969" for this suite.

• [SLOW TEST:16.211 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":339,"completed":105,"skipped":1667,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:45:25.879: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-2d5b6a2f-b6ad-42e4-9ac4-bfccd1f325b5
STEP: Creating a pod to test consume configMaps
Mar 11 16:45:25.947: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9d735f6a-c9ab-4413-9c1e-0020942d72be" in namespace "projected-3397" to be "Succeeded or Failed"
Mar 11 16:45:25.957: INFO: Pod "pod-projected-configmaps-9d735f6a-c9ab-4413-9c1e-0020942d72be": Phase="Pending", Reason="", readiness=false. Elapsed: 10.182569ms
Mar 11 16:45:27.964: INFO: Pod "pod-projected-configmaps-9d735f6a-c9ab-4413-9c1e-0020942d72be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016411597s
STEP: Saw pod success
Mar 11 16:45:27.964: INFO: Pod "pod-projected-configmaps-9d735f6a-c9ab-4413-9c1e-0020942d72be" satisfied condition "Succeeded or Failed"
Mar 11 16:45:27.967: INFO: Trying to get logs from node 198.18.167.130 pod pod-projected-configmaps-9d735f6a-c9ab-4413-9c1e-0020942d72be container agnhost-container: <nil>
STEP: delete the pod
Mar 11 16:45:27.992: INFO: Waiting for pod pod-projected-configmaps-9d735f6a-c9ab-4413-9c1e-0020942d72be to disappear
Mar 11 16:45:27.996: INFO: Pod pod-projected-configmaps-9d735f6a-c9ab-4413-9c1e-0020942d72be no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:45:27.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3397" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":106,"skipped":1670,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:45:28.022: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Mar 11 16:45:28.065: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:45:31.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9116" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":339,"completed":107,"skipped":1679,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:45:31.944: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-fee828fb-db6d-41b2-acee-44e1f1e23e5b
STEP: Creating a pod to test consume secrets
Mar 11 16:45:32.004: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4b67dee8-ff37-4739-b62d-0d4bc985577c" in namespace "projected-2865" to be "Succeeded or Failed"
Mar 11 16:45:32.007: INFO: Pod "pod-projected-secrets-4b67dee8-ff37-4739-b62d-0d4bc985577c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.827573ms
Mar 11 16:45:34.016: INFO: Pod "pod-projected-secrets-4b67dee8-ff37-4739-b62d-0d4bc985577c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011634099s
STEP: Saw pod success
Mar 11 16:45:34.016: INFO: Pod "pod-projected-secrets-4b67dee8-ff37-4739-b62d-0d4bc985577c" satisfied condition "Succeeded or Failed"
Mar 11 16:45:34.020: INFO: Trying to get logs from node 198.18.167.130 pod pod-projected-secrets-4b67dee8-ff37-4739-b62d-0d4bc985577c container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 11 16:45:34.044: INFO: Waiting for pod pod-projected-secrets-4b67dee8-ff37-4739-b62d-0d4bc985577c to disappear
Mar 11 16:45:34.048: INFO: Pod pod-projected-secrets-4b67dee8-ff37-4739-b62d-0d4bc985577c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:45:34.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2865" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":108,"skipped":1689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:45:34.062: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-a6a70baa-683b-44bc-afd3-2498ddab5ea4
STEP: Creating a pod to test consume secrets
Mar 11 16:45:34.126: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-66611009-234e-4d40-9ed5-09f5271f5eb0" in namespace "projected-7553" to be "Succeeded or Failed"
Mar 11 16:45:34.135: INFO: Pod "pod-projected-secrets-66611009-234e-4d40-9ed5-09f5271f5eb0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.18543ms
Mar 11 16:45:36.145: INFO: Pod "pod-projected-secrets-66611009-234e-4d40-9ed5-09f5271f5eb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019447305s
STEP: Saw pod success
Mar 11 16:45:36.145: INFO: Pod "pod-projected-secrets-66611009-234e-4d40-9ed5-09f5271f5eb0" satisfied condition "Succeeded or Failed"
Mar 11 16:45:36.149: INFO: Trying to get logs from node 198.18.167.130 pod pod-projected-secrets-66611009-234e-4d40-9ed5-09f5271f5eb0 container secret-volume-test: <nil>
STEP: delete the pod
Mar 11 16:45:36.175: INFO: Waiting for pod pod-projected-secrets-66611009-234e-4d40-9ed5-09f5271f5eb0 to disappear
Mar 11 16:45:36.178: INFO: Pod pod-projected-secrets-66611009-234e-4d40-9ed5-09f5271f5eb0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:45:36.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7553" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":109,"skipped":1723,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:45:36.192: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Mar 11 16:45:36.240: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-3536 proxy --unix-socket=/tmp/kubectl-proxy-unix727104373/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:45:36.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3536" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":339,"completed":110,"skipped":1729,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:45:36.323: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 11 16:45:36.371: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4052  e6ad6d95-e0da-4759-a1bf-b133f8aa88d6 45499 0 2022-03-11 16:45:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-11 16:45:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 16:45:36.372: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4052  e6ad6d95-e0da-4759-a1bf-b133f8aa88d6 45499 0 2022-03-11 16:45:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-11 16:45:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 11 16:45:46.387: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4052  e6ad6d95-e0da-4759-a1bf-b133f8aa88d6 45647 0 2022-03-11 16:45:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-11 16:45:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 16:45:46.387: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4052  e6ad6d95-e0da-4759-a1bf-b133f8aa88d6 45647 0 2022-03-11 16:45:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-11 16:45:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 11 16:45:56.400: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4052  e6ad6d95-e0da-4759-a1bf-b133f8aa88d6 45749 0 2022-03-11 16:45:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-11 16:45:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 16:45:56.400: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4052  e6ad6d95-e0da-4759-a1bf-b133f8aa88d6 45749 0 2022-03-11 16:45:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-11 16:45:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 11 16:46:06.417: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4052  e6ad6d95-e0da-4759-a1bf-b133f8aa88d6 45849 0 2022-03-11 16:45:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-11 16:45:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 16:46:06.417: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4052  e6ad6d95-e0da-4759-a1bf-b133f8aa88d6 45849 0 2022-03-11 16:45:36 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-03-11 16:45:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 11 16:46:16.435: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4052  b9a8e27f-fb31-4dbf-8946-8f56f73826c1 45950 0 2022-03-11 16:46:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-11 16:46:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 16:46:16.435: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4052  b9a8e27f-fb31-4dbf-8946-8f56f73826c1 45950 0 2022-03-11 16:46:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-11 16:46:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 11 16:46:26.445: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4052  b9a8e27f-fb31-4dbf-8946-8f56f73826c1 46047 0 2022-03-11 16:46:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-11 16:46:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 16:46:26.445: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4052  b9a8e27f-fb31-4dbf-8946-8f56f73826c1 46047 0 2022-03-11 16:46:16 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-03-11 16:46:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:46:36.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4052" for this suite.

• [SLOW TEST:60.139 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":339,"completed":111,"skipped":1732,"failed":0}
SSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:46:36.462: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Mar 11 16:46:38.551: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:46:40.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6970" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":339,"completed":112,"skipped":1739,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:46:40.618: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:46:40.664: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 11 16:46:49.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-8045 --namespace=crd-publish-openapi-8045 create -f -'
Mar 11 16:46:50.195: INFO: stderr: ""
Mar 11 16:46:50.195: INFO: stdout: "e2e-test-crd-publish-openapi-3821-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 11 16:46:50.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-8045 --namespace=crd-publish-openapi-8045 delete e2e-test-crd-publish-openapi-3821-crds test-cr'
Mar 11 16:46:50.298: INFO: stderr: ""
Mar 11 16:46:50.298: INFO: stdout: "e2e-test-crd-publish-openapi-3821-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 11 16:46:50.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-8045 --namespace=crd-publish-openapi-8045 apply -f -'
Mar 11 16:46:50.677: INFO: stderr: ""
Mar 11 16:46:50.677: INFO: stdout: "e2e-test-crd-publish-openapi-3821-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 11 16:46:50.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-8045 --namespace=crd-publish-openapi-8045 delete e2e-test-crd-publish-openapi-3821-crds test-cr'
Mar 11 16:46:50.787: INFO: stderr: ""
Mar 11 16:46:50.787: INFO: stdout: "e2e-test-crd-publish-openapi-3821-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 11 16:46:50.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-8045 explain e2e-test-crd-publish-openapi-3821-crds'
Mar 11 16:46:51.381: INFO: stderr: ""
Mar 11 16:46:51.381: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3821-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:46:59.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8045" for this suite.

• [SLOW TEST:19.039 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":339,"completed":113,"skipped":1754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:46:59.657: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 11 16:46:59.736: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 11 16:47:59.777: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:47:59.781: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Mar 11 16:48:01.885: INFO: found a healthy node: 198.18.167.130
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:48:12.021: INFO: pods created so far: [1 1 1]
Mar 11 16:48:12.022: INFO: length of pods created so far: 3
Mar 11 16:48:26.037: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:48:33.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1985" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:48:33.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1137" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:93.494 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":339,"completed":114,"skipped":1792,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:48:33.166: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Mar 11 16:48:33.239: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:48:35.246: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:48:37.249: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 198.18.16.160 on the node which pod1 resides and expect scheduled
Mar 11 16:48:37.260: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:48:39.269: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:48:41.266: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 198.18.16.160 but use UDP protocol on the node which pod2 resides
Mar 11 16:48:41.276: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:48:43.284: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:48:45.285: INFO: The status of Pod pod3 is Running (Ready = true)
Mar 11 16:48:45.301: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:48:47.313: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Mar 11 16:48:47.317: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 198.18.16.160 http://127.0.0.1:54323/hostname] Namespace:hostport-2718 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 16:48:47.317: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: checking connectivity from pod e2e-host-exec to serverIP: 198.18.16.160, port: 54323
Mar 11 16:48:47.409: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://198.18.16.160:54323/hostname] Namespace:hostport-2718 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 16:48:47.409: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: checking connectivity from pod e2e-host-exec to serverIP: 198.18.16.160, port: 54323 UDP
Mar 11 16:48:47.492: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 198.18.16.160 54323] Namespace:hostport-2718 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 16:48:47.492: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:48:52.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-2718" for this suite.

• [SLOW TEST:19.419 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":339,"completed":115,"skipped":1847,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:48:52.586: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:48:52.643: INFO: Creating ReplicaSet my-hostname-basic-bf9eabec-5bce-4ba5-8995-6950739f4d60
Mar 11 16:48:52.654: INFO: Pod name my-hostname-basic-bf9eabec-5bce-4ba5-8995-6950739f4d60: Found 0 pods out of 1
Mar 11 16:48:57.662: INFO: Pod name my-hostname-basic-bf9eabec-5bce-4ba5-8995-6950739f4d60: Found 1 pods out of 1
Mar 11 16:48:57.662: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-bf9eabec-5bce-4ba5-8995-6950739f4d60" is running
Mar 11 16:48:57.666: INFO: Pod "my-hostname-basic-bf9eabec-5bce-4ba5-8995-6950739f4d60-x4zw4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-11 16:48:52 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-11 16:48:54 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-11 16:48:54 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-11 16:48:52 +0000 UTC Reason: Message:}])
Mar 11 16:48:57.667: INFO: Trying to dial the pod
Mar 11 16:49:02.689: INFO: Controller my-hostname-basic-bf9eabec-5bce-4ba5-8995-6950739f4d60: Got expected result from replica 1 [my-hostname-basic-bf9eabec-5bce-4ba5-8995-6950739f4d60-x4zw4]: "my-hostname-basic-bf9eabec-5bce-4ba5-8995-6950739f4d60-x4zw4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:49:02.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9870" for this suite.

• [SLOW TEST:10.118 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":116,"skipped":1848,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:49:02.704: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-1926
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1926
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1926
Mar 11 16:49:02.793: INFO: Found 0 stateful pods, waiting for 1
Mar 11 16:49:12.802: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 11 16:49:12.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-1926 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 11 16:49:13.014: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 11 16:49:13.015: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 11 16:49:13.015: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 11 16:49:13.019: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 11 16:49:23.027: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 11 16:49:23.027: INFO: Waiting for statefulset status.replicas updated to 0
Mar 11 16:49:23.047: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999765s
Mar 11 16:49:24.057: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994296834s
Mar 11 16:49:25.065: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.983819107s
Mar 11 16:49:26.070: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.976912581s
Mar 11 16:49:27.078: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.971647993s
Mar 11 16:49:28.084: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.963682451s
Mar 11 16:49:29.092: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.957375515s
Mar 11 16:49:30.098: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.949780773s
Mar 11 16:49:31.107: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.943648945s
Mar 11 16:49:32.115: INFO: Verifying statefulset ss doesn't scale past 1 for another 934.689018ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1926
Mar 11 16:49:33.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-1926 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 11 16:49:33.305: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 11 16:49:33.305: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 11 16:49:33.306: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 11 16:49:33.311: INFO: Found 1 stateful pods, waiting for 3
Mar 11 16:49:43.320: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 11 16:49:43.321: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 11 16:49:43.321: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 11 16:49:43.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-1926 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 11 16:49:43.511: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 11 16:49:43.512: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 11 16:49:43.512: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 11 16:49:43.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-1926 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 11 16:49:43.692: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 11 16:49:43.692: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 11 16:49:43.692: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 11 16:49:43.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-1926 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 11 16:49:43.874: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 11 16:49:43.874: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 11 16:49:43.874: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 11 16:49:43.874: INFO: Waiting for statefulset status.replicas updated to 0
Mar 11 16:49:43.879: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 11 16:49:53.890: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 11 16:49:53.890: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 11 16:49:53.890: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 11 16:49:53.914: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999698s
Mar 11 16:49:54.923: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996643732s
Mar 11 16:49:55.937: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986632703s
Mar 11 16:49:56.944: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97354621s
Mar 11 16:49:57.957: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965630379s
Mar 11 16:49:58.963: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.953221443s
Mar 11 16:49:59.970: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.947734099s
Mar 11 16:50:00.979: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.940250126s
Mar 11 16:50:01.997: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.930533624s
Mar 11 16:50:03.004: INFO: Verifying statefulset ss doesn't scale past 3 for another 912.866299ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1926
Mar 11 16:50:04.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-1926 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 11 16:50:04.205: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 11 16:50:04.205: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 11 16:50:04.205: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 11 16:50:04.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-1926 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 11 16:50:04.389: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 11 16:50:04.389: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 11 16:50:04.389: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 11 16:50:04.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-1926 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 11 16:50:04.554: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 11 16:50:04.554: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 11 16:50:04.554: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 11 16:50:04.554: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Mar 11 16:50:34.582: INFO: Deleting all statefulset in ns statefulset-1926
Mar 11 16:50:34.586: INFO: Scaling statefulset ss to 0
Mar 11 16:50:34.604: INFO: Waiting for statefulset status.replicas updated to 0
Mar 11 16:50:34.608: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:50:34.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1926" for this suite.

• [SLOW TEST:91.960 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":339,"completed":117,"skipped":1864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:50:34.665: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Mar 11 16:50:36.761: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-7027 PodName:var-expansion-d898192b-2785-4e39-bc20-971a8ede7dc5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 16:50:36.761: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: test for file in mounted path
Mar 11 16:50:36.846: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-7027 PodName:var-expansion-d898192b-2785-4e39-bc20-971a8ede7dc5 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 16:50:36.846: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: updating the annotation value
Mar 11 16:50:37.467: INFO: Successfully updated pod "var-expansion-d898192b-2785-4e39-bc20-971a8ede7dc5"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Mar 11 16:50:37.473: INFO: Deleting pod "var-expansion-d898192b-2785-4e39-bc20-971a8ede7dc5" in namespace "var-expansion-7027"
Mar 11 16:50:37.494: INFO: Wait up to 5m0s for pod "var-expansion-d898192b-2785-4e39-bc20-971a8ede7dc5" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:51:21.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7027" for this suite.

• [SLOW TEST:46.861 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":339,"completed":118,"skipped":1900,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:51:21.528: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 11 16:51:24.639: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:51:24.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5455" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":119,"skipped":1916,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:51:24.687: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 11 16:51:24.744: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 16:51:33.711: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:52:07.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8030" for this suite.

• [SLOW TEST:42.496 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":339,"completed":120,"skipped":1937,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:52:07.185: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:52:07.253: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 11 16:52:12.260: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 11 16:52:12.261: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 11 16:52:14.267: INFO: Creating deployment "test-rollover-deployment"
Mar 11 16:52:14.279: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 11 16:52:16.289: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 11 16:52:16.297: INFO: Ensure that both replica sets have 1 created replica
Mar 11 16:52:16.303: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 11 16:52:16.319: INFO: Updating deployment test-rollover-deployment
Mar 11 16:52:16.320: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 11 16:52:18.332: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 11 16:52:18.339: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 11 16:52:18.345: INFO: all replica sets need to contain the pod-template-hash label
Mar 11 16:52:18.345: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614337, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 11 16:52:20.359: INFO: all replica sets need to contain the pod-template-hash label
Mar 11 16:52:20.359: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614337, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 11 16:52:22.357: INFO: all replica sets need to contain the pod-template-hash label
Mar 11 16:52:22.357: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614337, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 11 16:52:24.356: INFO: all replica sets need to contain the pod-template-hash label
Mar 11 16:52:24.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614337, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 11 16:52:26.359: INFO: all replica sets need to contain the pod-template-hash label
Mar 11 16:52:26.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614337, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782614334, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 11 16:52:28.359: INFO: 
Mar 11 16:52:28.360: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Mar 11 16:52:28.374: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7590  4bb2fcfa-08cc-40c1-a29b-bede43f2bf82 50483 2 2022-03-11 16:52:14 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-03-11 16:52:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-11 16:52:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009852ef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-11 16:52:14 +0000 UTC,LastTransitionTime:2022-03-11 16:52:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2022-03-11 16:52:27 +0000 UTC,LastTransitionTime:2022-03-11 16:52:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 11 16:52:28.379: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-7590  f6adc231-3ed3-4cfa-b874-c6c1c1ed8a0d 50471 2 2022-03-11 16:52:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4bb2fcfa-08cc-40c1-a29b-bede43f2bf82 0xc0098534f0 0xc0098534f1}] []  [{kube-controller-manager Update apps/v1 2022-03-11 16:52:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4bb2fcfa-08cc-40c1-a29b-bede43f2bf82\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009853578 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 11 16:52:28.379: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 11 16:52:28.379: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7590  8706d704-f995-45d3-86bf-526a27cc8ba0 50482 2 2022-03-11 16:52:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4bb2fcfa-08cc-40c1-a29b-bede43f2bf82 0xc009853297 0xc009853298}] []  [{e2e.test Update apps/v1 2022-03-11 16:52:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-11 16:52:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4bb2fcfa-08cc-40c1-a29b-bede43f2bf82\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc009853338 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 11 16:52:28.379: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-7590  e582917d-3b52-4f0d-a7eb-452eb0dcef98 50350 2 2022-03-11 16:52:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4bb2fcfa-08cc-40c1-a29b-bede43f2bf82 0xc0098533a7 0xc0098533a8}] []  [{kube-controller-manager Update apps/v1 2022-03-11 16:52:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4bb2fcfa-08cc-40c1-a29b-bede43f2bf82\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009853488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 11 16:52:28.383: INFO: Pod "test-rollover-deployment-98c5f4599-npg99" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-npg99 test-rollover-deployment-98c5f4599- deployment-7590  1ae88b3f-3943-4f9c-add7-93948a22935c 50367 0 2022-03-11 16:52:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 f6adc231-3ed3-4cfa-b874-c6c1c1ed8a0d 0xc009853b90 0xc009853b91}] []  [{kube-controller-manager Update v1 2022-03-11 16:52:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6adc231-3ed3-4cfa-b874-c6c1c1ed8a0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 16:52:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-frzxh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-frzxh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.167.130,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:52:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:52:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:52:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 16:52:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.167.130,PodIP:192.168.2.108,StartTime:2022-03-11 16:52:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 16:52:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://b69ca3355f842b1e2dcf17e07d2352677106a87938f82ec57ed9d34e7d50a772,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:52:28.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7590" for this suite.

• [SLOW TEST:21.212 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":339,"completed":121,"skipped":1951,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:52:28.398: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-201127d7-0cb8-4c69-99f5-f026b6f37d6c in namespace container-probe-1265
Mar 11 16:52:30.468: INFO: Started pod liveness-201127d7-0cb8-4c69-99f5-f026b6f37d6c in namespace container-probe-1265
STEP: checking the pod's current state and verifying that restartCount is present
Mar 11 16:52:30.472: INFO: Initial restart count of pod liveness-201127d7-0cb8-4c69-99f5-f026b6f37d6c is 0
Mar 11 16:52:50.563: INFO: Restart count of pod container-probe-1265/liveness-201127d7-0cb8-4c69-99f5-f026b6f37d6c is now 1 (20.09154124s elapsed)
Mar 11 16:53:10.642: INFO: Restart count of pod container-probe-1265/liveness-201127d7-0cb8-4c69-99f5-f026b6f37d6c is now 2 (40.169858197s elapsed)
Mar 11 16:53:30.712: INFO: Restart count of pod container-probe-1265/liveness-201127d7-0cb8-4c69-99f5-f026b6f37d6c is now 3 (1m0.240185251s elapsed)
Mar 11 16:53:50.775: INFO: Restart count of pod container-probe-1265/liveness-201127d7-0cb8-4c69-99f5-f026b6f37d6c is now 4 (1m20.302917853s elapsed)
Mar 11 16:54:53.037: INFO: Restart count of pod container-probe-1265/liveness-201127d7-0cb8-4c69-99f5-f026b6f37d6c is now 5 (2m22.564785916s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:54:53.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1265" for this suite.

• [SLOW TEST:144.669 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":339,"completed":122,"skipped":1964,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:54:53.068: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 16:54:53.615: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 16:54:56.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:54:56.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2216" for this suite.
STEP: Destroying namespace "webhook-2216-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":339,"completed":123,"skipped":1996,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:54:56.753: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Mar 11 16:54:57.347: INFO: created pod pod-service-account-defaultsa
Mar 11 16:54:57.347: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 11 16:54:57.354: INFO: created pod pod-service-account-mountsa
Mar 11 16:54:57.354: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 11 16:54:57.363: INFO: created pod pod-service-account-nomountsa
Mar 11 16:54:57.363: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 11 16:54:57.374: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 11 16:54:57.374: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 11 16:54:57.381: INFO: created pod pod-service-account-mountsa-mountspec
Mar 11 16:54:57.381: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 11 16:54:57.392: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 11 16:54:57.392: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 11 16:54:57.401: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 11 16:54:57.401: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 11 16:54:57.415: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 11 16:54:57.415: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 11 16:54:57.424: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 11 16:54:57.424: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:54:57.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6592" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":339,"completed":124,"skipped":2011,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:54:57.447: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:54:57.509: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 11 16:55:02.516: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Mar 11 16:55:02.535: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Mar 11 16:55:02.554: INFO: observed ReplicaSet test-rs in namespace replicaset-3331 with ReadyReplicas 1, AvailableReplicas 1
Mar 11 16:55:02.576: INFO: observed ReplicaSet test-rs in namespace replicaset-3331 with ReadyReplicas 1, AvailableReplicas 1
Mar 11 16:55:02.618: INFO: observed ReplicaSet test-rs in namespace replicaset-3331 with ReadyReplicas 1, AvailableReplicas 1
Mar 11 16:55:02.633: INFO: observed ReplicaSet test-rs in namespace replicaset-3331 with ReadyReplicas 1, AvailableReplicas 1
Mar 11 16:55:04.354: INFO: observed ReplicaSet test-rs in namespace replicaset-3331 with ReadyReplicas 2, AvailableReplicas 2
Mar 11 16:55:04.369: INFO: observed Replicaset test-rs in namespace replicaset-3331 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:55:04.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3331" for this suite.

• [SLOW TEST:6.944 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":339,"completed":125,"skipped":2036,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:55:04.391: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0311 16:55:14.580544      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 11 16:56:16.613: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Mar 11 16:56:16.613: INFO: Deleting pod "simpletest-rc-to-be-deleted-5j27h" in namespace "gc-9602"
Mar 11 16:56:16.634: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qsqd" in namespace "gc-9602"
Mar 11 16:56:16.662: INFO: Deleting pod "simpletest-rc-to-be-deleted-7x7v7" in namespace "gc-9602"
Mar 11 16:56:16.686: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvh9v" in namespace "gc-9602"
Mar 11 16:56:16.705: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddmp5" in namespace "gc-9602"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:56:16.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9602" for this suite.

• [SLOW TEST:72.357 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":339,"completed":126,"skipped":2042,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:56:16.748: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:56:16.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-359" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":339,"completed":127,"skipped":2045,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:56:16.859: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar 11 16:56:16.916: INFO: Waiting up to 5m0s for pod "downward-api-1bc01896-bdf7-425c-9616-cd75d9ee9a3b" in namespace "downward-api-1381" to be "Succeeded or Failed"
Mar 11 16:56:16.927: INFO: Pod "downward-api-1bc01896-bdf7-425c-9616-cd75d9ee9a3b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.510685ms
Mar 11 16:56:18.931: INFO: Pod "downward-api-1bc01896-bdf7-425c-9616-cd75d9ee9a3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014741209s
STEP: Saw pod success
Mar 11 16:56:18.931: INFO: Pod "downward-api-1bc01896-bdf7-425c-9616-cd75d9ee9a3b" satisfied condition "Succeeded or Failed"
Mar 11 16:56:18.934: INFO: Trying to get logs from node 198.18.167.130 pod downward-api-1bc01896-bdf7-425c-9616-cd75d9ee9a3b container dapi-container: <nil>
STEP: delete the pod
Mar 11 16:56:18.982: INFO: Waiting for pod downward-api-1bc01896-bdf7-425c-9616-cd75d9ee9a3b to disappear
Mar 11 16:56:18.985: INFO: Pod downward-api-1bc01896-bdf7-425c-9616-cd75d9ee9a3b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:56:18.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1381" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":339,"completed":128,"skipped":2056,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:56:18.999: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-3123
STEP: creating service affinity-clusterip-transition in namespace services-3123
STEP: creating replication controller affinity-clusterip-transition in namespace services-3123
I0311 16:56:19.086961      19 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-3123, replica count: 3
I0311 16:56:22.138706      19 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0311 16:56:25.140227      19 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 11 16:56:25.148: INFO: Creating new exec pod
Mar 11 16:56:28.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-3123 exec execpod-affinitytl5f4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Mar 11 16:56:28.379: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 11 16:56:28.379: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:56:28.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-3123 exec execpod-affinitytl5f4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.212.14 80'
Mar 11 16:56:28.585: INFO: stderr: "+ nc -v -t -w 2 10.108.212.14 80\nConnection to 10.108.212.14 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Mar 11 16:56:28.585: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 16:56:28.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-3123 exec execpod-affinitytl5f4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.212.14:80/ ; done'
Mar 11 16:56:28.867: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n"
Mar 11 16:56:28.868: INFO: stdout: "\naffinity-clusterip-transition-hzv8s\naffinity-clusterip-transition-hzv8s\naffinity-clusterip-transition-hzv8s\naffinity-clusterip-transition-hzv8s\naffinity-clusterip-transition-lw6qq\naffinity-clusterip-transition-hzv8s\naffinity-clusterip-transition-hzv8s\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-lw6qq\naffinity-clusterip-transition-hzv8s\naffinity-clusterip-transition-lw6qq\naffinity-clusterip-transition-lw6qq\naffinity-clusterip-transition-hzv8s\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-lw6qq\naffinity-clusterip-transition-lw6qq"
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-hzv8s
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-hzv8s
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-hzv8s
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-hzv8s
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-lw6qq
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-hzv8s
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-hzv8s
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-lw6qq
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-hzv8s
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-lw6qq
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-lw6qq
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-hzv8s
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-lw6qq
Mar 11 16:56:28.868: INFO: Received response from host: affinity-clusterip-transition-lw6qq
Mar 11 16:56:28.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-3123 exec execpod-affinitytl5f4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.212.14:80/ ; done'
Mar 11 16:56:29.152: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.212.14:80/\n"
Mar 11 16:56:29.152: INFO: stdout: "\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh\naffinity-clusterip-transition-hwjnh"
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Received response from host: affinity-clusterip-transition-hwjnh
Mar 11 16:56:29.152: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3123, will wait for the garbage collector to delete the pods
Mar 11 16:56:29.262: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.921603ms
Mar 11 16:56:29.363: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.264101ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:56:41.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3123" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:22.230 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":129,"skipped":2060,"failed":0}
SSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:56:41.233: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:56:41.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8505" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":339,"completed":130,"skipped":2066,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:56:41.354: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Mar 11 16:56:41.446: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:56:43.452: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Mar 11 16:56:43.469: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:56:45.474: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Mar 11 16:56:45.488: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 11 16:56:45.499: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 11 16:56:47.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 11 16:56:47.505: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 11 16:56:49.499: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 11 16:56:49.503: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 11 16:56:51.499: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 11 16:56:51.508: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 11 16:56:53.499: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 11 16:56:53.508: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 11 16:56:55.499: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 11 16:56:55.535: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 11 16:56:57.499: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 11 16:56:57.507: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 11 16:56:59.499: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 11 16:56:59.508: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 11 16:57:01.500: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 11 16:57:01.510: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:57:01.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4760" for this suite.

• [SLOW TEST:20.182 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":339,"completed":131,"skipped":2082,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:57:01.537: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-43ebb2b3-70ee-4110-9bf6-0684e52c96c8
STEP: Creating secret with name secret-projected-all-test-volume-0124b222-1023-4056-aec2-b0fda93a562e
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 11 16:57:01.623: INFO: Waiting up to 5m0s for pod "projected-volume-0a8c2777-d527-485f-b721-a9c99beb6c89" in namespace "projected-5310" to be "Succeeded or Failed"
Mar 11 16:57:01.635: INFO: Pod "projected-volume-0a8c2777-d527-485f-b721-a9c99beb6c89": Phase="Pending", Reason="", readiness=false. Elapsed: 12.322114ms
Mar 11 16:57:03.653: INFO: Pod "projected-volume-0a8c2777-d527-485f-b721-a9c99beb6c89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029589285s
STEP: Saw pod success
Mar 11 16:57:03.653: INFO: Pod "projected-volume-0a8c2777-d527-485f-b721-a9c99beb6c89" satisfied condition "Succeeded or Failed"
Mar 11 16:57:03.656: INFO: Trying to get logs from node 198.18.167.130 pod projected-volume-0a8c2777-d527-485f-b721-a9c99beb6c89 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 11 16:57:03.702: INFO: Waiting for pod projected-volume-0a8c2777-d527-485f-b721-a9c99beb6c89 to disappear
Mar 11 16:57:03.707: INFO: Pod projected-volume-0a8c2777-d527-485f-b721-a9c99beb6c89 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:57:03.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5310" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":339,"completed":132,"skipped":2087,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:57:03.727: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Mar 11 16:57:03.790: INFO: Waiting up to 5m0s for pod "test-pod-7f58d37c-e5a7-4640-b240-782be4b4f766" in namespace "svcaccounts-3649" to be "Succeeded or Failed"
Mar 11 16:57:03.793: INFO: Pod "test-pod-7f58d37c-e5a7-4640-b240-782be4b4f766": Phase="Pending", Reason="", readiness=false. Elapsed: 3.115144ms
Mar 11 16:57:05.799: INFO: Pod "test-pod-7f58d37c-e5a7-4640-b240-782be4b4f766": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009403783s
STEP: Saw pod success
Mar 11 16:57:05.799: INFO: Pod "test-pod-7f58d37c-e5a7-4640-b240-782be4b4f766" satisfied condition "Succeeded or Failed"
Mar 11 16:57:05.802: INFO: Trying to get logs from node 198.18.167.130 pod test-pod-7f58d37c-e5a7-4640-b240-782be4b4f766 container agnhost-container: <nil>
STEP: delete the pod
Mar 11 16:57:05.828: INFO: Waiting for pod test-pod-7f58d37c-e5a7-4640-b240-782be4b4f766 to disappear
Mar 11 16:57:05.831: INFO: Pod test-pod-7f58d37c-e5a7-4640-b240-782be4b4f766 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:57:05.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3649" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":339,"completed":133,"skipped":2092,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:57:05.850: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 16:57:05.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43952ce4-6cb1-417a-995f-171ba62b7684" in namespace "downward-api-3164" to be "Succeeded or Failed"
Mar 11 16:57:05.917: INFO: Pod "downwardapi-volume-43952ce4-6cb1-417a-995f-171ba62b7684": Phase="Pending", Reason="", readiness=false. Elapsed: 7.444701ms
Mar 11 16:57:07.922: INFO: Pod "downwardapi-volume-43952ce4-6cb1-417a-995f-171ba62b7684": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012487972s
STEP: Saw pod success
Mar 11 16:57:07.922: INFO: Pod "downwardapi-volume-43952ce4-6cb1-417a-995f-171ba62b7684" satisfied condition "Succeeded or Failed"
Mar 11 16:57:07.926: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-43952ce4-6cb1-417a-995f-171ba62b7684 container client-container: <nil>
STEP: delete the pod
Mar 11 16:57:07.954: INFO: Waiting for pod downwardapi-volume-43952ce4-6cb1-417a-995f-171ba62b7684 to disappear
Mar 11 16:57:07.958: INFO: Pod downwardapi-volume-43952ce4-6cb1-417a-995f-171ba62b7684 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:57:07.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3164" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":134,"skipped":2097,"failed":0}

------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:57:07.973: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-256
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-256
STEP: Deleting pre-stop pod
Mar 11 16:57:17.103: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:57:17.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-256" for this suite.

• [SLOW TEST:9.166 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":339,"completed":135,"skipped":2097,"failed":0}
SSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:57:17.139: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-52c642e1-9990-4e72-a433-357791a4456d in namespace container-probe-8275
Mar 11 16:57:19.231: INFO: Started pod liveness-52c642e1-9990-4e72-a433-357791a4456d in namespace container-probe-8275
STEP: checking the pod's current state and verifying that restartCount is present
Mar 11 16:57:19.234: INFO: Initial restart count of pod liveness-52c642e1-9990-4e72-a433-357791a4456d is 0
Mar 11 16:57:39.327: INFO: Restart count of pod container-probe-8275/liveness-52c642e1-9990-4e72-a433-357791a4456d is now 1 (20.092806931s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:57:39.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8275" for this suite.

• [SLOW TEST:22.254 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":136,"skipped":2104,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:57:39.394: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 11 16:57:39.457: INFO: Waiting up to 5m0s for pod "pod-69ec5e61-0fb4-4d24-9044-c94993d1b37e" in namespace "emptydir-8827" to be "Succeeded or Failed"
Mar 11 16:57:39.460: INFO: Pod "pod-69ec5e61-0fb4-4d24-9044-c94993d1b37e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.146151ms
Mar 11 16:57:41.468: INFO: Pod "pod-69ec5e61-0fb4-4d24-9044-c94993d1b37e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01092345s
Mar 11 16:57:43.477: INFO: Pod "pod-69ec5e61-0fb4-4d24-9044-c94993d1b37e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019596751s
STEP: Saw pod success
Mar 11 16:57:43.477: INFO: Pod "pod-69ec5e61-0fb4-4d24-9044-c94993d1b37e" satisfied condition "Succeeded or Failed"
Mar 11 16:57:43.480: INFO: Trying to get logs from node 198.18.167.130 pod pod-69ec5e61-0fb4-4d24-9044-c94993d1b37e container test-container: <nil>
STEP: delete the pod
Mar 11 16:57:43.526: INFO: Waiting for pod pod-69ec5e61-0fb4-4d24-9044-c94993d1b37e to disappear
Mar 11 16:57:43.529: INFO: Pod pod-69ec5e61-0fb4-4d24-9044-c94993d1b37e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:57:43.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8827" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":137,"skipped":2115,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:57:43.550: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:57:43.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1283 version'
Mar 11 16:57:43.689: INFO: stderr: ""
Mar 11 16:57:43.689: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.5\", GitCommit:\"aea7bbadd2fc0cd689de94a54e5b7b758869d691\", GitTreeState:\"clean\", BuildDate:\"2021-09-15T21:10:45Z\", GoVersion:\"go1.16.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"21+\", GitVersion:\"v1.21.5-eks-1-21\", GitCommit:\"aea7bbadd2fc0cd689de94a54e5b7b758869d691\", GitTreeState:\"clean\", BuildDate:\"2021-12-10T16:56:44Z\", GoVersion:\"go1.16.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:57:43.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1283" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":339,"completed":138,"skipped":2147,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:57:43.707: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-04d418e7-8583-403c-96d3-9eda27eaaacf
STEP: Creating a pod to test consume configMaps
Mar 11 16:57:43.780: INFO: Waiting up to 5m0s for pod "pod-configmaps-c37e2904-b429-47b2-83fa-5faeb22becbd" in namespace "configmap-1126" to be "Succeeded or Failed"
Mar 11 16:57:43.790: INFO: Pod "pod-configmaps-c37e2904-b429-47b2-83fa-5faeb22becbd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.904201ms
Mar 11 16:57:45.867: INFO: Pod "pod-configmaps-c37e2904-b429-47b2-83fa-5faeb22becbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.086891376s
STEP: Saw pod success
Mar 11 16:57:45.867: INFO: Pod "pod-configmaps-c37e2904-b429-47b2-83fa-5faeb22becbd" satisfied condition "Succeeded or Failed"
Mar 11 16:57:45.876: INFO: Trying to get logs from node 198.18.167.130 pod pod-configmaps-c37e2904-b429-47b2-83fa-5faeb22becbd container agnhost-container: <nil>
STEP: delete the pod
Mar 11 16:57:45.904: INFO: Waiting for pod pod-configmaps-c37e2904-b429-47b2-83fa-5faeb22becbd to disappear
Mar 11 16:57:45.908: INFO: Pod pod-configmaps-c37e2904-b429-47b2-83fa-5faeb22becbd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:57:45.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1126" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":139,"skipped":2152,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:57:45.925: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0311 16:57:47.054160      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 11 16:58:49.081: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:58:49.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6008" for this suite.

• [SLOW TEST:63.177 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":339,"completed":140,"skipped":2159,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:58:49.104: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 11 16:58:51.201: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:58:51.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2465" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":339,"completed":141,"skipped":2274,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:58:51.273: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4119.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4119.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4119.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4119.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4119.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4119.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4119.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 11.80.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.80.11_udp@PTR;check="$$(dig +tcp +noall +answer +search 11.80.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.80.11_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4119.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4119.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4119.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4119.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4119.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4119.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4119.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4119.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 11.80.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.80.11_udp@PTR;check="$$(dig +tcp +noall +answer +search 11.80.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.80.11_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 11 16:59:01.474: INFO: Unable to read wheezy_udp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:01.479: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:01.485: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:01.489: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:01.536: INFO: Unable to read jessie_udp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:01.540: INFO: Unable to read jessie_tcp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:01.544: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:01.548: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:01.580: INFO: Lookups using dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c failed for: [wheezy_udp@dns-test-service.dns-4119.svc.cluster.local wheezy_tcp@dns-test-service.dns-4119.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local jessie_udp@dns-test-service.dns-4119.svc.cluster.local jessie_tcp@dns-test-service.dns-4119.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local]

Mar 11 16:59:06.589: INFO: Unable to read wheezy_udp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:06.595: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:06.601: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:06.607: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:06.644: INFO: Unable to read jessie_udp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:06.650: INFO: Unable to read jessie_tcp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:06.656: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:06.663: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:06.693: INFO: Lookups using dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c failed for: [wheezy_udp@dns-test-service.dns-4119.svc.cluster.local wheezy_tcp@dns-test-service.dns-4119.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local jessie_udp@dns-test-service.dns-4119.svc.cluster.local jessie_tcp@dns-test-service.dns-4119.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local]

Mar 11 16:59:11.590: INFO: Unable to read wheezy_udp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:11.596: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:11.601: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:11.606: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:11.641: INFO: Unable to read jessie_udp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:11.645: INFO: Unable to read jessie_tcp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:11.651: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:11.655: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:11.684: INFO: Lookups using dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c failed for: [wheezy_udp@dns-test-service.dns-4119.svc.cluster.local wheezy_tcp@dns-test-service.dns-4119.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local jessie_udp@dns-test-service.dns-4119.svc.cluster.local jessie_tcp@dns-test-service.dns-4119.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local]

Mar 11 16:59:16.587: INFO: Unable to read wheezy_udp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:16.592: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:16.597: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:16.603: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:16.634: INFO: Unable to read jessie_udp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:16.639: INFO: Unable to read jessie_tcp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:16.645: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:16.650: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:16.676: INFO: Lookups using dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c failed for: [wheezy_udp@dns-test-service.dns-4119.svc.cluster.local wheezy_tcp@dns-test-service.dns-4119.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local jessie_udp@dns-test-service.dns-4119.svc.cluster.local jessie_tcp@dns-test-service.dns-4119.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local]

Mar 11 16:59:21.590: INFO: Unable to read wheezy_udp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:21.595: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:21.601: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:21.608: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:21.639: INFO: Unable to read jessie_udp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:21.644: INFO: Unable to read jessie_tcp@dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:21.649: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:21.653: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local from pod dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c: the server could not find the requested resource (get pods dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c)
Mar 11 16:59:21.693: INFO: Lookups using dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c failed for: [wheezy_udp@dns-test-service.dns-4119.svc.cluster.local wheezy_tcp@dns-test-service.dns-4119.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local jessie_udp@dns-test-service.dns-4119.svc.cluster.local jessie_tcp@dns-test-service.dns-4119.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4119.svc.cluster.local]

Mar 11 16:59:26.694: INFO: DNS probes using dns-4119/dns-test-59e83cad-25ed-4612-837f-6cc3fa53915c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:59:26.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4119" for this suite.

• [SLOW TEST:35.546 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":339,"completed":142,"skipped":2292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:59:26.821: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:59:26.886: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7946
I0311 16:59:26.916177      19 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7946, replica count: 1
I0311 16:59:27.968638      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0311 16:59:28.968978      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0311 16:59:29.969445      19 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 11 16:59:30.104: INFO: Created: latency-svc-7svx6
Mar 11 16:59:30.111: INFO: Got endpoints: latency-svc-7svx6 [41.516393ms]
Mar 11 16:59:30.141: INFO: Created: latency-svc-wc97m
Mar 11 16:59:30.143: INFO: Got endpoints: latency-svc-wc97m [31.753513ms]
Mar 11 16:59:30.161: INFO: Created: latency-svc-wr455
Mar 11 16:59:30.218: INFO: Got endpoints: latency-svc-wr455 [105.791334ms]
Mar 11 16:59:30.231: INFO: Created: latency-svc-lsgss
Mar 11 16:59:30.231: INFO: Got endpoints: latency-svc-lsgss [119.126159ms]
Mar 11 16:59:30.275: INFO: Created: latency-svc-5pgg7
Mar 11 16:59:30.278: INFO: Got endpoints: latency-svc-5pgg7 [165.443291ms]
Mar 11 16:59:30.304: INFO: Created: latency-svc-glb46
Mar 11 16:59:30.304: INFO: Got endpoints: latency-svc-glb46 [191.900036ms]
Mar 11 16:59:30.322: INFO: Created: latency-svc-pzq2g
Mar 11 16:59:30.347: INFO: Got endpoints: latency-svc-pzq2g [233.850996ms]
Mar 11 16:59:30.355: INFO: Created: latency-svc-f8m7b
Mar 11 16:59:30.359: INFO: Got endpoints: latency-svc-f8m7b [246.246949ms]
Mar 11 16:59:30.376: INFO: Created: latency-svc-p776v
Mar 11 16:59:30.398: INFO: Got endpoints: latency-svc-p776v [284.308017ms]
Mar 11 16:59:30.398: INFO: Created: latency-svc-lzvbp
Mar 11 16:59:30.407: INFO: Got endpoints: latency-svc-lzvbp [293.761491ms]
Mar 11 16:59:30.423: INFO: Created: latency-svc-8kkbj
Mar 11 16:59:30.435: INFO: Got endpoints: latency-svc-8kkbj [321.456875ms]
Mar 11 16:59:30.444: INFO: Created: latency-svc-lgqkb
Mar 11 16:59:30.459: INFO: Got endpoints: latency-svc-lgqkb [51.295417ms]
Mar 11 16:59:30.471: INFO: Created: latency-svc-sgkfs
Mar 11 16:59:30.485: INFO: Got endpoints: latency-svc-sgkfs [370.889542ms]
Mar 11 16:59:30.499: INFO: Created: latency-svc-lscmb
Mar 11 16:59:30.517: INFO: Created: latency-svc-fw42x
Mar 11 16:59:30.518: INFO: Got endpoints: latency-svc-lscmb [403.615038ms]
Mar 11 16:59:30.535: INFO: Got endpoints: latency-svc-fw42x [420.999506ms]
Mar 11 16:59:30.554: INFO: Created: latency-svc-42jlw
Mar 11 16:59:30.567: INFO: Got endpoints: latency-svc-42jlw [452.592486ms]
Mar 11 16:59:30.572: INFO: Created: latency-svc-66jw9
Mar 11 16:59:30.586: INFO: Got endpoints: latency-svc-66jw9 [471.296256ms]
Mar 11 16:59:30.592: INFO: Created: latency-svc-lr2pd
Mar 11 16:59:30.611: INFO: Got endpoints: latency-svc-lr2pd [467.839088ms]
Mar 11 16:59:30.619: INFO: Created: latency-svc-58wx4
Mar 11 16:59:30.627: INFO: Got endpoints: latency-svc-58wx4 [395.131955ms]
Mar 11 16:59:30.648: INFO: Created: latency-svc-9kz52
Mar 11 16:59:30.676: INFO: Got endpoints: latency-svc-9kz52 [458.185857ms]
Mar 11 16:59:30.677: INFO: Created: latency-svc-xx4wg
Mar 11 16:59:30.703: INFO: Got endpoints: latency-svc-xx4wg [425.213913ms]
Mar 11 16:59:30.708: INFO: Created: latency-svc-fpz5k
Mar 11 16:59:30.711: INFO: Got endpoints: latency-svc-fpz5k [407.18301ms]
Mar 11 16:59:30.720: INFO: Created: latency-svc-t9t27
Mar 11 16:59:30.732: INFO: Got endpoints: latency-svc-t9t27 [385.454703ms]
Mar 11 16:59:30.738: INFO: Created: latency-svc-rb5vk
Mar 11 16:59:30.757: INFO: Created: latency-svc-cfhtr
Mar 11 16:59:30.757: INFO: Got endpoints: latency-svc-rb5vk [397.710709ms]
Mar 11 16:59:30.775: INFO: Got endpoints: latency-svc-cfhtr [377.757024ms]
Mar 11 16:59:30.775: INFO: Created: latency-svc-flvr6
Mar 11 16:59:30.789: INFO: Got endpoints: latency-svc-flvr6 [353.319748ms]
Mar 11 16:59:30.795: INFO: Created: latency-svc-r7tc9
Mar 11 16:59:30.806: INFO: Got endpoints: latency-svc-r7tc9 [347.350857ms]
Mar 11 16:59:30.819: INFO: Created: latency-svc-zctbd
Mar 11 16:59:30.833: INFO: Got endpoints: latency-svc-zctbd [348.514469ms]
Mar 11 16:59:30.838: INFO: Created: latency-svc-w6llp
Mar 11 16:59:30.851: INFO: Got endpoints: latency-svc-w6llp [333.153354ms]
Mar 11 16:59:30.857: INFO: Created: latency-svc-gddmh
Mar 11 16:59:30.872: INFO: Got endpoints: latency-svc-gddmh [337.101556ms]
Mar 11 16:59:30.899: INFO: Created: latency-svc-hfv4g
Mar 11 16:59:30.911: INFO: Got endpoints: latency-svc-hfv4g [343.729849ms]
Mar 11 16:59:30.911: INFO: Created: latency-svc-khv7f
Mar 11 16:59:30.921: INFO: Got endpoints: latency-svc-khv7f [335.353244ms]
Mar 11 16:59:30.932: INFO: Created: latency-svc-j5rkd
Mar 11 16:59:30.952: INFO: Got endpoints: latency-svc-j5rkd [340.750352ms]
Mar 11 16:59:30.964: INFO: Created: latency-svc-qbqsf
Mar 11 16:59:30.979: INFO: Got endpoints: latency-svc-qbqsf [352.294711ms]
Mar 11 16:59:30.980: INFO: Created: latency-svc-9xldg
Mar 11 16:59:30.993: INFO: Created: latency-svc-dwgs7
Mar 11 16:59:31.082: INFO: Created: latency-svc-h5djl
Mar 11 16:59:31.351: INFO: Created: latency-svc-nzlb7
Mar 11 16:59:31.351: INFO: Created: latency-svc-tdkmx
Mar 11 16:59:31.367: INFO: Got endpoints: latency-svc-dwgs7 [663.816612ms]
Mar 11 16:59:31.367: INFO: Got endpoints: latency-svc-9xldg [691.163265ms]
Mar 11 16:59:31.368: INFO: Got endpoints: latency-svc-nzlb7 [635.437876ms]
Mar 11 16:59:31.368: INFO: Got endpoints: latency-svc-tdkmx [656.713823ms]
Mar 11 16:59:31.369: INFO: Got endpoints: latency-svc-h5djl [611.003325ms]
Mar 11 16:59:31.373: INFO: Created: latency-svc-l6qgh
Mar 11 16:59:31.373: INFO: Created: latency-svc-5krsq
Mar 11 16:59:31.374: INFO: Created: latency-svc-sc8s2
Mar 11 16:59:31.374: INFO: Created: latency-svc-f2zb7
Mar 11 16:59:31.374: INFO: Created: latency-svc-qpdr6
Mar 11 16:59:31.374: INFO: Created: latency-svc-5gksf
Mar 11 16:59:31.374: INFO: Created: latency-svc-ckkmf
Mar 11 16:59:31.374: INFO: Created: latency-svc-zdltt
Mar 11 16:59:31.383: INFO: Got endpoints: latency-svc-zdltt [607.070543ms]
Mar 11 16:59:31.385: INFO: Created: latency-svc-c9m58
Mar 11 16:59:31.385: INFO: Created: latency-svc-pwxxc
Mar 11 16:59:31.385: INFO: Got endpoints: latency-svc-qpdr6 [474.16483ms]
Mar 11 16:59:31.385: INFO: Got endpoints: latency-svc-c9m58 [596.440774ms]
Mar 11 16:59:31.386: INFO: Got endpoints: latency-svc-pwxxc [579.315941ms]
Mar 11 16:59:31.386: INFO: Got endpoints: latency-svc-5krsq [552.660254ms]
Mar 11 16:59:31.386: INFO: Got endpoints: latency-svc-sc8s2 [535.412783ms]
Mar 11 16:59:31.387: INFO: Got endpoints: latency-svc-f2zb7 [514.649192ms]
Mar 11 16:59:31.389: INFO: Got endpoints: latency-svc-5gksf [468.10715ms]
Mar 11 16:59:31.390: INFO: Got endpoints: latency-svc-ckkmf [436.846956ms]
Mar 11 16:59:31.390: INFO: Got endpoints: latency-svc-l6qgh [410.472752ms]
Mar 11 16:59:31.404: INFO: Created: latency-svc-ctsdn
Mar 11 16:59:31.414: INFO: Got endpoints: latency-svc-ctsdn [46.282974ms]
Mar 11 16:59:31.417: INFO: Created: latency-svc-wgxbx
Mar 11 16:59:31.440: INFO: Got endpoints: latency-svc-wgxbx [72.604387ms]
Mar 11 16:59:31.448: INFO: Created: latency-svc-27lpk
Mar 11 16:59:31.450: INFO: Got endpoints: latency-svc-27lpk [82.143334ms]
Mar 11 16:59:31.468: INFO: Created: latency-svc-txbq9
Mar 11 16:59:31.476: INFO: Got endpoints: latency-svc-txbq9 [108.044289ms]
Mar 11 16:59:31.490: INFO: Created: latency-svc-kbphr
Mar 11 16:59:31.503: INFO: Got endpoints: latency-svc-kbphr [134.890664ms]
Mar 11 16:59:31.532: INFO: Created: latency-svc-nws9g
Mar 11 16:59:31.547: INFO: Got endpoints: latency-svc-nws9g [163.221913ms]
Mar 11 16:59:31.559: INFO: Created: latency-svc-72thl
Mar 11 16:59:31.581: INFO: Got endpoints: latency-svc-72thl [195.173896ms]
Mar 11 16:59:31.582: INFO: Created: latency-svc-t7jx7
Mar 11 16:59:31.605: INFO: Created: latency-svc-n9224
Mar 11 16:59:31.605: INFO: Got endpoints: latency-svc-t7jx7 [218.80259ms]
Mar 11 16:59:31.615: INFO: Got endpoints: latency-svc-n9224 [229.895283ms]
Mar 11 16:59:31.633: INFO: Created: latency-svc-7tkbd
Mar 11 16:59:31.651: INFO: Got endpoints: latency-svc-7tkbd [264.957287ms]
Mar 11 16:59:31.656: INFO: Created: latency-svc-xtwgg
Mar 11 16:59:31.680: INFO: Got endpoints: latency-svc-xtwgg [294.737986ms]
Mar 11 16:59:31.698: INFO: Created: latency-svc-xtqpf
Mar 11 16:59:31.726: INFO: Got endpoints: latency-svc-xtqpf [336.217137ms]
Mar 11 16:59:31.739: INFO: Created: latency-svc-tgbrj
Mar 11 16:59:31.764: INFO: Created: latency-svc-g6jr8
Mar 11 16:59:31.765: INFO: Got endpoints: latency-svc-tgbrj [83.40975ms]
Mar 11 16:59:31.785: INFO: Got endpoints: latency-svc-g6jr8 [394.502599ms]
Mar 11 16:59:31.792: INFO: Created: latency-svc-tsd6k
Mar 11 16:59:31.809: INFO: Got endpoints: latency-svc-tsd6k [418.514044ms]
Mar 11 16:59:31.821: INFO: Created: latency-svc-vg5cr
Mar 11 16:59:31.835: INFO: Created: latency-svc-n7hfx
Mar 11 16:59:31.877: INFO: Got endpoints: latency-svc-vg5cr [487.750707ms]
Mar 11 16:59:31.878: INFO: Created: latency-svc-ql7ps
Mar 11 16:59:31.889: INFO: Created: latency-svc-6cqgc
Mar 11 16:59:31.919: INFO: Created: latency-svc-wt8hn
Mar 11 16:59:31.922: INFO: Got endpoints: latency-svc-n7hfx [507.906822ms]
Mar 11 16:59:31.942: INFO: Created: latency-svc-mnwgz
Mar 11 16:59:31.967: INFO: Created: latency-svc-x25xx
Mar 11 16:59:31.974: INFO: Got endpoints: latency-svc-ql7ps [534.027884ms]
Mar 11 16:59:31.983: INFO: Created: latency-svc-qfhgk
Mar 11 16:59:32.008: INFO: Created: latency-svc-5nmqn
Mar 11 16:59:32.017: INFO: Got endpoints: latency-svc-6cqgc [567.217448ms]
Mar 11 16:59:32.031: INFO: Created: latency-svc-pqfb5
Mar 11 16:59:32.043: INFO: Created: latency-svc-kc7vv
Mar 11 16:59:32.074: INFO: Got endpoints: latency-svc-wt8hn [597.434353ms]
Mar 11 16:59:32.079: INFO: Created: latency-svc-wrgqc
Mar 11 16:59:32.094: INFO: Created: latency-svc-dpllr
Mar 11 16:59:32.109: INFO: Got endpoints: latency-svc-mnwgz [605.039894ms]
Mar 11 16:59:32.122: INFO: Created: latency-svc-4gwmk
Mar 11 16:59:32.136: INFO: Created: latency-svc-59dv2
Mar 11 16:59:32.149: INFO: Created: latency-svc-vsp8m
Mar 11 16:59:32.167: INFO: Created: latency-svc-n56fm
Mar 11 16:59:32.167: INFO: Got endpoints: latency-svc-x25xx [620.543462ms]
Mar 11 16:59:32.184: INFO: Created: latency-svc-88nw2
Mar 11 16:59:32.208: INFO: Created: latency-svc-lccst
Mar 11 16:59:32.267: INFO: Got endpoints: latency-svc-qfhgk [686.145447ms]
Mar 11 16:59:32.272: INFO: Got endpoints: latency-svc-5nmqn [666.843443ms]
Mar 11 16:59:32.277: INFO: Created: latency-svc-cw8m7
Mar 11 16:59:32.306: INFO: Created: latency-svc-w4qxn
Mar 11 16:59:32.311: INFO: Got endpoints: latency-svc-pqfb5 [695.83203ms]
Mar 11 16:59:32.325: INFO: Created: latency-svc-h2g59
Mar 11 16:59:32.342: INFO: Created: latency-svc-n6l52
Mar 11 16:59:32.362: INFO: Got endpoints: latency-svc-kc7vv [710.85748ms]
Mar 11 16:59:32.363: INFO: Created: latency-svc-mkv76
Mar 11 16:59:32.382: INFO: Created: latency-svc-bfbc9
Mar 11 16:59:32.407: INFO: Created: latency-svc-pznrq
Mar 11 16:59:32.417: INFO: Got endpoints: latency-svc-wrgqc [690.792247ms]
Mar 11 16:59:32.453: INFO: Created: latency-svc-t9tdd
Mar 11 16:59:32.462: INFO: Got endpoints: latency-svc-dpllr [697.548687ms]
Mar 11 16:59:32.486: INFO: Created: latency-svc-64vq8
Mar 11 16:59:32.515: INFO: Got endpoints: latency-svc-4gwmk [729.490721ms]
Mar 11 16:59:32.542: INFO: Created: latency-svc-fxwsx
Mar 11 16:59:32.561: INFO: Got endpoints: latency-svc-59dv2 [751.891087ms]
Mar 11 16:59:32.588: INFO: Created: latency-svc-5nd55
Mar 11 16:59:32.614: INFO: Got endpoints: latency-svc-vsp8m [736.555184ms]
Mar 11 16:59:32.642: INFO: Created: latency-svc-mj8sx
Mar 11 16:59:32.660: INFO: Got endpoints: latency-svc-n56fm [738.028964ms]
Mar 11 16:59:32.681: INFO: Created: latency-svc-54vkt
Mar 11 16:59:32.712: INFO: Got endpoints: latency-svc-88nw2 [737.707559ms]
Mar 11 16:59:32.745: INFO: Created: latency-svc-sgtmw
Mar 11 16:59:32.768: INFO: Got endpoints: latency-svc-lccst [750.450568ms]
Mar 11 16:59:32.789: INFO: Created: latency-svc-xhkhf
Mar 11 16:59:32.813: INFO: Got endpoints: latency-svc-cw8m7 [738.523033ms]
Mar 11 16:59:32.834: INFO: Created: latency-svc-2khd8
Mar 11 16:59:32.857: INFO: Got endpoints: latency-svc-w4qxn [748.534042ms]
Mar 11 16:59:32.879: INFO: Created: latency-svc-pbqh5
Mar 11 16:59:32.912: INFO: Got endpoints: latency-svc-h2g59 [744.435556ms]
Mar 11 16:59:32.936: INFO: Created: latency-svc-szcdv
Mar 11 16:59:32.962: INFO: Got endpoints: latency-svc-n6l52 [690.621914ms]
Mar 11 16:59:32.983: INFO: Created: latency-svc-6shkz
Mar 11 16:59:33.004: INFO: Got endpoints: latency-svc-mkv76 [731.20414ms]
Mar 11 16:59:33.030: INFO: Created: latency-svc-xv5h9
Mar 11 16:59:33.060: INFO: Got endpoints: latency-svc-bfbc9 [749.168262ms]
Mar 11 16:59:33.083: INFO: Created: latency-svc-sk6rm
Mar 11 16:59:33.111: INFO: Got endpoints: latency-svc-pznrq [748.771577ms]
Mar 11 16:59:33.140: INFO: Created: latency-svc-cz4rh
Mar 11 16:59:33.174: INFO: Got endpoints: latency-svc-t9tdd [756.659325ms]
Mar 11 16:59:33.202: INFO: Created: latency-svc-n5vjm
Mar 11 16:59:33.220: INFO: Got endpoints: latency-svc-64vq8 [757.627802ms]
Mar 11 16:59:33.248: INFO: Created: latency-svc-52td2
Mar 11 16:59:33.259: INFO: Got endpoints: latency-svc-fxwsx [743.950124ms]
Mar 11 16:59:33.280: INFO: Created: latency-svc-d2mr9
Mar 11 16:59:33.311: INFO: Got endpoints: latency-svc-5nd55 [749.823075ms]
Mar 11 16:59:33.339: INFO: Created: latency-svc-4kbjm
Mar 11 16:59:33.364: INFO: Got endpoints: latency-svc-mj8sx [750.362301ms]
Mar 11 16:59:33.397: INFO: Created: latency-svc-lfltw
Mar 11 16:59:33.415: INFO: Got endpoints: latency-svc-54vkt [753.96023ms]
Mar 11 16:59:33.438: INFO: Created: latency-svc-l59dr
Mar 11 16:59:33.465: INFO: Got endpoints: latency-svc-sgtmw [753.281539ms]
Mar 11 16:59:33.502: INFO: Created: latency-svc-n9cx4
Mar 11 16:59:33.508: INFO: Got endpoints: latency-svc-xhkhf [740.29978ms]
Mar 11 16:59:33.538: INFO: Created: latency-svc-b8wgp
Mar 11 16:59:33.560: INFO: Got endpoints: latency-svc-2khd8 [746.892507ms]
Mar 11 16:59:33.584: INFO: Created: latency-svc-dl5jq
Mar 11 16:59:33.614: INFO: Got endpoints: latency-svc-pbqh5 [756.25377ms]
Mar 11 16:59:33.640: INFO: Created: latency-svc-6d85x
Mar 11 16:59:33.661: INFO: Got endpoints: latency-svc-szcdv [749.062998ms]
Mar 11 16:59:33.701: INFO: Created: latency-svc-z9bzb
Mar 11 16:59:33.711: INFO: Got endpoints: latency-svc-6shkz [749.254902ms]
Mar 11 16:59:33.739: INFO: Created: latency-svc-hjxrl
Mar 11 16:59:33.755: INFO: Got endpoints: latency-svc-xv5h9 [750.794145ms]
Mar 11 16:59:33.785: INFO: Created: latency-svc-7wmdr
Mar 11 16:59:33.807: INFO: Got endpoints: latency-svc-sk6rm [746.22004ms]
Mar 11 16:59:33.838: INFO: Created: latency-svc-wbhpd
Mar 11 16:59:33.862: INFO: Got endpoints: latency-svc-cz4rh [749.975657ms]
Mar 11 16:59:33.891: INFO: Created: latency-svc-tvw4x
Mar 11 16:59:33.911: INFO: Got endpoints: latency-svc-n5vjm [736.332841ms]
Mar 11 16:59:33.944: INFO: Created: latency-svc-4ssmh
Mar 11 16:59:33.957: INFO: Got endpoints: latency-svc-52td2 [735.30769ms]
Mar 11 16:59:33.984: INFO: Created: latency-svc-2hmdz
Mar 11 16:59:34.012: INFO: Got endpoints: latency-svc-d2mr9 [753.011267ms]
Mar 11 16:59:34.034: INFO: Created: latency-svc-xn9t4
Mar 11 16:59:34.062: INFO: Got endpoints: latency-svc-4kbjm [750.154233ms]
Mar 11 16:59:34.088: INFO: Created: latency-svc-6jlpn
Mar 11 16:59:34.117: INFO: Got endpoints: latency-svc-lfltw [752.648973ms]
Mar 11 16:59:34.168: INFO: Got endpoints: latency-svc-l59dr [753.429386ms]
Mar 11 16:59:34.175: INFO: Created: latency-svc-g6wcb
Mar 11 16:59:34.193: INFO: Created: latency-svc-cbps7
Mar 11 16:59:34.213: INFO: Got endpoints: latency-svc-n9cx4 [747.930919ms]
Mar 11 16:59:34.254: INFO: Created: latency-svc-fk677
Mar 11 16:59:34.263: INFO: Got endpoints: latency-svc-b8wgp [754.359626ms]
Mar 11 16:59:34.297: INFO: Created: latency-svc-rmvdx
Mar 11 16:59:34.313: INFO: Got endpoints: latency-svc-dl5jq [752.563416ms]
Mar 11 16:59:34.340: INFO: Created: latency-svc-25pmg
Mar 11 16:59:34.362: INFO: Got endpoints: latency-svc-6d85x [748.128974ms]
Mar 11 16:59:34.393: INFO: Created: latency-svc-wzzzn
Mar 11 16:59:34.404: INFO: Got endpoints: latency-svc-z9bzb [742.895628ms]
Mar 11 16:59:34.430: INFO: Created: latency-svc-jw5fq
Mar 11 16:59:34.459: INFO: Got endpoints: latency-svc-hjxrl [748.199529ms]
Mar 11 16:59:34.479: INFO: Created: latency-svc-hx42h
Mar 11 16:59:34.512: INFO: Got endpoints: latency-svc-7wmdr [757.214381ms]
Mar 11 16:59:34.535: INFO: Created: latency-svc-f4j8f
Mar 11 16:59:34.559: INFO: Got endpoints: latency-svc-wbhpd [752.388313ms]
Mar 11 16:59:34.584: INFO: Created: latency-svc-6jns4
Mar 11 16:59:34.609: INFO: Got endpoints: latency-svc-tvw4x [746.806891ms]
Mar 11 16:59:34.629: INFO: Created: latency-svc-xplwj
Mar 11 16:59:34.660: INFO: Got endpoints: latency-svc-4ssmh [748.905701ms]
Mar 11 16:59:34.683: INFO: Created: latency-svc-scqcd
Mar 11 16:59:34.707: INFO: Got endpoints: latency-svc-2hmdz [750.209528ms]
Mar 11 16:59:34.730: INFO: Created: latency-svc-mrlmh
Mar 11 16:59:34.758: INFO: Got endpoints: latency-svc-xn9t4 [745.228884ms]
Mar 11 16:59:34.781: INFO: Created: latency-svc-pfrw2
Mar 11 16:59:34.809: INFO: Got endpoints: latency-svc-6jlpn [747.613499ms]
Mar 11 16:59:34.834: INFO: Created: latency-svc-2xrph
Mar 11 16:59:34.862: INFO: Got endpoints: latency-svc-g6wcb [744.557185ms]
Mar 11 16:59:34.884: INFO: Created: latency-svc-hrhcg
Mar 11 16:59:34.913: INFO: Got endpoints: latency-svc-cbps7 [744.431816ms]
Mar 11 16:59:34.937: INFO: Created: latency-svc-wk6lm
Mar 11 16:59:34.964: INFO: Got endpoints: latency-svc-fk677 [751.227365ms]
Mar 11 16:59:34.993: INFO: Created: latency-svc-29mtm
Mar 11 16:59:35.013: INFO: Got endpoints: latency-svc-rmvdx [750.127936ms]
Mar 11 16:59:35.039: INFO: Created: latency-svc-68nbw
Mar 11 16:59:35.060: INFO: Got endpoints: latency-svc-25pmg [746.711135ms]
Mar 11 16:59:35.080: INFO: Created: latency-svc-ktb5n
Mar 11 16:59:35.111: INFO: Got endpoints: latency-svc-wzzzn [748.278835ms]
Mar 11 16:59:35.144: INFO: Created: latency-svc-bhf4t
Mar 11 16:59:35.158: INFO: Got endpoints: latency-svc-jw5fq [753.59193ms]
Mar 11 16:59:35.223: INFO: Got endpoints: latency-svc-hx42h [764.162957ms]
Mar 11 16:59:35.240: INFO: Created: latency-svc-wrc44
Mar 11 16:59:35.262: INFO: Created: latency-svc-hdbpx
Mar 11 16:59:35.267: INFO: Got endpoints: latency-svc-f4j8f [754.357147ms]
Mar 11 16:59:35.300: INFO: Created: latency-svc-f2sfm
Mar 11 16:59:35.315: INFO: Got endpoints: latency-svc-6jns4 [755.203263ms]
Mar 11 16:59:35.336: INFO: Created: latency-svc-h96rh
Mar 11 16:59:35.361: INFO: Got endpoints: latency-svc-xplwj [751.985936ms]
Mar 11 16:59:35.382: INFO: Created: latency-svc-7gb4s
Mar 11 16:59:35.411: INFO: Got endpoints: latency-svc-scqcd [751.288425ms]
Mar 11 16:59:35.440: INFO: Created: latency-svc-66969
Mar 11 16:59:35.463: INFO: Got endpoints: latency-svc-mrlmh [752.998515ms]
Mar 11 16:59:35.487: INFO: Created: latency-svc-cjmm2
Mar 11 16:59:35.516: INFO: Got endpoints: latency-svc-pfrw2 [757.651724ms]
Mar 11 16:59:35.540: INFO: Created: latency-svc-fqjlr
Mar 11 16:59:35.561: INFO: Got endpoints: latency-svc-2xrph [751.697194ms]
Mar 11 16:59:35.592: INFO: Created: latency-svc-rh4dw
Mar 11 16:59:35.610: INFO: Got endpoints: latency-svc-hrhcg [747.58857ms]
Mar 11 16:59:35.636: INFO: Created: latency-svc-4r5sv
Mar 11 16:59:35.663: INFO: Got endpoints: latency-svc-wk6lm [749.276282ms]
Mar 11 16:59:35.687: INFO: Created: latency-svc-l7gdn
Mar 11 16:59:35.710: INFO: Got endpoints: latency-svc-29mtm [745.492084ms]
Mar 11 16:59:35.740: INFO: Created: latency-svc-27mbl
Mar 11 16:59:35.756: INFO: Got endpoints: latency-svc-68nbw [742.707438ms]
Mar 11 16:59:35.781: INFO: Created: latency-svc-bwl49
Mar 11 16:59:35.814: INFO: Got endpoints: latency-svc-ktb5n [753.80898ms]
Mar 11 16:59:35.836: INFO: Created: latency-svc-g7lwl
Mar 11 16:59:35.859: INFO: Got endpoints: latency-svc-bhf4t [748.039254ms]
Mar 11 16:59:35.889: INFO: Created: latency-svc-sw56f
Mar 11 16:59:35.915: INFO: Got endpoints: latency-svc-wrc44 [756.592853ms]
Mar 11 16:59:35.953: INFO: Created: latency-svc-gwh2q
Mar 11 16:59:35.961: INFO: Got endpoints: latency-svc-hdbpx [736.573439ms]
Mar 11 16:59:35.989: INFO: Created: latency-svc-6m8kp
Mar 11 16:59:36.010: INFO: Got endpoints: latency-svc-f2sfm [739.292346ms]
Mar 11 16:59:36.036: INFO: Created: latency-svc-h5lqk
Mar 11 16:59:36.056: INFO: Got endpoints: latency-svc-h96rh [741.410733ms]
Mar 11 16:59:36.079: INFO: Created: latency-svc-wflxz
Mar 11 16:59:36.113: INFO: Got endpoints: latency-svc-7gb4s [752.039423ms]
Mar 11 16:59:36.134: INFO: Created: latency-svc-c98s7
Mar 11 16:59:36.172: INFO: Got endpoints: latency-svc-66969 [758.895226ms]
Mar 11 16:59:36.196: INFO: Created: latency-svc-kfjvs
Mar 11 16:59:36.215: INFO: Got endpoints: latency-svc-cjmm2 [752.171296ms]
Mar 11 16:59:36.236: INFO: Created: latency-svc-7r94c
Mar 11 16:59:36.268: INFO: Got endpoints: latency-svc-fqjlr [751.189437ms]
Mar 11 16:59:36.301: INFO: Created: latency-svc-jmzk4
Mar 11 16:59:36.332: INFO: Got endpoints: latency-svc-rh4dw [770.51444ms]
Mar 11 16:59:36.364: INFO: Created: latency-svc-2m7r5
Mar 11 16:59:36.365: INFO: Got endpoints: latency-svc-4r5sv [754.692627ms]
Mar 11 16:59:36.391: INFO: Created: latency-svc-pmhbb
Mar 11 16:59:36.411: INFO: Got endpoints: latency-svc-l7gdn [748.06645ms]
Mar 11 16:59:36.436: INFO: Created: latency-svc-clv9z
Mar 11 16:59:36.458: INFO: Got endpoints: latency-svc-27mbl [747.935859ms]
Mar 11 16:59:36.481: INFO: Created: latency-svc-s5t4j
Mar 11 16:59:36.509: INFO: Got endpoints: latency-svc-bwl49 [752.758148ms]
Mar 11 16:59:36.536: INFO: Created: latency-svc-bh9pt
Mar 11 16:59:36.559: INFO: Got endpoints: latency-svc-g7lwl [745.083917ms]
Mar 11 16:59:36.610: INFO: Got endpoints: latency-svc-sw56f [751.514075ms]
Mar 11 16:59:36.628: INFO: Created: latency-svc-t8w8w
Mar 11 16:59:36.652: INFO: Created: latency-svc-jbqjp
Mar 11 16:59:36.659: INFO: Got endpoints: latency-svc-gwh2q [744.198746ms]
Mar 11 16:59:36.684: INFO: Created: latency-svc-5hbrc
Mar 11 16:59:36.710: INFO: Got endpoints: latency-svc-6m8kp [749.395492ms]
Mar 11 16:59:36.730: INFO: Created: latency-svc-zxnjq
Mar 11 16:59:36.765: INFO: Got endpoints: latency-svc-h5lqk [753.932949ms]
Mar 11 16:59:36.786: INFO: Created: latency-svc-68qd6
Mar 11 16:59:36.810: INFO: Got endpoints: latency-svc-wflxz [753.247007ms]
Mar 11 16:59:36.831: INFO: Created: latency-svc-mtzgm
Mar 11 16:59:36.861: INFO: Got endpoints: latency-svc-c98s7 [747.476062ms]
Mar 11 16:59:36.882: INFO: Created: latency-svc-5ltcs
Mar 11 16:59:36.910: INFO: Got endpoints: latency-svc-kfjvs [737.960142ms]
Mar 11 16:59:36.936: INFO: Created: latency-svc-4nx29
Mar 11 16:59:36.960: INFO: Got endpoints: latency-svc-7r94c [744.421881ms]
Mar 11 16:59:36.989: INFO: Created: latency-svc-5xn8n
Mar 11 16:59:37.015: INFO: Got endpoints: latency-svc-jmzk4 [747.180764ms]
Mar 11 16:59:37.037: INFO: Created: latency-svc-6v52q
Mar 11 16:59:37.057: INFO: Got endpoints: latency-svc-2m7r5 [725.390323ms]
Mar 11 16:59:37.080: INFO: Created: latency-svc-6x62q
Mar 11 16:59:37.120: INFO: Got endpoints: latency-svc-pmhbb [755.820306ms]
Mar 11 16:59:37.192: INFO: Created: latency-svc-qjrxg
Mar 11 16:59:37.192: INFO: Got endpoints: latency-svc-clv9z [781.002929ms]
Mar 11 16:59:37.225: INFO: Got endpoints: latency-svc-s5t4j [767.486029ms]
Mar 11 16:59:37.227: INFO: Created: latency-svc-9knw8
Mar 11 16:59:37.266: INFO: Got endpoints: latency-svc-bh9pt [756.679411ms]
Mar 11 16:59:37.270: INFO: Created: latency-svc-7gj6k
Mar 11 16:59:37.294: INFO: Created: latency-svc-8lwrx
Mar 11 16:59:37.313: INFO: Got endpoints: latency-svc-t8w8w [753.090642ms]
Mar 11 16:59:37.351: INFO: Created: latency-svc-jfxs4
Mar 11 16:59:37.361: INFO: Got endpoints: latency-svc-jbqjp [750.194652ms]
Mar 11 16:59:37.414: INFO: Created: latency-svc-8zbdp
Mar 11 16:59:37.414: INFO: Got endpoints: latency-svc-5hbrc [755.157932ms]
Mar 11 16:59:37.442: INFO: Created: latency-svc-zbjxz
Mar 11 16:59:37.454: INFO: Got endpoints: latency-svc-zxnjq [743.75106ms]
Mar 11 16:59:37.481: INFO: Created: latency-svc-slzgt
Mar 11 16:59:37.511: INFO: Got endpoints: latency-svc-68qd6 [745.617999ms]
Mar 11 16:59:37.535: INFO: Created: latency-svc-rgx99
Mar 11 16:59:37.555: INFO: Got endpoints: latency-svc-mtzgm [745.619581ms]
Mar 11 16:59:37.583: INFO: Created: latency-svc-wf24d
Mar 11 16:59:37.614: INFO: Got endpoints: latency-svc-5ltcs [752.6337ms]
Mar 11 16:59:37.635: INFO: Created: latency-svc-jqt8l
Mar 11 16:59:37.663: INFO: Got endpoints: latency-svc-4nx29 [752.940765ms]
Mar 11 16:59:37.689: INFO: Created: latency-svc-zfp72
Mar 11 16:59:37.719: INFO: Got endpoints: latency-svc-5xn8n [759.752567ms]
Mar 11 16:59:37.749: INFO: Created: latency-svc-llgqz
Mar 11 16:59:37.755: INFO: Got endpoints: latency-svc-6v52q [739.945173ms]
Mar 11 16:59:37.781: INFO: Created: latency-svc-h7hmf
Mar 11 16:59:37.811: INFO: Got endpoints: latency-svc-6x62q [753.362935ms]
Mar 11 16:59:37.835: INFO: Created: latency-svc-nvpts
Mar 11 16:59:37.869: INFO: Got endpoints: latency-svc-qjrxg [748.494035ms]
Mar 11 16:59:37.913: INFO: Created: latency-svc-ddtq9
Mar 11 16:59:37.914: INFO: Got endpoints: latency-svc-9knw8 [721.551021ms]
Mar 11 16:59:37.953: INFO: Created: latency-svc-zkwhj
Mar 11 16:59:37.962: INFO: Got endpoints: latency-svc-7gj6k [737.059075ms]
Mar 11 16:59:38.073: INFO: Got endpoints: latency-svc-8lwrx [806.678069ms]
Mar 11 16:59:38.076: INFO: Got endpoints: latency-svc-jfxs4 [762.937085ms]
Mar 11 16:59:38.112: INFO: Got endpoints: latency-svc-8zbdp [751.417069ms]
Mar 11 16:59:38.159: INFO: Got endpoints: latency-svc-zbjxz [744.180497ms]
Mar 11 16:59:38.215: INFO: Got endpoints: latency-svc-slzgt [760.724962ms]
Mar 11 16:59:38.265: INFO: Got endpoints: latency-svc-rgx99 [753.946567ms]
Mar 11 16:59:38.309: INFO: Got endpoints: latency-svc-wf24d [749.084366ms]
Mar 11 16:59:38.358: INFO: Got endpoints: latency-svc-jqt8l [744.742744ms]
Mar 11 16:59:38.412: INFO: Got endpoints: latency-svc-zfp72 [747.922519ms]
Mar 11 16:59:38.464: INFO: Got endpoints: latency-svc-llgqz [743.94397ms]
Mar 11 16:59:38.512: INFO: Got endpoints: latency-svc-h7hmf [754.192288ms]
Mar 11 16:59:38.564: INFO: Got endpoints: latency-svc-nvpts [752.562939ms]
Mar 11 16:59:38.614: INFO: Got endpoints: latency-svc-ddtq9 [744.453001ms]
Mar 11 16:59:38.662: INFO: Got endpoints: latency-svc-zkwhj [748.184722ms]
Mar 11 16:59:38.662: INFO: Latencies: [31.753513ms 46.282974ms 51.295417ms 72.604387ms 82.143334ms 83.40975ms 105.791334ms 108.044289ms 119.126159ms 134.890664ms 163.221913ms 165.443291ms 191.900036ms 195.173896ms 218.80259ms 229.895283ms 233.850996ms 246.246949ms 264.957287ms 284.308017ms 293.761491ms 294.737986ms 321.456875ms 333.153354ms 335.353244ms 336.217137ms 337.101556ms 340.750352ms 343.729849ms 347.350857ms 348.514469ms 352.294711ms 353.319748ms 370.889542ms 377.757024ms 385.454703ms 394.502599ms 395.131955ms 397.710709ms 403.615038ms 407.18301ms 410.472752ms 418.514044ms 420.999506ms 425.213913ms 436.846956ms 452.592486ms 458.185857ms 467.839088ms 468.10715ms 471.296256ms 474.16483ms 487.750707ms 507.906822ms 514.649192ms 534.027884ms 535.412783ms 552.660254ms 567.217448ms 579.315941ms 596.440774ms 597.434353ms 605.039894ms 607.070543ms 611.003325ms 620.543462ms 635.437876ms 656.713823ms 663.816612ms 666.843443ms 686.145447ms 690.621914ms 690.792247ms 691.163265ms 695.83203ms 697.548687ms 710.85748ms 721.551021ms 725.390323ms 729.490721ms 731.20414ms 735.30769ms 736.332841ms 736.555184ms 736.573439ms 737.059075ms 737.707559ms 737.960142ms 738.028964ms 738.523033ms 739.292346ms 739.945173ms 740.29978ms 741.410733ms 742.707438ms 742.895628ms 743.75106ms 743.94397ms 743.950124ms 744.180497ms 744.198746ms 744.421881ms 744.431816ms 744.435556ms 744.453001ms 744.557185ms 744.742744ms 745.083917ms 745.228884ms 745.492084ms 745.617999ms 745.619581ms 746.22004ms 746.711135ms 746.806891ms 746.892507ms 747.180764ms 747.476062ms 747.58857ms 747.613499ms 747.922519ms 747.930919ms 747.935859ms 748.039254ms 748.06645ms 748.128974ms 748.184722ms 748.199529ms 748.278835ms 748.494035ms 748.534042ms 748.771577ms 748.905701ms 749.062998ms 749.084366ms 749.168262ms 749.254902ms 749.276282ms 749.395492ms 749.823075ms 749.975657ms 750.127936ms 750.154233ms 750.194652ms 750.209528ms 750.362301ms 750.450568ms 750.794145ms 751.189437ms 751.227365ms 751.288425ms 751.417069ms 751.514075ms 751.697194ms 751.891087ms 751.985936ms 752.039423ms 752.171296ms 752.388313ms 752.562939ms 752.563416ms 752.6337ms 752.648973ms 752.758148ms 752.940765ms 752.998515ms 753.011267ms 753.090642ms 753.247007ms 753.281539ms 753.362935ms 753.429386ms 753.59193ms 753.80898ms 753.932949ms 753.946567ms 753.96023ms 754.192288ms 754.357147ms 754.359626ms 754.692627ms 755.157932ms 755.203263ms 755.820306ms 756.25377ms 756.592853ms 756.659325ms 756.679411ms 757.214381ms 757.627802ms 757.651724ms 758.895226ms 759.752567ms 760.724962ms 762.937085ms 764.162957ms 767.486029ms 770.51444ms 781.002929ms 806.678069ms]
Mar 11 16:59:38.663: INFO: 50 %ile: 744.198746ms
Mar 11 16:59:38.663: INFO: 90 %ile: 754.692627ms
Mar 11 16:59:38.663: INFO: 99 %ile: 781.002929ms
Mar 11 16:59:38.663: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:59:38.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7946" for this suite.

• [SLOW TEST:11.867 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":339,"completed":143,"skipped":2352,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:59:38.688: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-7444d148-66fa-4888-b55b-2fc08dd76316
Mar 11 16:59:38.766: INFO: Pod name my-hostname-basic-7444d148-66fa-4888-b55b-2fc08dd76316: Found 0 pods out of 1
Mar 11 16:59:43.778: INFO: Pod name my-hostname-basic-7444d148-66fa-4888-b55b-2fc08dd76316: Found 1 pods out of 1
Mar 11 16:59:43.778: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7444d148-66fa-4888-b55b-2fc08dd76316" are running
Mar 11 16:59:43.783: INFO: Pod "my-hostname-basic-7444d148-66fa-4888-b55b-2fc08dd76316-nq8v6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-11 16:59:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-11 16:59:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-11 16:59:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-03-11 16:59:38 +0000 UTC Reason: Message:}])
Mar 11 16:59:43.784: INFO: Trying to dial the pod
Mar 11 16:59:48.809: INFO: Controller my-hostname-basic-7444d148-66fa-4888-b55b-2fc08dd76316: Got expected result from replica 1 [my-hostname-basic-7444d148-66fa-4888-b55b-2fc08dd76316-nq8v6]: "my-hostname-basic-7444d148-66fa-4888-b55b-2fc08dd76316-nq8v6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:59:48.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7840" for this suite.

• [SLOW TEST:10.153 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":144,"skipped":2361,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:59:48.842: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 11 16:59:49.039: INFO: Waiting up to 5m0s for pod "pod-5803561e-e1f9-4026-b867-8367794654be" in namespace "emptydir-7472" to be "Succeeded or Failed"
Mar 11 16:59:49.056: INFO: Pod "pod-5803561e-e1f9-4026-b867-8367794654be": Phase="Pending", Reason="", readiness=false. Elapsed: 17.032425ms
Mar 11 16:59:51.065: INFO: Pod "pod-5803561e-e1f9-4026-b867-8367794654be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026473975s
STEP: Saw pod success
Mar 11 16:59:51.065: INFO: Pod "pod-5803561e-e1f9-4026-b867-8367794654be" satisfied condition "Succeeded or Failed"
Mar 11 16:59:51.075: INFO: Trying to get logs from node 198.18.167.130 pod pod-5803561e-e1f9-4026-b867-8367794654be container test-container: <nil>
STEP: delete the pod
Mar 11 16:59:51.125: INFO: Waiting for pod pod-5803561e-e1f9-4026-b867-8367794654be to disappear
Mar 11 16:59:51.133: INFO: Pod pod-5803561e-e1f9-4026-b867-8367794654be no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:59:51.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7472" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":145,"skipped":2458,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:59:51.153: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:59:53.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6113" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":339,"completed":146,"skipped":2480,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:59:53.287: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-19442d86-ee1a-4ff6-b42f-d5123653ef69
STEP: Creating a pod to test consume configMaps
Mar 11 16:59:53.434: INFO: Waiting up to 5m0s for pod "pod-configmaps-a638826c-0dac-4b82-9a32-343257d4c207" in namespace "configmap-9872" to be "Succeeded or Failed"
Mar 11 16:59:53.442: INFO: Pod "pod-configmaps-a638826c-0dac-4b82-9a32-343257d4c207": Phase="Pending", Reason="", readiness=false. Elapsed: 8.54354ms
Mar 11 16:59:55.453: INFO: Pod "pod-configmaps-a638826c-0dac-4b82-9a32-343257d4c207": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019428481s
STEP: Saw pod success
Mar 11 16:59:55.456: INFO: Pod "pod-configmaps-a638826c-0dac-4b82-9a32-343257d4c207" satisfied condition "Succeeded or Failed"
Mar 11 16:59:55.465: INFO: Trying to get logs from node 198.18.167.130 pod pod-configmaps-a638826c-0dac-4b82-9a32-343257d4c207 container agnhost-container: <nil>
STEP: delete the pod
Mar 11 16:59:55.503: INFO: Waiting for pod pod-configmaps-a638826c-0dac-4b82-9a32-343257d4c207 to disappear
Mar 11 16:59:55.507: INFO: Pod pod-configmaps-a638826c-0dac-4b82-9a32-343257d4c207 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:59:55.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9872" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":147,"skipped":2500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:59:55.526: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 16:59:55.614: INFO: The status of Pod busybox-readonly-fs0e44936f-17c5-4f11-a70e-606e55063eb7 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 16:59:57.619: INFO: The status of Pod busybox-readonly-fs0e44936f-17c5-4f11-a70e-606e55063eb7 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 16:59:57.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4035" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":148,"skipped":2537,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 16:59:57.650: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Mar 11 17:00:03.771: INFO: 8 pods remaining
Mar 11 17:00:03.771: INFO: 0 pods has nil DeletionTimestamp
Mar 11 17:00:03.771: INFO: 
Mar 11 17:00:04.773: INFO: 8 pods remaining
Mar 11 17:00:04.773: INFO: 0 pods has nil DeletionTimestamp
Mar 11 17:00:04.773: INFO: 
Mar 11 17:00:05.774: INFO: 4 pods remaining
Mar 11 17:00:05.774: INFO: 0 pods has nil DeletionTimestamp
Mar 11 17:00:05.774: INFO: 
Mar 11 17:00:06.815: INFO: 1 pods remaining
Mar 11 17:00:06.815: INFO: 0 pods has nil DeletionTimestamp
Mar 11 17:00:06.815: INFO: 
STEP: Gathering metrics
W0311 17:00:07.786591      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 11 17:01:09.813: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:01:09.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4208" for this suite.

• [SLOW TEST:72.182 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":339,"completed":149,"skipped":2542,"failed":0}
SSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:01:09.832: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:01:09.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5582" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":339,"completed":150,"skipped":2548,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:01:09.956: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-b290b582-b066-4acf-b73c-5832105284c1
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:01:12.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2074" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":151,"skipped":2555,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:01:12.100: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:01:12.156: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 11 17:01:21.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-5075 --namespace=crd-publish-openapi-5075 create -f -'
Mar 11 17:01:22.550: INFO: stderr: ""
Mar 11 17:01:22.550: INFO: stdout: "e2e-test-crd-publish-openapi-2257-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 11 17:01:22.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-5075 --namespace=crd-publish-openapi-5075 delete e2e-test-crd-publish-openapi-2257-crds test-cr'
Mar 11 17:01:22.650: INFO: stderr: ""
Mar 11 17:01:22.650: INFO: stdout: "e2e-test-crd-publish-openapi-2257-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 11 17:01:22.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-5075 --namespace=crd-publish-openapi-5075 apply -f -'
Mar 11 17:01:23.226: INFO: stderr: ""
Mar 11 17:01:23.226: INFO: stdout: "e2e-test-crd-publish-openapi-2257-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 11 17:01:23.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-5075 --namespace=crd-publish-openapi-5075 delete e2e-test-crd-publish-openapi-2257-crds test-cr'
Mar 11 17:01:23.335: INFO: stderr: ""
Mar 11 17:01:23.335: INFO: stdout: "e2e-test-crd-publish-openapi-2257-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 11 17:01:23.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-5075 explain e2e-test-crd-publish-openapi-2257-crds'
Mar 11 17:01:23.906: INFO: stderr: ""
Mar 11 17:01:23.906: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2257-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:01:32.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5075" for this suite.

• [SLOW TEST:20.511 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":339,"completed":152,"skipped":2603,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:01:32.612: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-50158149-0f8f-4a7b-b403-1cb74f525e4b
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:01:32.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7369" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":339,"completed":153,"skipped":2616,"failed":0}
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:01:32.692: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:01:32.771: INFO: The status of Pod busybox-scheduling-eed8e5bd-f448-4943-8b99-45ce8f2051fa is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:01:34.785: INFO: The status of Pod busybox-scheduling-eed8e5bd-f448-4943-8b99-45ce8f2051fa is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:01:34.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9965" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":339,"completed":154,"skipped":2620,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:01:34.827: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:01:34.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4737" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":155,"skipped":2628,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:01:34.901: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Mar 11 17:01:34.973: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 11 17:02:35.028: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Mar 11 17:02:35.069: INFO: Created pod: pod0-sched-preemption-low-priority
Mar 11 17:02:35.099: INFO: Created pod: pod1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:01.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2511" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:86.379 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":339,"completed":156,"skipped":2637,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:01.281: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:03:01.809: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:03:04.867: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 11 17:03:04.895: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:04.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7383" for this suite.
STEP: Destroying namespace "webhook-7383-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":339,"completed":157,"skipped":2652,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:05.009: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 11 17:03:05.073: INFO: Waiting up to 5m0s for pod "pod-25b2c11e-4991-487e-9a3d-13ed76247e78" in namespace "emptydir-2234" to be "Succeeded or Failed"
Mar 11 17:03:05.081: INFO: Pod "pod-25b2c11e-4991-487e-9a3d-13ed76247e78": Phase="Pending", Reason="", readiness=false. Elapsed: 8.102544ms
Mar 11 17:03:07.135: INFO: Pod "pod-25b2c11e-4991-487e-9a3d-13ed76247e78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.062530516s
STEP: Saw pod success
Mar 11 17:03:07.235: INFO: Pod "pod-25b2c11e-4991-487e-9a3d-13ed76247e78" satisfied condition "Succeeded or Failed"
Mar 11 17:03:07.249: INFO: Trying to get logs from node 198.18.167.130 pod pod-25b2c11e-4991-487e-9a3d-13ed76247e78 container test-container: <nil>
STEP: delete the pod
Mar 11 17:03:07.293: INFO: Waiting for pod pod-25b2c11e-4991-487e-9a3d-13ed76247e78 to disappear
Mar 11 17:03:07.297: INFO: Pod pod-25b2c11e-4991-487e-9a3d-13ed76247e78 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:07.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2234" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":158,"skipped":2655,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:07.321: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Mar 11 17:03:07.369: INFO: namespace kubectl-1751
Mar 11 17:03:07.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1751 create -f -'
Mar 11 17:03:08.003: INFO: stderr: ""
Mar 11 17:03:08.003: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 11 17:03:09.012: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 11 17:03:09.012: INFO: Found 0 / 1
Mar 11 17:03:10.014: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 11 17:03:10.014: INFO: Found 1 / 1
Mar 11 17:03:10.014: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 11 17:03:10.019: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 11 17:03:10.019: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 11 17:03:10.019: INFO: wait on agnhost-primary startup in kubectl-1751 
Mar 11 17:03:10.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1751 logs agnhost-primary-zt2p2 agnhost-primary'
Mar 11 17:03:10.188: INFO: stderr: ""
Mar 11 17:03:10.188: INFO: stdout: "Paused\n"
STEP: exposing RC
Mar 11 17:03:10.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1751 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 11 17:03:10.400: INFO: stderr: ""
Mar 11 17:03:10.407: INFO: stdout: "service/rm2 exposed\n"
Mar 11 17:03:10.414: INFO: Service rm2 in namespace kubectl-1751 found.
STEP: exposing service
Mar 11 17:03:12.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1751 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 11 17:03:12.637: INFO: stderr: ""
Mar 11 17:03:12.637: INFO: stdout: "service/rm3 exposed\n"
Mar 11 17:03:12.646: INFO: Service rm3 in namespace kubectl-1751 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:14.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1751" for this suite.

• [SLOW TEST:7.355 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1223
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":339,"completed":159,"skipped":2667,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:14.676: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-9754
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 11 17:03:14.732: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 11 17:03:14.782: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:03:16.792: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:03:18.788: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:03:20.789: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:03:22.788: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:03:24.789: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:03:26.789: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:03:28.788: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:03:30.788: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:03:32.790: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:03:34.799: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 11 17:03:34.808: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Mar 11 17:03:36.843: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar 11 17:03:36.843: INFO: Breadth first check of 192.168.1.191 on host 198.18.16.160...
Mar 11 17:03:36.848: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.190:9080/dial?request=hostname&protocol=http&host=192.168.1.191&port=8080&tries=1'] Namespace:pod-network-test-9754 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:03:36.848: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:03:36.944: INFO: Waiting for responses: map[]
Mar 11 17:03:36.944: INFO: reached 192.168.1.191 after 0/1 tries
Mar 11 17:03:36.944: INFO: Breadth first check of 192.168.2.12 on host 198.18.167.130...
Mar 11 17:03:36.951: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.190:9080/dial?request=hostname&protocol=http&host=192.168.2.12&port=8080&tries=1'] Namespace:pod-network-test-9754 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:03:36.951: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:03:37.031: INFO: Waiting for responses: map[]
Mar 11 17:03:37.031: INFO: reached 192.168.2.12 after 0/1 tries
Mar 11 17:03:37.031: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:37.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9754" for this suite.

• [SLOW TEST:22.370 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":339,"completed":160,"skipped":2670,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:37.049: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:03:37.103: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:38.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5738" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":339,"completed":161,"skipped":2682,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:38.134: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
W0311 17:03:38.189410      19 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 11 17:03:38.208: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar 11 17:03:38.221: INFO: starting watch
STEP: patching
STEP: updating
Mar 11 17:03:38.249: INFO: waiting for watch events with expected annotations
Mar 11 17:03:38.249: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:38.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6787" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":339,"completed":162,"skipped":2706,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:38.360: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Mar 11 17:03:38.421: INFO: Major version: 1
STEP: Confirm minor version
Mar 11 17:03:38.422: INFO: cleanMinorVersion: 21
Mar 11 17:03:38.422: INFO: Minor version: 21+
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:38.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2793" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":339,"completed":163,"skipped":2713,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:38.441: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 11 17:03:38.512: INFO: Waiting up to 5m0s for pod "pod-b0a75796-77aa-4d2c-a4b1-0415977d4964" in namespace "emptydir-6569" to be "Succeeded or Failed"
Mar 11 17:03:38.521: INFO: Pod "pod-b0a75796-77aa-4d2c-a4b1-0415977d4964": Phase="Pending", Reason="", readiness=false. Elapsed: 7.625325ms
Mar 11 17:03:40.530: INFO: Pod "pod-b0a75796-77aa-4d2c-a4b1-0415977d4964": Phase="Running", Reason="", readiness=true. Elapsed: 2.016980209s
Mar 11 17:03:42.629: INFO: Pod "pod-b0a75796-77aa-4d2c-a4b1-0415977d4964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.116296192s
STEP: Saw pod success
Mar 11 17:03:42.629: INFO: Pod "pod-b0a75796-77aa-4d2c-a4b1-0415977d4964" satisfied condition "Succeeded or Failed"
Mar 11 17:03:42.692: INFO: Trying to get logs from node 198.18.167.130 pod pod-b0a75796-77aa-4d2c-a4b1-0415977d4964 container test-container: <nil>
STEP: delete the pod
Mar 11 17:03:42.756: INFO: Waiting for pod pod-b0a75796-77aa-4d2c-a4b1-0415977d4964 to disappear
Mar 11 17:03:42.760: INFO: Pod pod-b0a75796-77aa-4d2c-a4b1-0415977d4964 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:42.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6569" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":164,"skipped":2717,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:42.788: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 11 17:03:42.886: INFO: Waiting up to 5m0s for pod "pod-14eb6624-8530-4ec7-aac1-52f798584d1e" in namespace "emptydir-8857" to be "Succeeded or Failed"
Mar 11 17:03:42.896: INFO: Pod "pod-14eb6624-8530-4ec7-aac1-52f798584d1e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.474827ms
Mar 11 17:03:44.918: INFO: Pod "pod-14eb6624-8530-4ec7-aac1-52f798584d1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032039823s
STEP: Saw pod success
Mar 11 17:03:44.918: INFO: Pod "pod-14eb6624-8530-4ec7-aac1-52f798584d1e" satisfied condition "Succeeded or Failed"
Mar 11 17:03:44.924: INFO: Trying to get logs from node 198.18.167.130 pod pod-14eb6624-8530-4ec7-aac1-52f798584d1e container test-container: <nil>
STEP: delete the pod
Mar 11 17:03:44.964: INFO: Waiting for pod pod-14eb6624-8530-4ec7-aac1-52f798584d1e to disappear
Mar 11 17:03:44.968: INFO: Pod pod-14eb6624-8530-4ec7-aac1-52f798584d1e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:44.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8857" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":165,"skipped":2727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:44.985: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 11 17:03:45.072: INFO: Waiting up to 5m0s for pod "pod-b400f8b4-373f-4228-ba30-150629ae750f" in namespace "emptydir-573" to be "Succeeded or Failed"
Mar 11 17:03:45.086: INFO: Pod "pod-b400f8b4-373f-4228-ba30-150629ae750f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.124475ms
Mar 11 17:03:47.107: INFO: Pod "pod-b400f8b4-373f-4228-ba30-150629ae750f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034125393s
STEP: Saw pod success
Mar 11 17:03:47.108: INFO: Pod "pod-b400f8b4-373f-4228-ba30-150629ae750f" satisfied condition "Succeeded or Failed"
Mar 11 17:03:47.113: INFO: Trying to get logs from node 198.18.167.130 pod pod-b400f8b4-373f-4228-ba30-150629ae750f container test-container: <nil>
STEP: delete the pod
Mar 11 17:03:47.145: INFO: Waiting for pod pod-b400f8b4-373f-4228-ba30-150629ae750f to disappear
Mar 11 17:03:47.150: INFO: Pod pod-b400f8b4-373f-4228-ba30-150629ae750f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:47.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-573" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":166,"skipped":2767,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:47.174: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 11 17:03:49.439: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:49.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5180" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":167,"skipped":2773,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:49.488: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3712
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3712
I0311 17:03:49.619540      19 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3712, replica count: 2
Mar 11 17:03:52.671: INFO: Creating new exec pod
I0311 17:03:52.671178      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 11 17:03:57.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-3712 exec execpodsl2n7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 11 17:03:57.889: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 11 17:03:57.889: INFO: stdout: "externalname-service-rtsgg"
Mar 11 17:03:57.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-3712 exec execpodsl2n7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.62.28 80'
Mar 11 17:03:58.071: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.62.28 80\nConnection to 10.108.62.28 80 port [tcp/http] succeeded!\n"
Mar 11 17:03:58.071: INFO: stdout: ""
Mar 11 17:03:59.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-3712 exec execpodsl2n7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.62.28 80'
Mar 11 17:03:59.259: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.62.28 80\nConnection to 10.108.62.28 80 port [tcp/http] succeeded!\n"
Mar 11 17:03:59.260: INFO: stdout: "externalname-service-rtsgg"
Mar 11 17:03:59.260: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:03:59.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3712" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:9.839 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":339,"completed":168,"skipped":2779,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:03:59.327: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Mar 11 17:03:59.389: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9189  6e71dfd0-8aac-4cd3-aff9-3b7e8b5ceb49 62000 0 2022-03-11 17:03:59 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-03-11 17:03:59 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6g942,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6g942,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:03:59.396: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:04:01.403: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Mar 11 17:04:01.403: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9189 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:04:01.403: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Verifying customized DNS server is configured on pod...
Mar 11 17:04:01.496: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9189 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:04:01.496: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:04:01.603: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:04:01.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9189" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":339,"completed":169,"skipped":2818,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:04:01.650: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:04:08.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5380" for this suite.

• [SLOW TEST:7.102 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":339,"completed":170,"skipped":2821,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:04:08.752: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Mar 11 17:04:08.796: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Mar 11 17:04:09.492: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 11 17:04:11.592: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 11 17:04:13.600: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 11 17:04:15.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 11 17:04:17.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 11 17:04:19.604: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615049, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 11 17:04:24.163: INFO: Waited 2.543499838s for the sample-apiserver to be ready to handle requests.
I0311 17:04:25.228891      19 request.go:668] Waited for 1.024504381s due to client-side throttling, not priority and fairness, request: GET:https://10.96.0.1:443/apis/cilium.io/v2?timeout=32s
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Mar 11 17:04:25.732: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:04:26.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9948" for this suite.

• [SLOW TEST:17.792 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":339,"completed":171,"skipped":2825,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:04:26.546: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 11 17:04:26.620: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 11 17:04:26.634: INFO: Waiting for terminating namespaces to be deleted...
Mar 11 17:04:26.642: INFO: 
Logging pods the apiserver thinks is on node 198.18.16.160 before test
Mar 11 17:04:26.659: INFO: capi-kubeadm-bootstrap-controller-manager-694cc79bb7-m8mq5 from capi-kubeadm-bootstrap-system started at 2022-03-11 15:43:57 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.659: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:04:26.659: INFO: capi-controller-manager-689cd9b4fd-pkbpt from capi-system started at 2022-03-11 15:43:50 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.659: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:04:26.659: INFO: capv-controller-manager-6b467446b9-4qlm4 from capv-system started at 2022-03-11 15:44:30 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.659: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:04:26.659: INFO: cert-manager-7988d4fb6c-knnwq from cert-manager started at 2022-03-11 15:43:27 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.659: INFO: 	Container cert-manager ready: true, restart count 0
Mar 11 17:04:26.659: INFO: cert-manager-cainjector-6bc8dcdb64-8vk7b from cert-manager started at 2022-03-11 15:43:26 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.660: INFO: 	Container cert-manager ready: true, restart count 0
Mar 11 17:04:26.660: INFO: cert-manager-webhook-68979bfb95-ksn6x from cert-manager started at 2022-03-11 15:43:27 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.660: INFO: 	Container cert-manager ready: true, restart count 0
Mar 11 17:04:26.660: INFO: etcdadm-bootstrap-provider-controller-manager-74c86ffb56-l6l9z from etcdadm-bootstrap-provider-system started at 2022-03-11 15:44:04 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.660: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:04:26.660: INFO: etcdadm-controller-controller-manager-7894945688-sxr4k from etcdadm-controller-system started at 2022-03-11 16:43:28 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.660: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:04:26.660: INFO: cilium-operator-86d59d5c88-fvvbn from kube-system started at 2022-03-11 15:42:57 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.660: INFO: 	Container cilium-operator ready: true, restart count 0
Mar 11 17:04:26.660: INFO: cilium-qm8kg from kube-system started at 2022-03-11 15:42:57 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.660: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 11 17:04:26.661: INFO: coredns-745c7986c7-5r5r8 from kube-system started at 2022-03-11 16:43:28 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.661: INFO: 	Container coredns ready: true, restart count 0
Mar 11 17:04:26.661: INFO: coredns-745c7986c7-mmcqj from kube-system started at 2022-03-11 16:43:28 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.661: INFO: 	Container coredns ready: true, restart count 0
Mar 11 17:04:26.661: INFO: kube-proxy-8jj6k from kube-system started at 2022-03-11 15:42:19 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.661: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 11 17:04:26.661: INFO: vsphere-cloud-controller-manager-l6kzs from kube-system started at 2022-03-11 15:42:19 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.661: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 1
Mar 11 17:04:26.661: INFO: vsphere-csi-controller-576c9c8dc8-8vqvm from kube-system started at 2022-03-11 16:43:28 +0000 UTC (5 container statuses recorded)
Mar 11 17:04:26.661: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 11 17:04:26.662: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 11 17:04:26.662: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 11 17:04:26.662: INFO: 	Container vsphere-csi-controller ready: true, restart count 0
Mar 11 17:04:26.662: INFO: 	Container vsphere-syncer ready: true, restart count 0
Mar 11 17:04:26.662: INFO: vsphere-csi-node-zmmbm from kube-system started at 2022-03-11 15:42:19 +0000 UTC (3 container statuses recorded)
Mar 11 17:04:26.662: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 11 17:04:26.662: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 11 17:04:26.662: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Mar 11 17:04:26.662: INFO: sonobuoy from sonobuoy started at 2022-03-11 15:55:43 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.662: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 11 17:04:26.662: INFO: sonobuoy-systemd-logs-daemon-set-5218357723e74163-g6x4l from sonobuoy started at 2022-03-11 15:55:47 +0000 UTC (2 container statuses recorded)
Mar 11 17:04:26.662: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Mar 11 17:04:26.663: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 11 17:04:26.663: INFO: 
Logging pods the apiserver thinks is on node 198.18.167.130 before test
Mar 11 17:04:26.674: INFO: cilium-69rlw from kube-system started at 2022-03-11 15:42:57 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.674: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 11 17:04:26.674: INFO: kube-proxy-6trtj from kube-system started at 2022-03-11 15:42:21 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.674: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 11 17:04:26.674: INFO: vsphere-cloud-controller-manager-g9wgq from kube-system started at 2022-03-11 16:43:41 +0000 UTC (1 container statuses recorded)
Mar 11 17:04:26.674: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Mar 11 17:04:26.674: INFO: vsphere-csi-node-xgb98 from kube-system started at 2022-03-11 15:42:21 +0000 UTC (3 container statuses recorded)
Mar 11 17:04:26.674: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 11 17:04:26.674: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 11 17:04:26.674: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Mar 11 17:04:26.674: INFO: sonobuoy-systemd-logs-daemon-set-5218357723e74163-tgcrn from sonobuoy started at 2022-03-11 15:55:47 +0000 UTC (2 container statuses recorded)
Mar 11 17:04:26.674: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Mar 11 17:04:26.674: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node 198.18.16.160
STEP: verifying the node has the label node 198.18.167.130
Mar 11 17:04:26.809: INFO: Pod capi-kubeadm-bootstrap-controller-manager-694cc79bb7-m8mq5 requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.809: INFO: Pod capi-controller-manager-689cd9b4fd-pkbpt requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.809: INFO: Pod capv-controller-manager-6b467446b9-4qlm4 requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.809: INFO: Pod cert-manager-7988d4fb6c-knnwq requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.809: INFO: Pod cert-manager-cainjector-6bc8dcdb64-8vk7b requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.809: INFO: Pod cert-manager-webhook-68979bfb95-ksn6x requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.809: INFO: Pod etcdadm-bootstrap-provider-controller-manager-74c86ffb56-l6l9z requesting resource cpu=100m on Node 198.18.16.160
Mar 11 17:04:26.809: INFO: Pod etcdadm-controller-controller-manager-7894945688-sxr4k requesting resource cpu=100m on Node 198.18.16.160
Mar 11 17:04:26.810: INFO: Pod cilium-69rlw requesting resource cpu=0m on Node 198.18.167.130
Mar 11 17:04:26.810: INFO: Pod cilium-operator-86d59d5c88-fvvbn requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.810: INFO: Pod cilium-qm8kg requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.810: INFO: Pod coredns-745c7986c7-5r5r8 requesting resource cpu=100m on Node 198.18.16.160
Mar 11 17:04:26.810: INFO: Pod coredns-745c7986c7-mmcqj requesting resource cpu=100m on Node 198.18.16.160
Mar 11 17:04:26.810: INFO: Pod kube-proxy-6trtj requesting resource cpu=0m on Node 198.18.167.130
Mar 11 17:04:26.810: INFO: Pod kube-proxy-8jj6k requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.810: INFO: Pod vsphere-cloud-controller-manager-g9wgq requesting resource cpu=200m on Node 198.18.167.130
Mar 11 17:04:26.810: INFO: Pod vsphere-cloud-controller-manager-l6kzs requesting resource cpu=200m on Node 198.18.16.160
Mar 11 17:04:26.810: INFO: Pod vsphere-csi-controller-576c9c8dc8-8vqvm requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.810: INFO: Pod vsphere-csi-node-xgb98 requesting resource cpu=0m on Node 198.18.167.130
Mar 11 17:04:26.810: INFO: Pod vsphere-csi-node-zmmbm requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.810: INFO: Pod sonobuoy requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.811: INFO: Pod sonobuoy-systemd-logs-daemon-set-5218357723e74163-g6x4l requesting resource cpu=0m on Node 198.18.16.160
Mar 11 17:04:26.811: INFO: Pod sonobuoy-systemd-logs-daemon-set-5218357723e74163-tgcrn requesting resource cpu=0m on Node 198.18.167.130
STEP: Starting Pods to consume most of the cluster CPU.
Mar 11 17:04:26.811: INFO: Creating a pod which consumes cpu=1211m on Node 198.18.167.130
Mar 11 17:04:26.827: INFO: Creating a pod which consumes cpu=931m on Node 198.18.16.160
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0bca6c5a-b48f-4b0f-bb51-e36e2dc9465e.16db62632ef29615], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9795/filler-pod-0bca6c5a-b48f-4b0f-bb51-e36e2dc9465e to 198.18.16.160]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0bca6c5a-b48f-4b0f-bb51-e36e2dc9465e.16db626369b89cdf], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0bca6c5a-b48f-4b0f-bb51-e36e2dc9465e.16db62636d926456], Reason = [Created], Message = [Created container filler-pod-0bca6c5a-b48f-4b0f-bb51-e36e2dc9465e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0bca6c5a-b48f-4b0f-bb51-e36e2dc9465e.16db626374239391], Reason = [Started], Message = [Started container filler-pod-0bca6c5a-b48f-4b0f-bb51-e36e2dc9465e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e9b8ac6a-0c57-4364-847d-cb0caefd9fb0.16db62632d4286f8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9795/filler-pod-e9b8ac6a-0c57-4364-847d-cb0caefd9fb0 to 198.18.167.130]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e9b8ac6a-0c57-4364-847d-cb0caefd9fb0.16db626367b62467], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e9b8ac6a-0c57-4364-847d-cb0caefd9fb0.16db62636a8a339a], Reason = [Created], Message = [Created container filler-pod-e9b8ac6a-0c57-4364-847d-cb0caefd9fb0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e9b8ac6a-0c57-4364-847d-cb0caefd9fb0.16db62636fddaf15], Reason = [Started], Message = [Started container filler-pod-e9b8ac6a-0c57-4364-847d-cb0caefd9fb0]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16db6263a87afc15], Reason = [FailedScheduling], Message = [0/4 nodes are available: 2 Insufficient cpu, 2 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: removing the label node off the node 198.18.16.160
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 198.18.167.130
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:04:29.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9795" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":339,"completed":172,"skipped":2867,"failed":0}
SS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:04:30.004: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Mar 11 17:04:30.083: INFO: Waiting up to 5m0s for pod "client-containers-8ef6b515-b771-41b3-b4f3-31f518e1def0" in namespace "containers-6817" to be "Succeeded or Failed"
Mar 11 17:04:30.090: INFO: Pod "client-containers-8ef6b515-b771-41b3-b4f3-31f518e1def0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.652055ms
Mar 11 17:04:32.097: INFO: Pod "client-containers-8ef6b515-b771-41b3-b4f3-31f518e1def0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013076017s
Mar 11 17:04:34.105: INFO: Pod "client-containers-8ef6b515-b771-41b3-b4f3-31f518e1def0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021082068s
STEP: Saw pod success
Mar 11 17:04:34.106: INFO: Pod "client-containers-8ef6b515-b771-41b3-b4f3-31f518e1def0" satisfied condition "Succeeded or Failed"
Mar 11 17:04:34.111: INFO: Trying to get logs from node 198.18.16.160 pod client-containers-8ef6b515-b771-41b3-b4f3-31f518e1def0 container agnhost-container: <nil>
STEP: delete the pod
Mar 11 17:04:34.163: INFO: Waiting for pod client-containers-8ef6b515-b771-41b3-b4f3-31f518e1def0 to disappear
Mar 11 17:04:34.168: INFO: Pod client-containers-8ef6b515-b771-41b3-b4f3-31f518e1def0 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:04:34.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6817" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":339,"completed":173,"skipped":2869,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:04:34.185: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-9107
STEP: creating replication controller nodeport-test in namespace services-9107
I0311 17:04:34.286467      19 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-9107, replica count: 2
Mar 11 17:04:37.337: INFO: Creating new exec pod
I0311 17:04:37.337565      19 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 11 17:04:40.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-9107 exec execpodxlb8l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar 11 17:04:40.565: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 11 17:04:40.565: INFO: stdout: "nodeport-test-sxv8s"
Mar 11 17:04:40.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-9107 exec execpodxlb8l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.65.139 80'
Mar 11 17:04:40.762: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.65.139 80\nConnection to 10.103.65.139 80 port [tcp/http] succeeded!\n"
Mar 11 17:04:40.762: INFO: stdout: "nodeport-test-sxv8s"
Mar 11 17:04:40.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-9107 exec execpodxlb8l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 198.18.16.160 31146'
Mar 11 17:04:40.945: INFO: stderr: "+ nc -v -t -w 2 198.18.16.160 31146\n+ echo hostName\nConnection to 198.18.16.160 31146 port [tcp/*] succeeded!\n"
Mar 11 17:04:40.945: INFO: stdout: "nodeport-test-lxckr"
Mar 11 17:04:40.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-9107 exec execpodxlb8l -- /bin/sh -x -c echo hostName | nc -v -t -w 2 198.18.167.130 31146'
Mar 11 17:04:41.138: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 198.18.167.130 31146\nConnection to 198.18.167.130 31146 port [tcp/*] succeeded!\n"
Mar 11 17:04:41.138: INFO: stdout: "nodeport-test-lxckr"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:04:41.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9107" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:6.975 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":339,"completed":174,"skipped":2904,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:04:41.161: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-b2msv in namespace proxy-9325
I0311 17:04:41.248733      19 runners.go:190] Created replication controller with name: proxy-service-b2msv, namespace: proxy-9325, replica count: 1
I0311 17:04:42.300342      19 runners.go:190] proxy-service-b2msv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0311 17:04:43.300614      19 runners.go:190] proxy-service-b2msv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0311 17:04:44.301494      19 runners.go:190] proxy-service-b2msv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0311 17:04:45.302312      19 runners.go:190] proxy-service-b2msv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0311 17:04:46.303095      19 runners.go:190] proxy-service-b2msv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0311 17:04:47.303901      19 runners.go:190] proxy-service-b2msv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0311 17:04:48.303995      19 runners.go:190] proxy-service-b2msv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0311 17:04:49.304993      19 runners.go:190] proxy-service-b2msv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0311 17:04:50.305144      19 runners.go:190] proxy-service-b2msv Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0311 17:04:51.306110      19 runners.go:190] proxy-service-b2msv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 11 17:04:51.313: INFO: setup took 10.102011234s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 11 17:04:51.328: INFO: (0) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 13.294515ms)
Mar 11 17:04:51.328: INFO: (0) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 13.840842ms)
Mar 11 17:04:51.332: INFO: (0) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 15.400235ms)
Mar 11 17:04:51.333: INFO: (0) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 18.154874ms)
Mar 11 17:04:51.333: INFO: (0) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 16.861897ms)
Mar 11 17:04:51.335: INFO: (0) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 18.344591ms)
Mar 11 17:04:51.335: INFO: (0) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 18.72197ms)
Mar 11 17:04:51.335: INFO: (0) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 18.705929ms)
Mar 11 17:04:51.335: INFO: (0) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 18.944416ms)
Mar 11 17:04:51.336: INFO: (0) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 19.460628ms)
Mar 11 17:04:51.336: INFO: (0) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 19.739592ms)
Mar 11 17:04:51.336: INFO: (0) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 21.570952ms)
Mar 11 17:04:51.336: INFO: (0) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 20.010788ms)
Mar 11 17:04:51.337: INFO: (0) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 20.126463ms)
Mar 11 17:04:51.337: INFO: (0) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 20.756081ms)
Mar 11 17:04:51.337: INFO: (0) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 22.526157ms)
Mar 11 17:04:51.344: INFO: (1) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 6.42616ms)
Mar 11 17:04:51.344: INFO: (1) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 6.817322ms)
Mar 11 17:04:51.347: INFO: (1) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 9.897742ms)
Mar 11 17:04:51.347: INFO: (1) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 10.016932ms)
Mar 11 17:04:51.348: INFO: (1) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 10.301391ms)
Mar 11 17:04:51.348: INFO: (1) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 10.63764ms)
Mar 11 17:04:51.349: INFO: (1) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 11.363417ms)
Mar 11 17:04:51.349: INFO: (1) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 11.839721ms)
Mar 11 17:04:51.350: INFO: (1) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 12.022984ms)
Mar 11 17:04:51.350: INFO: (1) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 12.412493ms)
Mar 11 17:04:51.352: INFO: (1) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 14.597406ms)
Mar 11 17:04:51.352: INFO: (1) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 14.995104ms)
Mar 11 17:04:51.353: INFO: (1) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 15.864705ms)
Mar 11 17:04:51.354: INFO: (1) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 16.302793ms)
Mar 11 17:04:51.354: INFO: (1) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 16.550867ms)
Mar 11 17:04:51.355: INFO: (1) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 17.531739ms)
Mar 11 17:04:51.366: INFO: (2) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 10.931319ms)
Mar 11 17:04:51.367: INFO: (2) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 12.263614ms)
Mar 11 17:04:51.368: INFO: (2) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 12.124763ms)
Mar 11 17:04:51.368: INFO: (2) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 12.373428ms)
Mar 11 17:04:51.368: INFO: (2) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 12.596286ms)
Mar 11 17:04:51.368: INFO: (2) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 13.037655ms)
Mar 11 17:04:51.368: INFO: (2) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 12.800888ms)
Mar 11 17:04:51.368: INFO: (2) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 13.081542ms)
Mar 11 17:04:51.368: INFO: (2) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 12.930553ms)
Mar 11 17:04:51.369: INFO: (2) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 13.27908ms)
Mar 11 17:04:51.369: INFO: (2) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 13.992254ms)
Mar 11 17:04:51.370: INFO: (2) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 14.351685ms)
Mar 11 17:04:51.370: INFO: (2) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 14.233866ms)
Mar 11 17:04:51.370: INFO: (2) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 14.693126ms)
Mar 11 17:04:51.370: INFO: (2) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 14.754211ms)
Mar 11 17:04:51.370: INFO: (2) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 14.826303ms)
Mar 11 17:04:51.378: INFO: (3) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 7.413661ms)
Mar 11 17:04:51.379: INFO: (3) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 8.359597ms)
Mar 11 17:04:51.379: INFO: (3) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 8.525085ms)
Mar 11 17:04:51.380: INFO: (3) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 9.640135ms)
Mar 11 17:04:51.382: INFO: (3) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 11.026917ms)
Mar 11 17:04:51.382: INFO: (3) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 11.066751ms)
Mar 11 17:04:51.382: INFO: (3) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 11.218546ms)
Mar 11 17:04:51.382: INFO: (3) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 11.63949ms)
Mar 11 17:04:51.383: INFO: (3) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 11.914635ms)
Mar 11 17:04:51.384: INFO: (3) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 12.718906ms)
Mar 11 17:04:51.384: INFO: (3) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 12.752367ms)
Mar 11 17:04:51.384: INFO: (3) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 12.787184ms)
Mar 11 17:04:51.384: INFO: (3) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 13.140148ms)
Mar 11 17:04:51.384: INFO: (3) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 12.906963ms)
Mar 11 17:04:51.384: INFO: (3) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 13.203974ms)
Mar 11 17:04:51.384: INFO: (3) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 13.935556ms)
Mar 11 17:04:51.391: INFO: (4) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 6.3354ms)
Mar 11 17:04:51.392: INFO: (4) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 6.801876ms)
Mar 11 17:04:51.401: INFO: (4) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 15.645452ms)
Mar 11 17:04:51.401: INFO: (4) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 15.743683ms)
Mar 11 17:04:51.401: INFO: (4) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 16.652575ms)
Mar 11 17:04:51.402: INFO: (4) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 17.054282ms)
Mar 11 17:04:51.402: INFO: (4) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 16.846993ms)
Mar 11 17:04:51.402: INFO: (4) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 17.845298ms)
Mar 11 17:04:51.402: INFO: (4) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 17.772859ms)
Mar 11 17:04:51.403: INFO: (4) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 17.774621ms)
Mar 11 17:04:51.403: INFO: (4) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 18.046914ms)
Mar 11 17:04:51.403: INFO: (4) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 17.974491ms)
Mar 11 17:04:51.403: INFO: (4) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 18.259598ms)
Mar 11 17:04:51.404: INFO: (4) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 18.895383ms)
Mar 11 17:04:51.404: INFO: (4) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 19.28939ms)
Mar 11 17:04:51.405: INFO: (4) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 20.313774ms)
Mar 11 17:04:51.411: INFO: (5) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 5.705458ms)
Mar 11 17:04:51.411: INFO: (5) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 5.407443ms)
Mar 11 17:04:51.413: INFO: (5) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 6.990844ms)
Mar 11 17:04:51.413: INFO: (5) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 7.267115ms)
Mar 11 17:04:51.414: INFO: (5) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 7.419735ms)
Mar 11 17:04:51.414: INFO: (5) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 7.595926ms)
Mar 11 17:04:51.414: INFO: (5) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 8.15216ms)
Mar 11 17:04:51.414: INFO: (5) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 8.26674ms)
Mar 11 17:04:51.414: INFO: (5) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 9.225517ms)
Mar 11 17:04:51.414: INFO: (5) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 8.78486ms)
Mar 11 17:04:51.415: INFO: (5) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 8.717034ms)
Mar 11 17:04:51.415: INFO: (5) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 9.314879ms)
Mar 11 17:04:51.415: INFO: (5) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 9.47831ms)
Mar 11 17:04:51.416: INFO: (5) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 9.434463ms)
Mar 11 17:04:51.416: INFO: (5) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 9.33496ms)
Mar 11 17:04:51.416: INFO: (5) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 9.484475ms)
Mar 11 17:04:51.420: INFO: (6) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 3.524725ms)
Mar 11 17:04:51.420: INFO: (6) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 4.008956ms)
Mar 11 17:04:51.421: INFO: (6) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 4.30733ms)
Mar 11 17:04:51.422: INFO: (6) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 5.06939ms)
Mar 11 17:04:51.422: INFO: (6) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 5.882897ms)
Mar 11 17:04:51.423: INFO: (6) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 6.269583ms)
Mar 11 17:04:51.423: INFO: (6) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 6.869174ms)
Mar 11 17:04:51.424: INFO: (6) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 7.5353ms)
Mar 11 17:04:51.424: INFO: (6) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 7.936647ms)
Mar 11 17:04:51.424: INFO: (6) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 7.902407ms)
Mar 11 17:04:51.425: INFO: (6) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 8.592363ms)
Mar 11 17:04:51.425: INFO: (6) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 8.970125ms)
Mar 11 17:04:51.425: INFO: (6) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 8.568193ms)
Mar 11 17:04:51.425: INFO: (6) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 8.77745ms)
Mar 11 17:04:51.425: INFO: (6) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 8.389828ms)
Mar 11 17:04:51.425: INFO: (6) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 8.572983ms)
Mar 11 17:04:51.430: INFO: (7) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 3.868568ms)
Mar 11 17:04:51.435: INFO: (7) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 8.426671ms)
Mar 11 17:04:51.435: INFO: (7) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 8.820901ms)
Mar 11 17:04:51.435: INFO: (7) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 9.154295ms)
Mar 11 17:04:51.436: INFO: (7) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 9.836364ms)
Mar 11 17:04:51.436: INFO: (7) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 9.976111ms)
Mar 11 17:04:51.436: INFO: (7) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 9.794585ms)
Mar 11 17:04:51.437: INFO: (7) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 10.320755ms)
Mar 11 17:04:51.437: INFO: (7) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 9.969843ms)
Mar 11 17:04:51.437: INFO: (7) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 10.721773ms)
Mar 11 17:04:51.437: INFO: (7) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 11.344075ms)
Mar 11 17:04:51.437: INFO: (7) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 10.243663ms)
Mar 11 17:04:51.437: INFO: (7) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 11.206174ms)
Mar 11 17:04:51.437: INFO: (7) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 10.388567ms)
Mar 11 17:04:51.438: INFO: (7) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 11.298482ms)
Mar 11 17:04:51.438: INFO: (7) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 11.454353ms)
Mar 11 17:04:51.443: INFO: (8) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 4.610477ms)
Mar 11 17:04:51.445: INFO: (8) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 5.835583ms)
Mar 11 17:04:51.445: INFO: (8) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 6.45953ms)
Mar 11 17:04:51.445: INFO: (8) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 6.491113ms)
Mar 11 17:04:51.445: INFO: (8) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 6.958551ms)
Mar 11 17:04:51.446: INFO: (8) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 7.946005ms)
Mar 11 17:04:51.446: INFO: (8) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 7.86947ms)
Mar 11 17:04:51.447: INFO: (8) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 8.628686ms)
Mar 11 17:04:51.447: INFO: (8) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 8.418832ms)
Mar 11 17:04:51.447: INFO: (8) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 8.880759ms)
Mar 11 17:04:51.448: INFO: (8) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 9.662582ms)
Mar 11 17:04:51.449: INFO: (8) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 10.413352ms)
Mar 11 17:04:51.449: INFO: (8) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 10.447861ms)
Mar 11 17:04:51.449: INFO: (8) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 10.92322ms)
Mar 11 17:04:51.449: INFO: (8) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 10.469083ms)
Mar 11 17:04:51.449: INFO: (8) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 10.532456ms)
Mar 11 17:04:51.453: INFO: (9) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 3.255744ms)
Mar 11 17:04:51.454: INFO: (9) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 4.256022ms)
Mar 11 17:04:51.454: INFO: (9) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 4.137886ms)
Mar 11 17:04:51.455: INFO: (9) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 5.080092ms)
Mar 11 17:04:51.455: INFO: (9) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 5.786937ms)
Mar 11 17:04:51.455: INFO: (9) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 5.554164ms)
Mar 11 17:04:51.456: INFO: (9) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 6.673639ms)
Mar 11 17:04:51.456: INFO: (9) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 5.983325ms)
Mar 11 17:04:51.457: INFO: (9) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 7.186466ms)
Mar 11 17:04:51.457: INFO: (9) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 7.06896ms)
Mar 11 17:04:51.457: INFO: (9) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 7.311295ms)
Mar 11 17:04:51.458: INFO: (9) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 8.193423ms)
Mar 11 17:04:51.458: INFO: (9) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 7.56926ms)
Mar 11 17:04:51.458: INFO: (9) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 8.277021ms)
Mar 11 17:04:51.458: INFO: (9) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 8.603321ms)
Mar 11 17:04:51.458: INFO: (9) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 8.35752ms)
Mar 11 17:04:51.467: INFO: (10) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 8.740107ms)
Mar 11 17:04:51.468: INFO: (10) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 9.246286ms)
Mar 11 17:04:51.468: INFO: (10) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 9.141593ms)
Mar 11 17:04:51.468: INFO: (10) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 9.760437ms)
Mar 11 17:04:51.468: INFO: (10) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 9.505702ms)
Mar 11 17:04:51.468: INFO: (10) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 9.698025ms)
Mar 11 17:04:51.469: INFO: (10) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 10.398524ms)
Mar 11 17:04:51.469: INFO: (10) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 10.627813ms)
Mar 11 17:04:51.470: INFO: (10) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 10.882869ms)
Mar 11 17:04:51.470: INFO: (10) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 11.410838ms)
Mar 11 17:04:51.471: INFO: (10) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 11.890891ms)
Mar 11 17:04:51.471: INFO: (10) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 12.288662ms)
Mar 11 17:04:51.471: INFO: (10) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 12.519128ms)
Mar 11 17:04:51.471: INFO: (10) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 12.430583ms)
Mar 11 17:04:51.471: INFO: (10) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 12.316816ms)
Mar 11 17:04:51.472: INFO: (10) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 13.192748ms)
Mar 11 17:04:51.477: INFO: (11) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 4.625781ms)
Mar 11 17:04:51.477: INFO: (11) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 4.786589ms)
Mar 11 17:04:51.478: INFO: (11) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 5.354687ms)
Mar 11 17:04:51.478: INFO: (11) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 5.50224ms)
Mar 11 17:04:51.478: INFO: (11) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 6.246191ms)
Mar 11 17:04:51.481: INFO: (11) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 8.785031ms)
Mar 11 17:04:51.482: INFO: (11) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 9.172946ms)
Mar 11 17:04:51.482: INFO: (11) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 9.741782ms)
Mar 11 17:04:51.483: INFO: (11) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 10.689795ms)
Mar 11 17:04:51.484: INFO: (11) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 11.1144ms)
Mar 11 17:04:51.484: INFO: (11) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 11.612064ms)
Mar 11 17:04:51.484: INFO: (11) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 11.725308ms)
Mar 11 17:04:51.485: INFO: (11) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 12.43741ms)
Mar 11 17:04:51.485: INFO: (11) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 12.39697ms)
Mar 11 17:04:51.488: INFO: (11) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 15.365797ms)
Mar 11 17:04:51.492: INFO: (11) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 18.94371ms)
Mar 11 17:04:51.498: INFO: (12) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 5.936683ms)
Mar 11 17:04:51.501: INFO: (12) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 8.060318ms)
Mar 11 17:04:51.501: INFO: (12) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 8.911482ms)
Mar 11 17:04:51.501: INFO: (12) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 8.506414ms)
Mar 11 17:04:51.501: INFO: (12) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 8.449101ms)
Mar 11 17:04:51.502: INFO: (12) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 9.637954ms)
Mar 11 17:04:51.503: INFO: (12) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 10.783138ms)
Mar 11 17:04:51.504: INFO: (12) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 11.284214ms)
Mar 11 17:04:51.504: INFO: (12) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 11.877261ms)
Mar 11 17:04:51.504: INFO: (12) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 11.45709ms)
Mar 11 17:04:51.505: INFO: (12) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 12.887244ms)
Mar 11 17:04:51.505: INFO: (12) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 12.47852ms)
Mar 11 17:04:51.505: INFO: (12) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 12.700116ms)
Mar 11 17:04:51.505: INFO: (12) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 12.4944ms)
Mar 11 17:04:51.505: INFO: (12) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 13.029966ms)
Mar 11 17:04:51.507: INFO: (12) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 14.594226ms)
Mar 11 17:04:51.512: INFO: (13) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 4.76536ms)
Mar 11 17:04:51.514: INFO: (13) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 5.910535ms)
Mar 11 17:04:51.514: INFO: (13) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 6.134149ms)
Mar 11 17:04:51.514: INFO: (13) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 6.575506ms)
Mar 11 17:04:51.517: INFO: (13) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 8.592993ms)
Mar 11 17:04:51.517: INFO: (13) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 8.285639ms)
Mar 11 17:04:51.517: INFO: (13) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 8.863755ms)
Mar 11 17:04:51.517: INFO: (13) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 9.009677ms)
Mar 11 17:04:51.517: INFO: (13) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 9.374211ms)
Mar 11 17:04:51.521: INFO: (13) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 13.456475ms)
Mar 11 17:04:51.522: INFO: (13) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 13.474837ms)
Mar 11 17:04:51.522: INFO: (13) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 13.618615ms)
Mar 11 17:04:51.522: INFO: (13) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 13.902143ms)
Mar 11 17:04:51.522: INFO: (13) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 14.185274ms)
Mar 11 17:04:51.522: INFO: (13) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 14.666871ms)
Mar 11 17:04:51.527: INFO: (13) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 18.976305ms)
Mar 11 17:04:51.533: INFO: (14) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 5.459441ms)
Mar 11 17:04:51.534: INFO: (14) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 6.545203ms)
Mar 11 17:04:51.535: INFO: (14) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 7.285408ms)
Mar 11 17:04:51.535: INFO: (14) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 6.949326ms)
Mar 11 17:04:51.536: INFO: (14) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 7.346661ms)
Mar 11 17:04:51.536: INFO: (14) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 7.3234ms)
Mar 11 17:04:51.537: INFO: (14) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 9.563487ms)
Mar 11 17:04:51.537: INFO: (14) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 9.913675ms)
Mar 11 17:04:51.538: INFO: (14) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 9.674641ms)
Mar 11 17:04:51.538: INFO: (14) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 10.241746ms)
Mar 11 17:04:51.538: INFO: (14) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 10.232874ms)
Mar 11 17:04:51.538: INFO: (14) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 9.526658ms)
Mar 11 17:04:51.538: INFO: (14) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 9.910943ms)
Mar 11 17:04:51.538: INFO: (14) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 10.292324ms)
Mar 11 17:04:51.538: INFO: (14) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 10.346843ms)
Mar 11 17:04:51.539: INFO: (14) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 10.966981ms)
Mar 11 17:04:51.544: INFO: (15) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 5.312547ms)
Mar 11 17:04:51.544: INFO: (15) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 5.532673ms)
Mar 11 17:04:51.545: INFO: (15) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 6.095129ms)
Mar 11 17:04:51.545: INFO: (15) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 6.288561ms)
Mar 11 17:04:51.545: INFO: (15) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 6.359855ms)
Mar 11 17:04:51.547: INFO: (15) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 7.497827ms)
Mar 11 17:04:51.547: INFO: (15) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 7.509198ms)
Mar 11 17:04:51.547: INFO: (15) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 7.569459ms)
Mar 11 17:04:51.547: INFO: (15) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 8.125405ms)
Mar 11 17:04:51.547: INFO: (15) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 8.076096ms)
Mar 11 17:04:51.548: INFO: (15) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 9.081718ms)
Mar 11 17:04:51.548: INFO: (15) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 9.399686ms)
Mar 11 17:04:51.549: INFO: (15) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 9.494345ms)
Mar 11 17:04:51.550: INFO: (15) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 11.314358ms)
Mar 11 17:04:51.551: INFO: (15) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 11.538271ms)
Mar 11 17:04:51.551: INFO: (15) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 11.588026ms)
Mar 11 17:04:51.558: INFO: (16) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 7.015372ms)
Mar 11 17:04:51.563: INFO: (16) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 12.171826ms)
Mar 11 17:04:51.563: INFO: (16) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 12.422415ms)
Mar 11 17:04:51.563: INFO: (16) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 12.33535ms)
Mar 11 17:04:51.563: INFO: (16) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 11.705952ms)
Mar 11 17:04:51.563: INFO: (16) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 11.68499ms)
Mar 11 17:04:51.563: INFO: (16) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 11.379535ms)
Mar 11 17:04:51.563: INFO: (16) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 11.847458ms)
Mar 11 17:04:51.563: INFO: (16) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 11.294915ms)
Mar 11 17:04:51.563: INFO: (16) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 11.644973ms)
Mar 11 17:04:51.564: INFO: (16) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 12.792699ms)
Mar 11 17:04:51.564: INFO: (16) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 11.681548ms)
Mar 11 17:04:51.564: INFO: (16) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 12.157337ms)
Mar 11 17:04:51.564: INFO: (16) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 12.091574ms)
Mar 11 17:04:51.564: INFO: (16) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 11.823895ms)
Mar 11 17:04:51.564: INFO: (16) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 12.364429ms)
Mar 11 17:04:51.569: INFO: (17) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 4.941223ms)
Mar 11 17:04:51.570: INFO: (17) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 5.393942ms)
Mar 11 17:04:51.570: INFO: (17) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 5.321373ms)
Mar 11 17:04:51.570: INFO: (17) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 5.256703ms)
Mar 11 17:04:51.570: INFO: (17) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 5.264684ms)
Mar 11 17:04:51.570: INFO: (17) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 5.143568ms)
Mar 11 17:04:51.570: INFO: (17) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 5.465949ms)
Mar 11 17:04:51.573: INFO: (17) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 7.48324ms)
Mar 11 17:04:51.573: INFO: (17) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 7.07337ms)
Mar 11 17:04:51.573: INFO: (17) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 7.807277ms)
Mar 11 17:04:51.574: INFO: (17) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 8.756219ms)
Mar 11 17:04:51.574: INFO: (17) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 9.149427ms)
Mar 11 17:04:51.575: INFO: (17) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 9.393402ms)
Mar 11 17:04:51.575: INFO: (17) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 9.757333ms)
Mar 11 17:04:51.575: INFO: (17) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 9.377288ms)
Mar 11 17:04:51.576: INFO: (17) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 10.250151ms)
Mar 11 17:04:51.580: INFO: (18) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 3.924622ms)
Mar 11 17:04:51.580: INFO: (18) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 4.308442ms)
Mar 11 17:04:51.580: INFO: (18) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 4.10994ms)
Mar 11 17:04:51.581: INFO: (18) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 4.565478ms)
Mar 11 17:04:51.582: INFO: (18) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 5.172053ms)
Mar 11 17:04:51.582: INFO: (18) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 5.549443ms)
Mar 11 17:04:51.582: INFO: (18) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 5.716787ms)
Mar 11 17:04:51.582: INFO: (18) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 6.435549ms)
Mar 11 17:04:51.584: INFO: (18) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 7.641958ms)
Mar 11 17:04:51.584: INFO: (18) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 7.826969ms)
Mar 11 17:04:51.584: INFO: (18) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 7.183397ms)
Mar 11 17:04:51.585: INFO: (18) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 8.505159ms)
Mar 11 17:04:51.585: INFO: (18) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 8.366693ms)
Mar 11 17:04:51.585: INFO: (18) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 8.509138ms)
Mar 11 17:04:51.590: INFO: (18) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 13.607411ms)
Mar 11 17:04:51.591: INFO: (18) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 15.183312ms)
Mar 11 17:04:51.596: INFO: (19) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:462/proxy/: tls qux (200; 4.413497ms)
Mar 11 17:04:51.596: INFO: (19) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:460/proxy/: tls baz (200; 4.451934ms)
Mar 11 17:04:51.597: INFO: (19) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6/proxy/rewriteme">test</a> (200; 4.834114ms)
Mar 11 17:04:51.599: INFO: (19) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">... (200; 6.321116ms)
Mar 11 17:04:51.599: INFO: (19) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname2/proxy/: bar (200; 7.202021ms)
Mar 11 17:04:51.599: INFO: (19) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 6.708648ms)
Mar 11 17:04:51.599: INFO: (19) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:1080/proxy/rewriteme">test<... (200; 6.825905ms)
Mar 11 17:04:51.600: INFO: (19) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:162/proxy/: bar (200; 7.036072ms)
Mar 11 17:04:51.600: INFO: (19) /api/v1/namespaces/proxy-9325/pods/http:proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 7.470258ms)
Mar 11 17:04:51.600: INFO: (19) /api/v1/namespaces/proxy-9325/pods/proxy-service-b2msv-7p6c6:160/proxy/: foo (200; 7.999657ms)
Mar 11 17:04:51.601: INFO: (19) /api/v1/namespaces/proxy-9325/services/proxy-service-b2msv:portname1/proxy/: foo (200; 8.40721ms)
Mar 11 17:04:51.601: INFO: (19) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname1/proxy/: tls baz (200; 9.447409ms)
Mar 11 17:04:51.601: INFO: (19) /api/v1/namespaces/proxy-9325/services/https:proxy-service-b2msv:tlsportname2/proxy/: tls qux (200; 8.278109ms)
Mar 11 17:04:51.601: INFO: (19) /api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/: <a href="/api/v1/namespaces/proxy-9325/pods/https:proxy-service-b2msv-7p6c6:443/proxy/tlsrewritem... (200; 8.850959ms)
Mar 11 17:04:51.602: INFO: (19) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname2/proxy/: bar (200; 9.7435ms)
Mar 11 17:04:51.603: INFO: (19) /api/v1/namespaces/proxy-9325/services/http:proxy-service-b2msv:portname1/proxy/: foo (200; 10.065954ms)
STEP: deleting ReplicationController proxy-service-b2msv in namespace proxy-9325, will wait for the garbage collector to delete the pods
Mar 11 17:04:51.671: INFO: Deleting ReplicationController proxy-service-b2msv took: 12.696937ms
Mar 11 17:04:51.771: INFO: Terminating ReplicationController proxy-service-b2msv pods took: 100.298803ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:05:01.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9325" for this suite.

• [SLOW TEST:20.027 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":339,"completed":175,"skipped":2914,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:05:01.188: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 17:05:01.257: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42e9ec56-a5f6-4a11-9e52-43939fd13897" in namespace "downward-api-6291" to be "Succeeded or Failed"
Mar 11 17:05:01.270: INFO: Pod "downwardapi-volume-42e9ec56-a5f6-4a11-9e52-43939fd13897": Phase="Pending", Reason="", readiness=false. Elapsed: 12.718158ms
Mar 11 17:05:03.279: INFO: Pod "downwardapi-volume-42e9ec56-a5f6-4a11-9e52-43939fd13897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021144815s
STEP: Saw pod success
Mar 11 17:05:03.279: INFO: Pod "downwardapi-volume-42e9ec56-a5f6-4a11-9e52-43939fd13897" satisfied condition "Succeeded or Failed"
Mar 11 17:05:03.284: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-42e9ec56-a5f6-4a11-9e52-43939fd13897 container client-container: <nil>
STEP: delete the pod
Mar 11 17:05:03.313: INFO: Waiting for pod downwardapi-volume-42e9ec56-a5f6-4a11-9e52-43939fd13897 to disappear
Mar 11 17:05:03.318: INFO: Pod downwardapi-volume-42e9ec56-a5f6-4a11-9e52-43939fd13897 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:05:03.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6291" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":176,"skipped":2921,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:05:03.333: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:05:05.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4402" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":177,"skipped":2936,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:05:05.481: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:05:07.574: INFO: Deleting pod "var-expansion-a60b8642-cd5b-4edc-a235-e21a0634ff10" in namespace "var-expansion-7744"
Mar 11 17:05:07.586: INFO: Wait up to 5m0s for pod "var-expansion-a60b8642-cd5b-4edc-a235-e21a0634ff10" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:05:21.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7744" for this suite.

• [SLOW TEST:16.137 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":339,"completed":178,"skipped":2946,"failed":0}
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:05:21.619: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 11 17:05:22.037: INFO: Pod name wrapped-volume-race-102795f9-7abd-406c-b78a-ef8dffab09bb: Found 0 pods out of 5
Mar 11 17:05:27.047: INFO: Pod name wrapped-volume-race-102795f9-7abd-406c-b78a-ef8dffab09bb: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-102795f9-7abd-406c-b78a-ef8dffab09bb in namespace emptydir-wrapper-818, will wait for the garbage collector to delete the pods
Mar 11 17:08:17.162: INFO: Deleting ReplicationController wrapped-volume-race-102795f9-7abd-406c-b78a-ef8dffab09bb took: 11.851656ms
Mar 11 17:08:17.263: INFO: Terminating ReplicationController wrapped-volume-race-102795f9-7abd-406c-b78a-ef8dffab09bb pods took: 100.756618ms
STEP: Creating RC which spawns configmap-volume pods
Mar 11 17:08:29.200: INFO: Pod name wrapped-volume-race-c455f850-611c-4f62-adac-130927a6cd57: Found 0 pods out of 5
Mar 11 17:08:34.210: INFO: Pod name wrapped-volume-race-c455f850-611c-4f62-adac-130927a6cd57: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c455f850-611c-4f62-adac-130927a6cd57 in namespace emptydir-wrapper-818, will wait for the garbage collector to delete the pods
Mar 11 17:10:30.373: INFO: Deleting ReplicationController wrapped-volume-race-c455f850-611c-4f62-adac-130927a6cd57 took: 62.991165ms
Mar 11 17:10:30.474: INFO: Terminating ReplicationController wrapped-volume-race-c455f850-611c-4f62-adac-130927a6cd57 pods took: 101.007098ms
STEP: Creating RC which spawns configmap-volume pods
Mar 11 17:10:41.306: INFO: Pod name wrapped-volume-race-ae8633ad-264b-434f-a671-8da616cf7438: Found 0 pods out of 5
Mar 11 17:10:46.315: INFO: Pod name wrapped-volume-race-ae8633ad-264b-434f-a671-8da616cf7438: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ae8633ad-264b-434f-a671-8da616cf7438 in namespace emptydir-wrapper-818, will wait for the garbage collector to delete the pods
Mar 11 17:13:36.419: INFO: Deleting ReplicationController wrapped-volume-race-ae8633ad-264b-434f-a671-8da616cf7438 took: 12.340099ms
Mar 11 17:13:36.521: INFO: Terminating ReplicationController wrapped-volume-race-ae8633ad-264b-434f-a671-8da616cf7438 pods took: 102.288271ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:13:42.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-818" for this suite.

• [SLOW TEST:500.906 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":339,"completed":179,"skipped":2946,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:13:42.525: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1514
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Mar 11 17:13:42.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-9140 run e2e-test-httpd-pod --restart=Never --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Mar 11 17:13:43.241: INFO: stderr: ""
Mar 11 17:13:43.241: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1518
Mar 11 17:13:43.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-9140 delete pods e2e-test-httpd-pod'
Mar 11 17:13:51.104: INFO: stderr: ""
Mar 11 17:13:51.104: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:13:51.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9140" for this suite.

• [SLOW TEST:8.594 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":339,"completed":180,"skipped":2969,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:13:51.119: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:13:51.174: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:13:54.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4350" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":339,"completed":181,"skipped":2971,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:13:55.592: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:13:58.024: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 11 17:14:00.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615638, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615638, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615638, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782615638, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:14:03.085: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:14:03.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4557" for this suite.
STEP: Destroying namespace "webhook-4557-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.767 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":339,"completed":182,"skipped":2975,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:14:03.372: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-9af385fb-eb70-40c9-8c6e-26cc1484b16c
STEP: Creating a pod to test consume configMaps
Mar 11 17:14:03.444: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cc1fcb81-35b9-4277-8419-baf7ff2ea1ef" in namespace "projected-5636" to be "Succeeded or Failed"
Mar 11 17:14:03.453: INFO: Pod "pod-projected-configmaps-cc1fcb81-35b9-4277-8419-baf7ff2ea1ef": Phase="Pending", Reason="", readiness=false. Elapsed: 9.081369ms
Mar 11 17:14:05.463: INFO: Pod "pod-projected-configmaps-cc1fcb81-35b9-4277-8419-baf7ff2ea1ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018158325s
STEP: Saw pod success
Mar 11 17:14:05.463: INFO: Pod "pod-projected-configmaps-cc1fcb81-35b9-4277-8419-baf7ff2ea1ef" satisfied condition "Succeeded or Failed"
Mar 11 17:14:05.468: INFO: Trying to get logs from node 198.18.167.130 pod pod-projected-configmaps-cc1fcb81-35b9-4277-8419-baf7ff2ea1ef container agnhost-container: <nil>
STEP: delete the pod
Mar 11 17:14:05.518: INFO: Waiting for pod pod-projected-configmaps-cc1fcb81-35b9-4277-8419-baf7ff2ea1ef to disappear
Mar 11 17:14:05.523: INFO: Pod pod-projected-configmaps-cc1fcb81-35b9-4277-8419-baf7ff2ea1ef no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:14:05.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5636" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":183,"skipped":3023,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:14:05.544: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-9c47769b-9b86-45da-87a0-b2bcb7b63efa
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:14:05.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1098" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":339,"completed":184,"skipped":3043,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:14:05.611: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-7bdedf8b-e43e-4101-bc63-1908b7063e92
STEP: Creating a pod to test consume configMaps
Mar 11 17:14:05.677: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4af7a6d9-1504-486a-bb47-4cebc7c5eef3" in namespace "projected-4628" to be "Succeeded or Failed"
Mar 11 17:14:05.685: INFO: Pod "pod-projected-configmaps-4af7a6d9-1504-486a-bb47-4cebc7c5eef3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.812554ms
Mar 11 17:14:07.696: INFO: Pod "pod-projected-configmaps-4af7a6d9-1504-486a-bb47-4cebc7c5eef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018355223s
STEP: Saw pod success
Mar 11 17:14:07.696: INFO: Pod "pod-projected-configmaps-4af7a6d9-1504-486a-bb47-4cebc7c5eef3" satisfied condition "Succeeded or Failed"
Mar 11 17:14:07.701: INFO: Trying to get logs from node 198.18.167.130 pod pod-projected-configmaps-4af7a6d9-1504-486a-bb47-4cebc7c5eef3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 11 17:14:07.730: INFO: Waiting for pod pod-projected-configmaps-4af7a6d9-1504-486a-bb47-4cebc7c5eef3 to disappear
Mar 11 17:14:07.734: INFO: Pod pod-projected-configmaps-4af7a6d9-1504-486a-bb47-4cebc7c5eef3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:14:07.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4628" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":185,"skipped":3053,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:14:07.748: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Mar 11 17:14:07.821: INFO: observed Pod pod-test in namespace pods-9054 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar 11 17:14:07.827: INFO: observed Pod pod-test in namespace pods-9054 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:14:07 +0000 UTC  }]
Mar 11 17:14:07.853: INFO: observed Pod pod-test in namespace pods-9054 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:14:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:14:07 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:14:07 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:14:07 +0000 UTC  }]
Mar 11 17:14:09.177: INFO: Found Pod pod-test in namespace pods-9054 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:14:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:14:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:14:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:14:07 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Mar 11 17:14:09.193: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Mar 11 17:14:09.247: INFO: observed event type ADDED
Mar 11 17:14:09.247: INFO: observed event type MODIFIED
Mar 11 17:14:09.248: INFO: observed event type MODIFIED
Mar 11 17:14:09.248: INFO: observed event type MODIFIED
Mar 11 17:14:09.248: INFO: observed event type MODIFIED
Mar 11 17:14:09.248: INFO: observed event type MODIFIED
Mar 11 17:14:09.248: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:14:09.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9054" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":339,"completed":186,"skipped":3059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:14:09.266: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:14:09.312: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 11 17:14:18.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 --namespace=crd-publish-openapi-3106 create -f -'
Mar 11 17:14:19.458: INFO: stderr: ""
Mar 11 17:14:19.459: INFO: stdout: "e2e-test-crd-publish-openapi-6869-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 11 17:14:19.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 --namespace=crd-publish-openapi-3106 delete e2e-test-crd-publish-openapi-6869-crds test-foo'
Mar 11 17:14:19.566: INFO: stderr: ""
Mar 11 17:14:19.566: INFO: stdout: "e2e-test-crd-publish-openapi-6869-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 11 17:14:19.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 --namespace=crd-publish-openapi-3106 apply -f -'
Mar 11 17:14:20.021: INFO: stderr: ""
Mar 11 17:14:20.021: INFO: stdout: "e2e-test-crd-publish-openapi-6869-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 11 17:14:20.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 --namespace=crd-publish-openapi-3106 delete e2e-test-crd-publish-openapi-6869-crds test-foo'
Mar 11 17:14:20.144: INFO: stderr: ""
Mar 11 17:14:20.144: INFO: stdout: "e2e-test-crd-publish-openapi-6869-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 11 17:14:20.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 --namespace=crd-publish-openapi-3106 create -f -'
Mar 11 17:14:20.704: INFO: rc: 1
Mar 11 17:14:20.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 --namespace=crd-publish-openapi-3106 apply -f -'
Mar 11 17:14:21.246: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 11 17:14:21.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 --namespace=crd-publish-openapi-3106 create -f -'
Mar 11 17:14:21.795: INFO: rc: 1
Mar 11 17:14:21.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 --namespace=crd-publish-openapi-3106 apply -f -'
Mar 11 17:14:22.350: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 11 17:14:22.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 explain e2e-test-crd-publish-openapi-6869-crds'
Mar 11 17:14:22.780: INFO: stderr: ""
Mar 11 17:14:22.780: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6869-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 11 17:14:22.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 explain e2e-test-crd-publish-openapi-6869-crds.metadata'
Mar 11 17:14:23.336: INFO: stderr: ""
Mar 11 17:14:23.336: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6869-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 11 17:14:23.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 explain e2e-test-crd-publish-openapi-6869-crds.spec'
Mar 11 17:14:23.764: INFO: stderr: ""
Mar 11 17:14:23.764: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6869-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 11 17:14:23.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 explain e2e-test-crd-publish-openapi-6869-crds.spec.bars'
Mar 11 17:14:24.349: INFO: stderr: ""
Mar 11 17:14:24.349: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6869-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 11 17:14:24.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=crd-publish-openapi-3106 explain e2e-test-crd-publish-openapi-6869-crds.spec.bars2'
Mar 11 17:14:24.739: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:14:33.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3106" for this suite.

• [SLOW TEST:24.680 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":339,"completed":187,"skipped":3107,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:14:33.947: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar 11 17:14:34.013: INFO: Waiting up to 5m0s for pod "downward-api-c256e640-b2a8-4a9e-ba15-9feb872dad6f" in namespace "downward-api-462" to be "Succeeded or Failed"
Mar 11 17:14:34.019: INFO: Pod "downward-api-c256e640-b2a8-4a9e-ba15-9feb872dad6f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.1293ms
Mar 11 17:14:36.027: INFO: Pod "downward-api-c256e640-b2a8-4a9e-ba15-9feb872dad6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013769242s
STEP: Saw pod success
Mar 11 17:14:36.027: INFO: Pod "downward-api-c256e640-b2a8-4a9e-ba15-9feb872dad6f" satisfied condition "Succeeded or Failed"
Mar 11 17:14:36.031: INFO: Trying to get logs from node 198.18.167.130 pod downward-api-c256e640-b2a8-4a9e-ba15-9feb872dad6f container dapi-container: <nil>
STEP: delete the pod
Mar 11 17:14:36.059: INFO: Waiting for pod downward-api-c256e640-b2a8-4a9e-ba15-9feb872dad6f to disappear
Mar 11 17:14:36.062: INFO: Pod downward-api-c256e640-b2a8-4a9e-ba15-9feb872dad6f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:14:36.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-462" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":339,"completed":188,"skipped":3124,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:14:36.076: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-3265/configmap-test-50bd88c0-53c8-4082-8856-39466033c97a
STEP: Creating a pod to test consume configMaps
Mar 11 17:14:36.231: INFO: Waiting up to 5m0s for pod "pod-configmaps-5749c72f-33f2-40a6-a25d-a67c16118d82" in namespace "configmap-3265" to be "Succeeded or Failed"
Mar 11 17:14:36.242: INFO: Pod "pod-configmaps-5749c72f-33f2-40a6-a25d-a67c16118d82": Phase="Pending", Reason="", readiness=false. Elapsed: 11.269553ms
Mar 11 17:14:38.253: INFO: Pod "pod-configmaps-5749c72f-33f2-40a6-a25d-a67c16118d82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021633712s
STEP: Saw pod success
Mar 11 17:14:38.253: INFO: Pod "pod-configmaps-5749c72f-33f2-40a6-a25d-a67c16118d82" satisfied condition "Succeeded or Failed"
Mar 11 17:14:38.258: INFO: Trying to get logs from node 198.18.167.130 pod pod-configmaps-5749c72f-33f2-40a6-a25d-a67c16118d82 container env-test: <nil>
STEP: delete the pod
Mar 11 17:14:38.290: INFO: Waiting for pod pod-configmaps-5749c72f-33f2-40a6-a25d-a67c16118d82 to disappear
Mar 11 17:14:38.295: INFO: Pod pod-configmaps-5749c72f-33f2-40a6-a25d-a67c16118d82 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:14:38.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3265" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":339,"completed":189,"skipped":3132,"failed":0}
SSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:14:38.329: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:14:38.374: INFO: Creating pod...
Mar 11 17:14:38.395: INFO: Pod Quantity: 1 Status: Pending
Mar 11 17:14:39.400: INFO: Pod Quantity: 1 Status: Pending
Mar 11 17:14:40.403: INFO: Pod Status: Running
Mar 11 17:14:40.403: INFO: Creating service...
Mar 11 17:14:40.425: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/pods/agnhost/proxy/some/path/with/DELETE
Mar 11 17:14:40.431: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 11 17:14:40.432: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/pods/agnhost/proxy/some/path/with/GET
Mar 11 17:14:40.439: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 11 17:14:40.439: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/pods/agnhost/proxy/some/path/with/HEAD
Mar 11 17:14:40.443: INFO: http.Client request:HEAD | StatusCode:200
Mar 11 17:14:40.443: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/pods/agnhost/proxy/some/path/with/OPTIONS
Mar 11 17:14:40.448: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 11 17:14:40.448: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/pods/agnhost/proxy/some/path/with/PATCH
Mar 11 17:14:40.452: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 11 17:14:40.452: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/pods/agnhost/proxy/some/path/with/POST
Mar 11 17:14:40.456: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 11 17:14:40.456: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/pods/agnhost/proxy/some/path/with/PUT
Mar 11 17:14:40.459: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 11 17:14:40.460: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/services/test-service/proxy/some/path/with/DELETE
Mar 11 17:14:40.464: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 11 17:14:40.464: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/services/test-service/proxy/some/path/with/GET
Mar 11 17:14:40.470: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 11 17:14:40.470: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/services/test-service/proxy/some/path/with/HEAD
Mar 11 17:14:40.478: INFO: http.Client request:HEAD | StatusCode:200
Mar 11 17:14:40.478: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/services/test-service/proxy/some/path/with/OPTIONS
Mar 11 17:14:40.484: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 11 17:14:40.484: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/services/test-service/proxy/some/path/with/PATCH
Mar 11 17:14:40.490: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 11 17:14:40.490: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/services/test-service/proxy/some/path/with/POST
Mar 11 17:14:40.496: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 11 17:14:40.496: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9011/services/test-service/proxy/some/path/with/PUT
Mar 11 17:14:40.501: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:14:40.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9011" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":339,"completed":190,"skipped":3136,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:14:40.515: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:14:40.598: INFO: The status of Pod pod-secrets-69dc2858-81c8-46ad-b5f9-b339c1e76d0a is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:14:42.604: INFO: The status of Pod pod-secrets-69dc2858-81c8-46ad-b5f9-b339c1e76d0a is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:14:42.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7506" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":339,"completed":191,"skipped":3139,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:14:42.662: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:14:43.423: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:14:46.479: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:14:56.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2348" for this suite.
STEP: Destroying namespace "webhook-2348-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.172 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":339,"completed":192,"skipped":3139,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:14:56.834: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:14:56.904: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar 11 17:14:56.933: INFO: The status of Pod pod-logs-websocket-58d7b47b-71a4-4f91-bdb6-ca554ad98321 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:14:58.942: INFO: The status of Pod pod-logs-websocket-58d7b47b-71a4-4f91-bdb6-ca554ad98321 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:14:58.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7255" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":339,"completed":193,"skipped":3142,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:14:58.993: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1548
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Mar 11 17:14:59.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-4729 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Mar 11 17:14:59.169: INFO: stderr: ""
Mar 11 17:14:59.169: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 11 17:15:04.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-4729 get pod e2e-test-httpd-pod -o json'
Mar 11 17:15:04.343: INFO: stderr: ""
Mar 11 17:15:04.343: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-03-11T17:14:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4729\",\n        \"resourceVersion\": \"70174\",\n        \"uid\": \"b7e0533e-3ce8-47aa-b526-4122ce47a5af\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-4dr2w\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"198.18.167.130\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-4dr2w\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-11T17:14:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-11T17:15:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-11T17:15:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-03-11T17:14:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://0a7597a1ca359d91618c5c80d0ced2d999affc7c5c48fe73b11d377e9117330f\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-03-11T17:15:00Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"198.18.167.130\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.2.172\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.2.172\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-03-11T17:14:59Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 11 17:15:04.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-4729 replace -f -'
Mar 11 17:15:04.956: INFO: stderr: ""
Mar 11 17:15:04.956: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1552
Mar 11 17:15:04.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-4729 delete pods e2e-test-httpd-pod'
Mar 11 17:15:11.111: INFO: stderr: ""
Mar 11 17:15:11.111: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:15:11.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4729" for this suite.

• [SLOW TEST:12.135 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":339,"completed":194,"skipped":3163,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:15:11.128: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0311 17:15:51.272915      19 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Mar 11 17:16:53.305: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Mar 11 17:16:53.306: INFO: Deleting pod "simpletest.rc-48t5s" in namespace "gc-6583"
Mar 11 17:16:53.333: INFO: Deleting pod "simpletest.rc-5ql2z" in namespace "gc-6583"
Mar 11 17:16:53.350: INFO: Deleting pod "simpletest.rc-94j45" in namespace "gc-6583"
Mar 11 17:16:53.369: INFO: Deleting pod "simpletest.rc-97fr6" in namespace "gc-6583"
Mar 11 17:16:53.397: INFO: Deleting pod "simpletest.rc-g8lw7" in namespace "gc-6583"
Mar 11 17:16:53.422: INFO: Deleting pod "simpletest.rc-gxxhp" in namespace "gc-6583"
Mar 11 17:16:53.451: INFO: Deleting pod "simpletest.rc-hnlkw" in namespace "gc-6583"
Mar 11 17:16:53.484: INFO: Deleting pod "simpletest.rc-k99bv" in namespace "gc-6583"
Mar 11 17:16:53.503: INFO: Deleting pod "simpletest.rc-q2894" in namespace "gc-6583"
Mar 11 17:16:53.523: INFO: Deleting pod "simpletest.rc-t44jg" in namespace "gc-6583"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:16:53.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6583" for this suite.

• [SLOW TEST:102.429 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":339,"completed":195,"skipped":3173,"failed":0}
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:16:53.558: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Mar 11 17:16:53.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-6316 cluster-info'
Mar 11 17:16:53.702: INFO: stderr: ""
Mar 11 17:16:53.702: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:16:53.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6316" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":339,"completed":196,"skipped":3173,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:16:53.715: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:16:55.798: INFO: Deleting pod "var-expansion-73e59498-ed5c-4613-9348-ff6fec8ff0a2" in namespace "var-expansion-6470"
Mar 11 17:16:55.821: INFO: Wait up to 5m0s for pod "var-expansion-73e59498-ed5c-4613-9348-ff6fec8ff0a2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:17:11.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6470" for this suite.

• [SLOW TEST:18.142 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":339,"completed":197,"skipped":3185,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:17:11.858: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Mar 11 17:17:11.933: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-8006 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:17:12.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8006" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":339,"completed":198,"skipped":3198,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:17:12.036: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 17:17:12.104: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80248194-728a-441a-ba24-6f823a28446e" in namespace "downward-api-8136" to be "Succeeded or Failed"
Mar 11 17:17:12.109: INFO: Pod "downwardapi-volume-80248194-728a-441a-ba24-6f823a28446e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825175ms
Mar 11 17:17:14.119: INFO: Pod "downwardapi-volume-80248194-728a-441a-ba24-6f823a28446e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015624633s
STEP: Saw pod success
Mar 11 17:17:14.119: INFO: Pod "downwardapi-volume-80248194-728a-441a-ba24-6f823a28446e" satisfied condition "Succeeded or Failed"
Mar 11 17:17:14.125: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-80248194-728a-441a-ba24-6f823a28446e container client-container: <nil>
STEP: delete the pod
Mar 11 17:17:14.180: INFO: Waiting for pod downwardapi-volume-80248194-728a-441a-ba24-6f823a28446e to disappear
Mar 11 17:17:14.192: INFO: Pod downwardapi-volume-80248194-728a-441a-ba24-6f823a28446e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:17:14.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8136" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":199,"skipped":3236,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:17:14.226: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Mar 11 17:17:14.324: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:17:16.337: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Mar 11 17:17:16.364: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:17:18.373: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 11 17:17:18.406: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 11 17:17:18.409: INFO: Pod pod-with-poststart-http-hook still exists
Mar 11 17:17:20.409: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 11 17:17:20.418: INFO: Pod pod-with-poststart-http-hook still exists
Mar 11 17:17:22.409: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 11 17:17:22.418: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:17:22.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9415" for this suite.

• [SLOW TEST:8.208 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":339,"completed":200,"skipped":3244,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:17:22.434: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:17:22.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4661" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":339,"completed":201,"skipped":3257,"failed":0}

------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:17:22.527: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-24219149-4cfe-41ef-bf99-3f0991146359 in namespace container-probe-7278
Mar 11 17:17:24.594: INFO: Started pod busybox-24219149-4cfe-41ef-bf99-3f0991146359 in namespace container-probe-7278
STEP: checking the pod's current state and verifying that restartCount is present
Mar 11 17:17:24.598: INFO: Initial restart count of pod busybox-24219149-4cfe-41ef-bf99-3f0991146359 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:21:25.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7278" for this suite.

• [SLOW TEST:243.121 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":202,"skipped":3257,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:21:25.648: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-b180f2d9-b2a8-4c00-8205-58af315cad0c
STEP: Creating a pod to test consume configMaps
Mar 11 17:21:25.729: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3dd5ff05-d85b-4d23-af80-c95d13fd4c7e" in namespace "projected-4762" to be "Succeeded or Failed"
Mar 11 17:21:25.745: INFO: Pod "pod-projected-configmaps-3dd5ff05-d85b-4d23-af80-c95d13fd4c7e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.989235ms
Mar 11 17:21:27.751: INFO: Pod "pod-projected-configmaps-3dd5ff05-d85b-4d23-af80-c95d13fd4c7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022478961s
STEP: Saw pod success
Mar 11 17:21:27.753: INFO: Pod "pod-projected-configmaps-3dd5ff05-d85b-4d23-af80-c95d13fd4c7e" satisfied condition "Succeeded or Failed"
Mar 11 17:21:27.756: INFO: Trying to get logs from node 198.18.167.130 pod pod-projected-configmaps-3dd5ff05-d85b-4d23-af80-c95d13fd4c7e container agnhost-container: <nil>
STEP: delete the pod
Mar 11 17:21:27.807: INFO: Waiting for pod pod-projected-configmaps-3dd5ff05-d85b-4d23-af80-c95d13fd4c7e to disappear
Mar 11 17:21:27.810: INFO: Pod pod-projected-configmaps-3dd5ff05-d85b-4d23-af80-c95d13fd4c7e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:21:27.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4762" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":203,"skipped":3291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:21:27.832: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Mar 11 17:21:27.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-5055 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Mar 11 17:21:27.976: INFO: stderr: ""
Mar 11 17:21:27.976: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Mar 11 17:21:27.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-5055 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Mar 11 17:21:28.396: INFO: stderr: ""
Mar 11 17:21:28.396: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Mar 11 17:21:28.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-5055 delete pods e2e-test-httpd-pod'
Mar 11 17:21:41.099: INFO: stderr: ""
Mar 11 17:21:41.099: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:21:41.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5055" for this suite.

• [SLOW TEST:13.287 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:903
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":339,"completed":204,"skipped":3333,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:21:41.120: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Mar 11 17:21:41.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-543 create -f -'
Mar 11 17:21:41.801: INFO: stderr: ""
Mar 11 17:21:41.801: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 11 17:21:42.812: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 11 17:21:42.812: INFO: Found 0 / 1
Mar 11 17:21:43.807: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 11 17:21:43.807: INFO: Found 1 / 1
Mar 11 17:21:43.807: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 11 17:21:43.811: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 11 17:21:43.811: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 11 17:21:43.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-543 patch pod agnhost-primary-vltvw -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 11 17:21:43.916: INFO: stderr: ""
Mar 11 17:21:43.916: INFO: stdout: "pod/agnhost-primary-vltvw patched\n"
STEP: checking annotations
Mar 11 17:21:43.919: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 11 17:21:43.920: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:21:43.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-543" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":339,"completed":205,"skipped":3351,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:21:43.932: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 11 17:21:44.004: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3188  e792b59b-7c33-4eb3-9156-e3d3385e020e 74819 0 2022-03-11 17:21:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-03-11 17:21:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 17:21:44.004: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3188  e792b59b-7c33-4eb3-9156-e3d3385e020e 74820 0 2022-03-11 17:21:43 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-03-11 17:21:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:21:44.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3188" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":339,"completed":206,"skipped":3359,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:21:44.016: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Mar 11 17:21:44.073: INFO: Waiting up to 5m0s for pod "security-context-69b6c420-be90-45d8-aad5-bf7c02f4bbd6" in namespace "security-context-5637" to be "Succeeded or Failed"
Mar 11 17:21:44.081: INFO: Pod "security-context-69b6c420-be90-45d8-aad5-bf7c02f4bbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.275992ms
Mar 11 17:21:46.092: INFO: Pod "security-context-69b6c420-be90-45d8-aad5-bf7c02f4bbd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018753588s
STEP: Saw pod success
Mar 11 17:21:46.092: INFO: Pod "security-context-69b6c420-be90-45d8-aad5-bf7c02f4bbd6" satisfied condition "Succeeded or Failed"
Mar 11 17:21:46.097: INFO: Trying to get logs from node 198.18.167.130 pod security-context-69b6c420-be90-45d8-aad5-bf7c02f4bbd6 container test-container: <nil>
STEP: delete the pod
Mar 11 17:21:46.124: INFO: Waiting for pod security-context-69b6c420-be90-45d8-aad5-bf7c02f4bbd6 to disappear
Mar 11 17:21:46.144: INFO: Pod security-context-69b6c420-be90-45d8-aad5-bf7c02f4bbd6 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:21:46.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5637" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":207,"skipped":3381,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:21:46.163: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-779
STEP: creating service affinity-nodeport-transition in namespace services-779
STEP: creating replication controller affinity-nodeport-transition in namespace services-779
I0311 17:21:46.243385      19 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-779, replica count: 3
I0311 17:21:49.300454      19 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0311 17:21:52.301443      19 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 11 17:21:52.317: INFO: Creating new exec pod
Mar 11 17:21:55.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-779 exec execpod-affinityrm6dq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Mar 11 17:21:55.535: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 11 17:21:55.535: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 17:21:55.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-779 exec execpod-affinityrm6dq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.32.0 80'
Mar 11 17:21:55.721: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.32.0 80\nConnection to 10.101.32.0 80 port [tcp/http] succeeded!\n"
Mar 11 17:21:55.721: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 17:21:55.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-779 exec execpod-affinityrm6dq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 198.18.16.160 32493'
Mar 11 17:21:55.906: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 198.18.16.160 32493\nConnection to 198.18.16.160 32493 port [tcp/*] succeeded!\n"
Mar 11 17:21:55.906: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 17:21:55.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-779 exec execpod-affinityrm6dq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 198.18.167.130 32493'
Mar 11 17:21:56.085: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 198.18.167.130 32493\nConnection to 198.18.167.130 32493 port [tcp/*] succeeded!\n"
Mar 11 17:21:56.085: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 11 17:21:56.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-779 exec execpod-affinityrm6dq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://198.18.16.160:32493/ ; done'
Mar 11 17:21:56.379: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n"
Mar 11 17:21:56.379: INFO: stdout: "\naffinity-nodeport-transition-sn7qx\naffinity-nodeport-transition-8p9dc\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-sn7qx\naffinity-nodeport-transition-sn7qx\naffinity-nodeport-transition-sn7qx\naffinity-nodeport-transition-sn7qx\naffinity-nodeport-transition-8p9dc\naffinity-nodeport-transition-8p9dc\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-sn7qx\naffinity-nodeport-transition-8p9dc\naffinity-nodeport-transition-8p9dc\naffinity-nodeport-transition-sn7qx\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72"
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-sn7qx
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-8p9dc
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-sn7qx
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-sn7qx
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-sn7qx
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-sn7qx
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-8p9dc
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-8p9dc
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-sn7qx
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-8p9dc
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-8p9dc
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-sn7qx
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.379: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-779 exec execpod-affinityrm6dq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://198.18.16.160:32493/ ; done'
Mar 11 17:21:56.659: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n+ echo\n+ curl -q -s --connect-timeout 2 http://198.18.16.160:32493/\n"
Mar 11 17:21:56.659: INFO: stdout: "\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72\naffinity-nodeport-transition-2mt72"
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Received response from host: affinity-nodeport-transition-2mt72
Mar 11 17:21:56.659: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-779, will wait for the garbage collector to delete the pods
Mar 11 17:21:56.754: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.32694ms
Mar 11 17:21:56.856: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 102.03098ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:22:11.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-779" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:25.173 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":208,"skipped":3424,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:22:11.345: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6301.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6301.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 11 17:22:13.481: INFO: DNS probes using dns-6301/dns-test-43719621-949b-4596-884f-3485f2ace693 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:22:13.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6301" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":339,"completed":209,"skipped":3466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:22:13.527: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:22:13.610: INFO: Create a RollingUpdate DaemonSet
Mar 11 17:22:13.617: INFO: Check that daemon pods launch on every node of the cluster
Mar 11 17:22:13.622: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:22:13.622: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:22:13.626: INFO: Number of nodes with available pods: 0
Mar 11 17:22:13.626: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:22:14.637: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:22:14.637: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:22:14.641: INFO: Number of nodes with available pods: 0
Mar 11 17:22:14.641: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:22:15.634: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:22:15.634: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:22:15.647: INFO: Number of nodes with available pods: 2
Mar 11 17:22:15.647: INFO: Number of running nodes: 2, number of available pods: 2
Mar 11 17:22:15.647: INFO: Update the DaemonSet to trigger a rollout
Mar 11 17:22:15.659: INFO: Updating DaemonSet daemon-set
Mar 11 17:22:31.685: INFO: Roll back the DaemonSet before rollout is complete
Mar 11 17:22:31.700: INFO: Updating DaemonSet daemon-set
Mar 11 17:22:31.700: INFO: Make sure DaemonSet rollback is complete
Mar 11 17:22:31.707: INFO: Wrong image for pod: daemon-set-rvh74. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Mar 11 17:22:31.707: INFO: Pod daemon-set-rvh74 is not available
Mar 11 17:22:31.715: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:22:31.715: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:22:32.727: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:22:32.727: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:22:33.723: INFO: Pod daemon-set-q9w6b is not available
Mar 11 17:22:33.728: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:22:33.728: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5666, will wait for the garbage collector to delete the pods
Mar 11 17:22:33.800: INFO: Deleting DaemonSet.extensions daemon-set took: 11.482295ms
Mar 11 17:22:33.901: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.684259ms
Mar 11 17:22:41.209: INFO: Number of nodes with available pods: 0
Mar 11 17:22:41.210: INFO: Number of running nodes: 0, number of available pods: 0
Mar 11 17:22:41.217: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"75742"},"items":null}

Mar 11 17:22:41.221: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"75742"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:22:41.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5666" for this suite.

• [SLOW TEST:27.728 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":339,"completed":210,"skipped":3490,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:22:41.255: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:22:41.306: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:22:41.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9531" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":339,"completed":211,"skipped":3502,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:22:42.014: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 17:22:42.093: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c57dbfe8-989f-459f-870a-71a988dffeaa" in namespace "downward-api-8782" to be "Succeeded or Failed"
Mar 11 17:22:42.101: INFO: Pod "downwardapi-volume-c57dbfe8-989f-459f-870a-71a988dffeaa": Phase="Pending", Reason="", readiness=false. Elapsed: 7.450518ms
Mar 11 17:22:44.195: INFO: Pod "downwardapi-volume-c57dbfe8-989f-459f-870a-71a988dffeaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.101631461s
STEP: Saw pod success
Mar 11 17:22:44.195: INFO: Pod "downwardapi-volume-c57dbfe8-989f-459f-870a-71a988dffeaa" satisfied condition "Succeeded or Failed"
Mar 11 17:22:44.311: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-c57dbfe8-989f-459f-870a-71a988dffeaa container client-container: <nil>
STEP: delete the pod
Mar 11 17:22:44.608: INFO: Waiting for pod downwardapi-volume-c57dbfe8-989f-459f-870a-71a988dffeaa to disappear
Mar 11 17:22:44.618: INFO: Pod downwardapi-volume-c57dbfe8-989f-459f-870a-71a988dffeaa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:22:44.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8782" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":212,"skipped":3506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:22:44.670: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 11 17:22:47.326: INFO: Successfully updated pod "adopt-release-6z68p"
STEP: Checking that the Job readopts the Pod
Mar 11 17:22:47.327: INFO: Waiting up to 15m0s for pod "adopt-release-6z68p" in namespace "job-9363" to be "adopted"
Mar 11 17:22:47.345: INFO: Pod "adopt-release-6z68p": Phase="Running", Reason="", readiness=true. Elapsed: 18.243942ms
Mar 11 17:22:49.356: INFO: Pod "adopt-release-6z68p": Phase="Running", Reason="", readiness=true. Elapsed: 2.029093835s
Mar 11 17:22:49.356: INFO: Pod "adopt-release-6z68p" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 11 17:22:49.886: INFO: Successfully updated pod "adopt-release-6z68p"
STEP: Checking that the Job releases the Pod
Mar 11 17:22:49.886: INFO: Waiting up to 15m0s for pod "adopt-release-6z68p" in namespace "job-9363" to be "released"
Mar 11 17:22:49.902: INFO: Pod "adopt-release-6z68p": Phase="Running", Reason="", readiness=true. Elapsed: 16.001146ms
Mar 11 17:22:49.903: INFO: Pod "adopt-release-6z68p" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:22:49.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9363" for this suite.

• [SLOW TEST:5.271 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":339,"completed":213,"skipped":3538,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:22:49.946: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 11 17:22:50.011: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:22:55.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-417" for this suite.

• [SLOW TEST:5.578 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":339,"completed":214,"skipped":3545,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:22:55.523: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Mar 11 17:24:56.127: INFO: Successfully updated pod "var-expansion-d40225c1-4069-4703-bdab-bbaf91fc6870"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Mar 11 17:24:58.144: INFO: Deleting pod "var-expansion-d40225c1-4069-4703-bdab-bbaf91fc6870" in namespace "var-expansion-665"
Mar 11 17:24:58.155: INFO: Wait up to 5m0s for pod "var-expansion-d40225c1-4069-4703-bdab-bbaf91fc6870" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:25:30.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-665" for this suite.

• [SLOW TEST:154.657 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":339,"completed":215,"skipped":3556,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:25:30.181: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:25:30.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-938" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":339,"completed":216,"skipped":3560,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:25:30.273: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-411ff004-6729-47c5-82c5-9e73f965377f
STEP: Creating a pod to test consume secrets
Mar 11 17:25:30.335: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cac661b8-1b3f-4c32-a280-375e9771006c" in namespace "projected-5680" to be "Succeeded or Failed"
Mar 11 17:25:30.339: INFO: Pod "pod-projected-secrets-cac661b8-1b3f-4c32-a280-375e9771006c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103942ms
Mar 11 17:25:32.349: INFO: Pod "pod-projected-secrets-cac661b8-1b3f-4c32-a280-375e9771006c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013525091s
STEP: Saw pod success
Mar 11 17:25:32.349: INFO: Pod "pod-projected-secrets-cac661b8-1b3f-4c32-a280-375e9771006c" satisfied condition "Succeeded or Failed"
Mar 11 17:25:32.353: INFO: Trying to get logs from node 198.18.167.130 pod pod-projected-secrets-cac661b8-1b3f-4c32-a280-375e9771006c container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 11 17:25:32.391: INFO: Waiting for pod pod-projected-secrets-cac661b8-1b3f-4c32-a280-375e9771006c to disappear
Mar 11 17:25:32.394: INFO: Pod pod-projected-secrets-cac661b8-1b3f-4c32-a280-375e9771006c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:25:32.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5680" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":217,"skipped":3563,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:25:32.412: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-8181
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-8181
Mar 11 17:25:32.498: INFO: Found 0 stateful pods, waiting for 1
Mar 11 17:25:42.507: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Mar 11 17:25:42.551: INFO: Deleting all statefulset in ns statefulset-8181
Mar 11 17:25:42.557: INFO: Scaling statefulset ss to 0
Mar 11 17:26:02.578: INFO: Waiting for statefulset status.replicas updated to 0
Mar 11 17:26:02.584: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:26:02.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8181" for this suite.

• [SLOW TEST:30.237 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":339,"completed":218,"skipped":3581,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:26:02.649: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9888
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-9888
STEP: creating replication controller externalsvc in namespace services-9888
I0311 17:26:02.805262      19 runners.go:190] Created replication controller with name: externalsvc, namespace: services-9888, replica count: 2
I0311 17:26:05.856727      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 11 17:26:05.923: INFO: Creating new exec pod
Mar 11 17:26:09.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-9888 exec execpodsj4zm -- /bin/sh -x -c nslookup nodeport-service.services-9888.svc.cluster.local'
Mar 11 17:26:10.488: INFO: stderr: "+ nslookup nodeport-service.services-9888.svc.cluster.local\n"
Mar 11 17:26:10.488: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-9888.svc.cluster.local\tcanonical name = externalsvc.services-9888.svc.cluster.local.\nName:\texternalsvc.services-9888.svc.cluster.local\nAddress: 10.102.172.140\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9888, will wait for the garbage collector to delete the pods
Mar 11 17:26:10.553: INFO: Deleting ReplicationController externalsvc took: 9.318419ms
Mar 11 17:26:10.653: INFO: Terminating ReplicationController externalsvc pods took: 100.526611ms
Mar 11 17:26:21.194: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:26:21.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9888" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:18.578 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":339,"completed":219,"skipped":3595,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:26:21.230: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-pv6f
STEP: Creating a pod to test atomic-volume-subpath
Mar 11 17:26:21.332: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pv6f" in namespace "subpath-1215" to be "Succeeded or Failed"
Mar 11 17:26:21.340: INFO: Pod "pod-subpath-test-projected-pv6f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.20784ms
Mar 11 17:26:23.349: INFO: Pod "pod-subpath-test-projected-pv6f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016396201s
Mar 11 17:26:25.355: INFO: Pod "pod-subpath-test-projected-pv6f": Phase="Running", Reason="", readiness=true. Elapsed: 4.022866568s
Mar 11 17:26:27.363: INFO: Pod "pod-subpath-test-projected-pv6f": Phase="Running", Reason="", readiness=true. Elapsed: 6.031248417s
Mar 11 17:26:29.375: INFO: Pod "pod-subpath-test-projected-pv6f": Phase="Running", Reason="", readiness=true. Elapsed: 8.042816933s
Mar 11 17:26:31.380: INFO: Pod "pod-subpath-test-projected-pv6f": Phase="Running", Reason="", readiness=true. Elapsed: 10.047338299s
Mar 11 17:26:33.387: INFO: Pod "pod-subpath-test-projected-pv6f": Phase="Running", Reason="", readiness=true. Elapsed: 12.054632514s
Mar 11 17:26:35.394: INFO: Pod "pod-subpath-test-projected-pv6f": Phase="Running", Reason="", readiness=true. Elapsed: 14.061758935s
Mar 11 17:26:37.400: INFO: Pod "pod-subpath-test-projected-pv6f": Phase="Running", Reason="", readiness=true. Elapsed: 16.068022466s
Mar 11 17:26:39.410: INFO: Pod "pod-subpath-test-projected-pv6f": Phase="Running", Reason="", readiness=true. Elapsed: 18.077455879s
Mar 11 17:26:41.418: INFO: Pod "pod-subpath-test-projected-pv6f": Phase="Running", Reason="", readiness=true. Elapsed: 20.086153519s
Mar 11 17:26:43.426: INFO: Pod "pod-subpath-test-projected-pv6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.0938993s
STEP: Saw pod success
Mar 11 17:26:43.426: INFO: Pod "pod-subpath-test-projected-pv6f" satisfied condition "Succeeded or Failed"
Mar 11 17:26:43.431: INFO: Trying to get logs from node 198.18.167.130 pod pod-subpath-test-projected-pv6f container test-container-subpath-projected-pv6f: <nil>
STEP: delete the pod
Mar 11 17:26:43.480: INFO: Waiting for pod pod-subpath-test-projected-pv6f to disappear
Mar 11 17:26:43.488: INFO: Pod pod-subpath-test-projected-pv6f no longer exists
STEP: Deleting pod pod-subpath-test-projected-pv6f
Mar 11 17:26:43.488: INFO: Deleting pod "pod-subpath-test-projected-pv6f" in namespace "subpath-1215"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:26:43.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1215" for this suite.

• [SLOW TEST:22.285 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":339,"completed":220,"skipped":3611,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:26:43.516: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-f09bb5ab-d316-483e-be7e-cc63d59e7fa6
STEP: Creating a pod to test consume secrets
Mar 11 17:26:43.597: INFO: Waiting up to 5m0s for pod "pod-secrets-cdebdb13-6349-4197-b264-d830cd21dc5b" in namespace "secrets-2929" to be "Succeeded or Failed"
Mar 11 17:26:43.614: INFO: Pod "pod-secrets-cdebdb13-6349-4197-b264-d830cd21dc5b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.706929ms
Mar 11 17:26:45.623: INFO: Pod "pod-secrets-cdebdb13-6349-4197-b264-d830cd21dc5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026524759s
STEP: Saw pod success
Mar 11 17:26:45.623: INFO: Pod "pod-secrets-cdebdb13-6349-4197-b264-d830cd21dc5b" satisfied condition "Succeeded or Failed"
Mar 11 17:26:45.628: INFO: Trying to get logs from node 198.18.167.130 pod pod-secrets-cdebdb13-6349-4197-b264-d830cd21dc5b container secret-volume-test: <nil>
STEP: delete the pod
Mar 11 17:26:45.653: INFO: Waiting for pod pod-secrets-cdebdb13-6349-4197-b264-d830cd21dc5b to disappear
Mar 11 17:26:45.657: INFO: Pod pod-secrets-cdebdb13-6349-4197-b264-d830cd21dc5b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:26:45.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2929" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":221,"skipped":3624,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:26:45.675: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-a71b2722-1b4d-4366-ae05-9d82f2fa1fff
STEP: Creating a pod to test consume configMaps
Mar 11 17:26:45.738: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a407de55-e5da-4845-99dc-be9fc6f1bd18" in namespace "projected-6850" to be "Succeeded or Failed"
Mar 11 17:26:45.746: INFO: Pod "pod-projected-configmaps-a407de55-e5da-4845-99dc-be9fc6f1bd18": Phase="Pending", Reason="", readiness=false. Elapsed: 8.19741ms
Mar 11 17:26:47.755: INFO: Pod "pod-projected-configmaps-a407de55-e5da-4845-99dc-be9fc6f1bd18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017084365s
STEP: Saw pod success
Mar 11 17:26:47.755: INFO: Pod "pod-projected-configmaps-a407de55-e5da-4845-99dc-be9fc6f1bd18" satisfied condition "Succeeded or Failed"
Mar 11 17:26:47.760: INFO: Trying to get logs from node 198.18.167.130 pod pod-projected-configmaps-a407de55-e5da-4845-99dc-be9fc6f1bd18 container agnhost-container: <nil>
STEP: delete the pod
Mar 11 17:26:47.799: INFO: Waiting for pod pod-projected-configmaps-a407de55-e5da-4845-99dc-be9fc6f1bd18 to disappear
Mar 11 17:26:47.803: INFO: Pod pod-projected-configmaps-a407de55-e5da-4845-99dc-be9fc6f1bd18 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:26:47.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6850" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":222,"skipped":3635,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:26:47.817: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 11 17:26:47.879: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 11 17:26:47.891: INFO: Waiting for terminating namespaces to be deleted...
Mar 11 17:26:47.896: INFO: 
Logging pods the apiserver thinks is on node 198.18.16.160 before test
Mar 11 17:26:47.909: INFO: capi-kubeadm-bootstrap-controller-manager-694cc79bb7-m8mq5 from capi-kubeadm-bootstrap-system started at 2022-03-11 15:43:57 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.909: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:26:47.909: INFO: capi-controller-manager-689cd9b4fd-pkbpt from capi-system started at 2022-03-11 15:43:50 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.909: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:26:47.909: INFO: capv-controller-manager-6b467446b9-4qlm4 from capv-system started at 2022-03-11 15:44:30 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.909: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:26:47.909: INFO: cert-manager-7988d4fb6c-knnwq from cert-manager started at 2022-03-11 15:43:27 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.909: INFO: 	Container cert-manager ready: true, restart count 0
Mar 11 17:26:47.909: INFO: cert-manager-cainjector-6bc8dcdb64-8vk7b from cert-manager started at 2022-03-11 15:43:26 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.909: INFO: 	Container cert-manager ready: true, restart count 0
Mar 11 17:26:47.909: INFO: cert-manager-webhook-68979bfb95-ksn6x from cert-manager started at 2022-03-11 15:43:27 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.909: INFO: 	Container cert-manager ready: true, restart count 0
Mar 11 17:26:47.909: INFO: etcdadm-bootstrap-provider-controller-manager-74c86ffb56-l6l9z from etcdadm-bootstrap-provider-system started at 2022-03-11 15:44:04 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.909: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:26:47.909: INFO: etcdadm-controller-controller-manager-7894945688-sxr4k from etcdadm-controller-system started at 2022-03-11 16:43:28 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.909: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:26:47.910: INFO: cilium-operator-86d59d5c88-fvvbn from kube-system started at 2022-03-11 15:42:57 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.910: INFO: 	Container cilium-operator ready: true, restart count 0
Mar 11 17:26:47.910: INFO: cilium-qm8kg from kube-system started at 2022-03-11 15:42:57 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.910: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 11 17:26:47.910: INFO: coredns-745c7986c7-5r5r8 from kube-system started at 2022-03-11 16:43:28 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.910: INFO: 	Container coredns ready: true, restart count 0
Mar 11 17:26:47.910: INFO: coredns-745c7986c7-mmcqj from kube-system started at 2022-03-11 16:43:28 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.910: INFO: 	Container coredns ready: true, restart count 0
Mar 11 17:26:47.910: INFO: kube-proxy-8jj6k from kube-system started at 2022-03-11 15:42:19 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.910: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 11 17:26:47.910: INFO: vsphere-cloud-controller-manager-l6kzs from kube-system started at 2022-03-11 15:42:19 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.910: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 1
Mar 11 17:26:47.910: INFO: vsphere-csi-controller-576c9c8dc8-8vqvm from kube-system started at 2022-03-11 16:43:28 +0000 UTC (5 container statuses recorded)
Mar 11 17:26:47.910: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 11 17:26:47.910: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 11 17:26:47.910: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 11 17:26:47.910: INFO: 	Container vsphere-csi-controller ready: true, restart count 0
Mar 11 17:26:47.910: INFO: 	Container vsphere-syncer ready: true, restart count 0
Mar 11 17:26:47.910: INFO: vsphere-csi-node-zmmbm from kube-system started at 2022-03-11 15:42:19 +0000 UTC (3 container statuses recorded)
Mar 11 17:26:47.910: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 11 17:26:47.910: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 11 17:26:47.910: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Mar 11 17:26:47.910: INFO: sonobuoy from sonobuoy started at 2022-03-11 15:55:43 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.910: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 11 17:26:47.910: INFO: sonobuoy-systemd-logs-daemon-set-5218357723e74163-g6x4l from sonobuoy started at 2022-03-11 15:55:47 +0000 UTC (2 container statuses recorded)
Mar 11 17:26:47.910: INFO: 	Container sonobuoy-worker ready: false, restart count 10
Mar 11 17:26:47.911: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 11 17:26:47.911: INFO: 
Logging pods the apiserver thinks is on node 198.18.167.130 before test
Mar 11 17:26:47.923: INFO: cilium-69rlw from kube-system started at 2022-03-11 15:42:57 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.923: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 11 17:26:47.923: INFO: kube-proxy-6trtj from kube-system started at 2022-03-11 15:42:21 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.923: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 11 17:26:47.923: INFO: vsphere-cloud-controller-manager-g9wgq from kube-system started at 2022-03-11 16:43:41 +0000 UTC (1 container statuses recorded)
Mar 11 17:26:47.923: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Mar 11 17:26:47.923: INFO: vsphere-csi-node-xgb98 from kube-system started at 2022-03-11 15:42:21 +0000 UTC (3 container statuses recorded)
Mar 11 17:26:47.923: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 11 17:26:47.923: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 11 17:26:47.923: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Mar 11 17:26:47.923: INFO: sonobuoy-systemd-logs-daemon-set-5218357723e74163-tgcrn from sonobuoy started at 2022-03-11 15:55:47 +0000 UTC (2 container statuses recorded)
Mar 11 17:26:47.923: INFO: 	Container sonobuoy-worker ready: false, restart count 10
Mar 11 17:26:47.923: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-dc0b9e31-cb1d-43ca-bba4-0ad35bd3465d 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 198.18.167.130 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-dc0b9e31-cb1d-43ca-bba4-0ad35bd3465d off the node 198.18.167.130
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dc0b9e31-cb1d-43ca-bba4-0ad35bd3465d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:31:54.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8029" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:306.285 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":339,"completed":223,"skipped":3639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:31:54.108: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Mar 11 17:31:54.187: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:31:56.193: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:31:57.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-428" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":339,"completed":224,"skipped":3700,"failed":0}
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:31:57.237: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Mar 11 17:31:57.340: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:31:57.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-308" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":339,"completed":225,"skipped":3710,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:31:57.386: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:31:58.141: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:32:01.206: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:32:01.216: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9881-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:32:05.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2567" for this suite.
STEP: Destroying namespace "webhook-2567-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.638 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":339,"completed":226,"skipped":3722,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:32:06.039: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:32:12.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-461" for this suite.
STEP: Destroying namespace "nsdeletetest-5614" for this suite.
Mar 11 17:32:12.404: INFO: Namespace nsdeletetest-5614 was already deleted
STEP: Destroying namespace "nsdeletetest-1894" for this suite.

• [SLOW TEST:6.374 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":339,"completed":227,"skipped":3758,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:32:12.417: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Mar 11 17:32:12.475: INFO: created test-pod-1
Mar 11 17:32:12.487: INFO: created test-pod-2
Mar 11 17:32:12.504: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:32:12.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7309" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":339,"completed":228,"skipped":3777,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:32:12.588: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-9562/secret-test-5b846a56-6764-4cdd-bbb0-d3b20eaeb446
STEP: Creating a pod to test consume secrets
Mar 11 17:32:12.667: INFO: Waiting up to 5m0s for pod "pod-configmaps-f19281cc-30fb-40e6-8a27-88eb92e7560e" in namespace "secrets-9562" to be "Succeeded or Failed"
Mar 11 17:32:12.672: INFO: Pod "pod-configmaps-f19281cc-30fb-40e6-8a27-88eb92e7560e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.457701ms
Mar 11 17:32:14.679: INFO: Pod "pod-configmaps-f19281cc-30fb-40e6-8a27-88eb92e7560e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011780112s
Mar 11 17:32:16.690: INFO: Pod "pod-configmaps-f19281cc-30fb-40e6-8a27-88eb92e7560e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02274212s
STEP: Saw pod success
Mar 11 17:32:16.690: INFO: Pod "pod-configmaps-f19281cc-30fb-40e6-8a27-88eb92e7560e" satisfied condition "Succeeded or Failed"
Mar 11 17:32:16.696: INFO: Trying to get logs from node 198.18.167.130 pod pod-configmaps-f19281cc-30fb-40e6-8a27-88eb92e7560e container env-test: <nil>
STEP: delete the pod
Mar 11 17:32:16.751: INFO: Waiting for pod pod-configmaps-f19281cc-30fb-40e6-8a27-88eb92e7560e to disappear
Mar 11 17:32:16.756: INFO: Pod pod-configmaps-f19281cc-30fb-40e6-8a27-88eb92e7560e no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:32:16.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9562" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":229,"skipped":3791,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:32:16.777: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:32:16.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3360" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":339,"completed":230,"skipped":3845,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:32:16.838: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 11 17:32:16.900: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9823  ff8b9cc8-24ac-4c9a-aebb-df706130623e 82441 0 2022-03-11 17:32:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-11 17:32:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 17:32:16.900: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9823  ff8b9cc8-24ac-4c9a-aebb-df706130623e 82442 0 2022-03-11 17:32:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-11 17:32:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 11 17:32:16.919: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9823  ff8b9cc8-24ac-4c9a-aebb-df706130623e 82443 0 2022-03-11 17:32:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-11 17:32:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 17:32:16.919: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9823  ff8b9cc8-24ac-4c9a-aebb-df706130623e 82444 0 2022-03-11 17:32:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-03-11 17:32:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:32:16.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9823" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":339,"completed":231,"skipped":3851,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:32:16.933: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1308
STEP: creating the pod
Mar 11 17:32:16.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7877 create -f -'
Mar 11 17:32:17.686: INFO: stderr: ""
Mar 11 17:32:17.686: INFO: stdout: "pod/pause created\n"
Mar 11 17:32:17.686: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 11 17:32:17.686: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7877" to be "running and ready"
Mar 11 17:32:17.692: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.166315ms
Mar 11 17:32:19.701: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.014905617s
Mar 11 17:32:19.701: INFO: Pod "pause" satisfied condition "running and ready"
Mar 11 17:32:19.701: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 11 17:32:19.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7877 label pods pause testing-label=testing-label-value'
Mar 11 17:32:19.852: INFO: stderr: ""
Mar 11 17:32:19.852: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 11 17:32:19.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7877 get pod pause -L testing-label'
Mar 11 17:32:19.960: INFO: stderr: ""
Mar 11 17:32:19.960: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 11 17:32:19.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7877 label pods pause testing-label-'
Mar 11 17:32:20.072: INFO: stderr: ""
Mar 11 17:32:20.072: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 11 17:32:20.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7877 get pod pause -L testing-label'
Mar 11 17:32:20.172: INFO: stderr: ""
Mar 11 17:32:20.172: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: using delete to clean up resources
Mar 11 17:32:20.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7877 delete --grace-period=0 --force -f -'
Mar 11 17:32:20.290: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 11 17:32:20.290: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 11 17:32:20.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7877 get rc,svc -l name=pause --no-headers'
Mar 11 17:32:20.391: INFO: stderr: "No resources found in kubectl-7877 namespace.\n"
Mar 11 17:32:20.391: INFO: stdout: ""
Mar 11 17:32:20.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7877 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 11 17:32:20.489: INFO: stderr: ""
Mar 11 17:32:20.489: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:32:20.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7877" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":339,"completed":232,"skipped":3867,"failed":0}
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:32:20.506: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:32:20.571: INFO: Waiting up to 5m0s for pod "busybox-user-65534-845f2f64-a06d-466e-b69a-28c1fd540c9b" in namespace "security-context-test-5165" to be "Succeeded or Failed"
Mar 11 17:32:20.574: INFO: Pod "busybox-user-65534-845f2f64-a06d-466e-b69a-28c1fd540c9b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.135576ms
Mar 11 17:32:22.580: INFO: Pod "busybox-user-65534-845f2f64-a06d-466e-b69a-28c1fd540c9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.008926725s
Mar 11 17:32:24.588: INFO: Pod "busybox-user-65534-845f2f64-a06d-466e-b69a-28c1fd540c9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017445648s
Mar 11 17:32:24.588: INFO: Pod "busybox-user-65534-845f2f64-a06d-466e-b69a-28c1fd540c9b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:32:24.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5165" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":233,"skipped":3871,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:32:24.608: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-6526300b-cfb0-413c-984e-bafc1a26381c
STEP: Creating secret with name s-test-opt-upd-c467dc01-2e21-4278-ae53-1016b1a04262
STEP: Creating the pod
Mar 11 17:32:24.699: INFO: The status of Pod pod-projected-secrets-81b0cc95-5676-4680-bcec-9bd7a4e1c970 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:32:26.707: INFO: The status of Pod pod-projected-secrets-81b0cc95-5676-4680-bcec-9bd7a4e1c970 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:32:28.704: INFO: The status of Pod pod-projected-secrets-81b0cc95-5676-4680-bcec-9bd7a4e1c970 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-6526300b-cfb0-413c-984e-bafc1a26381c
STEP: Updating secret s-test-opt-upd-c467dc01-2e21-4278-ae53-1016b1a04262
STEP: Creating secret with name s-test-opt-create-c0da9ada-714e-48e9-bb68-1659afd3d5ec
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:33:37.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5270" for this suite.

• [SLOW TEST:72.650 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":234,"skipped":3886,"failed":0}
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:33:37.258: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 11 17:33:37.375: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:37.376: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:37.385: INFO: Number of nodes with available pods: 0
Mar 11 17:33:37.385: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:38.395: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:38.395: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:38.401: INFO: Number of nodes with available pods: 0
Mar 11 17:33:38.401: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:39.393: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:39.393: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:39.397: INFO: Number of nodes with available pods: 2
Mar 11 17:33:39.398: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 11 17:33:39.428: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:39.428: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:39.431: INFO: Number of nodes with available pods: 1
Mar 11 17:33:39.431: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:40.440: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:40.440: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:40.446: INFO: Number of nodes with available pods: 1
Mar 11 17:33:40.446: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:41.438: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:41.438: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:41.445: INFO: Number of nodes with available pods: 1
Mar 11 17:33:41.445: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:42.440: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:42.440: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:42.450: INFO: Number of nodes with available pods: 1
Mar 11 17:33:42.450: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:43.442: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:43.442: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:43.447: INFO: Number of nodes with available pods: 1
Mar 11 17:33:43.447: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:44.443: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:44.443: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:44.448: INFO: Number of nodes with available pods: 1
Mar 11 17:33:44.448: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:45.442: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:45.442: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:45.447: INFO: Number of nodes with available pods: 1
Mar 11 17:33:45.447: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:46.442: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:46.442: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:46.447: INFO: Number of nodes with available pods: 1
Mar 11 17:33:46.447: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:47.442: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:47.442: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:47.447: INFO: Number of nodes with available pods: 1
Mar 11 17:33:47.447: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:48.440: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:48.440: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:48.443: INFO: Number of nodes with available pods: 1
Mar 11 17:33:48.443: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:49.440: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:49.440: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:49.444: INFO: Number of nodes with available pods: 1
Mar 11 17:33:49.444: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:50.442: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:50.442: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:50.448: INFO: Number of nodes with available pods: 1
Mar 11 17:33:50.448: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:33:51.442: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:51.442: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:33:51.448: INFO: Number of nodes with available pods: 2
Mar 11 17:33:51.448: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5546, will wait for the garbage collector to delete the pods
Mar 11 17:33:51.520: INFO: Deleting DaemonSet.extensions daemon-set took: 12.382957ms
Mar 11 17:33:51.620: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.108191ms
Mar 11 17:34:01.131: INFO: Number of nodes with available pods: 0
Mar 11 17:34:01.131: INFO: Number of running nodes: 0, number of available pods: 0
Mar 11 17:34:01.136: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"83737"},"items":null}

Mar 11 17:34:01.143: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"83737"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:34:01.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5546" for this suite.

• [SLOW TEST:23.919 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":339,"completed":235,"skipped":3886,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:34:01.177: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:34:01.264: INFO: created pod
Mar 11 17:34:01.264: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6345" to be "Succeeded or Failed"
Mar 11 17:34:01.268: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.788719ms
Mar 11 17:34:03.283: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019570357s
STEP: Saw pod success
Mar 11 17:34:03.284: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar 11 17:34:33.285: INFO: polling logs
Mar 11 17:34:33.299: INFO: Pod logs: 
2022/03/11 17:34:02 OK: Got token
2022/03/11 17:34:02 validating with in-cluster discovery
2022/03/11 17:34:02 OK: got issuer https://kubernetes.default.svc.cluster.local
2022/03/11 17:34:02 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6345:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1647020641, NotBefore:1647020041, IssuedAt:1647020041, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6345", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b1756a57-4644-458f-aba8-f7d9dd032ccb"}}}
2022/03/11 17:34:02 OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
2022/03/11 17:34:02 OK: Validated signature on JWT
2022/03/11 17:34:02 OK: Got valid claims from token!
2022/03/11 17:34:02 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6345:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1647020641, NotBefore:1647020041, IssuedAt:1647020041, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6345", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b1756a57-4644-458f-aba8-f7d9dd032ccb"}}}

Mar 11 17:34:33.299: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:34:33.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6345" for this suite.

• [SLOW TEST:32.155 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":339,"completed":236,"skipped":3895,"failed":0}
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:34:33.332: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 17:34:33.420: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef37eabb-89c2-42ef-8cfb-4a919248751d" in namespace "projected-1892" to be "Succeeded or Failed"
Mar 11 17:34:33.430: INFO: Pod "downwardapi-volume-ef37eabb-89c2-42ef-8cfb-4a919248751d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.637099ms
Mar 11 17:34:35.448: INFO: Pod "downwardapi-volume-ef37eabb-89c2-42ef-8cfb-4a919248751d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02762017s
STEP: Saw pod success
Mar 11 17:34:35.448: INFO: Pod "downwardapi-volume-ef37eabb-89c2-42ef-8cfb-4a919248751d" satisfied condition "Succeeded or Failed"
Mar 11 17:34:35.454: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-ef37eabb-89c2-42ef-8cfb-4a919248751d container client-container: <nil>
STEP: delete the pod
Mar 11 17:34:35.489: INFO: Waiting for pod downwardapi-volume-ef37eabb-89c2-42ef-8cfb-4a919248751d to disappear
Mar 11 17:34:35.496: INFO: Pod downwardapi-volume-ef37eabb-89c2-42ef-8cfb-4a919248751d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:34:35.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1892" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":237,"skipped":3895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:34:35.515: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1386
STEP: creating an pod
Mar 11 17:34:35.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-9247 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 11 17:34:35.695: INFO: stderr: ""
Mar 11 17:34:35.695: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Mar 11 17:34:35.695: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 11 17:34:35.695: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9247" to be "running and ready, or succeeded"
Mar 11 17:34:35.707: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 11.963608ms
Mar 11 17:34:37.716: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.020509788s
Mar 11 17:34:37.716: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 11 17:34:37.716: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 11 17:34:37.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-9247 logs logs-generator logs-generator'
Mar 11 17:34:37.823: INFO: stderr: ""
Mar 11 17:34:37.823: INFO: stdout: "I0311 17:34:36.801373       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/tsb 552\nI0311 17:34:37.001666       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/qbc 369\nI0311 17:34:37.202146       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/hb4 525\nI0311 17:34:37.401423       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/krfh 350\nI0311 17:34:37.601943       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/8dx 431\nI0311 17:34:37.801464       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/9vm 574\n"
STEP: limiting log lines
Mar 11 17:34:37.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-9247 logs logs-generator logs-generator --tail=1'
Mar 11 17:34:37.938: INFO: stderr: ""
Mar 11 17:34:37.938: INFO: stdout: "I0311 17:34:37.801464       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/9vm 574\n"
Mar 11 17:34:37.938: INFO: got output "I0311 17:34:37.801464       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/9vm 574\n"
STEP: limiting log bytes
Mar 11 17:34:37.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-9247 logs logs-generator logs-generator --limit-bytes=1'
Mar 11 17:34:38.055: INFO: stderr: ""
Mar 11 17:34:38.055: INFO: stdout: "I"
Mar 11 17:34:38.055: INFO: got output "I"
STEP: exposing timestamps
Mar 11 17:34:38.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-9247 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 11 17:34:38.190: INFO: stderr: ""
Mar 11 17:34:38.190: INFO: stdout: "2022-03-11T17:34:38.002102815Z I0311 17:34:38.001884       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/lkn6 268\n"
Mar 11 17:34:38.190: INFO: got output "2022-03-11T17:34:38.002102815Z I0311 17:34:38.001884       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/lkn6 268\n"
STEP: restricting to a time range
Mar 11 17:34:40.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-9247 logs logs-generator logs-generator --since=1s'
Mar 11 17:34:40.872: INFO: stderr: ""
Mar 11 17:34:40.872: INFO: stdout: "I0311 17:34:40.001941       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/4vc5 474\nI0311 17:34:40.202287       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/2l2 400\nI0311 17:34:40.401534       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/q7z 503\nI0311 17:34:40.601880       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/wpq 385\nI0311 17:34:40.802332       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/dsdx 215\n"
Mar 11 17:34:40.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-9247 logs logs-generator logs-generator --since=24h'
Mar 11 17:34:40.993: INFO: stderr: ""
Mar 11 17:34:40.993: INFO: stdout: "I0311 17:34:36.801373       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/tsb 552\nI0311 17:34:37.001666       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/qbc 369\nI0311 17:34:37.202146       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/hb4 525\nI0311 17:34:37.401423       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/krfh 350\nI0311 17:34:37.601943       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/8dx 431\nI0311 17:34:37.801464       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/9vm 574\nI0311 17:34:38.001884       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/lkn6 268\nI0311 17:34:38.202034       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/zwlh 301\nI0311 17:34:38.402471       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/gd7z 369\nI0311 17:34:38.601832       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/wjtx 512\nI0311 17:34:38.802203       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/p9dg 446\nI0311 17:34:39.001502       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/mz5 473\nI0311 17:34:39.202163       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/lwc 544\nI0311 17:34:39.401464       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/2j4 403\nI0311 17:34:39.602053       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/2zq 555\nI0311 17:34:39.802426       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/c86 543\nI0311 17:34:40.001941       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/4vc5 474\nI0311 17:34:40.202287       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/2l2 400\nI0311 17:34:40.401534       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/q7z 503\nI0311 17:34:40.601880       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/wpq 385\nI0311 17:34:40.802332       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/dsdx 215\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1391
Mar 11 17:34:40.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-9247 delete pod logs-generator'
Mar 11 17:34:51.111: INFO: stderr: ""
Mar 11 17:34:51.111: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:34:51.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9247" for this suite.

• [SLOW TEST:15.617 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1383
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":339,"completed":238,"skipped":3918,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:34:51.131: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:34:51.785: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:34:54.827: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:34:54.832: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:34:59.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6278" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.670 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":339,"completed":239,"skipped":3920,"failed":0}
S
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:34:59.804: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Mar 11 17:34:59.940: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
Mar 11 17:35:01.980: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:35:04.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-3730" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":339,"completed":240,"skipped":3921,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:35:04.116: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Mar 11 17:35:04.198: INFO: Waiting up to 5m0s for pod "var-expansion-64c4b614-5b8f-4d1d-9471-883e2df81b91" in namespace "var-expansion-7893" to be "Succeeded or Failed"
Mar 11 17:35:04.251: INFO: Pod "var-expansion-64c4b614-5b8f-4d1d-9471-883e2df81b91": Phase="Pending", Reason="", readiness=false. Elapsed: 53.008195ms
Mar 11 17:35:06.258: INFO: Pod "var-expansion-64c4b614-5b8f-4d1d-9471-883e2df81b91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05936841s
Mar 11 17:35:08.263: INFO: Pod "var-expansion-64c4b614-5b8f-4d1d-9471-883e2df81b91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064951425s
STEP: Saw pod success
Mar 11 17:35:08.263: INFO: Pod "var-expansion-64c4b614-5b8f-4d1d-9471-883e2df81b91" satisfied condition "Succeeded or Failed"
Mar 11 17:35:08.267: INFO: Trying to get logs from node 198.18.167.130 pod var-expansion-64c4b614-5b8f-4d1d-9471-883e2df81b91 container dapi-container: <nil>
STEP: delete the pod
Mar 11 17:35:08.304: INFO: Waiting for pod var-expansion-64c4b614-5b8f-4d1d-9471-883e2df81b91 to disappear
Mar 11 17:35:08.308: INFO: Pod var-expansion-64c4b614-5b8f-4d1d-9471-883e2df81b91 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:35:08.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7893" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":339,"completed":241,"skipped":3921,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:35:08.324: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Mar 11 17:35:08.399: INFO: PodSpec: initContainers in spec.initContainers
Mar 11 17:35:54.422: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-021adc91-66de-47cb-be2b-48240ffad5bc", GenerateName:"", Namespace:"init-container-1636", SelfLink:"", UID:"81291a3f-dd7c-481c-b2fd-d075b667506a", ResourceVersion:"85156", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63782616908, loc:(*time.Location)(0x9e12f00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"399226667"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002042030), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002042048)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc002042060), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002042078)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-7v785", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0089a6b80), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-7v785", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-7v785", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.4.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-7v785", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0054c5548), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"198.18.167.130", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc003750a10), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0054c55d0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0054c55f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0054c55f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0054c55fc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002f60ce0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782616908, loc:(*time.Location)(0x9e12f00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782616908, loc:(*time.Location)(0x9e12f00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782616908, loc:(*time.Location)(0x9e12f00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782616908, loc:(*time.Location)(0x9e12f00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"198.18.167.130", PodIP:"192.168.2.147", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.2.147"}}, StartTime:(*v1.Time)(0xc0020420a8), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003750af0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003750b60)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"containerd://7da527a43e6c7809399780eeff5f7f5a5e1f3d9f8b43326d5b5a26ff18bbd485", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0089a6c00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0089a6be0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.4.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0054c5674)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:35:54.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1636" for this suite.

• [SLOW TEST:46.121 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":339,"completed":242,"skipped":3947,"failed":0}
SS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:35:54.446: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:35:54.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4061" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":339,"completed":243,"skipped":3949,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:35:54.537: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Mar 11 17:35:54.616: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 11 17:35:59.625: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:35:59.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-639" for this suite.

• [SLOW TEST:5.222 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":339,"completed":244,"skipped":3949,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:35:59.763: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Mar 11 17:35:59.840: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 11 17:35:59.841: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 11 17:35:59.851: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 11 17:35:59.857: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 11 17:35:59.888: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 11 17:35:59.889: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 11 17:35:59.907: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 11 17:35:59.908: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 11 17:36:03.694: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 11 17:36:03.694: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 11 17:36:04.688: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Mar 11 17:36:04.708: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Mar 11 17:36:04.710: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0
Mar 11 17:36:04.710: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0
Mar 11 17:36:04.710: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0
Mar 11 17:36:04.710: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0
Mar 11 17:36:04.710: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0
Mar 11 17:36:04.711: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0
Mar 11 17:36:04.711: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0
Mar 11 17:36:04.711: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 0
Mar 11 17:36:04.711: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1
Mar 11 17:36:04.711: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1
Mar 11 17:36:04.711: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:04.711: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:04.711: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:04.711: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:04.725: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:04.725: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:04.757: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:04.757: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:04.831: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:04.836: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:04.845: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1
Mar 11 17:36:04.845: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1
Mar 11 17:36:09.503: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:09.503: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:09.570: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1
STEP: listing Deployments
Mar 11 17:36:09.578: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Mar 11 17:36:09.600: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Mar 11 17:36:09.613: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 11 17:36:09.617: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 11 17:36:09.658: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 11 17:36:09.703: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 11 17:36:09.730: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 11 17:36:12.516: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 11 17:36:12.535: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 11 17:36:12.550: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 11 17:36:12.573: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 11 17:36:12.588: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 11 17:36:15.540: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Mar 11 17:36:15.609: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1
Mar 11 17:36:15.610: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1
Mar 11 17:36:15.610: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1
Mar 11 17:36:15.610: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1
Mar 11 17:36:15.610: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 1
Mar 11 17:36:15.610: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:15.610: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:15.610: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:15.611: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:15.611: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 2
Mar 11 17:36:15.611: INFO: observed Deployment test-deployment in namespace deployment-7031 with ReadyReplicas 3
STEP: deleting the Deployment
Mar 11 17:36:15.624: INFO: observed event type MODIFIED
Mar 11 17:36:15.625: INFO: observed event type MODIFIED
Mar 11 17:36:15.625: INFO: observed event type MODIFIED
Mar 11 17:36:15.625: INFO: observed event type MODIFIED
Mar 11 17:36:15.625: INFO: observed event type MODIFIED
Mar 11 17:36:15.625: INFO: observed event type MODIFIED
Mar 11 17:36:15.626: INFO: observed event type MODIFIED
Mar 11 17:36:15.626: INFO: observed event type MODIFIED
Mar 11 17:36:15.626: INFO: observed event type MODIFIED
Mar 11 17:36:15.627: INFO: observed event type MODIFIED
Mar 11 17:36:15.627: INFO: observed event type MODIFIED
Mar 11 17:36:15.627: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Mar 11 17:36:15.632: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:36:15.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7031" for this suite.

• [SLOW TEST:15.913 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":339,"completed":245,"skipped":3950,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:36:15.683: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Mar 11 17:36:15.787: INFO: Found Service test-service-4zdwv in namespace services-6596 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar 11 17:36:15.787: INFO: Service test-service-4zdwv created
STEP: Getting /status
Mar 11 17:36:15.793: INFO: Service test-service-4zdwv has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Mar 11 17:36:15.816: INFO: observed Service test-service-4zdwv in namespace services-6596 with annotations: map[] & LoadBalancer: {[]}
Mar 11 17:36:15.816: INFO: Found Service test-service-4zdwv in namespace services-6596 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar 11 17:36:15.817: INFO: Service test-service-4zdwv has service status patched
STEP: updating the ServiceStatus
Mar 11 17:36:15.848: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Mar 11 17:36:15.854: INFO: Observed Service test-service-4zdwv in namespace services-6596 with annotations: map[] & Conditions: {[]}
Mar 11 17:36:15.855: INFO: Observed event: &Service{ObjectMeta:{test-service-4zdwv  services-6596  843bcd7e-e629-4f59-b999-d2fa82a8b8b4 85652 0 2022-03-11 17:36:15 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-03-11 17:36:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}},"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}}}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.100.157.193,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,TopologyKeys:[],IPFamilyPolicy:*SingleStack,ClusterIPs:[10.100.157.193],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:nil,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar 11 17:36:15.855: INFO: Found Service test-service-4zdwv in namespace services-6596 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 11 17:36:15.856: INFO: Service test-service-4zdwv has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Mar 11 17:36:15.878: INFO: observed Service test-service-4zdwv in namespace services-6596 with labels: map[test-service-static:true]
Mar 11 17:36:15.878: INFO: observed Service test-service-4zdwv in namespace services-6596 with labels: map[test-service-static:true]
Mar 11 17:36:15.878: INFO: observed Service test-service-4zdwv in namespace services-6596 with labels: map[test-service-static:true]
Mar 11 17:36:15.878: INFO: Found Service test-service-4zdwv in namespace services-6596 with labels: map[test-service:patched test-service-static:true]
Mar 11 17:36:15.878: INFO: Service test-service-4zdwv patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Mar 11 17:36:15.906: INFO: Observed event: ADDED
Mar 11 17:36:15.906: INFO: Observed event: MODIFIED
Mar 11 17:36:15.906: INFO: Observed event: MODIFIED
Mar 11 17:36:15.906: INFO: Observed event: MODIFIED
Mar 11 17:36:15.906: INFO: Found Service test-service-4zdwv in namespace services-6596 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar 11 17:36:15.906: INFO: Service test-service-4zdwv deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:36:15.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6596" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":339,"completed":246,"skipped":3959,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:36:15.928: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Mar 11 17:36:15.998: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 11 17:36:15.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7353 create -f -'
Mar 11 17:36:17.115: INFO: stderr: ""
Mar 11 17:36:17.115: INFO: stdout: "service/agnhost-replica created\n"
Mar 11 17:36:17.115: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 11 17:36:17.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7353 create -f -'
Mar 11 17:36:17.682: INFO: stderr: ""
Mar 11 17:36:17.682: INFO: stdout: "service/agnhost-primary created\n"
Mar 11 17:36:17.683: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 11 17:36:17.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7353 create -f -'
Mar 11 17:36:18.324: INFO: stderr: ""
Mar 11 17:36:18.324: INFO: stdout: "service/frontend created\n"
Mar 11 17:36:18.324: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 11 17:36:18.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7353 create -f -'
Mar 11 17:36:18.915: INFO: stderr: ""
Mar 11 17:36:18.915: INFO: stdout: "deployment.apps/frontend created\n"
Mar 11 17:36:18.916: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 11 17:36:18.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7353 create -f -'
Mar 11 17:36:19.365: INFO: stderr: ""
Mar 11 17:36:19.365: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 11 17:36:19.365: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 11 17:36:19.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7353 create -f -'
Mar 11 17:36:19.995: INFO: stderr: ""
Mar 11 17:36:19.995: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Mar 11 17:36:19.995: INFO: Waiting for all frontend pods to be Running.
Mar 11 17:36:30.052: INFO: Waiting for frontend to serve content.
Mar 11 17:36:30.065: INFO: Trying to add a new entry to the guestbook.
Mar 11 17:36:30.076: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 11 17:36:30.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7353 delete --grace-period=0 --force -f -'
Mar 11 17:36:30.223: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 11 17:36:30.223: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Mar 11 17:36:30.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7353 delete --grace-period=0 --force -f -'
Mar 11 17:36:30.378: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 11 17:36:30.378: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar 11 17:36:30.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7353 delete --grace-period=0 --force -f -'
Mar 11 17:36:30.489: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 11 17:36:30.489: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 11 17:36:30.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7353 delete --grace-period=0 --force -f -'
Mar 11 17:36:30.605: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 11 17:36:30.605: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 11 17:36:30.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7353 delete --grace-period=0 --force -f -'
Mar 11 17:36:30.739: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 11 17:36:30.739: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Mar 11 17:36:30.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7353 delete --grace-period=0 --force -f -'
Mar 11 17:36:30.856: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 11 17:36:30.856: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:36:30.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7353" for this suite.

• [SLOW TEST:14.941 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:336
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":339,"completed":247,"skipped":3998,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:36:30.870: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Mar 11 17:36:30.943: INFO: Waiting up to 5m0s for pod "var-expansion-4099379c-f905-4cb4-8d35-1ee8f5894920" in namespace "var-expansion-9123" to be "Succeeded or Failed"
Mar 11 17:36:30.957: INFO: Pod "var-expansion-4099379c-f905-4cb4-8d35-1ee8f5894920": Phase="Pending", Reason="", readiness=false. Elapsed: 14.175083ms
Mar 11 17:36:32.963: INFO: Pod "var-expansion-4099379c-f905-4cb4-8d35-1ee8f5894920": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020696905s
Mar 11 17:36:34.972: INFO: Pod "var-expansion-4099379c-f905-4cb4-8d35-1ee8f5894920": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02898109s
STEP: Saw pod success
Mar 11 17:36:34.972: INFO: Pod "var-expansion-4099379c-f905-4cb4-8d35-1ee8f5894920" satisfied condition "Succeeded or Failed"
Mar 11 17:36:34.976: INFO: Trying to get logs from node 198.18.167.130 pod var-expansion-4099379c-f905-4cb4-8d35-1ee8f5894920 container dapi-container: <nil>
STEP: delete the pod
Mar 11 17:36:35.004: INFO: Waiting for pod var-expansion-4099379c-f905-4cb4-8d35-1ee8f5894920 to disappear
Mar 11 17:36:35.013: INFO: Pod var-expansion-4099379c-f905-4cb4-8d35-1ee8f5894920 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:36:35.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9123" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":339,"completed":248,"skipped":4000,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:36:35.030: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:36:35.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-3542 create -f -'
Mar 11 17:36:35.700: INFO: stderr: ""
Mar 11 17:36:35.700: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 11 17:36:35.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-3542 create -f -'
Mar 11 17:36:36.335: INFO: stderr: ""
Mar 11 17:36:36.335: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Mar 11 17:36:37.345: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 11 17:36:37.345: INFO: Found 0 / 1
Mar 11 17:36:38.347: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 11 17:36:38.347: INFO: Found 1 / 1
Mar 11 17:36:38.347: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 11 17:36:38.351: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 11 17:36:38.351: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 11 17:36:38.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-3542 describe pod agnhost-primary-n2t75'
Mar 11 17:36:38.483: INFO: stderr: ""
Mar 11 17:36:38.483: INFO: stdout: "Name:         agnhost-primary-n2t75\nNamespace:    kubectl-3542\nPriority:     0\nNode:         198.18.167.130/198.18.167.130\nStart Time:   Fri, 11 Mar 2022 17:36:35 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           192.168.2.114\nIPs:\n  IP:           192.168.2.114\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6a09d0f517c6ebf3d2f7d6f73feac54afabcb688db8f05455e393b18ad20d52c\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 11 Mar 2022 17:36:37 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t99hm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-t99hm:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-3542/agnhost-primary-n2t75 to 198.18.167.130\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Mar 11 17:36:38.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-3542 describe rc agnhost-primary'
Mar 11 17:36:38.622: INFO: stderr: ""
Mar 11 17:36:38.622: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3542\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-n2t75\n"
Mar 11 17:36:38.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-3542 describe service agnhost-primary'
Mar 11 17:36:38.730: INFO: stderr: ""
Mar 11 17:36:38.730: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3542\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.99.126.30\nIPs:               10.99.126.30\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.2.114:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 11 17:36:38.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-3542 describe node 198.18.151.115'
Mar 11 17:36:38.876: INFO: stderr: ""
Mar 11 17:36:38.876: INFO: stdout: "Name:               198.18.151.115\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-8gb.os-unknown\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=198.18.151.115\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-8gb.os-unknown\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 198.18.151.115\n                    cluster.x-k8s.io/cluster-name: prod\n                    cluster.x-k8s.io/cluster-namespace: eksa-system\n                    cluster.x-k8s.io/machine: prod-wg78l\n                    cluster.x-k8s.io/owner-kind: KubeadmControlPlane\n                    cluster.x-k8s.io/owner-name: prod\n                    csi.volume.kubernetes.io/nodeid: {\"csi.vsphere.vmware.com\":\"198.18.151.115\"}\n                    io.cilium.network.ipv4-cilium-host: 192.168.3.8\n                    io.cilium.network.ipv4-pod-cidr: 192.168.3.0/24\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 11 Mar 2022 15:42:52 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  198.18.151.115\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 11 Mar 2022 17:36:30 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 11 Mar 2022 15:43:23 +0000   Fri, 11 Mar 2022 15:43:23 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Fri, 11 Mar 2022 17:36:35 +0000   Fri, 11 Mar 2022 15:42:52 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 11 Mar 2022 17:36:35 +0000   Fri, 11 Mar 2022 15:42:52 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 11 Mar 2022 17:36:35 +0000   Fri, 11 Mar 2022 15:42:52 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 11 Mar 2022 17:36:35 +0000   Fri, 11 Mar 2022 15:43:32 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  Hostname:    198.18.151.115\n  InternalIP:  198.18.151.115\n  ExternalIP:  198.18.151.115\nCapacity:\n  cpu:                2\n  ephemeral-storage:  20624592Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7872524Ki\n  pods:               110\nAllocatable:\n  cpu:                1930m\n  ephemeral-storage:  17933882132\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7770124Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 df8e1a42b22d21f460d04b191375d786\n  System UUID:                df8e1a42-b22d-21f4-60d0-4b191375d786\n  Boot ID:                    97c6548d-d85d-4a74-bc56-af5b062751d4\n  Kernel Version:             5.10.93\n  OS Image:                   Bottlerocket OS 1.5.3 (vmware-k8s-1.21)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.8+bottlerocket\n  Kubelet Version:            v1.21.6\n  Kube-Proxy Version:         v1.21.6\nPodCIDR:                      192.168.3.0/24\nPodCIDRs:                     192.168.3.0/24\nProviderID:                   vsphere://421a8edf-2db2-f421-60d0-4b191375d786\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  eksa-system                 eksa-controller-manager-5c74596687-whrvs                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         110m\n  kube-system                 cilium-9fbzh                                               100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         113m\n  kube-system                 cilium-operator-86d59d5c88-4rm2k                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         113m\n  kube-system                 kube-apiserver-198.18.151.115                              250m (12%)    0 (0%)      0 (0%)           0 (0%)         113m\n  kube-system                 kube-controller-manager-198.18.151.115                     200m (10%)    0 (0%)      0 (0%)           0 (0%)         113m\n  kube-system                 kube-proxy-ttx7j                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         113m\n  kube-system                 kube-scheduler-198.18.151.115                              100m (5%)     0 (0%)      0 (0%)           0 (0%)         113m\n  kube-system                 kube-vip-198.18.151.115                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         113m\n  kube-system                 vsphere-cloud-controller-manager-l8h24                     200m (10%)    0 (0%)      0 (0%)           0 (0%)         113m\n  kube-system                 vsphere-csi-node-fgg85                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         113m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-5218357723e74163-fgfck    0 (0%)        0 (0%)      0 (0%)           0 (0%)         100m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                850m (44%)  0 (0%)\n  memory             100Mi (1%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Mar 11 17:36:38.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-3542 describe namespace kubectl-3542'
Mar 11 17:36:38.989: INFO: stderr: ""
Mar 11 17:36:38.989: INFO: stdout: "Name:         kubectl-3542\nLabels:       e2e-framework=kubectl\n              e2e-run=84b524fd-3b1e-4ba7-8d3e-349d0681232f\n              kubernetes.io/metadata.name=kubectl-3542\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:36:38.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3542" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":339,"completed":249,"skipped":4001,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:36:39.004: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:36:39.560: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:36:42.599: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:36:42.606: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2292-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:36:47.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-714" for this suite.
STEP: Destroying namespace "webhook-714-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.581 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":339,"completed":250,"skipped":4017,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:36:47.599: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-1bc1d5a8-c789-4a80-b161-e848f1aaec7c
STEP: Creating a pod to test consume secrets
Mar 11 17:36:47.710: INFO: Waiting up to 5m0s for pod "pod-secrets-ef47f0b1-9fdc-4d1c-af89-3b1cf9a54165" in namespace "secrets-8618" to be "Succeeded or Failed"
Mar 11 17:36:47.713: INFO: Pod "pod-secrets-ef47f0b1-9fdc-4d1c-af89-3b1cf9a54165": Phase="Pending", Reason="", readiness=false. Elapsed: 3.271219ms
Mar 11 17:36:49.724: INFO: Pod "pod-secrets-ef47f0b1-9fdc-4d1c-af89-3b1cf9a54165": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014573772s
Mar 11 17:36:51.731: INFO: Pod "pod-secrets-ef47f0b1-9fdc-4d1c-af89-3b1cf9a54165": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02079677s
STEP: Saw pod success
Mar 11 17:36:51.731: INFO: Pod "pod-secrets-ef47f0b1-9fdc-4d1c-af89-3b1cf9a54165" satisfied condition "Succeeded or Failed"
Mar 11 17:36:51.736: INFO: Trying to get logs from node 198.18.167.130 pod pod-secrets-ef47f0b1-9fdc-4d1c-af89-3b1cf9a54165 container secret-volume-test: <nil>
STEP: delete the pod
Mar 11 17:36:51.769: INFO: Waiting for pod pod-secrets-ef47f0b1-9fdc-4d1c-af89-3b1cf9a54165 to disappear
Mar 11 17:36:51.775: INFO: Pod pod-secrets-ef47f0b1-9fdc-4d1c-af89-3b1cf9a54165 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:36:51.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8618" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":251,"skipped":4023,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:36:51.797: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:37:14.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1016" for this suite.

• [SLOW TEST:22.532 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":339,"completed":252,"skipped":4057,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:37:14.329: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:37:22.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7090" for this suite.

• [SLOW TEST:8.088 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":339,"completed":253,"skipped":4068,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:37:22.418: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:37:22.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8592" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":339,"completed":254,"skipped":4071,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:37:22.543: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:37:23.108: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:37:26.156: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:37:26.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6394" for this suite.
STEP: Destroying namespace "webhook-6394-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":339,"completed":255,"skipped":4082,"failed":0}
SSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:37:26.359: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:37:26.435: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-41f63ec0-e73e-4393-920c-ba8b2cc9b2fd" in namespace "security-context-test-9158" to be "Succeeded or Failed"
Mar 11 17:37:26.448: INFO: Pod "busybox-privileged-false-41f63ec0-e73e-4393-920c-ba8b2cc9b2fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.727114ms
Mar 11 17:37:28.455: INFO: Pod "busybox-privileged-false-41f63ec0-e73e-4393-920c-ba8b2cc9b2fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019670678s
Mar 11 17:37:28.455: INFO: Pod "busybox-privileged-false-41f63ec0-e73e-4393-920c-ba8b2cc9b2fd" satisfied condition "Succeeded or Failed"
Mar 11 17:37:28.470: INFO: Got logs for pod "busybox-privileged-false-41f63ec0-e73e-4393-920c-ba8b2cc9b2fd": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:37:28.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9158" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":256,"skipped":4085,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:37:28.489: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-69223148-0dca-43c5-aecb-8d988c58b8f3
STEP: Creating a pod to test consume configMaps
Mar 11 17:37:28.552: INFO: Waiting up to 5m0s for pod "pod-configmaps-70280d7f-634f-4e2e-9ade-41ae3bd3f79d" in namespace "configmap-9359" to be "Succeeded or Failed"
Mar 11 17:37:28.555: INFO: Pod "pod-configmaps-70280d7f-634f-4e2e-9ade-41ae3bd3f79d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477376ms
Mar 11 17:37:30.562: INFO: Pod "pod-configmaps-70280d7f-634f-4e2e-9ade-41ae3bd3f79d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009865958s
Mar 11 17:37:32.569: INFO: Pod "pod-configmaps-70280d7f-634f-4e2e-9ade-41ae3bd3f79d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017072119s
STEP: Saw pod success
Mar 11 17:37:32.569: INFO: Pod "pod-configmaps-70280d7f-634f-4e2e-9ade-41ae3bd3f79d" satisfied condition "Succeeded or Failed"
Mar 11 17:37:32.572: INFO: Trying to get logs from node 198.18.167.130 pod pod-configmaps-70280d7f-634f-4e2e-9ade-41ae3bd3f79d container agnhost-container: <nil>
STEP: delete the pod
Mar 11 17:37:32.602: INFO: Waiting for pod pod-configmaps-70280d7f-634f-4e2e-9ade-41ae3bd3f79d to disappear
Mar 11 17:37:32.606: INFO: Pod pod-configmaps-70280d7f-634f-4e2e-9ade-41ae3bd3f79d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:37:32.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9359" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":257,"skipped":4102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:37:32.623: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-47b5f753-e46b-4234-b1af-789c7619b2c1
STEP: Creating configMap with name cm-test-opt-upd-59021457-8fcf-4a97-adf3-03c9dcc9b86f
STEP: Creating the pod
Mar 11 17:37:32.721: INFO: The status of Pod pod-configmaps-ef95eabb-57be-4bf0-84c7-11ed284818d5 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:37:34.729: INFO: The status of Pod pod-configmaps-ef95eabb-57be-4bf0-84c7-11ed284818d5 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:37:36.730: INFO: The status of Pod pod-configmaps-ef95eabb-57be-4bf0-84c7-11ed284818d5 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-47b5f753-e46b-4234-b1af-789c7619b2c1
STEP: Updating configmap cm-test-opt-upd-59021457-8fcf-4a97-adf3-03c9dcc9b86f
STEP: Creating configMap with name cm-test-opt-create-c253c949-9855-4c09-9a4c-ed1bd0c4d19e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:38:49.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9456" for this suite.

• [SLOW TEST:76.562 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":258,"skipped":4165,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:38:49.187: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6821
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6821
I0311 17:38:49.338155      19 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6821, replica count: 2
I0311 17:38:52.390987      19 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 11 17:38:52.391: INFO: Creating new exec pod
Mar 11 17:38:55.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-6821 exec execpod99vff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 11 17:38:55.635: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 11 17:38:55.635: INFO: stdout: ""
Mar 11 17:38:56.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-6821 exec execpod99vff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 11 17:38:56.842: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 11 17:38:56.842: INFO: stdout: ""
Mar 11 17:38:57.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-6821 exec execpod99vff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 11 17:38:57.818: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 11 17:38:57.818: INFO: stdout: ""
Mar 11 17:38:58.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-6821 exec execpod99vff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 11 17:38:58.818: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 11 17:38:58.818: INFO: stdout: ""
Mar 11 17:38:59.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-6821 exec execpod99vff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 11 17:38:59.835: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 11 17:38:59.835: INFO: stdout: "externalname-service-dbpmr"
Mar 11 17:38:59.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-6821 exec execpod99vff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.210.192 80'
Mar 11 17:39:00.031: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.210.192 80\nConnection to 10.100.210.192 80 port [tcp/http] succeeded!\n"
Mar 11 17:39:00.031: INFO: stdout: ""
Mar 11 17:39:01.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-6821 exec execpod99vff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.210.192 80'
Mar 11 17:39:01.264: INFO: stderr: "+ nc -v -t -w 2 10.100.210.192 80\n+ echo hostName\nConnection to 10.100.210.192 80 port [tcp/http] succeeded!\n"
Mar 11 17:39:01.264: INFO: stdout: "externalname-service-dbpmr"
Mar 11 17:39:01.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-6821 exec execpod99vff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 198.18.16.160 30709'
Mar 11 17:39:01.456: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 198.18.16.160 30709\nConnection to 198.18.16.160 30709 port [tcp/*] succeeded!\n"
Mar 11 17:39:01.456: INFO: stdout: "externalname-service-dbpmr"
Mar 11 17:39:01.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-6821 exec execpod99vff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 198.18.167.130 30709'
Mar 11 17:39:01.681: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 198.18.167.130 30709\nConnection to 198.18.167.130 30709 port [tcp/*] succeeded!\n"
Mar 11 17:39:01.681: INFO: stdout: ""
Mar 11 17:39:02.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-6821 exec execpod99vff -- /bin/sh -x -c echo hostName | nc -v -t -w 2 198.18.167.130 30709'
Mar 11 17:39:02.892: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 198.18.167.130 30709\nConnection to 198.18.167.130 30709 port [tcp/*] succeeded!\n"
Mar 11 17:39:02.892: INFO: stdout: "externalname-service-9pqq7"
Mar 11 17:39:02.892: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:39:02.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6821" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:13.812 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":339,"completed":259,"skipped":4167,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:39:02.999: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:39:16.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3256" for this suite.

• [SLOW TEST:13.168 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":339,"completed":260,"skipped":4181,"failed":0}
SS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:39:16.169: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Mar 11 17:39:16.246: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 11 17:40:16.346: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:40:16.352: INFO: Starting informer...
STEP: Starting pods...
Mar 11 17:40:16.575: INFO: Pod1 is running on 198.18.167.130. Tainting Node
Mar 11 17:40:18.810: INFO: Pod2 is running on 198.18.167.130. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar 11 17:40:25.320: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 11 17:40:51.115: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:40:51.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-331" for this suite.

• [SLOW TEST:95.020 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":339,"completed":261,"skipped":4183,"failed":0}
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:40:51.190: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 11 17:40:51.276: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4357  0b8ce808-e270-4387-8c9e-487479955f49 89551 0 2022-03-11 17:40:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-11 17:40:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 17:40:51.276: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4357  0b8ce808-e270-4387-8c9e-487479955f49 89552 0 2022-03-11 17:40:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-11 17:40:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 17:40:51.276: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4357  0b8ce808-e270-4387-8c9e-487479955f49 89553 0 2022-03-11 17:40:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-11 17:40:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 11 17:41:01.321: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4357  0b8ce808-e270-4387-8c9e-487479955f49 89677 0 2022-03-11 17:40:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-11 17:40:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 17:41:01.321: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4357  0b8ce808-e270-4387-8c9e-487479955f49 89678 0 2022-03-11 17:40:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-11 17:40:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 11 17:41:01.321: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4357  0b8ce808-e270-4387-8c9e-487479955f49 89679 0 2022-03-11 17:40:51 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-03-11 17:40:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:41:01.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4357" for this suite.

• [SLOW TEST:10.158 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":339,"completed":262,"skipped":4183,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:41:01.352: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:41:01.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2754" for this suite.
STEP: Destroying namespace "nspatchtest-afaa729d-d181-4ed3-8a6b-570e60a4260f-9910" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":339,"completed":263,"skipped":4185,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:41:01.528: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Mar 11 17:41:01.583: INFO: Waiting up to 5m0s for pod "security-context-2c515a26-b424-467e-91b3-0201feb6c9e9" in namespace "security-context-1601" to be "Succeeded or Failed"
Mar 11 17:41:01.593: INFO: Pod "security-context-2c515a26-b424-467e-91b3-0201feb6c9e9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.915581ms
Mar 11 17:41:03.600: INFO: Pod "security-context-2c515a26-b424-467e-91b3-0201feb6c9e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01692282s
STEP: Saw pod success
Mar 11 17:41:03.600: INFO: Pod "security-context-2c515a26-b424-467e-91b3-0201feb6c9e9" satisfied condition "Succeeded or Failed"
Mar 11 17:41:03.611: INFO: Trying to get logs from node 198.18.167.130 pod security-context-2c515a26-b424-467e-91b3-0201feb6c9e9 container test-container: <nil>
STEP: delete the pod
Mar 11 17:41:03.656: INFO: Waiting for pod security-context-2c515a26-b424-467e-91b3-0201feb6c9e9 to disappear
Mar 11 17:41:03.661: INFO: Pod security-context-2c515a26-b424-467e-91b3-0201feb6c9e9 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:41:03.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1601" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":264,"skipped":4195,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:41:03.673: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:41:03.735: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 11 17:41:08.742: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 11 17:41:08.742: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Mar 11 17:41:10.788: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6140  27d99c12-3939-4d7d-8a90-c0d2293bbf13 89886 1 2022-03-11 17:41:08 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-03-11 17:41:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-11 17:41:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000f37c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-03-11 17:41:08 +0000 UTC,LastTransitionTime:2022-03-11 17:41:08 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-5b4d99b59b" has successfully progressed.,LastUpdateTime:2022-03-11 17:41:10 +0000 UTC,LastTransitionTime:2022-03-11 17:41:08 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 11 17:41:10.791: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-6140  50cb0a01-320b-4db5-95f8-77250788b0d7 89875 1 2022-03-11 17:41:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 27d99c12-3939-4d7d-8a90-c0d2293bbf13 0xc0097ce057 0xc0097ce058}] []  [{kube-controller-manager Update apps/v1 2022-03-11 17:41:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27d99c12-3939-4d7d-8a90-c0d2293bbf13\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0097ce0e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 11 17:41:10.794: INFO: Pod "test-cleanup-deployment-5b4d99b59b-cbh4x" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-cbh4x test-cleanup-deployment-5b4d99b59b- deployment-6140  8b20df80-6244-4426-b797-5d6bd1d70e95 89874 0 2022-03-11 17:41:08 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b 50cb0a01-320b-4db5-95f8-77250788b0d7 0xc0097ce467 0xc0097ce468}] []  [{kube-controller-manager Update v1 2022-03-11 17:41:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50cb0a01-320b-4db5-95f8-77250788b0d7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:41:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2l7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2l7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.167.130,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:41:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:41:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:41:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:41:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.167.130,PodIP:192.168.2.163,StartTime:2022-03-11 17:41:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 17:41:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://fc370ab88e49f217d65e837aab82e8964d334e967455f806230950eaae5d1576,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:41:10.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6140" for this suite.

• [SLOW TEST:7.136 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":339,"completed":265,"skipped":4207,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:41:10.810: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Mar 11 17:41:10.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 create -f -'
Mar 11 17:41:11.419: INFO: stderr: ""
Mar 11 17:41:11.419: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 11 17:41:11.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 11 17:41:11.528: INFO: stderr: ""
Mar 11 17:41:11.528: INFO: stdout: "update-demo-nautilus-gn87f update-demo-nautilus-ngf45 "
Mar 11 17:41:11.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-gn87f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 11 17:41:11.622: INFO: stderr: ""
Mar 11 17:41:11.622: INFO: stdout: ""
Mar 11 17:41:11.622: INFO: update-demo-nautilus-gn87f is created but not running
Mar 11 17:41:16.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 11 17:41:16.718: INFO: stderr: ""
Mar 11 17:41:16.718: INFO: stdout: "update-demo-nautilus-gn87f update-demo-nautilus-ngf45 "
Mar 11 17:41:16.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-gn87f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 11 17:41:16.827: INFO: stderr: ""
Mar 11 17:41:16.827: INFO: stdout: ""
Mar 11 17:41:16.827: INFO: update-demo-nautilus-gn87f is created but not running
Mar 11 17:41:21.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 11 17:41:21.922: INFO: stderr: ""
Mar 11 17:41:21.922: INFO: stdout: "update-demo-nautilus-gn87f update-demo-nautilus-ngf45 "
Mar 11 17:41:21.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-gn87f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 11 17:41:22.023: INFO: stderr: ""
Mar 11 17:41:22.023: INFO: stdout: ""
Mar 11 17:41:22.023: INFO: update-demo-nautilus-gn87f is created but not running
Mar 11 17:41:27.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 11 17:41:27.118: INFO: stderr: ""
Mar 11 17:41:27.118: INFO: stdout: "update-demo-nautilus-gn87f update-demo-nautilus-ngf45 "
Mar 11 17:41:27.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-gn87f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 11 17:41:27.263: INFO: stderr: ""
Mar 11 17:41:27.263: INFO: stdout: "true"
Mar 11 17:41:27.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-gn87f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 11 17:41:27.373: INFO: stderr: ""
Mar 11 17:41:27.373: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar 11 17:41:27.373: INFO: validating pod update-demo-nautilus-gn87f
Mar 11 17:41:27.379: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 11 17:41:27.379: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 11 17:41:27.379: INFO: update-demo-nautilus-gn87f is verified up and running
Mar 11 17:41:27.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-ngf45 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 11 17:41:27.477: INFO: stderr: ""
Mar 11 17:41:27.477: INFO: stdout: "true"
Mar 11 17:41:27.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-ngf45 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 11 17:41:27.576: INFO: stderr: ""
Mar 11 17:41:27.576: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar 11 17:41:27.576: INFO: validating pod update-demo-nautilus-ngf45
Mar 11 17:41:27.582: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 11 17:41:27.582: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 11 17:41:27.582: INFO: update-demo-nautilus-ngf45 is verified up and running
STEP: scaling down the replication controller
Mar 11 17:41:27.586: INFO: scanned /root for discovery docs: <nil>
Mar 11 17:41:27.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 11 17:41:28.740: INFO: stderr: ""
Mar 11 17:41:28.740: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 11 17:41:28.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 11 17:41:28.846: INFO: stderr: ""
Mar 11 17:41:28.846: INFO: stdout: "update-demo-nautilus-gn87f update-demo-nautilus-ngf45 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 11 17:41:33.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 11 17:41:33.948: INFO: stderr: ""
Mar 11 17:41:33.948: INFO: stdout: "update-demo-nautilus-ngf45 "
Mar 11 17:41:33.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-ngf45 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 11 17:41:34.037: INFO: stderr: ""
Mar 11 17:41:34.037: INFO: stdout: "true"
Mar 11 17:41:34.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-ngf45 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 11 17:41:34.127: INFO: stderr: ""
Mar 11 17:41:34.127: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar 11 17:41:34.127: INFO: validating pod update-demo-nautilus-ngf45
Mar 11 17:41:34.132: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 11 17:41:34.132: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 11 17:41:34.132: INFO: update-demo-nautilus-ngf45 is verified up and running
STEP: scaling up the replication controller
Mar 11 17:41:34.135: INFO: scanned /root for discovery docs: <nil>
Mar 11 17:41:34.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 11 17:41:35.282: INFO: stderr: ""
Mar 11 17:41:35.282: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 11 17:41:35.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 11 17:41:35.393: INFO: stderr: ""
Mar 11 17:41:35.393: INFO: stdout: "update-demo-nautilus-df5c4 update-demo-nautilus-ngf45 "
Mar 11 17:41:35.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-df5c4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 11 17:41:35.485: INFO: stderr: ""
Mar 11 17:41:35.485: INFO: stdout: ""
Mar 11 17:41:35.485: INFO: update-demo-nautilus-df5c4 is created but not running
Mar 11 17:41:40.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 11 17:41:40.586: INFO: stderr: ""
Mar 11 17:41:40.586: INFO: stdout: "update-demo-nautilus-df5c4 update-demo-nautilus-ngf45 "
Mar 11 17:41:40.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-df5c4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 11 17:41:40.673: INFO: stderr: ""
Mar 11 17:41:40.673: INFO: stdout: "true"
Mar 11 17:41:40.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-df5c4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 11 17:41:40.768: INFO: stderr: ""
Mar 11 17:41:40.768: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar 11 17:41:40.768: INFO: validating pod update-demo-nautilus-df5c4
Mar 11 17:41:40.776: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 11 17:41:40.776: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 11 17:41:40.776: INFO: update-demo-nautilus-df5c4 is verified up and running
Mar 11 17:41:40.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-ngf45 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 11 17:41:40.862: INFO: stderr: ""
Mar 11 17:41:40.862: INFO: stdout: "true"
Mar 11 17:41:40.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods update-demo-nautilus-ngf45 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 11 17:41:40.975: INFO: stderr: ""
Mar 11 17:41:40.975: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar 11 17:41:40.975: INFO: validating pod update-demo-nautilus-ngf45
Mar 11 17:41:40.980: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 11 17:41:40.980: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 11 17:41:40.981: INFO: update-demo-nautilus-ngf45 is verified up and running
STEP: using delete to clean up resources
Mar 11 17:41:40.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 delete --grace-period=0 --force -f -'
Mar 11 17:41:41.124: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 11 17:41:41.124: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 11 17:41:41.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get rc,svc -l name=update-demo --no-headers'
Mar 11 17:41:41.238: INFO: stderr: "No resources found in kubectl-1953 namespace.\n"
Mar 11 17:41:41.238: INFO: stdout: ""
Mar 11 17:41:41.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-1953 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 11 17:41:41.343: INFO: stderr: ""
Mar 11 17:41:41.343: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:41:41.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1953" for this suite.

• [SLOW TEST:30.563 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":339,"completed":266,"skipped":4219,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:41:41.375: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 17:41:41.459: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae05f58c-cc05-482b-bdd3-ae0edf3cdefa" in namespace "projected-3031" to be "Succeeded or Failed"
Mar 11 17:41:41.470: INFO: Pod "downwardapi-volume-ae05f58c-cc05-482b-bdd3-ae0edf3cdefa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.182503ms
Mar 11 17:41:43.480: INFO: Pod "downwardapi-volume-ae05f58c-cc05-482b-bdd3-ae0edf3cdefa": Phase="Running", Reason="", readiness=true. Elapsed: 2.020467258s
Mar 11 17:41:45.491: INFO: Pod "downwardapi-volume-ae05f58c-cc05-482b-bdd3-ae0edf3cdefa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030756789s
STEP: Saw pod success
Mar 11 17:41:45.491: INFO: Pod "downwardapi-volume-ae05f58c-cc05-482b-bdd3-ae0edf3cdefa" satisfied condition "Succeeded or Failed"
Mar 11 17:41:45.494: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-ae05f58c-cc05-482b-bdd3-ae0edf3cdefa container client-container: <nil>
STEP: delete the pod
Mar 11 17:41:45.527: INFO: Waiting for pod downwardapi-volume-ae05f58c-cc05-482b-bdd3-ae0edf3cdefa to disappear
Mar 11 17:41:45.540: INFO: Pod downwardapi-volume-ae05f58c-cc05-482b-bdd3-ae0edf3cdefa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:41:45.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3031" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":267,"skipped":4238,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:41:45.563: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:42:13.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1514" for this suite.

• [SLOW TEST:28.167 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":339,"completed":268,"skipped":4246,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:42:13.730: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 17:42:13.801: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a1a43eb-cdf9-41a6-88b7-ad55c859dcaf" in namespace "downward-api-5736" to be "Succeeded or Failed"
Mar 11 17:42:13.805: INFO: Pod "downwardapi-volume-3a1a43eb-cdf9-41a6-88b7-ad55c859dcaf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.116263ms
Mar 11 17:42:15.814: INFO: Pod "downwardapi-volume-3a1a43eb-cdf9-41a6-88b7-ad55c859dcaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013004586s
STEP: Saw pod success
Mar 11 17:42:15.815: INFO: Pod "downwardapi-volume-3a1a43eb-cdf9-41a6-88b7-ad55c859dcaf" satisfied condition "Succeeded or Failed"
Mar 11 17:42:15.818: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-3a1a43eb-cdf9-41a6-88b7-ad55c859dcaf container client-container: <nil>
STEP: delete the pod
Mar 11 17:42:15.850: INFO: Waiting for pod downwardapi-volume-3a1a43eb-cdf9-41a6-88b7-ad55c859dcaf to disappear
Mar 11 17:42:15.854: INFO: Pod downwardapi-volume-3a1a43eb-cdf9-41a6-88b7-ad55c859dcaf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:42:15.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5736" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":269,"skipped":4261,"failed":0}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:42:15.871: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-9950
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 11 17:42:15.923: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 11 17:42:15.986: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:42:17.999: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:42:20.000: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:42:21.999: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:42:24.000: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:42:25.997: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:42:27.995: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:42:29.995: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:42:32.055: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:42:33.994: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:42:36.005: INFO: The status of Pod netserver-0 is Running (Ready = false)
Mar 11 17:42:37.994: INFO: The status of Pod netserver-0 is Running (Ready = true)
Mar 11 17:42:38.008: INFO: The status of Pod netserver-1 is Running (Ready = true)
STEP: Creating test pods
Mar 11 17:42:40.071: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Mar 11 17:42:40.071: INFO: Going to poll 192.168.1.31 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Mar 11 17:42:40.074: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.1.31 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9950 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:42:40.074: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:42:41.163: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 11 17:42:41.163: INFO: Going to poll 192.168.2.4 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Mar 11 17:42:41.171: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.2.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9950 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:42:41.171: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:42:42.255: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:42:42.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9950" for this suite.

• [SLOW TEST:26.399 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":270,"skipped":4266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:42:42.272: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Mar 11 17:42:42.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-2285 api-versions'
Mar 11 17:42:42.426: INFO: stderr: ""
Mar 11 17:42:42.426: INFO: stdout: "acme.cert-manager.io/v1\nacme.cert-manager.io/v1alpha2\nacme.cert-manager.io/v1alpha3\nacme.cert-manager.io/v1beta1\naddons.cluster.x-k8s.io/v1alpha3\naddons.cluster.x-k8s.io/v1alpha4\naddons.cluster.x-k8s.io/v1beta1\nadmissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\nanywhere.eks.amazonaws.com/v1alpha1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbootstrap.cluster.x-k8s.io/v1alpha3\nbootstrap.cluster.x-k8s.io/v1alpha4\nbootstrap.cluster.x-k8s.io/v1beta1\ncert-manager.io/v1\ncert-manager.io/v1alpha2\ncert-manager.io/v1alpha3\ncert-manager.io/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncilium.io/v2\ncluster.x-k8s.io/v1alpha3\ncluster.x-k8s.io/v1alpha4\ncluster.x-k8s.io/v1beta1\nclusterctl.cluster.x-k8s.io/v1alpha3\ncontrolplane.cluster.x-k8s.io/v1alpha3\ncontrolplane.cluster.x-k8s.io/v1alpha4\ncontrolplane.cluster.x-k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\netcdcluster.cluster.x-k8s.io/v1alpha3\netcdcluster.cluster.x-k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\ninfrastructure.cluster.x-k8s.io/v1alpha3\ninfrastructure.cluster.x-k8s.io/v1alpha4\ninfrastructure.cluster.x-k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:42:42.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2285" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":339,"completed":271,"skipped":4305,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:42:42.442: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3317 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3317;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3317 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3317;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3317.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3317.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3317.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3317.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3317.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3317.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3317.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3317.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3317.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3317.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3317.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 53.118.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.118.53_udp@PTR;check="$$(dig +tcp +noall +answer +search 53.118.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.118.53_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3317 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3317;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3317 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3317;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3317.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3317.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3317.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3317.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3317.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3317.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3317.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3317.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3317.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3317.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3317.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3317.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 53.118.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.118.53_udp@PTR;check="$$(dig +tcp +noall +answer +search 53.118.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.118.53_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 11 17:42:46.563: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.570: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.574: INFO: Unable to read wheezy_udp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.579: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.583: INFO: Unable to read wheezy_udp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.587: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.591: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.603: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.643: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.655: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.661: INFO: Unable to read jessie_udp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.665: INFO: Unable to read jessie_tcp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.668: INFO: Unable to read jessie_udp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.672: INFO: Unable to read jessie_tcp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.675: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.679: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:46.709: INFO: Lookups using dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3317 wheezy_tcp@dns-test-service.dns-3317 wheezy_udp@dns-test-service.dns-3317.svc wheezy_tcp@dns-test-service.dns-3317.svc wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3317 jessie_tcp@dns-test-service.dns-3317 jessie_udp@dns-test-service.dns-3317.svc jessie_tcp@dns-test-service.dns-3317.svc jessie_udp@_http._tcp.dns-test-service.dns-3317.svc jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc]

Mar 11 17:42:51.718: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.722: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.726: INFO: Unable to read wheezy_udp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.730: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.734: INFO: Unable to read wheezy_udp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.738: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.742: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.747: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.777: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.782: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.786: INFO: Unable to read jessie_udp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.790: INFO: Unable to read jessie_tcp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.794: INFO: Unable to read jessie_udp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.798: INFO: Unable to read jessie_tcp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.802: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.806: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:51.828: INFO: Lookups using dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3317 wheezy_tcp@dns-test-service.dns-3317 wheezy_udp@dns-test-service.dns-3317.svc wheezy_tcp@dns-test-service.dns-3317.svc wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3317 jessie_tcp@dns-test-service.dns-3317 jessie_udp@dns-test-service.dns-3317.svc jessie_tcp@dns-test-service.dns-3317.svc jessie_udp@_http._tcp.dns-test-service.dns-3317.svc jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc]

Mar 11 17:42:56.717: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.722: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.726: INFO: Unable to read wheezy_udp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.730: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.733: INFO: Unable to read wheezy_udp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.737: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.741: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.745: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.776: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.780: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.784: INFO: Unable to read jessie_udp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.788: INFO: Unable to read jessie_tcp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.792: INFO: Unable to read jessie_udp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.797: INFO: Unable to read jessie_tcp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.801: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.805: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:42:56.828: INFO: Lookups using dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3317 wheezy_tcp@dns-test-service.dns-3317 wheezy_udp@dns-test-service.dns-3317.svc wheezy_tcp@dns-test-service.dns-3317.svc wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3317 jessie_tcp@dns-test-service.dns-3317 jessie_udp@dns-test-service.dns-3317.svc jessie_tcp@dns-test-service.dns-3317.svc jessie_udp@_http._tcp.dns-test-service.dns-3317.svc jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc]

Mar 11 17:43:01.718: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.727: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.732: INFO: Unable to read wheezy_udp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.737: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.742: INFO: Unable to read wheezy_udp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.747: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.751: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.757: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.794: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.802: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.808: INFO: Unable to read jessie_udp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.813: INFO: Unable to read jessie_tcp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.817: INFO: Unable to read jessie_udp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.821: INFO: Unable to read jessie_tcp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.827: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.834: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:01.865: INFO: Lookups using dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3317 wheezy_tcp@dns-test-service.dns-3317 wheezy_udp@dns-test-service.dns-3317.svc wheezy_tcp@dns-test-service.dns-3317.svc wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3317 jessie_tcp@dns-test-service.dns-3317 jessie_udp@dns-test-service.dns-3317.svc jessie_tcp@dns-test-service.dns-3317.svc jessie_udp@_http._tcp.dns-test-service.dns-3317.svc jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc]

Mar 11 17:43:06.726: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.731: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.735: INFO: Unable to read wheezy_udp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.738: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.742: INFO: Unable to read wheezy_udp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.745: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.749: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.752: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.777: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.781: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.784: INFO: Unable to read jessie_udp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.789: INFO: Unable to read jessie_tcp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.792: INFO: Unable to read jessie_udp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.796: INFO: Unable to read jessie_tcp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.799: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.804: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:06.824: INFO: Lookups using dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3317 wheezy_tcp@dns-test-service.dns-3317 wheezy_udp@dns-test-service.dns-3317.svc wheezy_tcp@dns-test-service.dns-3317.svc wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3317 jessie_tcp@dns-test-service.dns-3317 jessie_udp@dns-test-service.dns-3317.svc jessie_tcp@dns-test-service.dns-3317.svc jessie_udp@_http._tcp.dns-test-service.dns-3317.svc jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc]

Mar 11 17:43:11.721: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.726: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.731: INFO: Unable to read wheezy_udp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.736: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.740: INFO: Unable to read wheezy_udp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.745: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.749: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.754: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.803: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.809: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.818: INFO: Unable to read jessie_udp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.826: INFO: Unable to read jessie_tcp@dns-test-service.dns-3317 from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.833: INFO: Unable to read jessie_udp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.840: INFO: Unable to read jessie_tcp@dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.848: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.853: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc from pod dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b: the server could not find the requested resource (get pods dns-test-e030465f-978f-406c-8233-c2a31b55ac0b)
Mar 11 17:43:11.907: INFO: Lookups using dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3317 wheezy_tcp@dns-test-service.dns-3317 wheezy_udp@dns-test-service.dns-3317.svc wheezy_tcp@dns-test-service.dns-3317.svc wheezy_udp@_http._tcp.dns-test-service.dns-3317.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3317.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3317 jessie_tcp@dns-test-service.dns-3317 jessie_udp@dns-test-service.dns-3317.svc jessie_tcp@dns-test-service.dns-3317.svc jessie_udp@_http._tcp.dns-test-service.dns-3317.svc jessie_tcp@_http._tcp.dns-test-service.dns-3317.svc]

Mar 11 17:43:16.822: INFO: DNS probes using dns-3317/dns-test-e030465f-978f-406c-8233-c2a31b55ac0b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:43:16.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3317" for this suite.

• [SLOW TEST:34.508 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":339,"completed":272,"skipped":4310,"failed":0}
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:43:16.951: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:43:21.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3554" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":339,"completed":273,"skipped":4310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:43:21.128: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-637
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-637
STEP: creating replication controller externalsvc in namespace services-637
I0311 17:43:21.246502      19 runners.go:190] Created replication controller with name: externalsvc, namespace: services-637, replica count: 2
I0311 17:43:24.298364      19 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 11 17:43:24.338: INFO: Creating new exec pod
Mar 11 17:43:26.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=services-637 exec execpodv5p58 -- /bin/sh -x -c nslookup clusterip-service.services-637.svc.cluster.local'
Mar 11 17:43:26.580: INFO: stderr: "+ nslookup clusterip-service.services-637.svc.cluster.local\n"
Mar 11 17:43:26.580: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-637.svc.cluster.local\tcanonical name = externalsvc.services-637.svc.cluster.local.\nName:\texternalsvc.services-637.svc.cluster.local\nAddress: 10.100.206.137\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-637, will wait for the garbage collector to delete the pods
Mar 11 17:43:26.655: INFO: Deleting ReplicationController externalsvc took: 20.331329ms
Mar 11 17:43:26.756: INFO: Terminating ReplicationController externalsvc pods took: 101.223892ms
Mar 11 17:43:41.190: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:43:41.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-637" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:20.095 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":339,"completed":274,"skipped":4338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:43:41.230: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:43:41.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5494" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":339,"completed":275,"skipped":4373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:43:41.372: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar 11 17:43:41.436: INFO: Waiting up to 5m0s for pod "downward-api-96e33790-75ea-4d2a-ad20-3167325849e1" in namespace "downward-api-9529" to be "Succeeded or Failed"
Mar 11 17:43:41.443: INFO: Pod "downward-api-96e33790-75ea-4d2a-ad20-3167325849e1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.681946ms
Mar 11 17:43:43.451: INFO: Pod "downward-api-96e33790-75ea-4d2a-ad20-3167325849e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014830841s
STEP: Saw pod success
Mar 11 17:43:43.451: INFO: Pod "downward-api-96e33790-75ea-4d2a-ad20-3167325849e1" satisfied condition "Succeeded or Failed"
Mar 11 17:43:43.456: INFO: Trying to get logs from node 198.18.167.130 pod downward-api-96e33790-75ea-4d2a-ad20-3167325849e1 container dapi-container: <nil>
STEP: delete the pod
Mar 11 17:43:43.495: INFO: Waiting for pod downward-api-96e33790-75ea-4d2a-ad20-3167325849e1 to disappear
Mar 11 17:43:43.500: INFO: Pod downward-api-96e33790-75ea-4d2a-ad20-3167325849e1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:43:43.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9529" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":339,"completed":276,"skipped":4403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:43:43.519: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0311 17:43:43.569339      19 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:45:01.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7738" for this suite.

• [SLOW TEST:78.116 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":339,"completed":277,"skipped":4435,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:45:01.636: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:45:01.703: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 11 17:45:03.756: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:45:04.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4061" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":339,"completed":278,"skipped":4465,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:45:04.789: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:45:05.565: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:45:08.605: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:45:08.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-388" for this suite.
STEP: Destroying namespace "webhook-388-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":339,"completed":279,"skipped":4496,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:45:08.786: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-8e1fc3fe-91d9-46df-9bbc-3b6fae41e051
STEP: Creating a pod to test consume configMaps
Mar 11 17:45:08.877: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f05fc291-4f4f-4c45-870a-297b8899b1c0" in namespace "projected-5898" to be "Succeeded or Failed"
Mar 11 17:45:08.887: INFO: Pod "pod-projected-configmaps-f05fc291-4f4f-4c45-870a-297b8899b1c0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.673655ms
Mar 11 17:45:10.899: INFO: Pod "pod-projected-configmaps-f05fc291-4f4f-4c45-870a-297b8899b1c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02110038s
Mar 11 17:45:12.907: INFO: Pod "pod-projected-configmaps-f05fc291-4f4f-4c45-870a-297b8899b1c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029908162s
STEP: Saw pod success
Mar 11 17:45:12.908: INFO: Pod "pod-projected-configmaps-f05fc291-4f4f-4c45-870a-297b8899b1c0" satisfied condition "Succeeded or Failed"
Mar 11 17:45:12.911: INFO: Trying to get logs from node 198.18.167.130 pod pod-projected-configmaps-f05fc291-4f4f-4c45-870a-297b8899b1c0 container agnhost-container: <nil>
STEP: delete the pod
Mar 11 17:45:12.941: INFO: Waiting for pod pod-projected-configmaps-f05fc291-4f4f-4c45-870a-297b8899b1c0 to disappear
Mar 11 17:45:12.944: INFO: Pod pod-projected-configmaps-f05fc291-4f4f-4c45-870a-297b8899b1c0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:45:12.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5898" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":280,"skipped":4504,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:45:12.963: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 11 17:45:13.039: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:45:21.757: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:45:54.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3447" for this suite.

• [SLOW TEST:41.634 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":339,"completed":281,"skipped":4507,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:45:54.597: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:45:55.397: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:45:58.440: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:45:58.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-427" for this suite.
STEP: Destroying namespace "webhook-427-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":339,"completed":282,"skipped":4535,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:45:58.577: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-998da585-7ad1-49f0-905a-cb852d5834fb
STEP: Creating a pod to test consume configMaps
Mar 11 17:45:58.658: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ea5e4d2-9539-43ea-a8ea-66375755c0ae" in namespace "configmap-309" to be "Succeeded or Failed"
Mar 11 17:45:58.665: INFO: Pod "pod-configmaps-0ea5e4d2-9539-43ea-a8ea-66375755c0ae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.201412ms
Mar 11 17:46:00.671: INFO: Pod "pod-configmaps-0ea5e4d2-9539-43ea-a8ea-66375755c0ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012645729s
STEP: Saw pod success
Mar 11 17:46:00.671: INFO: Pod "pod-configmaps-0ea5e4d2-9539-43ea-a8ea-66375755c0ae" satisfied condition "Succeeded or Failed"
Mar 11 17:46:00.674: INFO: Trying to get logs from node 198.18.167.130 pod pod-configmaps-0ea5e4d2-9539-43ea-a8ea-66375755c0ae container agnhost-container: <nil>
STEP: delete the pod
Mar 11 17:46:00.702: INFO: Waiting for pod pod-configmaps-0ea5e4d2-9539-43ea-a8ea-66375755c0ae to disappear
Mar 11 17:46:00.705: INFO: Pod pod-configmaps-0ea5e4d2-9539-43ea-a8ea-66375755c0ae no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:46:00.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-309" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":283,"skipped":4567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:46:00.718: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2248.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2248.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2248.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2248.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2248.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2248.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 11 17:46:02.813: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:02.817: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:02.823: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:02.827: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:02.839: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:02.843: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:02.847: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:02.851: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:02.858: INFO: Lookups using dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local]

Mar 11 17:46:07.865: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:07.869: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:07.873: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:07.877: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:07.889: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:07.893: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:07.897: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:07.901: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:07.908: INFO: Lookups using dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local]

Mar 11 17:46:12.869: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:12.878: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:12.884: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:12.889: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:12.903: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:12.908: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:12.913: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:12.918: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:12.929: INFO: Lookups using dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local]

Mar 11 17:46:17.866: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:17.880: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:17.887: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:17.895: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:17.915: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:17.920: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:17.926: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:17.937: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:17.949: INFO: Lookups using dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local]

Mar 11 17:46:22.870: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:22.876: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:22.880: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:22.885: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:22.904: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:22.909: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:22.922: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:22.929: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:22.942: INFO: Lookups using dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local]

Mar 11 17:46:27.866: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:27.872: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:27.878: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:27.884: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:27.904: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:27.909: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:27.913: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:27.919: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local from pod dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f: the server could not find the requested resource (get pods dns-test-c5a66149-3d53-42b9-afb9-d9356942434f)
Mar 11 17:46:27.931: INFO: Lookups using dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2248.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2248.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2248.svc.cluster.local jessie_udp@dns-test-service-2.dns-2248.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2248.svc.cluster.local]

Mar 11 17:46:32.916: INFO: DNS probes using dns-2248/dns-test-c5a66149-3d53-42b9-afb9-d9356942434f succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:46:32.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2248" for this suite.

• [SLOW TEST:32.269 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":339,"completed":284,"skipped":4597,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:46:32.990: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-452.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-452.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-452.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-452.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-452.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-452.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 11 17:46:37.112: INFO: DNS probes using dns-452/dns-test-f0babdc3-60ee-4c91-b80e-38a2c56399c3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:46:37.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-452" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":339,"completed":285,"skipped":4626,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:46:37.165: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Mar 11 17:46:37.233: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:46:39.239: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 11 17:46:40.281: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:46:41.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8808" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":339,"completed":286,"skipped":4627,"failed":0}
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:46:41.328: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:46:45.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-182" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":287,"skipped":4633,"failed":0}
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:46:45.436: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:46:45.516: INFO: The status of Pod busybox-host-aliases56c7de01-b29e-44a7-9fb8-7d916aad1d83 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:46:47.522: INFO: The status of Pod busybox-host-aliases56c7de01-b29e-44a7-9fb8-7d916aad1d83 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:46:47.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9814" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":288,"skipped":4637,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:46:47.550: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-83b98dba-22a3-4d45-a967-d7a555783b7e in namespace container-probe-5936
Mar 11 17:46:49.640: INFO: Started pod busybox-83b98dba-22a3-4d45-a967-d7a555783b7e in namespace container-probe-5936
STEP: checking the pod's current state and verifying that restartCount is present
Mar 11 17:46:49.645: INFO: Initial restart count of pod busybox-83b98dba-22a3-4d45-a967-d7a555783b7e is 0
Mar 11 17:47:39.948: INFO: Restart count of pod container-probe-5936/busybox-83b98dba-22a3-4d45-a967-d7a555783b7e is now 1 (50.302893095s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:47:39.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5936" for this suite.

• [SLOW TEST:52.458 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":289,"skipped":4648,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:47:40.011: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:47:40.069: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-3146
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:47:46.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-7068" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:47:46.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3146" for this suite.

• [SLOW TEST:6.257 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:75
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":339,"completed":290,"skipped":4676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:47:46.269: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-5336
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5336 to expose endpoints map[]
Mar 11 17:47:46.368: INFO: successfully validated that service multi-endpoint-test in namespace services-5336 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5336
Mar 11 17:47:46.391: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:47:48.398: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5336 to expose endpoints map[pod1:[100]]
Mar 11 17:47:48.417: INFO: successfully validated that service multi-endpoint-test in namespace services-5336 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5336
Mar 11 17:47:48.433: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:47:50.440: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5336 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 11 17:47:50.460: INFO: successfully validated that service multi-endpoint-test in namespace services-5336 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-5336
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5336 to expose endpoints map[pod2:[101]]
Mar 11 17:47:50.502: INFO: successfully validated that service multi-endpoint-test in namespace services-5336 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5336
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5336 to expose endpoints map[]
Mar 11 17:47:50.549: INFO: successfully validated that service multi-endpoint-test in namespace services-5336 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:47:50.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5336" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":339,"completed":291,"skipped":4705,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:47:50.586: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:47:50.643: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-8789c1ac-71f1-43f5-907d-6af2dff184cc" in namespace "security-context-test-2422" to be "Succeeded or Failed"
Mar 11 17:47:50.651: INFO: Pod "alpine-nnp-false-8789c1ac-71f1-43f5-907d-6af2dff184cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115402ms
Mar 11 17:47:52.657: INFO: Pod "alpine-nnp-false-8789c1ac-71f1-43f5-907d-6af2dff184cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01396643s
Mar 11 17:47:54.666: INFO: Pod "alpine-nnp-false-8789c1ac-71f1-43f5-907d-6af2dff184cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0234296s
Mar 11 17:47:54.667: INFO: Pod "alpine-nnp-false-8789c1ac-71f1-43f5-907d-6af2dff184cc" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:47:54.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2422" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":292,"skipped":4716,"failed":0}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:47:54.697: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:47:54.781: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 11 17:47:54.795: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:54.795: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:54.803: INFO: Number of nodes with available pods: 0
Mar 11 17:47:54.803: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:47:55.820: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:55.820: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:55.825: INFO: Number of nodes with available pods: 0
Mar 11 17:47:55.825: INFO: Node 198.18.16.160 is running more than one daemon pod
Mar 11 17:47:56.812: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:56.812: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:56.816: INFO: Number of nodes with available pods: 2
Mar 11 17:47:56.816: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 11 17:47:56.863: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:47:56.873: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:56.873: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:57.880: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:47:57.888: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:57.888: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:58.883: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:47:58.889: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:58.889: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:59.894: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:47:59.899: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:47:59.899: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:00.880: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:48:00.888: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:00.888: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:01.882: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:48:01.890: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:01.890: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:02.880: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:48:02.886: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:02.886: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:03.881: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:48:03.887: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:03.887: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:04.880: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:48:04.886: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:04.886: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:05.882: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:48:05.886: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:05.886: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:06.878: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:48:06.884: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:06.884: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:07.883: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:48:07.896: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:07.896: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:08.879: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:48:08.885: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:08.885: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:09.884: INFO: Pod daemon-set-chrh6 is not available
Mar 11 17:48:09.884: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:48:09.890: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:09.891: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:10.880: INFO: Pod daemon-set-chrh6 is not available
Mar 11 17:48:10.880: INFO: Wrong image for pod: daemon-set-vz248. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Mar 11 17:48:10.885: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:10.885: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:11.885: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:11.885: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:12.884: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:12.884: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:13.886: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:13.886: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:14.886: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:14.886: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:15.898: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:15.898: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:16.885: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:16.885: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:17.886: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:17.886: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:18.884: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:18.884: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:19.886: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:19.886: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:20.884: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:20.884: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:21.883: INFO: Pod daemon-set-hgt4w is not available
Mar 11 17:48:21.889: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:21.889: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 11 17:48:21.896: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:21.896: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:21.900: INFO: Number of nodes with available pods: 1
Mar 11 17:48:21.900: INFO: Node 198.18.167.130 is running more than one daemon pod
Mar 11 17:48:22.911: INFO: DaemonSet pods can't tolerate node 198.18.151.115 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:22.912: INFO: DaemonSet pods can't tolerate node 198.18.158.139 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 11 17:48:22.918: INFO: Number of nodes with available pods: 2
Mar 11 17:48:22.918: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-688, will wait for the garbage collector to delete the pods
Mar 11 17:48:23.023: INFO: Deleting DaemonSet.extensions daemon-set took: 17.099847ms
Mar 11 17:48:23.124: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.11457ms
Mar 11 17:48:31.130: INFO: Number of nodes with available pods: 0
Mar 11 17:48:31.131: INFO: Number of running nodes: 0, number of available pods: 0
Mar 11 17:48:31.134: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"95879"},"items":null}

Mar 11 17:48:31.137: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"95879"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:48:31.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-688" for this suite.

• [SLOW TEST:36.470 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":339,"completed":293,"skipped":4720,"failed":0}
SSSSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:48:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 11 17:48:33.252: INFO: &Pod{ObjectMeta:{send-events-fc4f1eeb-387f-4817-af8f-76b34150b67b  events-1660  2b935afb-239e-4b5c-8cef-fa24d5e1d045 95913 0 2022-03-11 17:48:31 +0000 UTC <nil> <nil> map[name:foo time:218128619] map[] [] []  [{e2e.test Update v1 2022-03-11 17:48:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:48:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gd8xl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gd8xl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.167.130,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:48:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:48:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:48:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:48:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.167.130,PodIP:192.168.2.58,StartTime:2022-03-11 17:48:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 17:48:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://ca5ed14d1a26d47e4534cbad87fdfb22a010e9ccc2eab515ccf37b6ae003ab4b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar 11 17:48:35.258: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 11 17:48:37.264: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:48:37.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1660" for this suite.

• [SLOW TEST:6.124 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":339,"completed":294,"skipped":4726,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:48:37.294: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 17:48:37.360: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb840e48-ef5f-4278-827f-9e40dcf0279d" in namespace "projected-5650" to be "Succeeded or Failed"
Mar 11 17:48:37.369: INFO: Pod "downwardapi-volume-eb840e48-ef5f-4278-827f-9e40dcf0279d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.961463ms
Mar 11 17:48:39.375: INFO: Pod "downwardapi-volume-eb840e48-ef5f-4278-827f-9e40dcf0279d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015018829s
STEP: Saw pod success
Mar 11 17:48:39.376: INFO: Pod "downwardapi-volume-eb840e48-ef5f-4278-827f-9e40dcf0279d" satisfied condition "Succeeded or Failed"
Mar 11 17:48:39.379: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-eb840e48-ef5f-4278-827f-9e40dcf0279d container client-container: <nil>
STEP: delete the pod
Mar 11 17:48:39.407: INFO: Waiting for pod downwardapi-volume-eb840e48-ef5f-4278-827f-9e40dcf0279d to disappear
Mar 11 17:48:39.415: INFO: Pod downwardapi-volume-eb840e48-ef5f-4278-827f-9e40dcf0279d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:48:39.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5650" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":295,"skipped":4742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:48:39.431: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-sqpn
STEP: Creating a pod to test atomic-volume-subpath
Mar 11 17:48:39.563: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sqpn" in namespace "subpath-2775" to be "Succeeded or Failed"
Mar 11 17:48:39.572: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Pending", Reason="", readiness=false. Elapsed: 9.006528ms
Mar 11 17:48:41.581: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Running", Reason="", readiness=true. Elapsed: 2.017902746s
Mar 11 17:48:43.587: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Running", Reason="", readiness=true. Elapsed: 4.024043582s
Mar 11 17:48:45.594: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Running", Reason="", readiness=true. Elapsed: 6.030455918s
Mar 11 17:48:47.599: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Running", Reason="", readiness=true. Elapsed: 8.036189011s
Mar 11 17:48:49.607: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Running", Reason="", readiness=true. Elapsed: 10.043378539s
Mar 11 17:48:51.613: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Running", Reason="", readiness=true. Elapsed: 12.049899975s
Mar 11 17:48:53.625: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Running", Reason="", readiness=true. Elapsed: 14.061589184s
Mar 11 17:48:55.630: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Running", Reason="", readiness=true. Elapsed: 16.067183237s
Mar 11 17:48:57.638: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Running", Reason="", readiness=true. Elapsed: 18.075210818s
Mar 11 17:48:59.643: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Running", Reason="", readiness=true. Elapsed: 20.080204847s
Mar 11 17:49:01.652: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Running", Reason="", readiness=true. Elapsed: 22.088990706s
Mar 11 17:49:03.663: INFO: Pod "pod-subpath-test-secret-sqpn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.099437483s
STEP: Saw pod success
Mar 11 17:49:03.663: INFO: Pod "pod-subpath-test-secret-sqpn" satisfied condition "Succeeded or Failed"
Mar 11 17:49:03.667: INFO: Trying to get logs from node 198.18.167.130 pod pod-subpath-test-secret-sqpn container test-container-subpath-secret-sqpn: <nil>
STEP: delete the pod
Mar 11 17:49:03.719: INFO: Waiting for pod pod-subpath-test-secret-sqpn to disappear
Mar 11 17:49:03.724: INFO: Pod pod-subpath-test-secret-sqpn no longer exists
STEP: Deleting pod pod-subpath-test-secret-sqpn
Mar 11 17:49:03.724: INFO: Deleting pod "pod-subpath-test-secret-sqpn" in namespace "subpath-2775"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:49:03.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2775" for this suite.

• [SLOW TEST:24.320 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":339,"completed":296,"skipped":4768,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:49:03.754: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 11 17:49:03.828: INFO: Waiting up to 5m0s for pod "pod-a3c1d77b-1988-477b-a601-da882b6f359b" in namespace "emptydir-3913" to be "Succeeded or Failed"
Mar 11 17:49:03.839: INFO: Pod "pod-a3c1d77b-1988-477b-a601-da882b6f359b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.67806ms
Mar 11 17:49:05.853: INFO: Pod "pod-a3c1d77b-1988-477b-a601-da882b6f359b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024147418s
STEP: Saw pod success
Mar 11 17:49:05.853: INFO: Pod "pod-a3c1d77b-1988-477b-a601-da882b6f359b" satisfied condition "Succeeded or Failed"
Mar 11 17:49:05.858: INFO: Trying to get logs from node 198.18.167.130 pod pod-a3c1d77b-1988-477b-a601-da882b6f359b container test-container: <nil>
STEP: delete the pod
Mar 11 17:49:05.898: INFO: Waiting for pod pod-a3c1d77b-1988-477b-a601-da882b6f359b to disappear
Mar 11 17:49:05.907: INFO: Pod pod-a3c1d77b-1988-477b-a601-da882b6f359b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:49:05.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3913" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":297,"skipped":4795,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:49:05.949: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 11 17:49:06.027: INFO: Waiting up to 5m0s for pod "pod-315776d8-63f8-4fdc-aca3-168979bcbae0" in namespace "emptydir-9120" to be "Succeeded or Failed"
Mar 11 17:49:06.038: INFO: Pod "pod-315776d8-63f8-4fdc-aca3-168979bcbae0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.722489ms
Mar 11 17:49:08.046: INFO: Pod "pod-315776d8-63f8-4fdc-aca3-168979bcbae0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018070208s
STEP: Saw pod success
Mar 11 17:49:08.046: INFO: Pod "pod-315776d8-63f8-4fdc-aca3-168979bcbae0" satisfied condition "Succeeded or Failed"
Mar 11 17:49:08.049: INFO: Trying to get logs from node 198.18.167.130 pod pod-315776d8-63f8-4fdc-aca3-168979bcbae0 container test-container: <nil>
STEP: delete the pod
Mar 11 17:49:08.077: INFO: Waiting for pod pod-315776d8-63f8-4fdc-aca3-168979bcbae0 to disappear
Mar 11 17:49:08.081: INFO: Pod pod-315776d8-63f8-4fdc-aca3-168979bcbae0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:49:08.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9120" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":298,"skipped":4811,"failed":0}
SSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:49:08.102: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:149
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 11 17:49:08.198: INFO: starting watch
STEP: patching
STEP: updating
Mar 11 17:49:08.218: INFO: waiting for watch events with expected annotations
Mar 11 17:49:08.219: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:49:08.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-5445" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":339,"completed":299,"skipped":4817,"failed":0}

------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:49:08.280: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-2806
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2806 to expose endpoints map[]
Mar 11 17:49:08.396: INFO: successfully validated that service endpoint-test2 in namespace services-2806 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2806
Mar 11 17:49:08.414: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:49:10.421: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2806 to expose endpoints map[pod1:[80]]
Mar 11 17:49:10.450: INFO: successfully validated that service endpoint-test2 in namespace services-2806 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-2806
Mar 11 17:49:10.475: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:49:12.487: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2806 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 11 17:49:12.513: INFO: successfully validated that service endpoint-test2 in namespace services-2806 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-2806
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2806 to expose endpoints map[pod2:[80]]
Mar 11 17:49:12.561: INFO: successfully validated that service endpoint-test2 in namespace services-2806 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-2806
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2806 to expose endpoints map[]
Mar 11 17:49:12.601: INFO: successfully validated that service endpoint-test2 in namespace services-2806 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:49:12.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2806" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":339,"completed":300,"skipped":4817,"failed":0}
SSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:49:12.668: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Mar 11 17:49:12.781: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Mar 11 17:49:12.787: INFO: starting watch
STEP: patching
STEP: updating
Mar 11 17:49:12.809: INFO: waiting for watch events with expected annotations
Mar 11 17:49:12.809: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:49:12.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5362" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":339,"completed":301,"skipped":4821,"failed":0}
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:49:12.878: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Mar 11 17:49:12.959: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:49:14.963: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Mar 11 17:49:15.000: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:49:17.011: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 11 17:49:17.017: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7575 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:49:17.017: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:49:17.122: INFO: Exec stderr: ""
Mar 11 17:49:17.122: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7575 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:49:17.122: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:49:17.204: INFO: Exec stderr: ""
Mar 11 17:49:17.204: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7575 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:49:17.204: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:49:17.290: INFO: Exec stderr: ""
Mar 11 17:49:17.291: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7575 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:49:17.291: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:49:17.377: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 11 17:49:17.377: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7575 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:49:17.377: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:49:17.461: INFO: Exec stderr: ""
Mar 11 17:49:17.461: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7575 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:49:17.461: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:49:17.556: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 11 17:49:17.556: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7575 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:49:17.556: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:49:17.629: INFO: Exec stderr: ""
Mar 11 17:49:17.629: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7575 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:49:17.629: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:49:17.706: INFO: Exec stderr: ""
Mar 11 17:49:17.706: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7575 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:49:17.706: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:49:18.041: INFO: Exec stderr: ""
Mar 11 17:49:18.041: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7575 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 11 17:49:18.041: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
Mar 11 17:49:18.150: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:49:18.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7575" for this suite.

• [SLOW TEST:5.305 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":302,"skipped":4825,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:49:18.183: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 11 17:49:18.263: INFO: Waiting up to 5m0s for pod "pod-8d96c3f8-a811-4976-9ae5-db78a356aaab" in namespace "emptydir-2813" to be "Succeeded or Failed"
Mar 11 17:49:18.273: INFO: Pod "pod-8d96c3f8-a811-4976-9ae5-db78a356aaab": Phase="Pending", Reason="", readiness=false. Elapsed: 9.748181ms
Mar 11 17:49:20.280: INFO: Pod "pod-8d96c3f8-a811-4976-9ae5-db78a356aaab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016520071s
STEP: Saw pod success
Mar 11 17:49:20.280: INFO: Pod "pod-8d96c3f8-a811-4976-9ae5-db78a356aaab" satisfied condition "Succeeded or Failed"
Mar 11 17:49:20.287: INFO: Trying to get logs from node 198.18.167.130 pod pod-8d96c3f8-a811-4976-9ae5-db78a356aaab container test-container: <nil>
STEP: delete the pod
Mar 11 17:49:20.317: INFO: Waiting for pod pod-8d96c3f8-a811-4976-9ae5-db78a356aaab to disappear
Mar 11 17:49:20.321: INFO: Pod pod-8d96c3f8-a811-4976-9ae5-db78a356aaab no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:49:20.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2813" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":303,"skipped":4827,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:49:20.343: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Mar 11 17:49:20.432: INFO: The status of Pod annotationupdate601c0ea2-7715-4132-88e9-5a63caf9c8c0 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:49:22.439: INFO: The status of Pod annotationupdate601c0ea2-7715-4132-88e9-5a63caf9c8c0 is Running (Ready = true)
Mar 11 17:49:22.974: INFO: Successfully updated pod "annotationupdate601c0ea2-7715-4132-88e9-5a63caf9c8c0"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:49:27.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9273" for this suite.

• [SLOW TEST:6.689 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":304,"skipped":4842,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:49:27.033: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fc955e47-644a-4571-82ba-a3b9d80b8edf
STEP: Creating the pod
Mar 11 17:49:27.112: INFO: The status of Pod pod-projected-configmaps-88833d07-3bf8-4af6-9f87-a3f08ddc1053 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:49:29.119: INFO: The status of Pod pod-projected-configmaps-88833d07-3bf8-4af6-9f87-a3f08ddc1053 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-fc955e47-644a-4571-82ba-a3b9d80b8edf
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:49:31.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-888" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":305,"skipped":4847,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:49:31.172: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:49:31.225: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Creating first CR 
Mar 11 17:49:33.810: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-11T17:49:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-11T17:49:33Z]] name:name1 resourceVersion:96967 uid:b8f8184a-2532-4def-9dda-a1fcb56ea5a9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 11 17:49:43.827: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-11T17:49:43Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-11T17:49:43Z]] name:name2 resourceVersion:97089 uid:8d61bc70-5547-4d46-a157-879f9714fa60] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 11 17:49:53.844: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-11T17:49:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-11T17:49:53Z]] name:name1 resourceVersion:97193 uid:b8f8184a-2532-4def-9dda-a1fcb56ea5a9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 11 17:50:03.861: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-11T17:49:43Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-11T17:50:03Z]] name:name2 resourceVersion:97293 uid:8d61bc70-5547-4d46-a157-879f9714fa60] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 11 17:50:13.874: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-11T17:49:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-11T17:49:53Z]] name:name1 resourceVersion:97387 uid:b8f8184a-2532-4def-9dda-a1fcb56ea5a9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 11 17:50:23.888: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-03-11T17:49:43Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-03-11T17:50:03Z]] name:name2 resourceVersion:97489 uid:8d61bc70-5547-4d46-a157-879f9714fa60] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:50:34.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-259" for this suite.

• [SLOW TEST:63.470 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":339,"completed":306,"skipped":4848,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:50:34.662: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Mar 11 17:50:34.784: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 11 17:50:34.793: INFO: Waiting for terminating namespaces to be deleted...
Mar 11 17:50:34.797: INFO: 
Logging pods the apiserver thinks is on node 198.18.16.160 before test
Mar 11 17:50:34.807: INFO: capi-kubeadm-bootstrap-controller-manager-694cc79bb7-m8mq5 from capi-kubeadm-bootstrap-system started at 2022-03-11 15:43:57 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.807: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:50:34.807: INFO: capi-controller-manager-689cd9b4fd-pkbpt from capi-system started at 2022-03-11 15:43:50 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.807: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:50:34.807: INFO: capv-controller-manager-6b467446b9-4qlm4 from capv-system started at 2022-03-11 15:44:30 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.807: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:50:34.807: INFO: cert-manager-7988d4fb6c-knnwq from cert-manager started at 2022-03-11 15:43:27 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.808: INFO: 	Container cert-manager ready: true, restart count 0
Mar 11 17:50:34.808: INFO: cert-manager-cainjector-6bc8dcdb64-8vk7b from cert-manager started at 2022-03-11 15:43:26 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.808: INFO: 	Container cert-manager ready: true, restart count 0
Mar 11 17:50:34.808: INFO: cert-manager-webhook-68979bfb95-ksn6x from cert-manager started at 2022-03-11 15:43:27 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.808: INFO: 	Container cert-manager ready: true, restart count 0
Mar 11 17:50:34.808: INFO: etcdadm-bootstrap-provider-controller-manager-74c86ffb56-l6l9z from etcdadm-bootstrap-provider-system started at 2022-03-11 15:44:04 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.808: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:50:34.808: INFO: etcdadm-controller-controller-manager-7894945688-sxr4k from etcdadm-controller-system started at 2022-03-11 16:43:28 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.808: INFO: 	Container manager ready: true, restart count 0
Mar 11 17:50:34.809: INFO: cilium-operator-86d59d5c88-fvvbn from kube-system started at 2022-03-11 15:42:57 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.809: INFO: 	Container cilium-operator ready: true, restart count 0
Mar 11 17:50:34.809: INFO: cilium-qm8kg from kube-system started at 2022-03-11 15:42:57 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.809: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 11 17:50:34.809: INFO: coredns-745c7986c7-5r5r8 from kube-system started at 2022-03-11 16:43:28 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.809: INFO: 	Container coredns ready: true, restart count 0
Mar 11 17:50:34.809: INFO: coredns-745c7986c7-mmcqj from kube-system started at 2022-03-11 16:43:28 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.809: INFO: 	Container coredns ready: true, restart count 0
Mar 11 17:50:34.810: INFO: kube-proxy-8jj6k from kube-system started at 2022-03-11 15:42:19 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.810: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 11 17:50:34.810: INFO: vsphere-cloud-controller-manager-l6kzs from kube-system started at 2022-03-11 15:42:19 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.810: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 1
Mar 11 17:50:34.810: INFO: vsphere-csi-controller-576c9c8dc8-8vqvm from kube-system started at 2022-03-11 16:43:28 +0000 UTC (5 container statuses recorded)
Mar 11 17:50:34.810: INFO: 	Container csi-attacher ready: true, restart count 0
Mar 11 17:50:34.810: INFO: 	Container csi-provisioner ready: true, restart count 0
Mar 11 17:50:34.810: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 11 17:50:34.810: INFO: 	Container vsphere-csi-controller ready: true, restart count 0
Mar 11 17:50:34.810: INFO: 	Container vsphere-syncer ready: true, restart count 0
Mar 11 17:50:34.811: INFO: vsphere-csi-node-zmmbm from kube-system started at 2022-03-11 15:42:19 +0000 UTC (3 container statuses recorded)
Mar 11 17:50:34.811: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 11 17:50:34.811: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 11 17:50:34.811: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Mar 11 17:50:34.811: INFO: sonobuoy from sonobuoy started at 2022-03-11 15:55:43 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.811: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 11 17:50:34.811: INFO: sonobuoy-systemd-logs-daemon-set-5218357723e74163-g6x4l from sonobuoy started at 2022-03-11 15:55:47 +0000 UTC (2 container statuses recorded)
Mar 11 17:50:34.811: INFO: 	Container sonobuoy-worker ready: false, restart count 15
Mar 11 17:50:34.811: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 11 17:50:34.811: INFO: 
Logging pods the apiserver thinks is on node 198.18.167.130 before test
Mar 11 17:50:34.821: INFO: cilium-69rlw from kube-system started at 2022-03-11 15:42:57 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.821: INFO: 	Container cilium-agent ready: true, restart count 0
Mar 11 17:50:34.821: INFO: kube-proxy-6trtj from kube-system started at 2022-03-11 15:42:21 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.821: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 11 17:50:34.821: INFO: vsphere-cloud-controller-manager-nmqmd from kube-system started at 2022-03-11 17:40:51 +0000 UTC (1 container statuses recorded)
Mar 11 17:50:34.822: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
Mar 11 17:50:34.822: INFO: vsphere-csi-node-xgb98 from kube-system started at 2022-03-11 15:42:21 +0000 UTC (3 container statuses recorded)
Mar 11 17:50:34.822: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 11 17:50:34.822: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 11 17:50:34.822: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Mar 11 17:50:34.822: INFO: sonobuoy-systemd-logs-daemon-set-5218357723e74163-tgcrn from sonobuoy started at 2022-03-11 15:55:47 +0000 UTC (2 container statuses recorded)
Mar 11 17:50:34.822: INFO: 	Container sonobuoy-worker ready: false, restart count 15
Mar 11 17:50:34.822: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16db64e7a7f19107], Reason = [FailedScheduling], Message = [0/4 nodes are available: 2 node(s) didn't match Pod's node affinity/selector, 2 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:50:35.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-694" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":339,"completed":307,"skipped":4858,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:50:35.876: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 17:50:35.940: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04b432ab-43cc-464d-acfb-d96487e430ce" in namespace "downward-api-6838" to be "Succeeded or Failed"
Mar 11 17:50:35.949: INFO: Pod "downwardapi-volume-04b432ab-43cc-464d-acfb-d96487e430ce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.047804ms
Mar 11 17:50:37.953: INFO: Pod "downwardapi-volume-04b432ab-43cc-464d-acfb-d96487e430ce": Phase="Running", Reason="", readiness=true. Elapsed: 2.012624925s
Mar 11 17:50:39.960: INFO: Pod "downwardapi-volume-04b432ab-43cc-464d-acfb-d96487e430ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019862896s
STEP: Saw pod success
Mar 11 17:50:39.961: INFO: Pod "downwardapi-volume-04b432ab-43cc-464d-acfb-d96487e430ce" satisfied condition "Succeeded or Failed"
Mar 11 17:50:39.964: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-04b432ab-43cc-464d-acfb-d96487e430ce container client-container: <nil>
STEP: delete the pod
Mar 11 17:50:39.992: INFO: Waiting for pod downwardapi-volume-04b432ab-43cc-464d-acfb-d96487e430ce to disappear
Mar 11 17:50:39.996: INFO: Pod downwardapi-volume-04b432ab-43cc-464d-acfb-d96487e430ce no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:50:39.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6838" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":339,"completed":308,"skipped":4858,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:50:40.019: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:50:40.662: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:50:43.706: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:50:43.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3842" for this suite.
STEP: Destroying namespace "webhook-3842-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":339,"completed":309,"skipped":4872,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:50:44.043: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:50:44.093: INFO: Creating deployment "webserver-deployment"
Mar 11 17:50:44.100: INFO: Waiting for observed generation 1
Mar 11 17:50:46.115: INFO: Waiting for all required pods to come up
Mar 11 17:50:46.122: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 11 17:50:58.138: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 11 17:50:58.144: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 11 17:50:58.156: INFO: Updating deployment webserver-deployment
Mar 11 17:50:58.156: INFO: Waiting for observed generation 2
Mar 11 17:51:00.170: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 11 17:51:00.174: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 11 17:51:00.178: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 11 17:51:00.192: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 11 17:51:00.192: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 11 17:51:00.199: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 11 17:51:00.217: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 11 17:51:00.217: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 11 17:51:00.235: INFO: Updating deployment webserver-deployment
Mar 11 17:51:00.235: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 11 17:51:00.242: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 11 17:51:02.253: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Mar 11 17:51:02.261: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2705  f57fc82d-8085-42e6-aec4-75c071306044 98307 3 2022-03-11 17:50:44 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-03-11 17:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2022-03-11 17:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0026995f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-03-11 17:51:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2022-03-11 17:51:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 11 17:51:02.265: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-2705  fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 98302 3 2022-03-11 17:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment f57fc82d-8085-42e6-aec4-75c071306044 0xc0026999f7 0xc0026999f8}] []  [{kube-controller-manager Update apps/v1 2022-03-11 17:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f57fc82d-8085-42e6-aec4-75c071306044\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002699a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 11 17:51:02.265: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 11 17:51:02.265: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-2705  f003f1ee-3c22-40ae-8489-2b7328f3934a 98303 3 2022-03-11 17:50:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment f57fc82d-8085-42e6-aec4-75c071306044 0xc002699ad7 0xc002699ad8}] []  [{kube-controller-manager Update apps/v1 2022-03-11 17:50:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f57fc82d-8085-42e6-aec4-75c071306044\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002699b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 11 17:51:02.274: INFO: Pod "webserver-deployment-795d758f88-2dm8r" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2dm8r webserver-deployment-795d758f88- deployment-2705  81c6632a-e619-4619-bbc7-0d9949e4cb59 98294 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1e007 0xc003b1e008}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zstdt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zstdt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.274: INFO: Pod "webserver-deployment-795d758f88-2fvth" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-2fvth webserver-deployment-795d758f88- deployment-2705  1ec865fd-b332-4184-8ec4-787859315bf1 98168 0 2022-03-11 17:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1e180 0xc003b1e181}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wz7gx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wz7gx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.16.160,PodIP:,StartTime:2022-03-11 17:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.274: INFO: Pod "webserver-deployment-795d758f88-5xphv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5xphv webserver-deployment-795d758f88- deployment-2705  331374de-6dd8-4a5c-9fee-8b3a67fb1536 98285 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1e377 0xc003b1e378}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-24tnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-24tnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.275: INFO: Pod "webserver-deployment-795d758f88-62zbb" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-62zbb webserver-deployment-795d758f88- deployment-2705  dc82cb1d-e49d-4b57-a78f-0f38d2abc5f5 98177 0 2022-03-11 17:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1e4f0 0xc003b1e4f1}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vqcvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vqcvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.16.160,PodIP:,StartTime:2022-03-11 17:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.275: INFO: Pod "webserver-deployment-795d758f88-6xm8n" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6xm8n webserver-deployment-795d758f88- deployment-2705  eaafa524-5b68-4c96-98ed-2f7d499bd5ca 98240 0 2022-03-11 17:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1e6d7 0xc003b1e6d8}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fv5mw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fv5mw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.167.130,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.167.130,PodIP:192.168.2.210,StartTime:2022-03-11 17:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.275: INFO: Pod "webserver-deployment-795d758f88-7wv75" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7wv75 webserver-deployment-795d758f88- deployment-2705  0e6feee9-a0c0-42fe-a494-b5f296579c1a 98270 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1e8e7 0xc003b1e8e8}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-96wbl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-96wbl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.276: INFO: Pod "webserver-deployment-795d758f88-8bx7g" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8bx7g webserver-deployment-795d758f88- deployment-2705  fcbef97d-c004-404f-8ebf-0aac295d39d9 98269 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1ea60 0xc003b1ea61}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zvn5z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zvn5z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.276: INFO: Pod "webserver-deployment-795d758f88-c4s87" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c4s87 webserver-deployment-795d758f88- deployment-2705  007f5bcd-9c73-4256-9af6-5390cae5a6ab 98159 0 2022-03-11 17:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1ebf0 0xc003b1ebf1}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fxjhn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fxjhn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.16.160,PodIP:,StartTime:2022-03-11 17:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.276: INFO: Pod "webserver-deployment-795d758f88-f78t5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-f78t5 webserver-deployment-795d758f88- deployment-2705  a64816b9-7144-41be-a400-93c1bfcdef14 98289 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1f457 0xc003b1f458}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4lj4l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4lj4l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.276: INFO: Pod "webserver-deployment-795d758f88-lz5h6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lz5h6 webserver-deployment-795d758f88- deployment-2705  6c233396-42a6-4298-98c4-d18bf92449be 98279 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1f6c0 0xc003b1f6c1}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jbqmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jbqmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.277: INFO: Pod "webserver-deployment-795d758f88-ps2gr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-ps2gr webserver-deployment-795d758f88- deployment-2705  a960b3cb-77fe-4e47-b2f2-e59fe143717d 98191 0 2022-03-11 17:50:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1f890 0xc003b1f891}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:50:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlzwk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlzwk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.16.160,PodIP:,StartTime:2022-03-11 17:50:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.277: INFO: Pod "webserver-deployment-795d758f88-thl55" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-thl55 webserver-deployment-795d758f88- deployment-2705  2aa6888e-2f95-409c-965a-49a6935b2edf 98286 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1fa77 0xc003b1fa78}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qc84d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qc84d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.277: INFO: Pod "webserver-deployment-795d758f88-z4t6v" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-z4t6v webserver-deployment-795d758f88- deployment-2705  4b29172d-6824-43f8-bdc8-b8cfeaa5538a 98257 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a 0xc003b1fbf0 0xc003b1fbf1}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fcfb4fd8-5e81-4b57-a6c7-f4d9326b0e7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j9ctt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j9ctt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.278: INFO: Pod "webserver-deployment-847dcfb7fb-2h2p9" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2h2p9 webserver-deployment-847dcfb7fb- deployment-2705  5ba0863e-63cd-4477-b8bb-5a230ee81353 98274 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc003b1fd60 0xc003b1fd61}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rw987,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rw987,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.278: INFO: Pod "webserver-deployment-847dcfb7fb-2jn5r" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2jn5r webserver-deployment-847dcfb7fb- deployment-2705  8b9a1d61-a9ed-4a32-b2fb-b1c6e2b83827 98115 0 2022-03-11 17:50:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc003b1fec0 0xc003b1fec1}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:50:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l4r2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l4r2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.167.130,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.167.130,PodIP:192.168.2.201,StartTime:2022-03-11 17:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 17:50:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://b63ace3898bb72d2aa899998fa6660b4f5f0d7118829d980c7e7d581061649b9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.278: INFO: Pod "webserver-deployment-847dcfb7fb-5l28s" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-5l28s webserver-deployment-847dcfb7fb- deployment-2705  b9df170f-4b6f-4d4c-a0b0-eceee29c0829 98277 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0036f8137 0xc0036f8138}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sgnwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sgnwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.278: INFO: Pod "webserver-deployment-847dcfb7fb-8fltl" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8fltl webserver-deployment-847dcfb7fb- deployment-2705  ff7cbe4f-6c5f-41e2-9e5b-358bfed63f94 98275 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0036f8320 0xc0036f8321}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8ww8q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8ww8q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.279: INFO: Pod "webserver-deployment-847dcfb7fb-8kl4q" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8kl4q webserver-deployment-847dcfb7fb- deployment-2705  e1f2f13b-e09d-45d3-801d-956dce6a6236 98107 0 2022-03-11 17:50:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0036f8680 0xc0036f8681}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:50:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d5jvj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d5jvj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.167.130,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.167.130,PodIP:192.168.2.10,StartTime:2022-03-11 17:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 17:50:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://ddb4e64f9d9738e6e6181e1708ce073670f12f61ad6444dffda8b78ec90f3917,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.279: INFO: Pod "webserver-deployment-847dcfb7fb-8z9lf" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-8z9lf webserver-deployment-847dcfb7fb- deployment-2705  34b8fb20-f5a0-4854-aba5-042e05083d67 98293 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0036f8897 0xc0036f8898}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2nh9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2nh9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.279: INFO: Pod "webserver-deployment-847dcfb7fb-9mnt8" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-9mnt8 webserver-deployment-847dcfb7fb- deployment-2705  999bb8be-6c4f-4d73-ab68-24097f056bf2 98296 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0036f8b80 0xc0036f8b81}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-76w4c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76w4c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.280: INFO: Pod "webserver-deployment-847dcfb7fb-cdwgd" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-cdwgd webserver-deployment-847dcfb7fb- deployment-2705  fd0b32a6-9a9c-485d-a8ff-e466881a8116 98312 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0036f8d30 0xc0036f8d31}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sn77z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sn77z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.16.160,PodIP:,StartTime:2022-03-11 17:51:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.280: INFO: Pod "webserver-deployment-847dcfb7fb-czjgl" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-czjgl webserver-deployment-847dcfb7fb- deployment-2705  6c11fee1-424c-40c7-82a3-ed31d0a4dbf7 98292 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0036f8f27 0xc0036f8f28}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zhshh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zhshh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.280: INFO: Pod "webserver-deployment-847dcfb7fb-dbrrc" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-dbrrc webserver-deployment-847dcfb7fb- deployment-2705  ac411d4a-9a8f-4671-8510-a44eb35f5cc3 98119 0 2022-03-11 17:50:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0036f9220 0xc0036f9221}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:50:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-knsdn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-knsdn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.167.130,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.167.130,PodIP:192.168.2.88,StartTime:2022-03-11 17:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 17:50:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://e19c1e959478367869b4826aa44e4aabd9f0a19f5de5bd2264464c41f573806e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.280: INFO: Pod "webserver-deployment-847dcfb7fb-frt4f" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-frt4f webserver-deployment-847dcfb7fb- deployment-2705  46d9bfa9-a50f-41d8-89ea-f02a73f21b4c 97908 0 2022-03-11 17:50:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0036f97c7 0xc0036f97c8}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:50:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.1.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pktwj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pktwj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.16.160,PodIP:192.168.1.232,StartTime:2022-03-11 17:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 17:50:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://5a94661e6ce23d5058cd9505b45fbd2482319c80b9fb113b84618441c339637f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.1.232,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.281: INFO: Pod "webserver-deployment-847dcfb7fb-g2j42" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-g2j42 webserver-deployment-847dcfb7fb- deployment-2705  dfe18ce0-db45-45aa-992a-66ab3b5675f8 98259 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0036f9a67 0xc0036f9a68}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rbl9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbl9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.281: INFO: Pod "webserver-deployment-847dcfb7fb-h5qhr" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-h5qhr webserver-deployment-847dcfb7fb- deployment-2705  81ef6f58-c119-4a58-b0ad-aa51df62c6ea 98096 0 2022-03-11 17:50:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0036f9c70 0xc0036f9c71}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:50:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xffgn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xffgn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.167.130,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.167.130,PodIP:192.168.2.216,StartTime:2022-03-11 17:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 17:50:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://14d0d2f33583129c761ed00835fd786f0900d585aa4b25e924d5cbff553ede61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.281: INFO: Pod "webserver-deployment-847dcfb7fb-hcr4x" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-hcr4x webserver-deployment-847dcfb7fb- deployment-2705  d9bcd1e8-1c58-45ed-98c3-6f8e4be0c8dc 98287 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0036f9f97 0xc0036f9f98}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xd74x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xd74x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.282: INFO: Pod "webserver-deployment-847dcfb7fb-jgph6" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-jgph6 webserver-deployment-847dcfb7fb- deployment-2705  2c4c9a1b-bb21-4f93-87cf-0d0359653814 98084 0 2022-03-11 17:50:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0044c2100 0xc0044c2101}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:50:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ctllm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ctllm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.167.130,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.167.130,PodIP:192.168.2.209,StartTime:2022-03-11 17:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 17:50:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://9433b80f9fe76e7bc500f0f0d6cd1eed942ee5d5d10418f5a712e8c189f56fc5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.209,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.282: INFO: Pod "webserver-deployment-847dcfb7fb-qjbng" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-qjbng webserver-deployment-847dcfb7fb- deployment-2705  dd0ef4f5-4bf3-458c-9082-85a91800fcc6 98291 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0044c22e7 0xc0044c22e8}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rfjcl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rfjcl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.282: INFO: Pod "webserver-deployment-847dcfb7fb-sbfd5" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-sbfd5 webserver-deployment-847dcfb7fb- deployment-2705  8be7725e-749f-4142-b021-0baa8ac815fa 98081 0 2022-03-11 17:50:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0044c2450 0xc0044c2451}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:50:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.193\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dpm25,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dpm25,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.167.130,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.167.130,PodIP:192.168.2.193,StartTime:2022-03-11 17:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 17:50:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://db01ad3f1967504d10a176189dcad6281ec1919c92a9248d6ea08b0f573f1e25,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.193,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.282: INFO: Pod "webserver-deployment-847dcfb7fb-spqlx" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-spqlx webserver-deployment-847dcfb7fb- deployment-2705  86f1a780-a1f6-4a41-bb5c-3977e711d24d 98053 0 2022-03-11 17:50:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0044c2637 0xc0044c2638}] []  [{kube-controller-manager Update v1 2022-03-11 17:50:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:50:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.2.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ktw8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ktw8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.167.130,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:50:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.167.130,PodIP:192.168.2.212,StartTime:2022-03-11 17:50:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-03-11 17:50:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://123c810d13884bf6fd6c54c70fe5e019ee06997a1c284df7b5b8dd83c7954512,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.2.212,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.283: INFO: Pod "webserver-deployment-847dcfb7fb-xxq7h" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-xxq7h webserver-deployment-847dcfb7fb- deployment-2705  47607193-82ba-46fc-aea2-17d75af7907f 98273 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0044c2817 0xc0044c2818}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-54rgl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-54rgl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:198.18.16.160,PodIP:,StartTime:2022-03-11 17:51:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 11 17:51:02.283: INFO: Pod "webserver-deployment-847dcfb7fb-z6zgp" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-z6zgp webserver-deployment-847dcfb7fb- deployment-2705  be50654c-cdce-4170-a7a9-e8cab34cd4a1 98271 0 2022-03-11 17:51:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb f003f1ee-3c22-40ae-8489-2b7328f3934a 0xc0044c2a17 0xc0044c2a18}] []  [{kube-controller-manager Update v1 2022-03-11 17:51:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f003f1ee-3c22-40ae-8489-2b7328f3934a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pd4l7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pd4l7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:198.18.16.160,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-03-11 17:51:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:51:02.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2705" for this suite.

• [SLOW TEST:18.255 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":339,"completed":310,"skipped":4883,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:51:02.298: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 11 17:51:02.371: INFO: Waiting up to 5m0s for pod "pod-a052c090-0de3-48ce-a64f-c2bf843018a7" in namespace "emptydir-2745" to be "Succeeded or Failed"
Mar 11 17:51:02.375: INFO: Pod "pod-a052c090-0de3-48ce-a64f-c2bf843018a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.14119ms
Mar 11 17:51:04.381: INFO: Pod "pod-a052c090-0de3-48ce-a64f-c2bf843018a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009627809s
Mar 11 17:51:06.391: INFO: Pod "pod-a052c090-0de3-48ce-a64f-c2bf843018a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019259178s
Mar 11 17:51:08.396: INFO: Pod "pod-a052c090-0de3-48ce-a64f-c2bf843018a7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.024554742s
Mar 11 17:51:10.413: INFO: Pod "pod-a052c090-0de3-48ce-a64f-c2bf843018a7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041652635s
Mar 11 17:51:12.418: INFO: Pod "pod-a052c090-0de3-48ce-a64f-c2bf843018a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.047016726s
STEP: Saw pod success
Mar 11 17:51:12.418: INFO: Pod "pod-a052c090-0de3-48ce-a64f-c2bf843018a7" satisfied condition "Succeeded or Failed"
Mar 11 17:51:12.423: INFO: Trying to get logs from node 198.18.16.160 pod pod-a052c090-0de3-48ce-a64f-c2bf843018a7 container test-container: <nil>
STEP: delete the pod
Mar 11 17:51:12.991: INFO: Waiting for pod pod-a052c090-0de3-48ce-a64f-c2bf843018a7 to disappear
Mar 11 17:51:12.996: INFO: Pod pod-a052c090-0de3-48ce-a64f-c2bf843018a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:51:12.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2745" for this suite.

• [SLOW TEST:10.718 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":311,"skipped":4885,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:51:13.018: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Mar 11 17:51:15.161: INFO: running pods: 0 < 3
Mar 11 17:51:17.177: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:51:19.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-27" for this suite.

• [SLOW TEST:6.171 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":339,"completed":312,"skipped":4952,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:51:19.190: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-01a777ba-db8f-4fb6-9126-2461b3cd1b47
STEP: Creating a pod to test consume secrets
Mar 11 17:51:19.272: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ca4a8763-566b-436f-a094-b735ade59c6a" in namespace "projected-500" to be "Succeeded or Failed"
Mar 11 17:51:19.282: INFO: Pod "pod-projected-secrets-ca4a8763-566b-436f-a094-b735ade59c6a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.830405ms
Mar 11 17:51:21.291: INFO: Pod "pod-projected-secrets-ca4a8763-566b-436f-a094-b735ade59c6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019111534s
Mar 11 17:51:23.298: INFO: Pod "pod-projected-secrets-ca4a8763-566b-436f-a094-b735ade59c6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026034092s
STEP: Saw pod success
Mar 11 17:51:23.299: INFO: Pod "pod-projected-secrets-ca4a8763-566b-436f-a094-b735ade59c6a" satisfied condition "Succeeded or Failed"
Mar 11 17:51:23.302: INFO: Trying to get logs from node 198.18.167.130 pod pod-projected-secrets-ca4a8763-566b-436f-a094-b735ade59c6a container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 11 17:51:23.335: INFO: Waiting for pod pod-projected-secrets-ca4a8763-566b-436f-a094-b735ade59c6a to disappear
Mar 11 17:51:23.338: INFO: Pod pod-projected-secrets-ca4a8763-566b-436f-a094-b735ade59c6a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:51:23.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-500" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":313,"skipped":4966,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:51:23.354: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Mar 11 17:51:23.434: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Mar 11 17:51:23.470: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:51:23.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7832" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":339,"completed":314,"skipped":4981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:51:23.532: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-tk55
STEP: Creating a pod to test atomic-volume-subpath
Mar 11 17:51:23.612: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tk55" in namespace "subpath-4811" to be "Succeeded or Failed"
Mar 11 17:51:23.627: INFO: Pod "pod-subpath-test-configmap-tk55": Phase="Pending", Reason="", readiness=false. Elapsed: 14.74508ms
Mar 11 17:51:25.633: INFO: Pod "pod-subpath-test-configmap-tk55": Phase="Running", Reason="", readiness=true. Elapsed: 2.021018676s
Mar 11 17:51:27.642: INFO: Pod "pod-subpath-test-configmap-tk55": Phase="Running", Reason="", readiness=true. Elapsed: 4.029953164s
Mar 11 17:51:29.652: INFO: Pod "pod-subpath-test-configmap-tk55": Phase="Running", Reason="", readiness=true. Elapsed: 6.040070864s
Mar 11 17:51:31.662: INFO: Pod "pod-subpath-test-configmap-tk55": Phase="Running", Reason="", readiness=true. Elapsed: 8.04959269s
Mar 11 17:51:33.670: INFO: Pod "pod-subpath-test-configmap-tk55": Phase="Running", Reason="", readiness=true. Elapsed: 10.05789662s
Mar 11 17:51:35.678: INFO: Pod "pod-subpath-test-configmap-tk55": Phase="Running", Reason="", readiness=true. Elapsed: 12.065904721s
Mar 11 17:51:37.684: INFO: Pod "pod-subpath-test-configmap-tk55": Phase="Running", Reason="", readiness=true. Elapsed: 14.07170482s
Mar 11 17:51:39.693: INFO: Pod "pod-subpath-test-configmap-tk55": Phase="Running", Reason="", readiness=true. Elapsed: 16.081278113s
Mar 11 17:51:41.699: INFO: Pod "pod-subpath-test-configmap-tk55": Phase="Running", Reason="", readiness=true. Elapsed: 18.087460886s
Mar 11 17:51:43.707: INFO: Pod "pod-subpath-test-configmap-tk55": Phase="Running", Reason="", readiness=true. Elapsed: 20.094618484s
Mar 11 17:51:45.714: INFO: Pod "pod-subpath-test-configmap-tk55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.102411533s
STEP: Saw pod success
Mar 11 17:51:45.714: INFO: Pod "pod-subpath-test-configmap-tk55" satisfied condition "Succeeded or Failed"
Mar 11 17:51:45.718: INFO: Trying to get logs from node 198.18.167.130 pod pod-subpath-test-configmap-tk55 container test-container-subpath-configmap-tk55: <nil>
STEP: delete the pod
Mar 11 17:51:45.743: INFO: Waiting for pod pod-subpath-test-configmap-tk55 to disappear
Mar 11 17:51:45.747: INFO: Pod pod-subpath-test-configmap-tk55 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tk55
Mar 11 17:51:45.748: INFO: Deleting pod "pod-subpath-test-configmap-tk55" in namespace "subpath-4811"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:51:45.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4811" for this suite.

• [SLOW TEST:22.238 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":339,"completed":315,"skipped":5028,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:51:45.771: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-8251
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-8251
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8251
Mar 11 17:51:45.853: INFO: Found 0 stateful pods, waiting for 1
Mar 11 17:51:55.861: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 11 17:51:55.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-8251 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 11 17:51:56.452: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 11 17:51:56.452: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 11 17:51:56.452: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 11 17:51:56.459: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 11 17:52:06.467: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 11 17:52:06.467: INFO: Waiting for statefulset status.replicas updated to 0
Mar 11 17:52:06.491: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 11 17:52:06.492: INFO: ss-0  198.18.167.130  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:51:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:51:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:51:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:51:45 +0000 UTC  }]
Mar 11 17:52:06.492: INFO: 
Mar 11 17:52:06.492: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 11 17:52:07.500: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995276916s
Mar 11 17:52:08.507: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.98774832s
Mar 11 17:52:09.515: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980132102s
Mar 11 17:52:10.525: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.973427537s
Mar 11 17:52:11.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963281139s
Mar 11 17:52:12.541: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.957402514s
Mar 11 17:52:13.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.945607209s
Mar 11 17:52:14.557: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.938733537s
Mar 11 17:52:15.569: INFO: Verifying statefulset ss doesn't scale past 3 for another 928.037879ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8251
Mar 11 17:52:16.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-8251 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 11 17:52:16.775: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 11 17:52:16.775: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 11 17:52:16.775: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 11 17:52:16.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-8251 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 11 17:52:16.957: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 11 17:52:16.957: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 11 17:52:16.957: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 11 17:52:16.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-8251 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 11 17:52:17.135: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 11 17:52:17.136: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 11 17:52:17.136: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 11 17:52:17.142: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 11 17:52:17.142: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 11 17:52:17.142: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 11 17:52:17.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-8251 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 11 17:52:17.349: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 11 17:52:17.349: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 11 17:52:17.349: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 11 17:52:17.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-8251 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 11 17:52:17.550: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 11 17:52:17.550: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 11 17:52:17.550: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 11 17:52:17.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=statefulset-8251 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 11 17:52:17.727: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 11 17:52:17.727: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 11 17:52:17.727: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 11 17:52:17.727: INFO: Waiting for statefulset status.replicas updated to 0
Mar 11 17:52:17.731: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 11 17:52:27.743: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 11 17:52:27.743: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 11 17:52:27.743: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 11 17:52:27.760: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 11 17:52:27.760: INFO: ss-0  198.18.167.130  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:51:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:51:45 +0000 UTC  }]
Mar 11 17:52:27.760: INFO: ss-1  198.18.167.130  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:06 +0000 UTC  }]
Mar 11 17:52:27.760: INFO: ss-2  198.18.167.130  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:06 +0000 UTC  }]
Mar 11 17:52:27.761: INFO: 
Mar 11 17:52:27.761: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 11 17:52:28.770: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Mar 11 17:52:28.770: INFO: ss-0  198.18.167.130  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:51:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:51:45 +0000 UTC  }]
Mar 11 17:52:28.770: INFO: ss-1  198.18.167.130  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:06 +0000 UTC  }]
Mar 11 17:52:28.770: INFO: ss-2  198.18.167.130  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-03-11 17:52:06 +0000 UTC  }]
Mar 11 17:52:28.770: INFO: 
Mar 11 17:52:28.770: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 11 17:52:29.776: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.984867072s
Mar 11 17:52:30.782: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.979330942s
Mar 11 17:52:31.787: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.972841134s
Mar 11 17:52:32.795: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.967212137s
Mar 11 17:52:33.800: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.959475241s
Mar 11 17:52:34.807: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.954799487s
Mar 11 17:52:35.814: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.94705206s
Mar 11 17:52:36.822: INFO: Verifying statefulset ss doesn't scale past 0 for another 940.864012ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8251
Mar 11 17:52:37.834: INFO: Scaling statefulset ss to 0
Mar 11 17:52:37.850: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Mar 11 17:52:37.853: INFO: Deleting all statefulset in ns statefulset-8251
Mar 11 17:52:37.855: INFO: Scaling statefulset ss to 0
Mar 11 17:52:37.866: INFO: Waiting for statefulset status.replicas updated to 0
Mar 11 17:52:37.868: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:52:37.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8251" for this suite.

• [SLOW TEST:52.133 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":339,"completed":316,"skipped":5047,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:52:37.907: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-00e27ff4-ef97-48df-99b8-a3d731920b17
STEP: Creating a pod to test consume secrets
Mar 11 17:52:37.977: INFO: Waiting up to 5m0s for pod "pod-secrets-c4c8decf-67d9-4e3c-b969-248a10652542" in namespace "secrets-2233" to be "Succeeded or Failed"
Mar 11 17:52:37.981: INFO: Pod "pod-secrets-c4c8decf-67d9-4e3c-b969-248a10652542": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231219ms
Mar 11 17:52:39.990: INFO: Pod "pod-secrets-c4c8decf-67d9-4e3c-b969-248a10652542": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013104525s
STEP: Saw pod success
Mar 11 17:52:39.990: INFO: Pod "pod-secrets-c4c8decf-67d9-4e3c-b969-248a10652542" satisfied condition "Succeeded or Failed"
Mar 11 17:52:39.995: INFO: Trying to get logs from node 198.18.167.130 pod pod-secrets-c4c8decf-67d9-4e3c-b969-248a10652542 container secret-volume-test: <nil>
STEP: delete the pod
Mar 11 17:52:40.026: INFO: Waiting for pod pod-secrets-c4c8decf-67d9-4e3c-b969-248a10652542 to disappear
Mar 11 17:52:40.031: INFO: Pod pod-secrets-c4c8decf-67d9-4e3c-b969-248a10652542 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:52:40.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2233" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":317,"skipped":5065,"failed":0}
SSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:52:40.048: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-f55b869b-71df-4611-8301-b3c3c819ab1f
STEP: Creating a pod to test consume secrets
Mar 11 17:52:40.131: INFO: Waiting up to 5m0s for pod "pod-secrets-05ba84c4-7a91-4be4-9931-2894000f830b" in namespace "secrets-1067" to be "Succeeded or Failed"
Mar 11 17:52:40.135: INFO: Pod "pod-secrets-05ba84c4-7a91-4be4-9931-2894000f830b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.703948ms
Mar 11 17:52:42.142: INFO: Pod "pod-secrets-05ba84c4-7a91-4be4-9931-2894000f830b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010279581s
STEP: Saw pod success
Mar 11 17:52:42.142: INFO: Pod "pod-secrets-05ba84c4-7a91-4be4-9931-2894000f830b" satisfied condition "Succeeded or Failed"
Mar 11 17:52:42.147: INFO: Trying to get logs from node 198.18.167.130 pod pod-secrets-05ba84c4-7a91-4be4-9931-2894000f830b container secret-env-test: <nil>
STEP: delete the pod
Mar 11 17:52:42.178: INFO: Waiting for pod pod-secrets-05ba84c4-7a91-4be4-9931-2894000f830b to disappear
Mar 11 17:52:42.182: INFO: Pod pod-secrets-05ba84c4-7a91-4be4-9931-2894000f830b no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:52:42.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1067" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":339,"completed":318,"skipped":5069,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:52:42.196: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Mar 11 17:52:42.289: INFO: Waiting up to 5m0s for pod "downward-api-3a3c2ed3-8fcc-4368-96f0-1d5f4a2d76df" in namespace "downward-api-2021" to be "Succeeded or Failed"
Mar 11 17:52:42.300: INFO: Pod "downward-api-3a3c2ed3-8fcc-4368-96f0-1d5f4a2d76df": Phase="Pending", Reason="", readiness=false. Elapsed: 11.198408ms
Mar 11 17:52:44.309: INFO: Pod "downward-api-3a3c2ed3-8fcc-4368-96f0-1d5f4a2d76df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019563265s
STEP: Saw pod success
Mar 11 17:52:44.309: INFO: Pod "downward-api-3a3c2ed3-8fcc-4368-96f0-1d5f4a2d76df" satisfied condition "Succeeded or Failed"
Mar 11 17:52:44.313: INFO: Trying to get logs from node 198.18.167.130 pod downward-api-3a3c2ed3-8fcc-4368-96f0-1d5f4a2d76df container dapi-container: <nil>
STEP: delete the pod
Mar 11 17:52:44.346: INFO: Waiting for pod downward-api-3a3c2ed3-8fcc-4368-96f0-1d5f4a2d76df to disappear
Mar 11 17:52:44.351: INFO: Pod downward-api-3a3c2ed3-8fcc-4368-96f0-1d5f4a2d76df no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:52:44.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2021" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":339,"completed":319,"skipped":5069,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:52:44.364: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-a26eb008-46ca-487f-b071-bbc08128a4a4
STEP: Creating a pod to test consume secrets
Mar 11 17:52:44.432: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6c129ae3-15b6-4c5a-b04f-a08aa9aded7a" in namespace "projected-5823" to be "Succeeded or Failed"
Mar 11 17:52:44.443: INFO: Pod "pod-projected-secrets-6c129ae3-15b6-4c5a-b04f-a08aa9aded7a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.002964ms
Mar 11 17:52:46.450: INFO: Pod "pod-projected-secrets-6c129ae3-15b6-4c5a-b04f-a08aa9aded7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01768455s
Mar 11 17:52:48.458: INFO: Pod "pod-projected-secrets-6c129ae3-15b6-4c5a-b04f-a08aa9aded7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025915744s
STEP: Saw pod success
Mar 11 17:52:48.458: INFO: Pod "pod-projected-secrets-6c129ae3-15b6-4c5a-b04f-a08aa9aded7a" satisfied condition "Succeeded or Failed"
Mar 11 17:52:48.462: INFO: Trying to get logs from node 198.18.167.130 pod pod-projected-secrets-6c129ae3-15b6-4c5a-b04f-a08aa9aded7a container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 11 17:52:48.489: INFO: Waiting for pod pod-projected-secrets-6c129ae3-15b6-4c5a-b04f-a08aa9aded7a to disappear
Mar 11 17:52:48.492: INFO: Pod pod-projected-secrets-6c129ae3-15b6-4c5a-b04f-a08aa9aded7a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:52:48.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5823" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":320,"skipped":5083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:52:48.511: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Mar 11 17:52:48.571: INFO: Waiting up to 5m0s for pod "client-containers-76559480-4ff4-413e-94fd-e50116057cd1" in namespace "containers-7124" to be "Succeeded or Failed"
Mar 11 17:52:48.579: INFO: Pod "client-containers-76559480-4ff4-413e-94fd-e50116057cd1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.064961ms
Mar 11 17:52:50.586: INFO: Pod "client-containers-76559480-4ff4-413e-94fd-e50116057cd1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015196113s
STEP: Saw pod success
Mar 11 17:52:50.586: INFO: Pod "client-containers-76559480-4ff4-413e-94fd-e50116057cd1" satisfied condition "Succeeded or Failed"
Mar 11 17:52:50.589: INFO: Trying to get logs from node 198.18.167.130 pod client-containers-76559480-4ff4-413e-94fd-e50116057cd1 container agnhost-container: <nil>
STEP: delete the pod
Mar 11 17:52:50.619: INFO: Waiting for pod client-containers-76559480-4ff4-413e-94fd-e50116057cd1 to disappear
Mar 11 17:52:50.622: INFO: Pod client-containers-76559480-4ff4-413e-94fd-e50116057cd1 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:52:50.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7124" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":339,"completed":321,"skipped":5113,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:52:50.639: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Mar 11 17:52:50.718: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:52:52.725: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Mar 11 17:52:52.742: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:52:54.750: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 11 17:52:54.772: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 11 17:52:54.777: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 11 17:52:56.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 11 17:52:56.787: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 11 17:52:58.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 11 17:52:58.789: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 11 17:53:00.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 11 17:53:00.785: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 11 17:53:02.778: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 11 17:53:02.784: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:53:02.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1752" for this suite.

• [SLOW TEST:12.158 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":339,"completed":322,"skipped":5119,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:53:02.800: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-309f3b08-a344-4439-9b19-a1c5a150d850
STEP: Creating a pod to test consume configMaps
Mar 11 17:53:02.891: INFO: Waiting up to 5m0s for pod "pod-configmaps-9c8be504-c68c-4b43-ab6c-fd0851abdfde" in namespace "configmap-4029" to be "Succeeded or Failed"
Mar 11 17:53:02.897: INFO: Pod "pod-configmaps-9c8be504-c68c-4b43-ab6c-fd0851abdfde": Phase="Pending", Reason="", readiness=false. Elapsed: 5.799869ms
Mar 11 17:53:04.903: INFO: Pod "pod-configmaps-9c8be504-c68c-4b43-ab6c-fd0851abdfde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011076215s
STEP: Saw pod success
Mar 11 17:53:04.903: INFO: Pod "pod-configmaps-9c8be504-c68c-4b43-ab6c-fd0851abdfde" satisfied condition "Succeeded or Failed"
Mar 11 17:53:04.907: INFO: Trying to get logs from node 198.18.167.130 pod pod-configmaps-9c8be504-c68c-4b43-ab6c-fd0851abdfde container agnhost-container: <nil>
STEP: delete the pod
Mar 11 17:53:04.931: INFO: Waiting for pod pod-configmaps-9c8be504-c68c-4b43-ab6c-fd0851abdfde to disappear
Mar 11 17:53:04.934: INFO: Pod pod-configmaps-9c8be504-c68c-4b43-ab6c-fd0851abdfde no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:53:04.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4029" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":323,"skipped":5125,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:53:04.947: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 11 17:53:07.100: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:53:07.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7889" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":324,"skipped":5142,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:53:07.150: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:53:07.212: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: creating the pod
STEP: submitting the pod to kubernetes
Mar 11 17:53:07.235: INFO: The status of Pod pod-exec-websocket-cd301936-84cf-4b9c-95bf-dc8b039cd975 is Pending, waiting for it to be Running (with Ready = true)
Mar 11 17:53:09.243: INFO: The status of Pod pod-exec-websocket-cd301936-84cf-4b9c-95bf-dc8b039cd975 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:53:09.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5173" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":339,"completed":325,"skipped":5150,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:53:09.351: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Mar 11 17:53:09.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7777 create -f -'
Mar 11 17:53:10.189: INFO: stderr: ""
Mar 11 17:53:10.189: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Mar 11 17:53:10.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7777 diff -f -'
Mar 11 17:53:10.764: INFO: rc: 1
Mar 11 17:53:10.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7777 delete -f -'
Mar 11 17:53:10.875: INFO: stderr: ""
Mar 11 17:53:10.875: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:53:10.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7777" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":339,"completed":326,"skipped":5164,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:53:10.896: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6018.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6018.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 11 17:53:13.058: INFO: DNS probes using dns-test-5813bded-bd89-4a11-912f-0cb6777e79b2 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6018.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6018.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 11 17:53:15.176: INFO: File wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local from pod  dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 11 17:53:15.181: INFO: File jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local from pod  dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 11 17:53:15.181: INFO: Lookups using dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 failed for: [wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local]

Mar 11 17:53:20.197: INFO: File wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local from pod  dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 11 17:53:20.206: INFO: File jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local from pod  dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 11 17:53:20.206: INFO: Lookups using dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 failed for: [wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local]

Mar 11 17:53:25.189: INFO: File wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local from pod  dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 11 17:53:25.194: INFO: File jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local from pod  dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 11 17:53:25.194: INFO: Lookups using dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 failed for: [wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local]

Mar 11 17:53:30.187: INFO: File wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local from pod  dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 11 17:53:30.193: INFO: File jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local from pod  dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 11 17:53:30.194: INFO: Lookups using dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 failed for: [wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local]

Mar 11 17:53:35.190: INFO: File wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local from pod  dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 11 17:53:35.195: INFO: File jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local from pod  dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 11 17:53:35.195: INFO: Lookups using dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 failed for: [wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local]

Mar 11 17:53:40.188: INFO: File wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local from pod  dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 11 17:53:40.193: INFO: File jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local from pod  dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 11 17:53:40.193: INFO: Lookups using dns-6018/dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 failed for: [wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local]

Mar 11 17:53:45.193: INFO: DNS probes using dns-test-f0459a64-237b-4a26-a6e1-0a9f2d3de094 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6018.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6018.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6018.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6018.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 11 17:53:47.323: INFO: DNS probes using dns-test-59167fd9-4974-4c87-b7d1-de4adf777aac succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:53:47.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6018" for this suite.

• [SLOW TEST:36.502 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":339,"completed":327,"skipped":5194,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:53:47.401: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Mar 11 17:53:47.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7654 create -f -'
Mar 11 17:53:48.026: INFO: stderr: ""
Mar 11 17:53:48.026: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 11 17:53:48.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 11 17:53:48.153: INFO: stderr: ""
Mar 11 17:53:48.153: INFO: stdout: "update-demo-nautilus-km2rj update-demo-nautilus-q2rtf "
Mar 11 17:53:48.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7654 get pods update-demo-nautilus-km2rj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 11 17:53:48.255: INFO: stderr: ""
Mar 11 17:53:48.255: INFO: stdout: ""
Mar 11 17:53:48.255: INFO: update-demo-nautilus-km2rj is created but not running
Mar 11 17:53:53.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7654 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 11 17:53:53.385: INFO: stderr: ""
Mar 11 17:53:53.385: INFO: stdout: "update-demo-nautilus-km2rj update-demo-nautilus-q2rtf "
Mar 11 17:53:53.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7654 get pods update-demo-nautilus-km2rj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 11 17:53:53.505: INFO: stderr: ""
Mar 11 17:53:53.505: INFO: stdout: "true"
Mar 11 17:53:53.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7654 get pods update-demo-nautilus-km2rj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 11 17:53:53.621: INFO: stderr: ""
Mar 11 17:53:53.621: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar 11 17:53:53.621: INFO: validating pod update-demo-nautilus-km2rj
Mar 11 17:53:53.628: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 11 17:53:53.628: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 11 17:53:53.628: INFO: update-demo-nautilus-km2rj is verified up and running
Mar 11 17:53:53.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7654 get pods update-demo-nautilus-q2rtf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 11 17:53:53.723: INFO: stderr: ""
Mar 11 17:53:53.723: INFO: stdout: "true"
Mar 11 17:53:53.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7654 get pods update-demo-nautilus-q2rtf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 11 17:53:53.826: INFO: stderr: ""
Mar 11 17:53:53.826: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Mar 11 17:53:53.826: INFO: validating pod update-demo-nautilus-q2rtf
Mar 11 17:53:53.831: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 11 17:53:53.831: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 11 17:53:53.831: INFO: update-demo-nautilus-q2rtf is verified up and running
STEP: using delete to clean up resources
Mar 11 17:53:53.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7654 delete --grace-period=0 --force -f -'
Mar 11 17:53:53.956: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 11 17:53:53.956: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 11 17:53:53.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7654 get rc,svc -l name=update-demo --no-headers'
Mar 11 17:53:54.070: INFO: stderr: "No resources found in kubectl-7654 namespace.\n"
Mar 11 17:53:54.070: INFO: stdout: ""
Mar 11 17:53:54.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=kubectl-7654 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 11 17:53:54.165: INFO: stderr: ""
Mar 11 17:53:54.165: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:53:54.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7654" for this suite.

• [SLOW TEST:6.780 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":339,"completed":328,"skipped":5199,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:53:54.181: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Mar 11 17:53:54.258: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a4cfcca-1b08-45bd-946f-74b47984d2df" in namespace "projected-4953" to be "Succeeded or Failed"
Mar 11 17:53:54.262: INFO: Pod "downwardapi-volume-9a4cfcca-1b08-45bd-946f-74b47984d2df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.301278ms
Mar 11 17:53:56.271: INFO: Pod "downwardapi-volume-9a4cfcca-1b08-45bd-946f-74b47984d2df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012992587s
STEP: Saw pod success
Mar 11 17:53:56.271: INFO: Pod "downwardapi-volume-9a4cfcca-1b08-45bd-946f-74b47984d2df" satisfied condition "Succeeded or Failed"
Mar 11 17:53:56.275: INFO: Trying to get logs from node 198.18.167.130 pod downwardapi-volume-9a4cfcca-1b08-45bd-946f-74b47984d2df container client-container: <nil>
STEP: delete the pod
Mar 11 17:53:56.304: INFO: Waiting for pod downwardapi-volume-9a4cfcca-1b08-45bd-946f-74b47984d2df to disappear
Mar 11 17:53:56.307: INFO: Pod downwardapi-volume-9a4cfcca-1b08-45bd-946f-74b47984d2df no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:53:56.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4953" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":329,"skipped":5210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:53:56.324: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Mar 11 17:54:16.603: INFO: EndpointSlice for Service endpointslice-6305/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:54:26.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6305" for this suite.

• [SLOW TEST:30.305 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":339,"completed":330,"skipped":5233,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:54:26.630: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:54:31.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3390" for this suite.

• [SLOW TEST:5.213 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":339,"completed":331,"skipped":5240,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:54:31.842: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:54:31.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6049" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":339,"completed":332,"skipped":5254,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:54:31.923: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:54:32.283: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 11 17:54:34.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782618072, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782618072, loc:(*time.Location)(0x9e12f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63782618072, loc:(*time.Location)(0x9e12f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63782618072, loc:(*time.Location)(0x9e12f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:54:37.342: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 11 17:54:39.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-070390966 --namespace=webhook-423 attach --namespace=webhook-423 to-be-attached-pod -i -c=container1'
Mar 11 17:54:39.502: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:54:39.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-423" for this suite.
STEP: Destroying namespace "webhook-423-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.669 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":339,"completed":333,"skipped":5254,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:54:39.593: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-b5434319-953a-4468-9d42-ba4ff11caaa8
STEP: Creating a pod to test consume secrets
Mar 11 17:54:39.681: INFO: Waiting up to 5m0s for pod "pod-secrets-a3e04940-a57e-41b5-b140-ed4eee31550b" in namespace "secrets-5132" to be "Succeeded or Failed"
Mar 11 17:54:39.695: INFO: Pod "pod-secrets-a3e04940-a57e-41b5-b140-ed4eee31550b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.135812ms
Mar 11 17:54:41.701: INFO: Pod "pod-secrets-a3e04940-a57e-41b5-b140-ed4eee31550b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019553541s
STEP: Saw pod success
Mar 11 17:54:41.701: INFO: Pod "pod-secrets-a3e04940-a57e-41b5-b140-ed4eee31550b" satisfied condition "Succeeded or Failed"
Mar 11 17:54:41.704: INFO: Trying to get logs from node 198.18.167.130 pod pod-secrets-a3e04940-a57e-41b5-b140-ed4eee31550b container secret-volume-test: <nil>
STEP: delete the pod
Mar 11 17:54:41.739: INFO: Waiting for pod pod-secrets-a3e04940-a57e-41b5-b140-ed4eee31550b to disappear
Mar 11 17:54:41.742: INFO: Pod pod-secrets-a3e04940-a57e-41b5-b140-ed4eee31550b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:54:41.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5132" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":334,"skipped":5329,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:54:41.757: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:55:41.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6899" for this suite.

• [SLOW TEST:60.105 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":339,"completed":335,"skipped":5349,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:55:41.865: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:55:42.895: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:55:45.940: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:55:58.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3339" for this suite.
STEP: Destroying namespace "webhook-3339-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.372 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":339,"completed":336,"skipped":5368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:55:58.237: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-f3fc6eda-809c-4370-b71c-b4acfdda62df
STEP: Creating a pod to test consume configMaps
Mar 11 17:55:58.334: INFO: Waiting up to 5m0s for pod "pod-configmaps-22b2008b-0d3c-4581-971e-a84703e23516" in namespace "configmap-2238" to be "Succeeded or Failed"
Mar 11 17:55:58.344: INFO: Pod "pod-configmaps-22b2008b-0d3c-4581-971e-a84703e23516": Phase="Pending", Reason="", readiness=false. Elapsed: 8.503366ms
Mar 11 17:56:00.354: INFO: Pod "pod-configmaps-22b2008b-0d3c-4581-971e-a84703e23516": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018922801s
STEP: Saw pod success
Mar 11 17:56:00.354: INFO: Pod "pod-configmaps-22b2008b-0d3c-4581-971e-a84703e23516" satisfied condition "Succeeded or Failed"
Mar 11 17:56:00.358: INFO: Trying to get logs from node 198.18.167.130 pod pod-configmaps-22b2008b-0d3c-4581-971e-a84703e23516 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 11 17:56:00.394: INFO: Waiting for pod pod-configmaps-22b2008b-0d3c-4581-971e-a84703e23516 to disappear
Mar 11 17:56:00.401: INFO: Pod pod-configmaps-22b2008b-0d3c-4581-971e-a84703e23516 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:56:00.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2238" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":337,"skipped":5406,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:56:00.416: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 11 17:56:00.939: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 11 17:56:04.002: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Mar 11 17:56:04.010: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:56:07.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7882" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.353 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":339,"completed":338,"skipped":5412,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Mar 11 17:56:08.770: INFO: >>> kubeConfig: /tmp/kubeconfig-070390966
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Mar 11 17:56:08.851: INFO: Waiting up to 5m0s for pod "var-expansion-c7be79d2-036e-4459-807c-3736399307cd" in namespace "var-expansion-2046" to be "Succeeded or Failed"
Mar 11 17:56:08.869: INFO: Pod "var-expansion-c7be79d2-036e-4459-807c-3736399307cd": Phase="Pending", Reason="", readiness=false. Elapsed: 17.50746ms
Mar 11 17:56:10.876: INFO: Pod "var-expansion-c7be79d2-036e-4459-807c-3736399307cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024918727s
STEP: Saw pod success
Mar 11 17:56:10.877: INFO: Pod "var-expansion-c7be79d2-036e-4459-807c-3736399307cd" satisfied condition "Succeeded or Failed"
Mar 11 17:56:10.881: INFO: Trying to get logs from node 198.18.167.130 pod var-expansion-c7be79d2-036e-4459-807c-3736399307cd container dapi-container: <nil>
STEP: delete the pod
Mar 11 17:56:10.910: INFO: Waiting for pod var-expansion-c7be79d2-036e-4459-807c-3736399307cd to disappear
Mar 11 17:56:10.917: INFO: Pod var-expansion-c7be79d2-036e-4459-807c-3736399307cd no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Mar 11 17:56:10.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2046" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":339,"completed":339,"skipped":5426,"failed":0}
SSSSSMar 11 17:56:10.937: INFO: Running AfterSuite actions on all nodes
Mar 11 17:56:10.937: INFO: Running AfterSuite actions on node 1
Mar 11 17:56:10.937: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":339,"completed":339,"skipped":5431,"failed":0}

Ran 339 of 5770 Specs in 7210.328 seconds
SUCCESS! -- 339 Passed | 0 Failed | 0 Pending | 5431 Skipped
PASS

Ginkgo ran 1 suite in 2h0m12.698330882s
Test Suite Passed
