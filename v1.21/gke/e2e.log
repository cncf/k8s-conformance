I0712 20:30:21.295910      20 e2e.go:129] Starting e2e run "9c130501-7225-4214-917d-490219467f1b" on Ginkgo node 1
{"msg":"Test Suite starting","total":339,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1626121818 - Will randomize all specs
Will run 339 of 5771 specs

Jul 12 20:30:21.312: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:30:21.315: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 12 20:30:21.341: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 12 20:30:21.431: INFO: 19 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 12 20:30:21.431: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Jul 12 20:30:21.431: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 12 20:30:21.443: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'fluentbit-gke' (0 seconds elapsed)
Jul 12 20:30:21.443: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'gke-metrics-agent' (0 seconds elapsed)
Jul 12 20:30:21.443: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'gke-metrics-agent-windows' (0 seconds elapsed)
Jul 12 20:30:21.443: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 12 20:30:21.443: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'metadata-proxy-v0.1' (0 seconds elapsed)
Jul 12 20:30:21.443: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Jul 12 20:30:21.443: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'pdcsi-node' (0 seconds elapsed)
Jul 12 20:30:21.443: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'pdcsi-node-windows' (0 seconds elapsed)
Jul 12 20:30:21.443: INFO: e2e test version: v1.21.1
Jul 12 20:30:21.450: INFO: kube-apiserver version: v1.21.1-gke.2200
Jul 12 20:30:21.450: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:30:21.458: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:30:21.459: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
W0712 20:30:21.499890      20 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jul 12 20:30:21.499: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jul 12 20:30:21.509: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jul 12 20:30:21.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8364 create -f -'
Jul 12 20:30:22.379: INFO: stderr: ""
Jul 12 20:30:22.379: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 12 20:30:22.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8364 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 12 20:30:22.474: INFO: stderr: ""
Jul 12 20:30:22.474: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0
Jul 12 20:30:27.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8364 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 12 20:30:27.585: INFO: stderr: ""
Jul 12 20:30:27.585: INFO: stdout: "update-demo-nautilus-dj8tx update-demo-nautilus-ws868 "
Jul 12 20:30:27.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8364 get pods update-demo-nautilus-dj8tx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 12 20:30:27.679: INFO: stderr: ""
Jul 12 20:30:27.679: INFO: stdout: ""
Jul 12 20:30:27.679: INFO: update-demo-nautilus-dj8tx is created but not running
Jul 12 20:30:32.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8364 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 12 20:30:32.821: INFO: stderr: ""
Jul 12 20:30:32.821: INFO: stdout: "update-demo-nautilus-dj8tx update-demo-nautilus-ws868 "
Jul 12 20:30:32.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8364 get pods update-demo-nautilus-dj8tx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 12 20:30:32.920: INFO: stderr: ""
Jul 12 20:30:32.920: INFO: stdout: "true"
Jul 12 20:30:32.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8364 get pods update-demo-nautilus-dj8tx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 12 20:30:33.014: INFO: stderr: ""
Jul 12 20:30:33.014: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 12 20:30:33.014: INFO: validating pod update-demo-nautilus-dj8tx
Jul 12 20:30:33.024: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 20:30:33.024: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 20:30:33.024: INFO: update-demo-nautilus-dj8tx is verified up and running
Jul 12 20:30:33.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8364 get pods update-demo-nautilus-ws868 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 12 20:30:33.122: INFO: stderr: ""
Jul 12 20:30:33.122: INFO: stdout: "true"
Jul 12 20:30:33.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8364 get pods update-demo-nautilus-ws868 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 12 20:30:33.215: INFO: stderr: ""
Jul 12 20:30:33.215: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 12 20:30:33.215: INFO: validating pod update-demo-nautilus-ws868
Jul 12 20:30:33.223: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 20:30:33.223: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 20:30:33.223: INFO: update-demo-nautilus-ws868 is verified up and running
STEP: using delete to clean up resources
Jul 12 20:30:33.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8364 delete --grace-period=0 --force -f -'
Jul 12 20:30:33.327: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 20:30:33.327: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 12 20:30:33.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8364 get rc,svc -l name=update-demo --no-headers'
Jul 12 20:30:33.435: INFO: stderr: "No resources found in kubectl-8364 namespace.\n"
Jul 12 20:30:33.435: INFO: stdout: ""
Jul 12 20:30:33.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8364 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 12 20:30:33.544: INFO: stderr: ""
Jul 12 20:30:33.544: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:30:33.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8364" for this suite.

• [SLOW TEST:12.097 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":339,"completed":1,"skipped":20,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:30:33.556: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jul 12 20:30:33.622: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:30:35.631: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:30:37.638: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:30:39.661: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jul 12 20:30:39.760: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:30:41.776: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:30:43.785: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:30:45.768: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jul 12 20:30:45.780: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 12 20:30:45.784: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 12 20:30:47.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 12 20:30:47.811: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 12 20:30:49.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 12 20:30:49.789: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 12 20:30:51.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 12 20:30:51.798: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 12 20:30:53.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 12 20:30:53.793: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 12 20:30:55.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 12 20:30:55.793: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 12 20:30:57.785: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 12 20:30:57.800: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 12 20:30:59.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 12 20:30:59.820: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:30:59.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2988" for this suite.

• [SLOW TEST:26.327 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":339,"completed":2,"skipped":29,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:30:59.883: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:30:59.951: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul 12 20:30:59.970: INFO: The status of Pod pod-exec-websocket-bdb35fa2-0d08-4796-8ab5-aeb4a98df619 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:31:01.981: INFO: The status of Pod pod-exec-websocket-bdb35fa2-0d08-4796-8ab5-aeb4a98df619 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:31:02.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1419" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":339,"completed":3,"skipped":40,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:31:02.127: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-4d31491b-fece-4acd-8601-6dccf41e2cf3
STEP: Creating a pod to test consume secrets
Jul 12 20:31:02.234: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9ecbb378-25bd-4567-bc49-49f3fa9c654b" in namespace "projected-151" to be "Succeeded or Failed"
Jul 12 20:31:02.240: INFO: Pod "pod-projected-secrets-9ecbb378-25bd-4567-bc49-49f3fa9c654b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.416316ms
Jul 12 20:31:04.246: INFO: Pod "pod-projected-secrets-9ecbb378-25bd-4567-bc49-49f3fa9c654b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012160213s
Jul 12 20:31:06.254: INFO: Pod "pod-projected-secrets-9ecbb378-25bd-4567-bc49-49f3fa9c654b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019439564s
Jul 12 20:31:08.278: INFO: Pod "pod-projected-secrets-9ecbb378-25bd-4567-bc49-49f3fa9c654b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043916534s
STEP: Saw pod success
Jul 12 20:31:08.278: INFO: Pod "pod-projected-secrets-9ecbb378-25bd-4567-bc49-49f3fa9c654b" satisfied condition "Succeeded or Failed"
Jul 12 20:31:08.286: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-1pbx pod pod-projected-secrets-9ecbb378-25bd-4567-bc49-49f3fa9c654b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 12 20:31:08.324: INFO: Waiting for pod pod-projected-secrets-9ecbb378-25bd-4567-bc49-49f3fa9c654b to disappear
Jul 12 20:31:08.331: INFO: Pod pod-projected-secrets-9ecbb378-25bd-4567-bc49-49f3fa9c654b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:31:08.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-151" for this suite.

• [SLOW TEST:6.214 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":4,"skipped":105,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:31:08.342: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jul 12 20:31:08.409: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the sample API server.
Jul 12 20:31:08.940: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jul 12 20:31:11.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718669, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 20:31:13.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718669, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 20:31:15.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718669, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 20:31:17.034: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718669, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 20:31:19.035: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718669, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761718668, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-64f6b9dc99\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 20:31:24.291: INFO: Waited 3.242223415s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jul 12 20:31:24.883: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:31:25.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5206" for this suite.

• [SLOW TEST:17.392 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":339,"completed":5,"skipped":116,"failed":0}
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:31:25.734: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 12 20:31:28.130: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:31:28.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3467" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":6,"skipped":117,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:31:28.168: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:31:28.284: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-80427dd3-30d6-475d-b117-480c5688b2b3" in namespace "security-context-test-5601" to be "Succeeded or Failed"
Jul 12 20:31:28.311: INFO: Pod "alpine-nnp-false-80427dd3-30d6-475d-b117-480c5688b2b3": Phase="Pending", Reason="", readiness=false. Elapsed: 27.060422ms
Jul 12 20:31:30.319: INFO: Pod "alpine-nnp-false-80427dd3-30d6-475d-b117-480c5688b2b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035034876s
Jul 12 20:31:32.333: INFO: Pod "alpine-nnp-false-80427dd3-30d6-475d-b117-480c5688b2b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048995711s
Jul 12 20:31:32.333: INFO: Pod "alpine-nnp-false-80427dd3-30d6-475d-b117-480c5688b2b3" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:31:32.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5601" for this suite.
•{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":7,"skipped":122,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:31:32.384: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jul 12 20:31:38.611: INFO: 0 pods remaining
Jul 12 20:31:38.611: INFO: 0 pods has nil DeletionTimestamp
Jul 12 20:31:38.611: INFO: 
STEP: Gathering metrics
W0712 20:31:39.572734      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0712 20:31:39.572765      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0712 20:31:39.572773      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 12 20:31:39.572: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:31:39.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-642" for this suite.

• [SLOW TEST:7.212 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":339,"completed":8,"skipped":134,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:31:39.597: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:31:56.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7982" for this suite.

• [SLOW TEST:16.418 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":339,"completed":9,"skipped":147,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:31:56.017: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 12 20:31:58.154: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:31:58.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3826" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":339,"completed":10,"skipped":153,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:31:58.263: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jul 12 20:31:58.361: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:32:00.373: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jul 12 20:32:00.394: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:32:02.407: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 12 20:32:02.429: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 12 20:32:02.433: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 12 20:32:04.434: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 12 20:32:04.448: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 12 20:32:06.434: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 12 20:32:06.440: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 12 20:32:08.433: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 12 20:32:08.443: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:32:08.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5246" for this suite.

• [SLOW TEST:10.191 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":339,"completed":11,"skipped":160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:32:08.456: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: mirroring a new custom Endpoint
Jul 12 20:32:08.591: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
STEP: mirroring deletion of a custom Endpoint
Jul 12 20:32:10.623: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:32:12.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5484" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":339,"completed":12,"skipped":255,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:32:12.653: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:32:12.775: INFO: The status of Pod pod-secrets-72026bac-952b-4409-b1d0-e452578eb524 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:32:14.786: INFO: The status of Pod pod-secrets-72026bac-952b-4409-b1d0-e452578eb524 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:32:14.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6714" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":339,"completed":13,"skipped":277,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:32:14.836: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:32:14.892: INFO: Waiting up to 5m0s for pod "busybox-user-65534-cc736509-5d0e-499a-bafb-de2387ec0053" in namespace "security-context-test-175" to be "Succeeded or Failed"
Jul 12 20:32:14.904: INFO: Pod "busybox-user-65534-cc736509-5d0e-499a-bafb-de2387ec0053": Phase="Pending", Reason="", readiness=false. Elapsed: 11.740643ms
Jul 12 20:32:16.918: INFO: Pod "busybox-user-65534-cc736509-5d0e-499a-bafb-de2387ec0053": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025703738s
Jul 12 20:32:16.918: INFO: Pod "busybox-user-65534-cc736509-5d0e-499a-bafb-de2387ec0053" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:32:16.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-175" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":14,"skipped":298,"failed":0}
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:32:16.936: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:32:17.005: INFO: The status of Pod busybox-readonly-fsc11e3ea1-fce5-425f-a9dd-249bcf06432f is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:32:19.014: INFO: The status of Pod busybox-readonly-fsc11e3ea1-fce5-425f-a9dd-249bcf06432f is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:32:19.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6293" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":15,"skipped":305,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:32:19.034: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-8fd36400-6682-4e92-8129-eeceeb46deff
STEP: Creating a pod to test consume secrets
Jul 12 20:32:19.088: INFO: Waiting up to 5m0s for pod "pod-secrets-47e22ff3-fc7f-49dd-b398-573a7e60f0cb" in namespace "secrets-1078" to be "Succeeded or Failed"
Jul 12 20:32:19.098: INFO: Pod "pod-secrets-47e22ff3-fc7f-49dd-b398-573a7e60f0cb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0722ms
Jul 12 20:32:21.107: INFO: Pod "pod-secrets-47e22ff3-fc7f-49dd-b398-573a7e60f0cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019184843s
STEP: Saw pod success
Jul 12 20:32:21.108: INFO: Pod "pod-secrets-47e22ff3-fc7f-49dd-b398-573a7e60f0cb" satisfied condition "Succeeded or Failed"
Jul 12 20:32:21.112: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-g6xf pod pod-secrets-47e22ff3-fc7f-49dd-b398-573a7e60f0cb container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 20:32:21.133: INFO: Waiting for pod pod-secrets-47e22ff3-fc7f-49dd-b398-573a7e60f0cb to disappear
Jul 12 20:32:21.137: INFO: Pod pod-secrets-47e22ff3-fc7f-49dd-b398-573a7e60f0cb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:32:21.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1078" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":16,"skipped":306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:32:21.150: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:32:21.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7903" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":339,"completed":17,"skipped":339,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:32:21.224: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-86e4463f-94f7-4df3-9147-3e021e7beb30
STEP: Creating the pod
Jul 12 20:32:21.312: INFO: The status of Pod pod-configmaps-2d725030-85b5-4bc0-8e8d-35f6ec207883 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:32:23.333: INFO: The status of Pod pod-configmaps-2d725030-85b5-4bc0-8e8d-35f6ec207883 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-86e4463f-94f7-4df3-9147-3e021e7beb30
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:32:25.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8706" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":18,"skipped":352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:32:25.379: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-4beab923-887b-4a2e-9f83-32e02552a0b8
STEP: Creating a pod to test consume configMaps
Jul 12 20:32:25.446: INFO: Waiting up to 5m0s for pod "pod-configmaps-c57861fb-4324-45ae-bbfa-ebad2afae78c" in namespace "configmap-3775" to be "Succeeded or Failed"
Jul 12 20:32:25.455: INFO: Pod "pod-configmaps-c57861fb-4324-45ae-bbfa-ebad2afae78c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.717861ms
Jul 12 20:32:27.471: INFO: Pod "pod-configmaps-c57861fb-4324-45ae-bbfa-ebad2afae78c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025472372s
STEP: Saw pod success
Jul 12 20:32:27.471: INFO: Pod "pod-configmaps-c57861fb-4324-45ae-bbfa-ebad2afae78c" satisfied condition "Succeeded or Failed"
Jul 12 20:32:27.476: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-1pbx pod pod-configmaps-c57861fb-4324-45ae-bbfa-ebad2afae78c container agnhost-container: <nil>
STEP: delete the pod
Jul 12 20:32:27.503: INFO: Waiting for pod pod-configmaps-c57861fb-4324-45ae-bbfa-ebad2afae78c to disappear
Jul 12 20:32:27.509: INFO: Pod pod-configmaps-c57861fb-4324-45ae-bbfa-ebad2afae78c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:32:27.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3775" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":19,"skipped":374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:32:27.527: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 12 20:32:27.656: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4457  17c05441-d282-4429-939e-49509de048ce 7572 0 2021-07-12 20:32:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-12 20:32:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 20:32:27.656: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4457  17c05441-d282-4429-939e-49509de048ce 7573 0 2021-07-12 20:32:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-12 20:32:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 20:32:27.656: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4457  17c05441-d282-4429-939e-49509de048ce 7575 0 2021-07-12 20:32:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-12 20:32:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 12 20:32:37.727: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4457  17c05441-d282-4429-939e-49509de048ce 7645 0 2021-07-12 20:32:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-12 20:32:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 20:32:37.728: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4457  17c05441-d282-4429-939e-49509de048ce 7646 0 2021-07-12 20:32:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-12 20:32:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 20:32:37.728: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4457  17c05441-d282-4429-939e-49509de048ce 7647 0 2021-07-12 20:32:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-07-12 20:32:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:32:37.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4457" for this suite.

• [SLOW TEST:10.218 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":339,"completed":20,"skipped":402,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:32:37.746: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jul 12 20:32:58.325: INFO: EndpointSlice for Service endpointslice-9026/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:33:08.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9026" for this suite.

• [SLOW TEST:30.620 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":339,"completed":21,"skipped":403,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:33:08.365: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-09e17743-f63b-49eb-973e-5d42d67c1c19 in namespace container-probe-4571
Jul 12 20:33:10.446: INFO: Started pod liveness-09e17743-f63b-49eb-973e-5d42d67c1c19 in namespace container-probe-4571
STEP: checking the pod's current state and verifying that restartCount is present
Jul 12 20:33:10.449: INFO: Initial restart count of pod liveness-09e17743-f63b-49eb-973e-5d42d67c1c19 is 0
Jul 12 20:33:30.558: INFO: Restart count of pod container-probe-4571/liveness-09e17743-f63b-49eb-973e-5d42d67c1c19 is now 1 (20.108984177s elapsed)
Jul 12 20:33:50.664: INFO: Restart count of pod container-probe-4571/liveness-09e17743-f63b-49eb-973e-5d42d67c1c19 is now 2 (40.215245052s elapsed)
Jul 12 20:34:10.785: INFO: Restart count of pod container-probe-4571/liveness-09e17743-f63b-49eb-973e-5d42d67c1c19 is now 3 (1m0.335424997s elapsed)
Jul 12 20:34:30.895: INFO: Restart count of pod container-probe-4571/liveness-09e17743-f63b-49eb-973e-5d42d67c1c19 is now 4 (1m20.44625118s elapsed)
Jul 12 20:35:43.311: INFO: Restart count of pod container-probe-4571/liveness-09e17743-f63b-49eb-973e-5d42d67c1c19 is now 5 (2m32.862294707s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:35:43.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4571" for this suite.

• [SLOW TEST:154.999 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":339,"completed":22,"skipped":405,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:35:43.364: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5797
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5797
I0712 20:35:43.679949      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5797, replica count: 2
I0712 20:35:46.733866      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 20:35:46.734: INFO: Creating new exec pod
Jul 12 20:35:49.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5797 exec execpodbbwvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 12 20:35:51.014: INFO: rc: 1
Jul 12 20:35:51.014: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5797 exec execpodbbwvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80:
Command stdout:

stderr:
+ echo hostName
+ nc -v -t -w 2 externalname-service 80
nc: connect to externalname-service port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 12 20:35:52.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5797 exec execpodbbwvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 12 20:35:53.317: INFO: rc: 1
Jul 12 20:35:53.317: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5797 exec execpodbbwvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80:
Command stdout:

stderr:
+ echo hostName
+ nc -v -t -w 2 externalname-service 80
nc: connect to externalname-service port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 12 20:35:54.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5797 exec execpodbbwvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 12 20:35:54.238: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 12 20:35:54.238: INFO: stdout: "externalname-service-d975q"
Jul 12 20:35:54.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5797 exec execpodbbwvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.133.10 80'
Jul 12 20:35:54.420: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.133.10 80\nConnection to 10.37.133.10 80 port [tcp/http] succeeded!\n"
Jul 12 20:35:54.420: INFO: stdout: ""
Jul 12 20:35:55.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5797 exec execpodbbwvj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.133.10 80'
Jul 12 20:35:55.636: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.133.10 80\nConnection to 10.37.133.10 80 port [tcp/http] succeeded!\n"
Jul 12 20:35:55.636: INFO: stdout: "externalname-service-d975q"
Jul 12 20:35:55.636: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:35:55.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5797" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:12.345 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":339,"completed":23,"skipped":405,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:35:55.709: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 12 20:35:55.872: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9093  90a3d3bd-1fdf-4d96-a4d6-a7c015534623 8822 0 2021-07-12 20:35:55 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-07-12 20:35:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 20:35:55.872: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9093  90a3d3bd-1fdf-4d96-a4d6-a7c015534623 8823 0 2021-07-12 20:35:55 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-07-12 20:35:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:35:55.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9093" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":339,"completed":24,"skipped":411,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:35:55.888: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:35:56.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-741" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":339,"completed":25,"skipped":430,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:35:56.037: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-1f9c0664-951f-4984-b6d7-06617f0bf62e
STEP: Creating a pod to test consume configMaps
Jul 12 20:35:56.121: INFO: Waiting up to 5m0s for pod "pod-configmaps-6aa46788-4e37-4f3a-8000-8c15cf19e82d" in namespace "configmap-3993" to be "Succeeded or Failed"
Jul 12 20:35:56.127: INFO: Pod "pod-configmaps-6aa46788-4e37-4f3a-8000-8c15cf19e82d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.933642ms
Jul 12 20:35:58.142: INFO: Pod "pod-configmaps-6aa46788-4e37-4f3a-8000-8c15cf19e82d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021028295s
STEP: Saw pod success
Jul 12 20:35:58.142: INFO: Pod "pod-configmaps-6aa46788-4e37-4f3a-8000-8c15cf19e82d" satisfied condition "Succeeded or Failed"
Jul 12 20:35:58.148: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-configmaps-6aa46788-4e37-4f3a-8000-8c15cf19e82d container configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 20:35:58.206: INFO: Waiting for pod pod-configmaps-6aa46788-4e37-4f3a-8000-8c15cf19e82d to disappear
Jul 12 20:35:58.220: INFO: Pod pod-configmaps-6aa46788-4e37-4f3a-8000-8c15cf19e82d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:35:58.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3993" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":26,"skipped":433,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:35:58.242: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 20:35:58.368: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a1d0da4-c637-4f81-a491-a3350cf03d32" in namespace "downward-api-4050" to be "Succeeded or Failed"
Jul 12 20:35:58.376: INFO: Pod "downwardapi-volume-8a1d0da4-c637-4f81-a491-a3350cf03d32": Phase="Pending", Reason="", readiness=false. Elapsed: 7.640814ms
Jul 12 20:36:00.381: INFO: Pod "downwardapi-volume-8a1d0da4-c637-4f81-a491-a3350cf03d32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013254863s
STEP: Saw pod success
Jul 12 20:36:00.382: INFO: Pod "downwardapi-volume-8a1d0da4-c637-4f81-a491-a3350cf03d32" satisfied condition "Succeeded or Failed"
Jul 12 20:36:00.385: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-8a1d0da4-c637-4f81-a491-a3350cf03d32 container client-container: <nil>
STEP: delete the pod
Jul 12 20:36:00.415: INFO: Waiting for pod downwardapi-volume-8a1d0da4-c637-4f81-a491-a3350cf03d32 to disappear
Jul 12 20:36:00.419: INFO: Pod downwardapi-volume-8a1d0da4-c637-4f81-a491-a3350cf03d32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:36:00.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4050" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":27,"skipped":434,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:36:00.429: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-6e98ea61-b1ab-4c9f-96d3-124d09890925
STEP: Creating a pod to test consume secrets
Jul 12 20:36:00.534: INFO: Waiting up to 5m0s for pod "pod-secrets-b46cf6ee-0877-4f5b-8695-2f804cb4d1f2" in namespace "secrets-7354" to be "Succeeded or Failed"
Jul 12 20:36:00.543: INFO: Pod "pod-secrets-b46cf6ee-0877-4f5b-8695-2f804cb4d1f2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.084909ms
Jul 12 20:36:02.558: INFO: Pod "pod-secrets-b46cf6ee-0877-4f5b-8695-2f804cb4d1f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023491953s
STEP: Saw pod success
Jul 12 20:36:02.558: INFO: Pod "pod-secrets-b46cf6ee-0877-4f5b-8695-2f804cb4d1f2" satisfied condition "Succeeded or Failed"
Jul 12 20:36:02.564: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-secrets-b46cf6ee-0877-4f5b-8695-2f804cb4d1f2 container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 20:36:02.593: INFO: Waiting for pod pod-secrets-b46cf6ee-0877-4f5b-8695-2f804cb4d1f2 to disappear
Jul 12 20:36:02.610: INFO: Pod pod-secrets-b46cf6ee-0877-4f5b-8695-2f804cb4d1f2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:36:02.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7354" for this suite.
STEP: Destroying namespace "secret-namespace-3581" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":339,"completed":28,"skipped":450,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:36:02.637: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 12 20:36:02.713: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 12 20:37:02.773: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:37:02.777: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jul 12 20:37:06.879: INFO: found a healthy node: gke-gke-1-21-default-pool-f67064dc-3tj7
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:37:14.999: INFO: pods created so far: [1 1 1]
Jul 12 20:37:14.999: INFO: length of pods created so far: 3
Jul 12 20:37:27.022: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:37:34.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3543" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:37:34.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6382" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:91.540 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":339,"completed":29,"skipped":456,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:37:34.179: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:37:34.274: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 12 20:37:34.286: INFO: Number of nodes with available pods: 0
Jul 12 20:37:34.286: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 12 20:37:34.332: INFO: Number of nodes with available pods: 0
Jul 12 20:37:34.332: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:35.341: INFO: Number of nodes with available pods: 0
Jul 12 20:37:35.341: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:36.340: INFO: Number of nodes with available pods: 0
Jul 12 20:37:36.340: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:37.344: INFO: Number of nodes with available pods: 0
Jul 12 20:37:37.344: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:38.340: INFO: Number of nodes with available pods: 0
Jul 12 20:37:38.340: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:39.349: INFO: Number of nodes with available pods: 0
Jul 12 20:37:39.349: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:40.341: INFO: Number of nodes with available pods: 0
Jul 12 20:37:40.341: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:41.339: INFO: Number of nodes with available pods: 1
Jul 12 20:37:41.339: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 12 20:37:41.398: INFO: Number of nodes with available pods: 0
Jul 12 20:37:41.398: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 12 20:37:41.425: INFO: Number of nodes with available pods: 0
Jul 12 20:37:41.425: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:42.435: INFO: Number of nodes with available pods: 0
Jul 12 20:37:42.435: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:43.450: INFO: Number of nodes with available pods: 0
Jul 12 20:37:43.450: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:44.446: INFO: Number of nodes with available pods: 0
Jul 12 20:37:44.446: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:45.438: INFO: Number of nodes with available pods: 0
Jul 12 20:37:45.438: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:46.447: INFO: Number of nodes with available pods: 0
Jul 12 20:37:46.447: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:47.432: INFO: Number of nodes with available pods: 0
Jul 12 20:37:47.432: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:48.438: INFO: Number of nodes with available pods: 0
Jul 12 20:37:48.438: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 20:37:49.433: INFO: Number of nodes with available pods: 1
Jul 12 20:37:49.433: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7912, will wait for the garbage collector to delete the pods
Jul 12 20:37:49.507: INFO: Deleting DaemonSet.extensions daemon-set took: 14.696297ms
Jul 12 20:37:49.607: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.472266ms
Jul 12 20:37:58.036: INFO: Number of nodes with available pods: 0
Jul 12 20:37:58.036: INFO: Number of running nodes: 0, number of available pods: 0
Jul 12 20:37:58.065: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9678"},"items":null}

Jul 12 20:37:58.074: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9678"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:37:58.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7912" for this suite.

• [SLOW TEST:23.976 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":339,"completed":30,"skipped":486,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:37:58.155: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3128.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3128.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3128.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3128.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3128.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3128.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3128.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3128.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3128.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3128.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3128.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3128.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3128.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3128.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3128.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3128.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3128.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3128.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 20:38:12.440: INFO: DNS probes using dns-3128/dns-test-2a94f50b-6699-4851-aa75-d8007d2cf857 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:38:12.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3128" for this suite.

• [SLOW TEST:14.472 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":339,"completed":31,"skipped":489,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:38:12.628: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:38:12.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6692" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":339,"completed":32,"skipped":490,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:38:12.761: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5904.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5904.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5904.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5904.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5904.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5904.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 20:38:14.928: INFO: DNS probes using dns-5904/dns-test-fd4f9a1c-1ca4-4a35-8e28-e693ac2d74c3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:38:14.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5904" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":339,"completed":33,"skipped":497,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:38:14.996: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-9qrh
STEP: Creating a pod to test atomic-volume-subpath
Jul 12 20:38:15.106: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9qrh" in namespace "subpath-6939" to be "Succeeded or Failed"
Jul 12 20:38:15.118: INFO: Pod "pod-subpath-test-configmap-9qrh": Phase="Pending", Reason="", readiness=false. Elapsed: 11.713352ms
Jul 12 20:38:17.127: INFO: Pod "pod-subpath-test-configmap-9qrh": Phase="Running", Reason="", readiness=true. Elapsed: 2.020511125s
Jul 12 20:38:19.134: INFO: Pod "pod-subpath-test-configmap-9qrh": Phase="Running", Reason="", readiness=true. Elapsed: 4.028454083s
Jul 12 20:38:21.141: INFO: Pod "pod-subpath-test-configmap-9qrh": Phase="Running", Reason="", readiness=true. Elapsed: 6.035342711s
Jul 12 20:38:23.163: INFO: Pod "pod-subpath-test-configmap-9qrh": Phase="Running", Reason="", readiness=true. Elapsed: 8.056814929s
Jul 12 20:38:25.172: INFO: Pod "pod-subpath-test-configmap-9qrh": Phase="Running", Reason="", readiness=true. Elapsed: 10.066466796s
Jul 12 20:38:27.182: INFO: Pod "pod-subpath-test-configmap-9qrh": Phase="Running", Reason="", readiness=true. Elapsed: 12.075594355s
Jul 12 20:38:29.192: INFO: Pod "pod-subpath-test-configmap-9qrh": Phase="Running", Reason="", readiness=true. Elapsed: 14.085709286s
Jul 12 20:38:31.199: INFO: Pod "pod-subpath-test-configmap-9qrh": Phase="Running", Reason="", readiness=true. Elapsed: 16.092989276s
Jul 12 20:38:33.209: INFO: Pod "pod-subpath-test-configmap-9qrh": Phase="Running", Reason="", readiness=true. Elapsed: 18.103353206s
Jul 12 20:38:35.219: INFO: Pod "pod-subpath-test-configmap-9qrh": Phase="Running", Reason="", readiness=true. Elapsed: 20.112846941s
Jul 12 20:38:37.236: INFO: Pod "pod-subpath-test-configmap-9qrh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.129991535s
STEP: Saw pod success
Jul 12 20:38:37.236: INFO: Pod "pod-subpath-test-configmap-9qrh" satisfied condition "Succeeded or Failed"
Jul 12 20:38:37.245: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-subpath-test-configmap-9qrh container test-container-subpath-configmap-9qrh: <nil>
STEP: delete the pod
Jul 12 20:38:37.290: INFO: Waiting for pod pod-subpath-test-configmap-9qrh to disappear
Jul 12 20:38:37.296: INFO: Pod pod-subpath-test-configmap-9qrh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9qrh
Jul 12 20:38:37.296: INFO: Deleting pod "pod-subpath-test-configmap-9qrh" in namespace "subpath-6939"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:38:37.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6939" for this suite.

• [SLOW TEST:22.322 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":339,"completed":34,"skipped":503,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:38:37.319: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:149
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 12 20:38:37.522: INFO: starting watch
STEP: patching
STEP: updating
Jul 12 20:38:37.580: INFO: waiting for watch events with expected annotations
Jul 12 20:38:37.580: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:38:37.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2440" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":339,"completed":35,"skipped":520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:38:37.681: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:38:37.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-276" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":339,"completed":36,"skipped":580,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:38:37.804: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:38:37.893: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul 12 20:38:37.928: INFO: The status of Pod pod-logs-websocket-380794a4-3d4e-4687-95a2-acf2154877d3 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:38:39.946: INFO: The status of Pod pod-logs-websocket-380794a4-3d4e-4687-95a2-acf2154877d3 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:38:40.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4654" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":339,"completed":37,"skipped":656,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:38:40.073: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:38:40.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6541" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":339,"completed":38,"skipped":659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:38:40.188: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jul 12 20:38:40.231: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 12 20:39:40.274: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:39:40.278: INFO: Starting informer...
STEP: Starting pods...
Jul 12 20:39:40.505: INFO: Pod1 is running on gke-gke-1-21-default-pool-f67064dc-3tj7. Tainting Node
Jul 12 20:39:42.751: INFO: Pod2 is running on gke-gke-1-21-default-pool-f67064dc-3tj7. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jul 12 20:39:49.620: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul 12 20:40:09.678: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:40:09.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-2609" for this suite.

• [SLOW TEST:89.535 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":339,"completed":39,"skipped":688,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:40:09.727: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-5070958c-09a2-4cbc-ba9d-59c9858fd538
STEP: Creating a pod to test consume secrets
Jul 12 20:40:09.818: INFO: Waiting up to 5m0s for pod "pod-secrets-64700153-ea9f-4255-8f3b-96b6dde7a783" in namespace "secrets-4619" to be "Succeeded or Failed"
Jul 12 20:40:09.827: INFO: Pod "pod-secrets-64700153-ea9f-4255-8f3b-96b6dde7a783": Phase="Pending", Reason="", readiness=false. Elapsed: 8.159009ms
Jul 12 20:40:11.836: INFO: Pod "pod-secrets-64700153-ea9f-4255-8f3b-96b6dde7a783": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017377368s
STEP: Saw pod success
Jul 12 20:40:11.836: INFO: Pod "pod-secrets-64700153-ea9f-4255-8f3b-96b6dde7a783" satisfied condition "Succeeded or Failed"
Jul 12 20:40:11.840: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-secrets-64700153-ea9f-4255-8f3b-96b6dde7a783 container secret-env-test: <nil>
STEP: delete the pod
Jul 12 20:40:11.877: INFO: Waiting for pod pod-secrets-64700153-ea9f-4255-8f3b-96b6dde7a783 to disappear
Jul 12 20:40:11.880: INFO: Pod pod-secrets-64700153-ea9f-4255-8f3b-96b6dde7a783 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:40:11.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4619" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":339,"completed":40,"skipped":708,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:40:11.896: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-0a182f53-a02c-42a3-a60b-18e4dd35e90b
STEP: Creating configMap with name cm-test-opt-upd-29760911-0c19-4677-b1b6-a3ffcb75115b
STEP: Creating the pod
Jul 12 20:40:12.017: INFO: The status of Pod pod-configmaps-6e2c01f3-fe94-4faf-bec6-249ea4277adc is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:40:14.027: INFO: The status of Pod pod-configmaps-6e2c01f3-fe94-4faf-bec6-249ea4277adc is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-0a182f53-a02c-42a3-a60b-18e4dd35e90b
STEP: Updating configmap cm-test-opt-upd-29760911-0c19-4677-b1b6-a3ffcb75115b
STEP: Creating configMap with name cm-test-opt-create-66ddcba1-0bb9-422b-a2bb-da67913ef0d2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:40:16.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6726" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":41,"skipped":725,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:40:16.105: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-ec8cba0e-392d-4cdd-b944-d9fed0727ec7
STEP: Creating a pod to test consume configMaps
Jul 12 20:40:16.170: INFO: Waiting up to 5m0s for pod "pod-configmaps-e566560b-2efc-4659-a536-c3bf1c9d7b7f" in namespace "configmap-9969" to be "Succeeded or Failed"
Jul 12 20:40:16.174: INFO: Pod "pod-configmaps-e566560b-2efc-4659-a536-c3bf1c9d7b7f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.236733ms
Jul 12 20:40:18.183: INFO: Pod "pod-configmaps-e566560b-2efc-4659-a536-c3bf1c9d7b7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012995579s
STEP: Saw pod success
Jul 12 20:40:18.183: INFO: Pod "pod-configmaps-e566560b-2efc-4659-a536-c3bf1c9d7b7f" satisfied condition "Succeeded or Failed"
Jul 12 20:40:18.187: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-configmaps-e566560b-2efc-4659-a536-c3bf1c9d7b7f container agnhost-container: <nil>
STEP: delete the pod
Jul 12 20:40:18.210: INFO: Waiting for pod pod-configmaps-e566560b-2efc-4659-a536-c3bf1c9d7b7f to disappear
Jul 12 20:40:18.215: INFO: Pod pod-configmaps-e566560b-2efc-4659-a536-c3bf1c9d7b7f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:40:18.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9969" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":42,"skipped":741,"failed":0}
SSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:40:18.227: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:40:20.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3270" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":339,"completed":43,"skipped":744,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:40:20.313: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 12 20:40:20.373: INFO: Waiting up to 5m0s for pod "pod-ae4a16de-bade-4d7d-b13e-6b6ca10d4320" in namespace "emptydir-853" to be "Succeeded or Failed"
Jul 12 20:40:20.377: INFO: Pod "pod-ae4a16de-bade-4d7d-b13e-6b6ca10d4320": Phase="Pending", Reason="", readiness=false. Elapsed: 4.257558ms
Jul 12 20:40:22.398: INFO: Pod "pod-ae4a16de-bade-4d7d-b13e-6b6ca10d4320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025156646s
STEP: Saw pod success
Jul 12 20:40:22.398: INFO: Pod "pod-ae4a16de-bade-4d7d-b13e-6b6ca10d4320" satisfied condition "Succeeded or Failed"
Jul 12 20:40:22.409: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-1pbx pod pod-ae4a16de-bade-4d7d-b13e-6b6ca10d4320 container test-container: <nil>
STEP: delete the pod
Jul 12 20:40:22.470: INFO: Waiting for pod pod-ae4a16de-bade-4d7d-b13e-6b6ca10d4320 to disappear
Jul 12 20:40:22.478: INFO: Pod pod-ae4a16de-bade-4d7d-b13e-6b6ca10d4320 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:40:22.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-853" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":44,"skipped":744,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:40:22.502: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:40:22.569: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 12 20:40:29.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-9425 --namespace=crd-publish-openapi-9425 create -f -'
Jul 12 20:40:30.809: INFO: stderr: ""
Jul 12 20:40:30.809: INFO: stdout: "e2e-test-crd-publish-openapi-848-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 12 20:40:30.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-9425 --namespace=crd-publish-openapi-9425 delete e2e-test-crd-publish-openapi-848-crds test-cr'
Jul 12 20:40:30.930: INFO: stderr: ""
Jul 12 20:40:30.930: INFO: stdout: "e2e-test-crd-publish-openapi-848-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul 12 20:40:30.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-9425 --namespace=crd-publish-openapi-9425 apply -f -'
Jul 12 20:40:31.350: INFO: stderr: ""
Jul 12 20:40:31.350: INFO: stdout: "e2e-test-crd-publish-openapi-848-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 12 20:40:31.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-9425 --namespace=crd-publish-openapi-9425 delete e2e-test-crd-publish-openapi-848-crds test-cr'
Jul 12 20:40:31.453: INFO: stderr: ""
Jul 12 20:40:31.453: INFO: stdout: "e2e-test-crd-publish-openapi-848-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jul 12 20:40:31.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-9425 explain e2e-test-crd-publish-openapi-848-crds'
Jul 12 20:40:31.779: INFO: stderr: ""
Jul 12 20:40:31.779: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-848-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:40:36.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9425" for this suite.

• [SLOW TEST:14.056 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":339,"completed":45,"skipped":747,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:40:36.560: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jul 12 20:40:40.636: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9975 PodName:pod-sharedvolume-51ed805b-dd42-465b-b435-c4cc810cbe2a ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:40:40.636: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:40:40.765: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:40:40.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9975" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":339,"completed":46,"skipped":753,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:40:40.781: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jul 12 20:40:40.840: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:40:42.865: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:40:44.855: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jul 12 20:40:44.924: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:40:46.939: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 12 20:40:46.946: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4492 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:40:46.946: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:40:47.026: INFO: Exec stderr: ""
Jul 12 20:40:47.026: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4492 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:40:47.027: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:40:47.124: INFO: Exec stderr: ""
Jul 12 20:40:47.125: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4492 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:40:47.125: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:40:47.205: INFO: Exec stderr: ""
Jul 12 20:40:47.206: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4492 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:40:47.206: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:40:47.271: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 12 20:40:47.271: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4492 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:40:47.271: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:40:47.344: INFO: Exec stderr: ""
Jul 12 20:40:47.344: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4492 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:40:47.344: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:40:47.412: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 12 20:40:47.413: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4492 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:40:47.413: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:40:47.478: INFO: Exec stderr: ""
Jul 12 20:40:47.478: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4492 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:40:47.478: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:40:47.542: INFO: Exec stderr: ""
Jul 12 20:40:47.542: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4492 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:40:47.542: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:40:47.613: INFO: Exec stderr: ""
Jul 12 20:40:47.613: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4492 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:40:47.613: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:40:47.705: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:40:47.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4492" for this suite.

• [SLOW TEST:6.948 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":47,"skipped":787,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:40:47.733: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:40:47.799: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 12 20:40:47.836: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 12 20:40:52.880: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 12 20:40:54.920: INFO: Creating deployment "test-rolling-update-deployment"
Jul 12 20:40:54.947: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 12 20:40:54.973: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 12 20:40:57.073: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 12 20:40:57.100: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719255, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719255, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719256, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719255, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-585b757574\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 20:40:59.146: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 12 20:40:59.217: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6925  d3ff2d5b-8f4f-4f12-b673-da7bcd9ac405 10992 1 2021-07-12 20:40:54 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-07-12 20:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 20:40:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045cf3e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-12 20:40:55 +0000 UTC,LastTransitionTime:2021-07-12 20:40:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-585b757574" has successfully progressed.,LastUpdateTime:2021-07-12 20:40:57 +0000 UTC,LastTransitionTime:2021-07-12 20:40:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 12 20:40:59.257: INFO: New ReplicaSet "test-rolling-update-deployment-585b757574" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-585b757574  deployment-6925  927a4e1c-cb2e-4a73-9d8c-f90d02cde242 10985 1 2021-07-12 20:40:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d3ff2d5b-8f4f-4f12-b673-da7bcd9ac405 0xc004600c47 0xc004600c48}] []  [{kube-controller-manager Update apps/v1 2021-07-12 20:40:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3ff2d5b-8f4f-4f12-b673-da7bcd9ac405\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 585b757574,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004600d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 12 20:40:59.257: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 12 20:40:59.257: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6925  35d68ed0-5c37-4307-ba0a-619b34b07ef1 10991 2 2021-07-12 20:40:47 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d3ff2d5b-8f4f-4f12-b673-da7bcd9ac405 0xc004600ab7 0xc004600ab8}] []  [{e2e.test Update apps/v1 2021-07-12 20:40:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 20:40:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d3ff2d5b-8f4f-4f12-b673-da7bcd9ac405\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004600bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 20:40:59.277: INFO: Pod "test-rolling-update-deployment-585b757574-v5c8g" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-585b757574-v5c8g test-rolling-update-deployment-585b757574- deployment-6925  d96a26df-88df-4fff-83f4-b4660eb25cbc 10984 0 2021-07-12 20:40:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:585b757574] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-585b757574 927a4e1c-cb2e-4a73-9d8c-f90d02cde242 0xc004601427 0xc004601428}] []  [{kube-controller-manager Update v1 2021-07-12 20:40:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"927a4e1c-cb2e-4a73-9d8c-f90d02cde242\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 20:40:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.1.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gswn2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gswn2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 20:40:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 20:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 20:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 20:40:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.27,PodIP:10.40.1.41,StartTime:2021-07-12 20:40:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 20:40:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://28a707891ed95f7c9fb986e7b542c77bbbf982938680d14f9d7c388feb2bae22,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.1.41,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:40:59.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6925" for this suite.

• [SLOW TEST:11.594 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":48,"skipped":798,"failed":0}
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:40:59.329: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1386
STEP: creating an pod
Jul 12 20:41:00.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1500 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.32 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul 12 20:41:00.391: INFO: stderr: ""
Jul 12 20:41:00.391: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for log generator to start.
Jul 12 20:41:00.391: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul 12 20:41:00.391: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1500" to be "running and ready, or succeeded"
Jul 12 20:41:00.432: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 40.696317ms
Jul 12 20:41:02.445: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.053783483s
Jul 12 20:41:02.445: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul 12 20:41:02.445: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jul 12 20:41:02.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1500 logs logs-generator logs-generator'
Jul 12 20:41:02.580: INFO: stderr: ""
Jul 12 20:41:02.580: INFO: stdout: "I0712 20:41:01.388409       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/swn 277\nI0712 20:41:01.588540       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/j25 452\nI0712 20:41:01.789501       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/97f 511\nI0712 20:41:01.988901       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/dbh 301\nI0712 20:41:02.189377       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/l5qr 594\nI0712 20:41:02.388822       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/w7j7 377\n"
STEP: limiting log lines
Jul 12 20:41:02.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1500 logs logs-generator logs-generator --tail=1'
Jul 12 20:41:02.699: INFO: stderr: ""
Jul 12 20:41:02.699: INFO: stdout: "I0712 20:41:02.589375       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xc2 308\n"
Jul 12 20:41:02.699: INFO: got output "I0712 20:41:02.589375       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xc2 308\n"
STEP: limiting log bytes
Jul 12 20:41:02.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1500 logs logs-generator logs-generator --limit-bytes=1'
Jul 12 20:41:02.815: INFO: stderr: ""
Jul 12 20:41:02.815: INFO: stdout: "I"
Jul 12 20:41:02.815: INFO: got output "I"
STEP: exposing timestamps
Jul 12 20:41:02.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1500 logs logs-generator logs-generator --tail=1 --timestamps'
Jul 12 20:41:02.954: INFO: stderr: ""
Jul 12 20:41:02.954: INFO: stdout: "2021-07-12T20:41:02.789257103Z I0712 20:41:02.788982       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/mdkk 216\n"
Jul 12 20:41:02.954: INFO: got output "2021-07-12T20:41:02.789257103Z I0712 20:41:02.788982       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/mdkk 216\n"
STEP: restricting to a time range
Jul 12 20:41:05.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1500 logs logs-generator logs-generator --since=1s'
Jul 12 20:41:05.584: INFO: stderr: ""
Jul 12 20:41:05.584: INFO: stdout: "I0712 20:41:04.588813       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/mpfr 379\nI0712 20:41:04.789269       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/85dz 450\nI0712 20:41:04.988677       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/q7w 296\nI0712 20:41:05.189098       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/jvc 253\nI0712 20:41:05.388474       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/x84g 430\n"
Jul 12 20:41:05.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1500 logs logs-generator logs-generator --since=24h'
Jul 12 20:41:05.696: INFO: stderr: ""
Jul 12 20:41:05.696: INFO: stdout: "I0712 20:41:01.388409       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/swn 277\nI0712 20:41:01.588540       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/j25 452\nI0712 20:41:01.789501       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/97f 511\nI0712 20:41:01.988901       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/dbh 301\nI0712 20:41:02.189377       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/l5qr 594\nI0712 20:41:02.388822       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/w7j7 377\nI0712 20:41:02.589375       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xc2 308\nI0712 20:41:02.788982       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/mdkk 216\nI0712 20:41:02.989435       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/fm8x 556\nI0712 20:41:03.189045       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/xmq 431\nI0712 20:41:03.388435       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/lhv 543\nI0712 20:41:03.588927       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/nxm 281\nI0712 20:41:03.789283       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/bjmn 294\nI0712 20:41:03.988683       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/6wh 598\nI0712 20:41:04.189311       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/nfj 505\nI0712 20:41:04.388597       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/qv6r 386\nI0712 20:41:04.588813       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/mpfr 379\nI0712 20:41:04.789269       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/85dz 450\nI0712 20:41:04.988677       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/q7w 296\nI0712 20:41:05.189098       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/jvc 253\nI0712 20:41:05.388474       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/x84g 430\nI0712 20:41:05.588814       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/bfw6 581\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1391
Jul 12 20:41:05.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1500 delete pod logs-generator'
Jul 12 20:41:11.584: INFO: stderr: ""
Jul 12 20:41:11.584: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:41:11.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1500" for this suite.

• [SLOW TEST:12.274 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1383
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":339,"completed":49,"skipped":798,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:41:11.603: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1851
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1851
STEP: creating replication controller externalsvc in namespace services-1851
I0712 20:41:11.896268      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1851, replica count: 2
I0712 20:41:14.947263      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jul 12 20:41:15.006: INFO: Creating new exec pod
Jul 12 20:41:19.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1851 exec execpod6phj9 -- /bin/sh -x -c nslookup nodeport-service.services-1851.svc.cluster.local'
Jul 12 20:41:19.367: INFO: stderr: "+ nslookup nodeport-service.services-1851.svc.cluster.local\n"
Jul 12 20:41:19.367: INFO: stdout: "Server:\t\t10.37.128.10\nAddress:\t10.37.128.10#53\n\nnodeport-service.services-1851.svc.cluster.local\tcanonical name = externalsvc.services-1851.svc.cluster.local.\nName:\texternalsvc.services-1851.svc.cluster.local\nAddress: 10.37.143.157\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1851, will wait for the garbage collector to delete the pods
Jul 12 20:41:19.472: INFO: Deleting ReplicationController externalsvc took: 23.553957ms
Jul 12 20:41:19.573: INFO: Terminating ReplicationController externalsvc pods took: 101.158205ms
Jul 12 20:41:31.693: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:41:31.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1851" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:20.156 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":339,"completed":50,"skipped":807,"failed":0}
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:41:31.761: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 12 20:41:31.803: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:41:41.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5877" for this suite.

• [SLOW TEST:9.897 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":339,"completed":51,"skipped":807,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:41:41.659: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 20:41:42.584: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 20:41:45.647: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Jul 12 20:41:45.880: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:41:46.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1742" for this suite.
STEP: Destroying namespace "webhook-1742-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":339,"completed":52,"skipped":813,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:41:46.322: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:41:46.431: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 12 20:41:46.449: INFO: Number of nodes with available pods: 0
Jul 12 20:41:46.449: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 20:41:47.478: INFO: Number of nodes with available pods: 0
Jul 12 20:41:47.478: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 20:41:48.463: INFO: Number of nodes with available pods: 2
Jul 12 20:41:48.463: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 20:41:49.491: INFO: Number of nodes with available pods: 2
Jul 12 20:41:49.491: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 20:41:50.473: INFO: Number of nodes with available pods: 2
Jul 12 20:41:50.474: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 20:41:51.482: INFO: Number of nodes with available pods: 2
Jul 12 20:41:51.482: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 20:41:52.511: INFO: Number of nodes with available pods: 2
Jul 12 20:41:52.511: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 20:41:53.552: INFO: Number of nodes with available pods: 3
Jul 12 20:41:53.552: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 12 20:41:53.627: INFO: Wrong image for pod: daemon-set-6nsg2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:53.627: INFO: Wrong image for pod: daemon-set-9p5p4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:53.627: INFO: Wrong image for pod: daemon-set-tqjld. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:54.665: INFO: Wrong image for pod: daemon-set-9p5p4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:54.665: INFO: Wrong image for pod: daemon-set-tqjld. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:55.661: INFO: Wrong image for pod: daemon-set-9p5p4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:55.661: INFO: Wrong image for pod: daemon-set-tqjld. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:56.666: INFO: Wrong image for pod: daemon-set-9p5p4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:56.666: INFO: Wrong image for pod: daemon-set-tqjld. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:57.668: INFO: Wrong image for pod: daemon-set-9p5p4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:57.669: INFO: Wrong image for pod: daemon-set-tqjld. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:58.665: INFO: Wrong image for pod: daemon-set-9p5p4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:58.665: INFO: Pod daemon-set-f8wqs is not available
Jul 12 20:41:58.665: INFO: Wrong image for pod: daemon-set-tqjld. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:59.663: INFO: Wrong image for pod: daemon-set-9p5p4. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:41:59.664: INFO: Pod daemon-set-f8wqs is not available
Jul 12 20:41:59.664: INFO: Wrong image for pod: daemon-set-tqjld. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:42:00.659: INFO: Wrong image for pod: daemon-set-tqjld. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:42:01.660: INFO: Wrong image for pod: daemon-set-tqjld. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:42:02.673: INFO: Pod daemon-set-bwrr7 is not available
Jul 12 20:42:02.673: INFO: Wrong image for pod: daemon-set-tqjld. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:42:03.659: INFO: Pod daemon-set-bwrr7 is not available
Jul 12 20:42:03.659: INFO: Wrong image for pod: daemon-set-tqjld. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.32, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1.
Jul 12 20:42:06.661: INFO: Pod daemon-set-kbscl is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 12 20:42:06.674: INFO: Number of nodes with available pods: 2
Jul 12 20:42:06.674: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 20:42:07.692: INFO: Number of nodes with available pods: 2
Jul 12 20:42:07.692: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 20:42:08.686: INFO: Number of nodes with available pods: 3
Jul 12 20:42:08.686: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3791, will wait for the garbage collector to delete the pods
Jul 12 20:42:08.768: INFO: Deleting DaemonSet.extensions daemon-set took: 6.507577ms
Jul 12 20:42:08.969: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.592747ms
Jul 12 20:42:21.676: INFO: Number of nodes with available pods: 0
Jul 12 20:42:21.676: INFO: Number of running nodes: 0, number of available pods: 0
Jul 12 20:42:21.679: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11690"},"items":null}

Jul 12 20:42:21.683: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11690"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:42:21.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3791" for this suite.

• [SLOW TEST:35.383 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":339,"completed":53,"skipped":857,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:42:21.707: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test service account token: 
Jul 12 20:42:21.799: INFO: Waiting up to 5m0s for pod "test-pod-deeb9108-a2b1-4625-8493-74df69919bce" in namespace "svcaccounts-7739" to be "Succeeded or Failed"
Jul 12 20:42:21.807: INFO: Pod "test-pod-deeb9108-a2b1-4625-8493-74df69919bce": Phase="Pending", Reason="", readiness=false. Elapsed: 7.88647ms
Jul 12 20:42:23.817: INFO: Pod "test-pod-deeb9108-a2b1-4625-8493-74df69919bce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017838598s
STEP: Saw pod success
Jul 12 20:42:23.817: INFO: Pod "test-pod-deeb9108-a2b1-4625-8493-74df69919bce" satisfied condition "Succeeded or Failed"
Jul 12 20:42:23.822: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod test-pod-deeb9108-a2b1-4625-8493-74df69919bce container agnhost-container: <nil>
STEP: delete the pod
Jul 12 20:42:23.849: INFO: Waiting for pod test-pod-deeb9108-a2b1-4625-8493-74df69919bce to disappear
Jul 12 20:42:23.858: INFO: Pod test-pod-deeb9108-a2b1-4625-8493-74df69919bce no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:42:23.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7739" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":339,"completed":54,"skipped":891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:42:23.883: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 20:42:23.975: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce1dce7b-3a20-49bf-a1af-a1f4feb64f64" in namespace "projected-6653" to be "Succeeded or Failed"
Jul 12 20:42:23.983: INFO: Pod "downwardapi-volume-ce1dce7b-3a20-49bf-a1af-a1f4feb64f64": Phase="Pending", Reason="", readiness=false. Elapsed: 7.891039ms
Jul 12 20:42:25.995: INFO: Pod "downwardapi-volume-ce1dce7b-3a20-49bf-a1af-a1f4feb64f64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019195021s
STEP: Saw pod success
Jul 12 20:42:25.995: INFO: Pod "downwardapi-volume-ce1dce7b-3a20-49bf-a1af-a1f4feb64f64" satisfied condition "Succeeded or Failed"
Jul 12 20:42:25.999: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-ce1dce7b-3a20-49bf-a1af-a1f4feb64f64 container client-container: <nil>
STEP: delete the pod
Jul 12 20:42:26.030: INFO: Waiting for pod downwardapi-volume-ce1dce7b-3a20-49bf-a1af-a1f4feb64f64 to disappear
Jul 12 20:42:26.034: INFO: Pod downwardapi-volume-ce1dce7b-3a20-49bf-a1af-a1f4feb64f64 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:42:26.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6653" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":55,"skipped":931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:42:26.049: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:42:43.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5667" for this suite.

• [SLOW TEST:17.165 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":339,"completed":56,"skipped":969,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:42:43.215: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 20:42:44.523: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 20:42:47.547: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:42:47.556: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6907-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:42:51.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2116" for this suite.
STEP: Destroying namespace "webhook-2116-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.078 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":339,"completed":57,"skipped":969,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:42:51.294: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 12 20:42:51.393: INFO: Waiting up to 5m0s for pod "pod-869ef112-e125-4d32-87f6-e94cd7bbb9d5" in namespace "emptydir-9412" to be "Succeeded or Failed"
Jul 12 20:42:51.409: INFO: Pod "pod-869ef112-e125-4d32-87f6-e94cd7bbb9d5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.022531ms
Jul 12 20:42:53.452: INFO: Pod "pod-869ef112-e125-4d32-87f6-e94cd7bbb9d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.058666883s
STEP: Saw pod success
Jul 12 20:42:53.452: INFO: Pod "pod-869ef112-e125-4d32-87f6-e94cd7bbb9d5" satisfied condition "Succeeded or Failed"
Jul 12 20:42:53.461: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-869ef112-e125-4d32-87f6-e94cd7bbb9d5 container test-container: <nil>
STEP: delete the pod
Jul 12 20:42:53.504: INFO: Waiting for pod pod-869ef112-e125-4d32-87f6-e94cd7bbb9d5 to disappear
Jul 12 20:42:53.511: INFO: Pod pod-869ef112-e125-4d32-87f6-e94cd7bbb9d5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:42:53.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9412" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":58,"skipped":994,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:42:53.545: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:42:53.649: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:43:02.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8394" for this suite.

• [SLOW TEST:9.690 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":339,"completed":59,"skipped":1014,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:43:03.236: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jul 12 20:43:04.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-4720 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Jul 12 20:43:04.237: INFO: stderr: ""
Jul 12 20:43:04.237: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jul 12 20:43:04.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-4720 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-1"}]}} --dry-run=server'
Jul 12 20:43:04.888: INFO: stderr: ""
Jul 12 20:43:04.888: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jul 12 20:43:04.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-4720 delete pods e2e-test-httpd-pod'
Jul 12 20:43:11.587: INFO: stderr: ""
Jul 12 20:43:11.587: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:43:11.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4720" for this suite.

• [SLOW TEST:8.397 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:903
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":339,"completed":60,"skipped":1023,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:43:11.633: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul 12 20:43:11.767: INFO: Waiting up to 5m0s for pod "downward-api-0329a3f3-f4d1-4c2b-b830-3a187c13b00c" in namespace "downward-api-5712" to be "Succeeded or Failed"
Jul 12 20:43:11.774: INFO: Pod "downward-api-0329a3f3-f4d1-4c2b-b830-3a187c13b00c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.618016ms
Jul 12 20:43:13.885: INFO: Pod "downward-api-0329a3f3-f4d1-4c2b-b830-3a187c13b00c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.118116186s
STEP: Saw pod success
Jul 12 20:43:13.886: INFO: Pod "downward-api-0329a3f3-f4d1-4c2b-b830-3a187c13b00c" satisfied condition "Succeeded or Failed"
Jul 12 20:43:13.935: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downward-api-0329a3f3-f4d1-4c2b-b830-3a187c13b00c container dapi-container: <nil>
STEP: delete the pod
Jul 12 20:43:14.107: INFO: Waiting for pod downward-api-0329a3f3-f4d1-4c2b-b830-3a187c13b00c to disappear
Jul 12 20:43:14.205: INFO: Pod downward-api-0329a3f3-f4d1-4c2b-b830-3a187c13b00c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:43:14.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5712" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":339,"completed":61,"skipped":1045,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:43:14.331: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-8009
STEP: creating service affinity-clusterip in namespace services-8009
STEP: creating replication controller affinity-clusterip in namespace services-8009
I0712 20:43:14.736114      20 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-8009, replica count: 3
I0712 20:43:17.787587      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 20:43:20.789431      20 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 20:43:20.812: INFO: Creating new exec pod
Jul 12 20:43:23.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8009 exec execpod-affinityszmj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jul 12 20:43:24.020: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jul 12 20:43:24.020: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 20:43:24.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8009 exec execpod-affinityszmj6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.134.95 80'
Jul 12 20:43:24.227: INFO: stderr: "+ nc -v -t -w 2 10.37.134.95 80\n+ echo hostName\nConnection to 10.37.134.95 80 port [tcp/http] succeeded!\n"
Jul 12 20:43:24.228: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 20:43:24.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8009 exec execpod-affinityszmj6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.37.134.95:80/ ; done'
Jul 12 20:43:24.562: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.134.95:80/\n"
Jul 12 20:43:24.562: INFO: stdout: "\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl\naffinity-clusterip-xbvgl"
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Received response from host: affinity-clusterip-xbvgl
Jul 12 20:43:24.562: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-8009, will wait for the garbage collector to delete the pods
Jul 12 20:43:24.661: INFO: Deleting ReplicationController affinity-clusterip took: 13.179558ms
Jul 12 20:43:24.861: INFO: Terminating ReplicationController affinity-clusterip pods took: 200.186188ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:43:31.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8009" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:17.388 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":62,"skipped":1059,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:43:31.719: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:43:31.772: INFO: Creating ReplicaSet my-hostname-basic-b995cc54-62b6-43cb-8974-34542e59e0e7
Jul 12 20:43:31.786: INFO: Pod name my-hostname-basic-b995cc54-62b6-43cb-8974-34542e59e0e7: Found 0 pods out of 1
Jul 12 20:43:36.803: INFO: Pod name my-hostname-basic-b995cc54-62b6-43cb-8974-34542e59e0e7: Found 1 pods out of 1
Jul 12 20:43:36.804: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b995cc54-62b6-43cb-8974-34542e59e0e7" is running
Jul 12 20:43:36.808: INFO: Pod "my-hostname-basic-b995cc54-62b6-43cb-8974-34542e59e0e7-tzd2b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 20:43:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 20:43:33 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 20:43:33 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 20:43:31 +0000 UTC Reason: Message:}])
Jul 12 20:43:36.809: INFO: Trying to dial the pod
Jul 12 20:43:41.832: INFO: Controller my-hostname-basic-b995cc54-62b6-43cb-8974-34542e59e0e7: Got expected result from replica 1 [my-hostname-basic-b995cc54-62b6-43cb-8974-34542e59e0e7-tzd2b]: "my-hostname-basic-b995cc54-62b6-43cb-8974-34542e59e0e7-tzd2b", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:43:41.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2495" for this suite.

• [SLOW TEST:10.125 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":63,"skipped":1064,"failed":0}
SSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:43:41.847: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:43:42.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5189" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":64,"skipped":1071,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:43:42.069: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-0ef31753-dda5-45bf-b441-dabeae237b7a
STEP: Creating a pod to test consume secrets
Jul 12 20:43:42.174: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b6a5dcf2-79d4-4091-937b-c871d0ea8709" in namespace "projected-2607" to be "Succeeded or Failed"
Jul 12 20:43:42.189: INFO: Pod "pod-projected-secrets-b6a5dcf2-79d4-4091-937b-c871d0ea8709": Phase="Pending", Reason="", readiness=false. Elapsed: 14.881748ms
Jul 12 20:43:44.210: INFO: Pod "pod-projected-secrets-b6a5dcf2-79d4-4091-937b-c871d0ea8709": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035583976s
STEP: Saw pod success
Jul 12 20:43:44.210: INFO: Pod "pod-projected-secrets-b6a5dcf2-79d4-4091-937b-c871d0ea8709" satisfied condition "Succeeded or Failed"
Jul 12 20:43:44.217: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-projected-secrets-b6a5dcf2-79d4-4091-937b-c871d0ea8709 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 12 20:43:44.258: INFO: Waiting for pod pod-projected-secrets-b6a5dcf2-79d4-4091-937b-c871d0ea8709 to disappear
Jul 12 20:43:44.266: INFO: Pod pod-projected-secrets-b6a5dcf2-79d4-4091-937b-c871d0ea8709 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:43:44.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2607" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":65,"skipped":1071,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:43:44.282: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jul 12 20:43:44.387: INFO: Waiting up to 5m0s for pod "security-context-c4b5fbe0-e8d2-42ce-a480-b47ee3e6b66c" in namespace "security-context-1348" to be "Succeeded or Failed"
Jul 12 20:43:44.399: INFO: Pod "security-context-c4b5fbe0-e8d2-42ce-a480-b47ee3e6b66c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.701091ms
Jul 12 20:43:46.407: INFO: Pod "security-context-c4b5fbe0-e8d2-42ce-a480-b47ee3e6b66c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020349155s
STEP: Saw pod success
Jul 12 20:43:46.407: INFO: Pod "security-context-c4b5fbe0-e8d2-42ce-a480-b47ee3e6b66c" satisfied condition "Succeeded or Failed"
Jul 12 20:43:46.412: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod security-context-c4b5fbe0-e8d2-42ce-a480-b47ee3e6b66c container test-container: <nil>
STEP: delete the pod
Jul 12 20:43:46.434: INFO: Waiting for pod security-context-c4b5fbe0-e8d2-42ce-a480-b47ee3e6b66c to disappear
Jul 12 20:43:46.438: INFO: Pod security-context-c4b5fbe0-e8d2-42ce-a480-b47ee3e6b66c no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:43:46.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1348" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":66,"skipped":1091,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:43:46.446: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 12 20:43:46.507: INFO: Waiting up to 5m0s for pod "pod-f17c2293-8374-47b1-b42c-75ad8415a9ae" in namespace "emptydir-1031" to be "Succeeded or Failed"
Jul 12 20:43:46.513: INFO: Pod "pod-f17c2293-8374-47b1-b42c-75ad8415a9ae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.403454ms
Jul 12 20:43:48.527: INFO: Pod "pod-f17c2293-8374-47b1-b42c-75ad8415a9ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01989235s
Jul 12 20:43:50.542: INFO: Pod "pod-f17c2293-8374-47b1-b42c-75ad8415a9ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034677818s
STEP: Saw pod success
Jul 12 20:43:50.542: INFO: Pod "pod-f17c2293-8374-47b1-b42c-75ad8415a9ae" satisfied condition "Succeeded or Failed"
Jul 12 20:43:50.548: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-f17c2293-8374-47b1-b42c-75ad8415a9ae container test-container: <nil>
STEP: delete the pod
Jul 12 20:43:50.578: INFO: Waiting for pod pod-f17c2293-8374-47b1-b42c-75ad8415a9ae to disappear
Jul 12 20:43:50.581: INFO: Pod pod-f17c2293-8374-47b1-b42c-75ad8415a9ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:43:50.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1031" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":67,"skipped":1099,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:43:50.594: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jul 12 20:43:50.657: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 12 20:43:50.657: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 12 20:43:50.675: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 12 20:43:50.675: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 12 20:43:50.765: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 12 20:43:50.765: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 12 20:43:50.828: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 12 20:43:50.829: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 12 20:43:52.673: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul 12 20:43:52.673: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul 12 20:43:52.706: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jul 12 20:43:52.736: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jul 12 20:43:52.740: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0
Jul 12 20:43:52.740: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0
Jul 12 20:43:52.740: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0
Jul 12 20:43:52.740: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0
Jul 12 20:43:52.743: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0
Jul 12 20:43:52.743: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0
Jul 12 20:43:52.743: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0
Jul 12 20:43:52.743: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 0
Jul 12 20:43:52.743: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1
Jul 12 20:43:52.743: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1
Jul 12 20:43:52.743: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:52.743: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:52.744: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:52.744: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:52.765: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:52.765: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:52.803: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:52.803: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:52.988: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:52.989: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:53.036: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1
Jul 12 20:43:53.036: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1
Jul 12 20:43:54.704: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:54.704: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:54.795: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1
STEP: listing Deployments
Jul 12 20:43:54.815: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jul 12 20:43:54.837: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jul 12 20:43:54.845: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 12 20:43:54.858: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 12 20:43:54.914: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 12 20:43:54.974: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 12 20:43:55.025: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 12 20:43:56.732: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 12 20:43:56.820: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jul 12 20:43:57.033: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 12 20:43:57.168: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 12 20:43:58.734: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jul 12 20:43:58.920: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1
Jul 12 20:43:58.920: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1
Jul 12 20:43:58.921: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1
Jul 12 20:43:58.921: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1
Jul 12 20:43:58.922: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 1
Jul 12 20:43:58.922: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:58.922: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 3
Jul 12 20:43:58.922: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:58.922: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 2
Jul 12 20:43:58.922: INFO: observed Deployment test-deployment in namespace deployment-8476 with ReadyReplicas 3
STEP: deleting the Deployment
Jul 12 20:43:58.954: INFO: observed event type MODIFIED
Jul 12 20:43:58.954: INFO: observed event type MODIFIED
Jul 12 20:43:58.955: INFO: observed event type MODIFIED
Jul 12 20:43:58.955: INFO: observed event type MODIFIED
Jul 12 20:43:58.955: INFO: observed event type MODIFIED
Jul 12 20:43:58.955: INFO: observed event type MODIFIED
Jul 12 20:43:58.955: INFO: observed event type MODIFIED
Jul 12 20:43:58.957: INFO: observed event type MODIFIED
Jul 12 20:43:58.957: INFO: observed event type MODIFIED
Jul 12 20:43:58.957: INFO: observed event type MODIFIED
Jul 12 20:43:58.958: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 12 20:43:58.971: INFO: Log out all the ReplicaSets if there is no deployment created
Jul 12 20:43:58.986: INFO: ReplicaSet "test-deployment-7b4c744884":
&ReplicaSet{ObjectMeta:{test-deployment-7b4c744884  deployment-8476  7571b5d1-cbb8-4551-8be7-9e31ba4f7f0d 12638 3 2021-07-12 20:43:50 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment b0c55e5f-36f2-407e-8d9c-83c188c340f9 0xc0044a6d87 0xc0044a6d88}] []  [{kube-controller-manager Update apps/v1 2021-07-12 20:43:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b0c55e5f-36f2-407e-8d9c-83c188c340f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b4c744884,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b4c744884 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0044a6df0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:43:58.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8476" for this suite.

• [SLOW TEST:8.441 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":339,"completed":68,"skipped":1100,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:43:59.039: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create deployment with httpd image
Jul 12 20:43:59.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-5689 create -f -'
Jul 12 20:43:59.551: INFO: stderr: ""
Jul 12 20:43:59.551: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jul 12 20:43:59.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-5689 diff -f -'
Jul 12 20:43:59.908: INFO: rc: 1
Jul 12 20:43:59.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-5689 delete -f -'
Jul 12 20:44:00.013: INFO: stderr: ""
Jul 12 20:44:00.013: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:44:00.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5689" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":339,"completed":69,"skipped":1108,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:44:00.060: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:44:10.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4883" for this suite.

• [SLOW TEST:10.366 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":339,"completed":70,"skipped":1147,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:44:10.427: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Starting the proxy
Jul 12 20:44:10.477: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-3761 proxy --unix-socket=/tmp/kubectl-proxy-unix371617674/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:44:10.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3761" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":339,"completed":71,"skipped":1150,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:44:10.565: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 20:44:10.890: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 12 20:44:12.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719450, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719450, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719450, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719450, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 20:44:15.936: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jul 12 20:44:18.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=webhook-1183 attach --namespace=webhook-1183 to-be-attached-pod -i -c=container1'
Jul 12 20:44:18.122: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:44:18.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1183" for this suite.
STEP: Destroying namespace "webhook-1183-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.657 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":339,"completed":72,"skipped":1155,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:44:18.224: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4979
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4979
I0712 20:44:18.331441      20 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4979, replica count: 2
I0712 20:44:21.382431      20 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 20:44:21.383: INFO: Creating new exec pod
Jul 12 20:44:24.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 12 20:44:26.650: INFO: rc: 1
Jul 12 20:44:26.650: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80:
Command stdout:

stderr:
+ echo hostName
+ nc -v -t -w 2 externalname-service 80
nc: connect to externalname-service port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 12 20:44:27.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 12 20:44:27.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 12 20:44:27.871: INFO: stdout: "externalname-service-9b4gw"
Jul 12 20:44:27.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.132.241 80'
Jul 12 20:44:28.054: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.132.241 80\nConnection to 10.37.132.241 80 port [tcp/http] succeeded!\n"
Jul 12 20:44:28.054: INFO: stdout: ""
Jul 12 20:44:29.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.132.241 80'
Jul 12 20:44:29.264: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.132.241 80\nConnection to 10.37.132.241 80 port [tcp/http] succeeded!\n"
Jul 12 20:44:29.264: INFO: stdout: ""
Jul 12 20:44:30.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.132.241 80'
Jul 12 20:44:30.257: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.132.241 80\nConnection to 10.37.132.241 80 port [tcp/http] succeeded!\n"
Jul 12 20:44:30.257: INFO: stdout: ""
Jul 12 20:44:31.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.132.241 80'
Jul 12 20:44:31.253: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.132.241 80\nConnection to 10.37.132.241 80 port [tcp/http] succeeded!\n"
Jul 12 20:44:31.253: INFO: stdout: ""
Jul 12 20:44:32.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.132.241 80'
Jul 12 20:44:32.219: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.132.241 80\nConnection to 10.37.132.241 80 port [tcp/http] succeeded!\n"
Jul 12 20:44:32.219: INFO: stdout: "externalname-service-5wt87"
Jul 12 20:44:32.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.26 31950'
Jul 12 20:44:32.393: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.26 31950\nConnection to 10.128.0.26 31950 port [tcp/*] succeeded!\n"
Jul 12 20:44:32.393: INFO: stdout: ""
Jul 12 20:44:33.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.26 31950'
Jul 12 20:44:33.575: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.26 31950\nConnection to 10.128.0.26 31950 port [tcp/*] succeeded!\n"
Jul 12 20:44:33.575: INFO: stdout: ""
Jul 12 20:44:34.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.26 31950'
Jul 12 20:44:34.584: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.26 31950\nConnection to 10.128.0.26 31950 port [tcp/*] succeeded!\n"
Jul 12 20:44:34.584: INFO: stdout: ""
Jul 12 20:44:35.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.26 31950'
Jul 12 20:44:35.583: INFO: stderr: "+ nc -v -t -w 2 10.128.0.26 31950\n+ echo hostName\nConnection to 10.128.0.26 31950 port [tcp/*] succeeded!\n"
Jul 12 20:44:35.583: INFO: stdout: "externalname-service-5wt87"
Jul 12 20:44:35.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4979 exec execpodx8gxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.27 31950'
Jul 12 20:44:35.777: INFO: stderr: "+ nc -v -t -w 2 10.128.0.27 31950\n+ echo hostName\nConnection to 10.128.0.27 31950 port [tcp/*] succeeded!\n"
Jul 12 20:44:35.777: INFO: stdout: "externalname-service-5wt87"
Jul 12 20:44:35.777: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:44:35.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4979" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:17.619 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":339,"completed":73,"skipped":1160,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:44:35.848: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:44:36.698: INFO: Checking APIGroup: apiregistration.k8s.io
Jul 12 20:44:36.699: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jul 12 20:44:36.699: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.699: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jul 12 20:44:36.699: INFO: Checking APIGroup: apps
Jul 12 20:44:36.701: INFO: PreferredVersion.GroupVersion: apps/v1
Jul 12 20:44:36.701: INFO: Versions found [{apps/v1 v1}]
Jul 12 20:44:36.701: INFO: apps/v1 matches apps/v1
Jul 12 20:44:36.701: INFO: Checking APIGroup: events.k8s.io
Jul 12 20:44:36.703: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jul 12 20:44:36.703: INFO: Versions found [{events.k8s.io/v1 v1}]
Jul 12 20:44:36.703: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jul 12 20:44:36.703: INFO: Checking APIGroup: authentication.k8s.io
Jul 12 20:44:36.705: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jul 12 20:44:36.705: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.705: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jul 12 20:44:36.705: INFO: Checking APIGroup: authorization.k8s.io
Jul 12 20:44:36.706: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jul 12 20:44:36.706: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.706: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jul 12 20:44:36.706: INFO: Checking APIGroup: autoscaling
Jul 12 20:44:36.708: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jul 12 20:44:36.708: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jul 12 20:44:36.708: INFO: autoscaling/v1 matches autoscaling/v1
Jul 12 20:44:36.708: INFO: Checking APIGroup: batch
Jul 12 20:44:36.709: INFO: PreferredVersion.GroupVersion: batch/v1
Jul 12 20:44:36.709: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jul 12 20:44:36.709: INFO: batch/v1 matches batch/v1
Jul 12 20:44:36.709: INFO: Checking APIGroup: certificates.k8s.io
Jul 12 20:44:36.711: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jul 12 20:44:36.711: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.711: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jul 12 20:44:36.711: INFO: Checking APIGroup: networking.k8s.io
Jul 12 20:44:36.712: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jul 12 20:44:36.712: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.712: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jul 12 20:44:36.712: INFO: Checking APIGroup: extensions
Jul 12 20:44:36.713: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jul 12 20:44:36.713: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jul 12 20:44:36.713: INFO: extensions/v1beta1 matches extensions/v1beta1
Jul 12 20:44:36.713: INFO: Checking APIGroup: policy
Jul 12 20:44:36.714: INFO: PreferredVersion.GroupVersion: policy/v1
Jul 12 20:44:36.714: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jul 12 20:44:36.714: INFO: policy/v1 matches policy/v1
Jul 12 20:44:36.714: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jul 12 20:44:36.715: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jul 12 20:44:36.715: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.715: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jul 12 20:44:36.715: INFO: Checking APIGroup: storage.k8s.io
Jul 12 20:44:36.717: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jul 12 20:44:36.717: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.717: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jul 12 20:44:36.717: INFO: Checking APIGroup: admissionregistration.k8s.io
Jul 12 20:44:36.718: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jul 12 20:44:36.718: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.718: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jul 12 20:44:36.718: INFO: Checking APIGroup: apiextensions.k8s.io
Jul 12 20:44:36.719: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jul 12 20:44:36.719: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.719: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jul 12 20:44:36.719: INFO: Checking APIGroup: scheduling.k8s.io
Jul 12 20:44:36.720: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jul 12 20:44:36.720: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.720: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jul 12 20:44:36.720: INFO: Checking APIGroup: coordination.k8s.io
Jul 12 20:44:36.721: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jul 12 20:44:36.721: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.721: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jul 12 20:44:36.722: INFO: Checking APIGroup: node.k8s.io
Jul 12 20:44:36.723: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jul 12 20:44:36.723: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.723: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jul 12 20:44:36.723: INFO: Checking APIGroup: discovery.k8s.io
Jul 12 20:44:36.724: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jul 12 20:44:36.724: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.724: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jul 12 20:44:36.724: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jul 12 20:44:36.725: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Jul 12 20:44:36.725: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.725: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Jul 12 20:44:36.725: INFO: Checking APIGroup: cloud.google.com
Jul 12 20:44:36.726: INFO: PreferredVersion.GroupVersion: cloud.google.com/v1
Jul 12 20:44:36.726: INFO: Versions found [{cloud.google.com/v1 v1} {cloud.google.com/v1beta1 v1beta1}]
Jul 12 20:44:36.726: INFO: cloud.google.com/v1 matches cloud.google.com/v1
Jul 12 20:44:36.726: INFO: Checking APIGroup: networking.gke.io
Jul 12 20:44:36.727: INFO: PreferredVersion.GroupVersion: networking.gke.io/v1
Jul 12 20:44:36.728: INFO: Versions found [{networking.gke.io/v1 v1} {networking.gke.io/v1beta2 v1beta2} {networking.gke.io/v1beta1 v1beta1}]
Jul 12 20:44:36.728: INFO: networking.gke.io/v1 matches networking.gke.io/v1
Jul 12 20:44:36.728: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jul 12 20:44:36.730: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jul 12 20:44:36.730: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.730: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jul 12 20:44:36.730: INFO: Checking APIGroup: migration.k8s.io
Jul 12 20:44:36.732: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Jul 12 20:44:36.732: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Jul 12 20:44:36.732: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Jul 12 20:44:36.732: INFO: Checking APIGroup: nodemanagement.gke.io
Jul 12 20:44:36.736: INFO: PreferredVersion.GroupVersion: nodemanagement.gke.io/v1alpha1
Jul 12 20:44:36.736: INFO: Versions found [{nodemanagement.gke.io/v1alpha1 v1alpha1}]
Jul 12 20:44:36.736: INFO: nodemanagement.gke.io/v1alpha1 matches nodemanagement.gke.io/v1alpha1
Jul 12 20:44:36.736: INFO: Checking APIGroup: metrics.k8s.io
Jul 12 20:44:36.740: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jul 12 20:44:36.740: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jul 12 20:44:36.740: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:44:36.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1241" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":339,"completed":74,"skipped":1173,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:44:36.751: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:44:51.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8246" for this suite.
STEP: Destroying namespace "nsdeletetest-9122" for this suite.
Jul 12 20:44:52.012: INFO: Namespace nsdeletetest-9122 was already deleted
STEP: Destroying namespace "nsdeletetest-4858" for this suite.

• [SLOW TEST:15.265 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":339,"completed":75,"skipped":1188,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:44:52.018: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:44:52.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1793" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":339,"completed":76,"skipped":1204,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:44:52.338: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-downwardapi-m8qd
STEP: Creating a pod to test atomic-volume-subpath
Jul 12 20:44:52.482: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-m8qd" in namespace "subpath-3564" to be "Succeeded or Failed"
Jul 12 20:44:52.492: INFO: Pod "pod-subpath-test-downwardapi-m8qd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.045431ms
Jul 12 20:44:54.511: INFO: Pod "pod-subpath-test-downwardapi-m8qd": Phase="Running", Reason="", readiness=true. Elapsed: 2.028214572s
Jul 12 20:44:56.529: INFO: Pod "pod-subpath-test-downwardapi-m8qd": Phase="Running", Reason="", readiness=true. Elapsed: 4.046915098s
Jul 12 20:44:58.547: INFO: Pod "pod-subpath-test-downwardapi-m8qd": Phase="Running", Reason="", readiness=true. Elapsed: 6.065008033s
Jul 12 20:45:00.556: INFO: Pod "pod-subpath-test-downwardapi-m8qd": Phase="Running", Reason="", readiness=true. Elapsed: 8.073966574s
Jul 12 20:45:02.563: INFO: Pod "pod-subpath-test-downwardapi-m8qd": Phase="Running", Reason="", readiness=true. Elapsed: 10.080141686s
Jul 12 20:45:04.573: INFO: Pod "pod-subpath-test-downwardapi-m8qd": Phase="Running", Reason="", readiness=true. Elapsed: 12.09075729s
Jul 12 20:45:06.582: INFO: Pod "pod-subpath-test-downwardapi-m8qd": Phase="Running", Reason="", readiness=true. Elapsed: 14.099453569s
Jul 12 20:45:08.591: INFO: Pod "pod-subpath-test-downwardapi-m8qd": Phase="Running", Reason="", readiness=true. Elapsed: 16.108580149s
Jul 12 20:45:10.602: INFO: Pod "pod-subpath-test-downwardapi-m8qd": Phase="Running", Reason="", readiness=true. Elapsed: 18.119620631s
Jul 12 20:45:12.616: INFO: Pod "pod-subpath-test-downwardapi-m8qd": Phase="Running", Reason="", readiness=true. Elapsed: 20.133612567s
Jul 12 20:45:14.626: INFO: Pod "pod-subpath-test-downwardapi-m8qd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.143905239s
STEP: Saw pod success
Jul 12 20:45:14.627: INFO: Pod "pod-subpath-test-downwardapi-m8qd" satisfied condition "Succeeded or Failed"
Jul 12 20:45:14.630: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-subpath-test-downwardapi-m8qd container test-container-subpath-downwardapi-m8qd: <nil>
STEP: delete the pod
Jul 12 20:45:14.653: INFO: Waiting for pod pod-subpath-test-downwardapi-m8qd to disappear
Jul 12 20:45:14.658: INFO: Pod pod-subpath-test-downwardapi-m8qd no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-m8qd
Jul 12 20:45:14.658: INFO: Deleting pod "pod-subpath-test-downwardapi-m8qd" in namespace "subpath-3564"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:45:14.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3564" for this suite.

• [SLOW TEST:22.337 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":339,"completed":77,"skipped":1210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:45:14.679: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-04992578-48bd-4093-8b88-85f96ca5ea3b
STEP: Creating a pod to test consume secrets
Jul 12 20:45:14.738: INFO: Waiting up to 5m0s for pod "pod-secrets-460d6add-5e00-4b3b-87d3-c6965ae3f679" in namespace "secrets-8381" to be "Succeeded or Failed"
Jul 12 20:45:14.758: INFO: Pod "pod-secrets-460d6add-5e00-4b3b-87d3-c6965ae3f679": Phase="Pending", Reason="", readiness=false. Elapsed: 19.468068ms
Jul 12 20:45:16.768: INFO: Pod "pod-secrets-460d6add-5e00-4b3b-87d3-c6965ae3f679": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.030151986s
STEP: Saw pod success
Jul 12 20:45:16.768: INFO: Pod "pod-secrets-460d6add-5e00-4b3b-87d3-c6965ae3f679" satisfied condition "Succeeded or Failed"
Jul 12 20:45:16.772: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-secrets-460d6add-5e00-4b3b-87d3-c6965ae3f679 container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 20:45:16.796: INFO: Waiting for pod pod-secrets-460d6add-5e00-4b3b-87d3-c6965ae3f679 to disappear
Jul 12 20:45:16.802: INFO: Pod pod-secrets-460d6add-5e00-4b3b-87d3-c6965ae3f679 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:45:16.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8381" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":78,"skipped":1243,"failed":0}
S
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:45:16.811: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Request ServerVersion
STEP: Confirm major version
Jul 12 20:45:16.861: INFO: Major version: 1
STEP: Confirm minor version
Jul 12 20:45:16.861: INFO: cleanMinorVersion: 21
Jul 12 20:45:16.861: INFO: Minor version: 21
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:45:16.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1817" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":339,"completed":79,"skipped":1244,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:45:16.871: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8479.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8479.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 20:45:21.005: INFO: DNS probes using dns-8479/dns-test-437013a4-9519-4e7d-b2e5-2843fe30bb23 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:45:21.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8479" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":339,"completed":80,"skipped":1250,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:45:21.055: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jul 12 20:45:21.097: INFO: namespace kubectl-3524
Jul 12 20:45:21.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-3524 create -f -'
Jul 12 20:45:21.458: INFO: stderr: ""
Jul 12 20:45:21.458: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 12 20:45:22.503: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 20:45:22.503: INFO: Found 0 / 1
Jul 12 20:45:23.471: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 20:45:23.472: INFO: Found 1 / 1
Jul 12 20:45:23.472: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 12 20:45:23.479: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 20:45:23.479: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 12 20:45:23.479: INFO: wait on agnhost-primary startup in kubectl-3524 
Jul 12 20:45:23.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-3524 logs agnhost-primary-hq29f agnhost-primary'
Jul 12 20:45:23.599: INFO: stderr: ""
Jul 12 20:45:23.599: INFO: stdout: "Paused\n"
STEP: exposing RC
Jul 12 20:45:23.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-3524 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jul 12 20:45:23.750: INFO: stderr: ""
Jul 12 20:45:23.750: INFO: stdout: "service/rm2 exposed\n"
Jul 12 20:45:23.771: INFO: Service rm2 in namespace kubectl-3524 found.
STEP: exposing service
Jul 12 20:45:25.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-3524 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jul 12 20:45:25.898: INFO: stderr: ""
Jul 12 20:45:25.898: INFO: stdout: "service/rm3 exposed\n"
Jul 12 20:45:25.906: INFO: Service rm3 in namespace kubectl-3524 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:45:27.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3524" for this suite.

• [SLOW TEST:6.912 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1223
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":339,"completed":81,"skipped":1266,"failed":0}
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:45:27.968: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:45:30.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3421" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":82,"skipped":1272,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:45:30.169: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption is created
Jul 12 20:45:30.230: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:45:32.235: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:45:33.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8076" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":339,"completed":83,"skipped":1285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:45:33.289: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:45:33.349: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 12 20:45:34.415: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:45:34.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3976" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":339,"completed":84,"skipped":1307,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:45:34.461: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:46:34.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-531" for this suite.

• [SLOW TEST:60.147 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":339,"completed":85,"skipped":1369,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:46:34.608: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:46:34.675: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 12 20:46:39.690: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 12 20:46:39.690: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 12 20:46:41.706: INFO: Creating deployment "test-rollover-deployment"
Jul 12 20:46:41.725: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 12 20:46:43.752: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 12 20:46:43.768: INFO: Ensure that both replica sets have 1 created replica
Jul 12 20:46:43.778: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 12 20:46:43.798: INFO: Updating deployment test-rollover-deployment
Jul 12 20:46:43.798: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 12 20:46:45.827: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 12 20:46:45.834: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 12 20:46:45.841: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 20:46:45.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719605, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 20:46:47.853: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 20:46:47.853: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719605, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 20:46:49.854: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 20:46:49.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719605, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 20:46:51.853: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 20:46:51.853: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719605, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 20:46:53.875: INFO: all replica sets need to contain the pod-template-hash label
Jul 12 20:46:53.876: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719605, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719601, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-98c5f4599\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 20:46:55.865: INFO: 
Jul 12 20:46:55.865: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 12 20:46:55.877: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-741  7807fd45-d8a1-4537-a261-827407ddbdd5 14138 2 2021-07-12 20:46:41 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-12 20:46:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 20:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c00fa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-07-12 20:46:41 +0000 UTC,LastTransitionTime:2021-07-12 20:46:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-98c5f4599" has successfully progressed.,LastUpdateTime:2021-07-12 20:46:55 +0000 UTC,LastTransitionTime:2021-07-12 20:46:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 12 20:46:55.881: INFO: New ReplicaSet "test-rollover-deployment-98c5f4599" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-98c5f4599  deployment-741  26e05c09-c5ae-4ddb-b00d-8a18f40e2697 14130 2 2021-07-12 20:46:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 7807fd45-d8a1-4537-a261-827407ddbdd5 0xc003c01bb0 0xc003c01bb1}] []  [{kube-controller-manager Update apps/v1 2021-07-12 20:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7807fd45-d8a1-4537-a261-827407ddbdd5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 98c5f4599,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c01c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 12 20:46:55.881: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 12 20:46:55.881: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-741  2b31915e-91a2-4c48-81bb-fdfc36662f20 14137 2 2021-07-12 20:46:34 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 7807fd45-d8a1-4537-a261-827407ddbdd5 0xc003c01727 0xc003c01728}] []  [{e2e.test Update apps/v1 2021-07-12 20:46:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 20:46:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7807fd45-d8a1-4537-a261-827407ddbdd5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c01968 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 20:46:55.881: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-741  93ce4871-5f9b-41d7-9b96-f48522da487a 14064 2 2021-07-12 20:46:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 7807fd45-d8a1-4537-a261-827407ddbdd5 0xc003c01ab7 0xc003c01ab8}] []  [{kube-controller-manager Update apps/v1 2021-07-12 20:46:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7807fd45-d8a1-4537-a261-827407ddbdd5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c01b48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 20:46:55.886: INFO: Pod "test-rollover-deployment-98c5f4599-qjhfj" is available:
&Pod{ObjectMeta:{test-rollover-deployment-98c5f4599-qjhfj test-rollover-deployment-98c5f4599- deployment-741  bbcd159d-9d9b-4b5d-8490-0012dfaab26a 14079 0 2021-07-12 20:46:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:98c5f4599] map[] [{apps/v1 ReplicaSet test-rollover-deployment-98c5f4599 26e05c09-c5ae-4ddb-b00d-8a18f40e2697 0xc003d2e370 0xc003d2e371}] []  [{kube-controller-manager Update v1 2021-07-12 20:46:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"26e05c09-c5ae-4ddb-b00d-8a18f40e2697\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 20:46:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.1.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-phzsd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-phzsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 20:46:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 20:46:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 20:46:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 20:46:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.27,PodIP:10.40.1.93,StartTime:2021-07-12 20:46:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 20:46:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://a5644fc78585c09d6ca10b719346066f5ebade880de15aabd1165e7562a082bc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.1.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:46:55.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-741" for this suite.

• [SLOW TEST:21.291 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":339,"completed":86,"skipped":1401,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:46:55.903: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
W0712 20:46:55.952427      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:48:01.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3103" for this suite.

• [SLOW TEST:66.189 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":339,"completed":87,"skipped":1412,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:48:02.092: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jul 12 20:48:02.206: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:48:25.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7109" for this suite.

• [SLOW TEST:23.356 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":339,"completed":88,"skipped":1412,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:48:25.448: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 20:48:26.060: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 20:48:29.159: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:48:29.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9151" for this suite.
STEP: Destroying namespace "webhook-9151-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":339,"completed":89,"skipped":1427,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:48:29.323: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 12 20:48:29.955: INFO: starting watch
STEP: patching
STEP: updating
Jul 12 20:48:29.975: INFO: waiting for watch events with expected annotations
Jul 12 20:48:29.975: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:48:30.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6177" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":339,"completed":90,"skipped":1446,"failed":0}
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:48:30.066: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:48:30.139: INFO: The status of Pod busybox-scheduling-21846b8a-049c-4fc0-9641-b0ac10229c6b is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:48:32.147: INFO: The status of Pod busybox-scheduling-21846b8a-049c-4fc0-9641-b0ac10229c6b is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:48:32.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-345" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":339,"completed":91,"skipped":1449,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:48:32.183: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0712 20:48:32.240947      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:50:00.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-490" for this suite.

• [SLOW TEST:88.294 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":339,"completed":92,"skipped":1466,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:50:00.479: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:50:00.559: INFO: Creating deployment "test-recreate-deployment"
Jul 12 20:50:00.567: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 12 20:50:00.582: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jul 12 20:50:02.604: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 12 20:50:02.609: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 12 20:50:02.627: INFO: Updating deployment test-recreate-deployment
Jul 12 20:50:02.627: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 12 20:50:02.854: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2441  37b05924-1503-4bf6-aaf9-b29ebf71b9d5 15298 2 2021-07-12 20:50:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-12 20:50:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 20:50:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000261a78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-12 20:50:02 +0000 UTC,LastTransitionTime:2021-07-12 20:50:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-85d47dcb4" is progressing.,LastUpdateTime:2021-07-12 20:50:02 +0000 UTC,LastTransitionTime:2021-07-12 20:50:00 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul 12 20:50:02.860: INFO: New ReplicaSet "test-recreate-deployment-85d47dcb4" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-85d47dcb4  deployment-2441  a3ecda60-515b-48f4-b840-c948e12ed0e2 15295 1 2021-07-12 20:50:02 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 37b05924-1503-4bf6-aaf9-b29ebf71b9d5 0xc000451030 0xc000451031}] []  [{kube-controller-manager Update apps/v1 2021-07-12 20:50:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37b05924-1503-4bf6-aaf9-b29ebf71b9d5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 85d47dcb4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00011f188 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 20:50:02.860: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 12 20:50:02.860: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6cb8b65c46  deployment-2441  0a382eed-b816-4542-8b2f-76ba9c14e6f8 15289 2 2021-07-12 20:50:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 37b05924-1503-4bf6-aaf9-b29ebf71b9d5 0xc000450ac7 0xc000450ac8}] []  [{kube-controller-manager Update apps/v1 2021-07-12 20:50:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"37b05924-1503-4bf6-aaf9-b29ebf71b9d5\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6cb8b65c46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6cb8b65c46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000450e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 20:50:02.865: INFO: Pod "test-recreate-deployment-85d47dcb4-nq8g4" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-85d47dcb4-nq8g4 test-recreate-deployment-85d47dcb4- deployment-2441  a7c929c0-7e37-411f-b682-b42579b4a3d8 15297 0 2021-07-12 20:50:02 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:85d47dcb4] map[] [{apps/v1 ReplicaSet test-recreate-deployment-85d47dcb4 a3ecda60-515b-48f4-b840-c948e12ed0e2 0xc003e80540 0xc003e80541}] []  [{kube-controller-manager Update v1 2021-07-12 20:50:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3ecda60-515b-48f4-b840-c948e12ed0e2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 20:50:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9ndmh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9ndmh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 20:50:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 20:50:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 20:50:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 20:50:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.27,PodIP:,StartTime:2021-07-12 20:50:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:50:02.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2441" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":339,"completed":93,"skipped":1484,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:50:02.886: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 12 20:50:05.514: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7798 pod-service-account-d274e566-ed72-4210-9a6f-200750cf4a18 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 12 20:50:05.768: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7798 pod-service-account-d274e566-ed72-4210-9a6f-200750cf4a18 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 12 20:50:05.953: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7798 pod-service-account-d274e566-ed72-4210-9a6f-200750cf4a18 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:50:06.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7798" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":339,"completed":94,"skipped":1551,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:50:06.274: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:50:06.330: INFO: The status of Pod test-webserver-93c4ddc3-2e7a-42ee-a6ae-0ae6ae49db52 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:50:08.348: INFO: The status of Pod test-webserver-93c4ddc3-2e7a-42ee-a6ae-0ae6ae49db52 is Running (Ready = false)
Jul 12 20:50:10.338: INFO: The status of Pod test-webserver-93c4ddc3-2e7a-42ee-a6ae-0ae6ae49db52 is Running (Ready = false)
Jul 12 20:50:12.344: INFO: The status of Pod test-webserver-93c4ddc3-2e7a-42ee-a6ae-0ae6ae49db52 is Running (Ready = false)
Jul 12 20:50:14.335: INFO: The status of Pod test-webserver-93c4ddc3-2e7a-42ee-a6ae-0ae6ae49db52 is Running (Ready = false)
Jul 12 20:50:16.337: INFO: The status of Pod test-webserver-93c4ddc3-2e7a-42ee-a6ae-0ae6ae49db52 is Running (Ready = false)
Jul 12 20:50:18.339: INFO: The status of Pod test-webserver-93c4ddc3-2e7a-42ee-a6ae-0ae6ae49db52 is Running (Ready = false)
Jul 12 20:50:20.341: INFO: The status of Pod test-webserver-93c4ddc3-2e7a-42ee-a6ae-0ae6ae49db52 is Running (Ready = false)
Jul 12 20:50:22.334: INFO: The status of Pod test-webserver-93c4ddc3-2e7a-42ee-a6ae-0ae6ae49db52 is Running (Ready = false)
Jul 12 20:50:24.353: INFO: The status of Pod test-webserver-93c4ddc3-2e7a-42ee-a6ae-0ae6ae49db52 is Running (Ready = false)
Jul 12 20:50:26.363: INFO: The status of Pod test-webserver-93c4ddc3-2e7a-42ee-a6ae-0ae6ae49db52 is Running (Ready = false)
Jul 12 20:50:28.341: INFO: The status of Pod test-webserver-93c4ddc3-2e7a-42ee-a6ae-0ae6ae49db52 is Running (Ready = true)
Jul 12 20:50:28.345: INFO: Container started at 2021-07-12 20:50:07 +0000 UTC, pod became ready at 2021-07-12 20:50:26 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:50:28.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1224" for this suite.

• [SLOW TEST:22.081 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":339,"completed":95,"skipped":1557,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:50:28.356: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:50:28.416: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-18a66c42-195a-4ce8-bfe0-88c1b67f7bf5" in namespace "security-context-test-6093" to be "Succeeded or Failed"
Jul 12 20:50:28.423: INFO: Pod "busybox-privileged-false-18a66c42-195a-4ce8-bfe0-88c1b67f7bf5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.506585ms
Jul 12 20:50:30.431: INFO: Pod "busybox-privileged-false-18a66c42-195a-4ce8-bfe0-88c1b67f7bf5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014546914s
Jul 12 20:50:30.431: INFO: Pod "busybox-privileged-false-18a66c42-195a-4ce8-bfe0-88c1b67f7bf5" satisfied condition "Succeeded or Failed"
Jul 12 20:50:30.442: INFO: Got logs for pod "busybox-privileged-false-18a66c42-195a-4ce8-bfe0-88c1b67f7bf5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:50:30.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6093" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":96,"skipped":1604,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:50:30.453: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jul 12 20:50:30.501: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:50:35.055: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:50:53.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7673" for this suite.

• [SLOW TEST:23.473 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":339,"completed":97,"skipped":1607,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:50:53.928: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-2b21ef14-d78f-4fb2-9069-769f9842cd2e
STEP: Creating a pod to test consume configMaps
Jul 12 20:50:54.055: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-600e229b-87fa-479e-8a18-cf7b83260ac3" in namespace "projected-4420" to be "Succeeded or Failed"
Jul 12 20:50:54.072: INFO: Pod "pod-projected-configmaps-600e229b-87fa-479e-8a18-cf7b83260ac3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.834093ms
Jul 12 20:50:56.080: INFO: Pod "pod-projected-configmaps-600e229b-87fa-479e-8a18-cf7b83260ac3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024158821s
STEP: Saw pod success
Jul 12 20:50:56.080: INFO: Pod "pod-projected-configmaps-600e229b-87fa-479e-8a18-cf7b83260ac3" satisfied condition "Succeeded or Failed"
Jul 12 20:50:56.084: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-projected-configmaps-600e229b-87fa-479e-8a18-cf7b83260ac3 container agnhost-container: <nil>
STEP: delete the pod
Jul 12 20:50:56.109: INFO: Waiting for pod pod-projected-configmaps-600e229b-87fa-479e-8a18-cf7b83260ac3 to disappear
Jul 12 20:50:56.113: INFO: Pod pod-projected-configmaps-600e229b-87fa-479e-8a18-cf7b83260ac3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:50:56.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4420" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":98,"skipped":1692,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:50:56.124: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 12 20:50:56.232: INFO: Waiting up to 5m0s for pod "pod-229f15b2-615e-4d86-a6e3-05ce9d0c4073" in namespace "emptydir-1628" to be "Succeeded or Failed"
Jul 12 20:50:56.269: INFO: Pod "pod-229f15b2-615e-4d86-a6e3-05ce9d0c4073": Phase="Pending", Reason="", readiness=false. Elapsed: 35.997763ms
Jul 12 20:50:58.290: INFO: Pod "pod-229f15b2-615e-4d86-a6e3-05ce9d0c4073": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.057854937s
STEP: Saw pod success
Jul 12 20:50:58.291: INFO: Pod "pod-229f15b2-615e-4d86-a6e3-05ce9d0c4073" satisfied condition "Succeeded or Failed"
Jul 12 20:50:58.295: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-229f15b2-615e-4d86-a6e3-05ce9d0c4073 container test-container: <nil>
STEP: delete the pod
Jul 12 20:50:58.322: INFO: Waiting for pod pod-229f15b2-615e-4d86-a6e3-05ce9d0c4073 to disappear
Jul 12 20:50:58.327: INFO: Pod pod-229f15b2-615e-4d86-a6e3-05ce9d0c4073 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:50:58.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1628" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":99,"skipped":1696,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:50:58.349: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name cm-test-opt-del-bfdb2a1f-d1a5-4c8c-aae0-123bfba872bb
STEP: Creating configMap with name cm-test-opt-upd-29c45aa3-f0b7-47f6-8a7d-7cb7e517d429
STEP: Creating the pod
Jul 12 20:50:58.491: INFO: The status of Pod pod-projected-configmaps-5545eecd-6640-477f-b89d-3d4784b6f46f is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:51:00.501: INFO: The status of Pod pod-projected-configmaps-5545eecd-6640-477f-b89d-3d4784b6f46f is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-bfdb2a1f-d1a5-4c8c-aae0-123bfba872bb
STEP: Updating configmap cm-test-opt-upd-29c45aa3-f0b7-47f6-8a7d-7cb7e517d429
STEP: Creating configMap with name cm-test-opt-create-15147192-874f-4244-9346-8fb89e4b3552
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:51:02.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2452" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":100,"skipped":1702,"failed":0}
S
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:51:02.615: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jul 12 20:51:02.693: INFO: PodSpec: initContainers in spec.initContainers
Jul 12 20:51:51.407: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-af34da21-9670-4672-bfc4-50c96dc4a254", GenerateName:"", Namespace:"init-container-8773", SelfLink:"", UID:"3a1c515c-f16e-4971-8b6a-6b9698552b9e", ResourceVersion:"16017", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63761719862, loc:(*time.Location)(0x9dc0820)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"693306458"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001f2a1c8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001f2a1e0)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc001f2a1f8), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001f2a210)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-jlmk8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0041a2260), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jlmk8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jlmk8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.4.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jlmk8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0041107c0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"gke-gke-1-21-default-pool-f67064dc-3tj7", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002fba380), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0041108a0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0041108c0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0041108c8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0041108cc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004a30d80), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719862, loc:(*time.Location)(0x9dc0820)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719862, loc:(*time.Location)(0x9dc0820)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719862, loc:(*time.Location)(0x9dc0820)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761719862, loc:(*time.Location)(0x9dc0820)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.128.0.27", PodIP:"10.40.1.108", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.40.1.108"}}, StartTime:(*v1.Time)(0xc001f2a240), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002fba460)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002fba4d0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:39e1e963e5310e9c313bad51523be012ede7b35bb9316517d19089a010356592", ContainerID:"containerd://7abb448287de0c34a2d20c14208e41bb679335d9c18635da0a1f5cf79807af40", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041a22e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-1", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041a22c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.4.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc00411099f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:51:51.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8773" for this suite.

• [SLOW TEST:48.807 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":339,"completed":101,"skipped":1703,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:51:51.425: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul 12 20:51:51.645: INFO: The status of Pod pod-update-147c7954-6189-4a9d-8c95-c429908267a4 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:51:53.652: INFO: The status of Pod pod-update-147c7954-6189-4a9d-8c95-c429908267a4 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 12 20:51:54.181: INFO: Successfully updated pod "pod-update-147c7954-6189-4a9d-8c95-c429908267a4"
STEP: verifying the updated pod is in kubernetes
Jul 12 20:51:54.203: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:51:54.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5086" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":339,"completed":102,"skipped":1748,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:51:54.215: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:01.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6456" for this suite.

• [SLOW TEST:7.107 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":339,"completed":103,"skipped":1754,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:01.325: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-7acf34e0-ada8-40e3-98f1-4b21232df5f6
STEP: Creating a pod to test consume secrets
Jul 12 20:52:01.401: INFO: Waiting up to 5m0s for pod "pod-secrets-6dc744d6-bdb6-4803-858e-c14ce8fd4aee" in namespace "secrets-1172" to be "Succeeded or Failed"
Jul 12 20:52:01.409: INFO: Pod "pod-secrets-6dc744d6-bdb6-4803-858e-c14ce8fd4aee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.335986ms
Jul 12 20:52:03.417: INFO: Pod "pod-secrets-6dc744d6-bdb6-4803-858e-c14ce8fd4aee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015503061s
Jul 12 20:52:05.428: INFO: Pod "pod-secrets-6dc744d6-bdb6-4803-858e-c14ce8fd4aee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026598304s
STEP: Saw pod success
Jul 12 20:52:05.428: INFO: Pod "pod-secrets-6dc744d6-bdb6-4803-858e-c14ce8fd4aee" satisfied condition "Succeeded or Failed"
Jul 12 20:52:05.434: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-secrets-6dc744d6-bdb6-4803-858e-c14ce8fd4aee container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 20:52:05.457: INFO: Waiting for pod pod-secrets-6dc744d6-bdb6-4803-858e-c14ce8fd4aee to disappear
Jul 12 20:52:05.468: INFO: Pod pod-secrets-6dc744d6-bdb6-4803-858e-c14ce8fd4aee no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:05.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1172" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":104,"skipped":1755,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:05.479: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 12 20:52:05.534: INFO: Waiting up to 5m0s for pod "pod-28499f60-773c-43fb-aac4-20892bc34c82" in namespace "emptydir-851" to be "Succeeded or Failed"
Jul 12 20:52:05.538: INFO: Pod "pod-28499f60-773c-43fb-aac4-20892bc34c82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.169506ms
Jul 12 20:52:07.549: INFO: Pod "pod-28499f60-773c-43fb-aac4-20892bc34c82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015081505s
STEP: Saw pod success
Jul 12 20:52:07.549: INFO: Pod "pod-28499f60-773c-43fb-aac4-20892bc34c82" satisfied condition "Succeeded or Failed"
Jul 12 20:52:07.553: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-28499f60-773c-43fb-aac4-20892bc34c82 container test-container: <nil>
STEP: delete the pod
Jul 12 20:52:07.586: INFO: Waiting for pod pod-28499f60-773c-43fb-aac4-20892bc34c82 to disappear
Jul 12 20:52:07.591: INFO: Pod pod-28499f60-773c-43fb-aac4-20892bc34c82 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:07.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-851" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":105,"skipped":1764,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:07.603: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replication controller my-hostname-basic-99037203-1943-4712-be3a-acf281820e13
Jul 12 20:52:07.736: INFO: Pod name my-hostname-basic-99037203-1943-4712-be3a-acf281820e13: Found 0 pods out of 1
Jul 12 20:52:12.795: INFO: Pod name my-hostname-basic-99037203-1943-4712-be3a-acf281820e13: Found 1 pods out of 1
Jul 12 20:52:12.795: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-99037203-1943-4712-be3a-acf281820e13" are running
Jul 12 20:52:12.805: INFO: Pod "my-hostname-basic-99037203-1943-4712-be3a-acf281820e13-5hfwz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 20:52:07 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 20:52:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 20:52:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-07-12 20:52:07 +0000 UTC Reason: Message:}])
Jul 12 20:52:12.806: INFO: Trying to dial the pod
Jul 12 20:52:17.884: INFO: Controller my-hostname-basic-99037203-1943-4712-be3a-acf281820e13: Got expected result from replica 1 [my-hostname-basic-99037203-1943-4712-be3a-acf281820e13-5hfwz]: "my-hostname-basic-99037203-1943-4712-be3a-acf281820e13-5hfwz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:17.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7719" for this suite.

• [SLOW TEST:10.331 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":339,"completed":106,"skipped":1785,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:17.935: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating Agnhost RC
Jul 12 20:52:18.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1548 create -f -'
Jul 12 20:52:19.053: INFO: stderr: ""
Jul 12 20:52:19.054: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 12 20:52:20.059: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 20:52:20.059: INFO: Found 0 / 1
Jul 12 20:52:21.059: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 20:52:21.059: INFO: Found 1 / 1
Jul 12 20:52:21.059: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 12 20:52:21.063: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 20:52:21.064: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 12 20:52:21.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1548 patch pod agnhost-primary-jz2n8 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 12 20:52:21.163: INFO: stderr: ""
Jul 12 20:52:21.163: INFO: stdout: "pod/agnhost-primary-jz2n8 patched\n"
STEP: checking annotations
Jul 12 20:52:21.167: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 20:52:21.167: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:21.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1548" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":339,"completed":107,"skipped":1792,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:21.177: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap that has name configmap-test-emptyKey-a0495d5a-2455-42dd-920a-b39ff4f4f471
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:21.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2224" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":339,"completed":108,"skipped":1805,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:21.279: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-6db9634a-2cc8-4d5a-9823-234ffff94cb3
STEP: Creating a pod to test consume configMaps
Jul 12 20:52:21.348: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cc34994d-7aa1-45aa-8e91-99696d66f49f" in namespace "projected-7002" to be "Succeeded or Failed"
Jul 12 20:52:21.363: INFO: Pod "pod-projected-configmaps-cc34994d-7aa1-45aa-8e91-99696d66f49f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.917552ms
Jul 12 20:52:23.387: INFO: Pod "pod-projected-configmaps-cc34994d-7aa1-45aa-8e91-99696d66f49f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03853511s
STEP: Saw pod success
Jul 12 20:52:23.388: INFO: Pod "pod-projected-configmaps-cc34994d-7aa1-45aa-8e91-99696d66f49f" satisfied condition "Succeeded or Failed"
Jul 12 20:52:23.395: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-projected-configmaps-cc34994d-7aa1-45aa-8e91-99696d66f49f container agnhost-container: <nil>
STEP: delete the pod
Jul 12 20:52:23.425: INFO: Waiting for pod pod-projected-configmaps-cc34994d-7aa1-45aa-8e91-99696d66f49f to disappear
Jul 12 20:52:23.442: INFO: Pod pod-projected-configmaps-cc34994d-7aa1-45aa-8e91-99696d66f49f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:23.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7002" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":339,"completed":109,"skipped":1826,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:23.456: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jul 12 20:52:23.736: INFO: The status of Pod annotationupdate57d0d1ec-d5b8-4c9e-a564-1674c00bc59d is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:52:25.745: INFO: The status of Pod annotationupdate57d0d1ec-d5b8-4c9e-a564-1674c00bc59d is Running (Ready = true)
Jul 12 20:52:26.490: INFO: Successfully updated pod "annotationupdate57d0d1ec-d5b8-4c9e-a564-1674c00bc59d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:28.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9371" for this suite.

• [SLOW TEST:5.157 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":110,"skipped":1827,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:28.614: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-projected-all-test-volume-24653b11-193d-4a4b-aedf-9115bdd233a3
STEP: Creating secret with name secret-projected-all-test-volume-821b683c-7714-4afd-b6df-f3541bce5194
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 12 20:52:28.734: INFO: Waiting up to 5m0s for pod "projected-volume-062e598d-bee7-468d-bb2e-00c8c605bffc" in namespace "projected-4315" to be "Succeeded or Failed"
Jul 12 20:52:28.777: INFO: Pod "projected-volume-062e598d-bee7-468d-bb2e-00c8c605bffc": Phase="Pending", Reason="", readiness=false. Elapsed: 43.005149ms
Jul 12 20:52:30.787: INFO: Pod "projected-volume-062e598d-bee7-468d-bb2e-00c8c605bffc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.052893721s
STEP: Saw pod success
Jul 12 20:52:30.787: INFO: Pod "projected-volume-062e598d-bee7-468d-bb2e-00c8c605bffc" satisfied condition "Succeeded or Failed"
Jul 12 20:52:30.793: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod projected-volume-062e598d-bee7-468d-bb2e-00c8c605bffc container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 12 20:52:30.821: INFO: Waiting for pod projected-volume-062e598d-bee7-468d-bb2e-00c8c605bffc to disappear
Jul 12 20:52:30.830: INFO: Pod projected-volume-062e598d-bee7-468d-bb2e-00c8c605bffc no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:30.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4315" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":339,"completed":111,"skipped":1837,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:30.851: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 20:52:30.918: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ce40cf4-1664-4cc6-8286-fa51d477230d" in namespace "projected-1701" to be "Succeeded or Failed"
Jul 12 20:52:30.925: INFO: Pod "downwardapi-volume-2ce40cf4-1664-4cc6-8286-fa51d477230d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.506769ms
Jul 12 20:52:32.933: INFO: Pod "downwardapi-volume-2ce40cf4-1664-4cc6-8286-fa51d477230d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015561719s
STEP: Saw pod success
Jul 12 20:52:32.933: INFO: Pod "downwardapi-volume-2ce40cf4-1664-4cc6-8286-fa51d477230d" satisfied condition "Succeeded or Failed"
Jul 12 20:52:32.941: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-2ce40cf4-1664-4cc6-8286-fa51d477230d container client-container: <nil>
STEP: delete the pod
Jul 12 20:52:32.985: INFO: Waiting for pod downwardapi-volume-2ce40cf4-1664-4cc6-8286-fa51d477230d to disappear
Jul 12 20:52:32.992: INFO: Pod downwardapi-volume-2ce40cf4-1664-4cc6-8286-fa51d477230d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:32.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1701" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":112,"skipped":1849,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:33.015: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Service
STEP: watching for the Service to be added
Jul 12 20:52:33.144: INFO: Found Service test-service-d77ds in namespace services-9096 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jul 12 20:52:33.144: INFO: Service test-service-d77ds created
STEP: Getting /status
Jul 12 20:52:33.157: INFO: Service test-service-d77ds has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jul 12 20:52:33.261: INFO: observed Service test-service-d77ds in namespace services-9096 with annotations: map[cloud.google.com/neg:{"ingress":true}] & LoadBalancer: {[]}
Jul 12 20:52:33.262: INFO: Found Service test-service-d77ds in namespace services-9096 with annotations: map[cloud.google.com/neg:{"ingress":true} patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jul 12 20:52:33.263: INFO: Service test-service-d77ds has service status patched
STEP: updating the ServiceStatus
Jul 12 20:52:33.293: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jul 12 20:52:33.301: INFO: Observed Service test-service-d77ds in namespace services-9096 with annotations: map[cloud.google.com/neg:{"ingress":true}] & Conditions: {[]}
Jul 12 20:52:33.301: INFO: Observed event: &Service{ObjectMeta:{test-service-d77ds  services-9096  e79e061b-381f-4f62-b546-0e8aebcd52e7 16437 0 2021-07-12 20:52:33 +0000 UTC <nil> <nil> map[test-service-static:true] map[cloud.google.com/neg:{"ingress":true} patchedstatus:true] [] []  [{e2e.test Update v1 2021-07-12 20:52:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:patchedstatus":{}},"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}}}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.37.139.60,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,TopologyKeys:[],IPFamilyPolicy:*SingleStack,ClusterIPs:[10.37.139.60],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:nil,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jul 12 20:52:33.302: INFO: Found Service test-service-d77ds in namespace services-9096 with annotations: map[cloud.google.com/neg:{"ingress":true} patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 12 20:52:33.302: INFO: Service test-service-d77ds has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jul 12 20:52:33.403: INFO: observed Service test-service-d77ds in namespace services-9096 with labels: map[test-service-static:true]
Jul 12 20:52:33.403: INFO: observed Service test-service-d77ds in namespace services-9096 with labels: map[test-service-static:true]
Jul 12 20:52:33.404: INFO: observed Service test-service-d77ds in namespace services-9096 with labels: map[test-service-static:true]
Jul 12 20:52:33.404: INFO: Found Service test-service-d77ds in namespace services-9096 with labels: map[test-service:patched test-service-static:true]
Jul 12 20:52:33.404: INFO: Service test-service-d77ds patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jul 12 20:52:33.430: INFO: Observed event: ADDED
Jul 12 20:52:33.430: INFO: Observed event: MODIFIED
Jul 12 20:52:33.431: INFO: Observed event: MODIFIED
Jul 12 20:52:33.431: INFO: Observed event: MODIFIED
Jul 12 20:52:33.431: INFO: Found Service test-service-d77ds in namespace services-9096 with labels: map[test-service:patched test-service-static:true] & annotations: map[cloud.google.com/neg:{"ingress":true} patchedstatus:true]
Jul 12 20:52:33.432: INFO: Service test-service-d77ds deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:33.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9096" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":339,"completed":113,"skipped":1938,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:33.451: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 12 20:52:44.833: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0712 20:52:44.833317      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0712 20:52:44.833348      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0712 20:52:44.833354      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 12 20:52:44.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-5r8zd" in namespace "gc-8332"
Jul 12 20:52:44.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dlxm" in namespace "gc-8332"
Jul 12 20:52:45.008: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9c8k" in namespace "gc-8332"
Jul 12 20:52:45.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-bw25v" in namespace "gc-8332"
Jul 12 20:52:45.103: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsr7g" in namespace "gc-8332"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:45.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8332" for this suite.

• [SLOW TEST:11.718 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":339,"completed":114,"skipped":1944,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:45.170: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating cluster-info
Jul 12 20:52:45.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7690 cluster-info'
Jul 12 20:52:45.482: INFO: stderr: ""
Jul 12 20:52:45.482: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.37.128.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:45.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7690" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":339,"completed":115,"skipped":1953,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:45.501: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test env composition
Jul 12 20:52:45.589: INFO: Waiting up to 5m0s for pod "var-expansion-aa84df9f-5def-41c7-b248-04ef487bf0c7" in namespace "var-expansion-683" to be "Succeeded or Failed"
Jul 12 20:52:45.598: INFO: Pod "var-expansion-aa84df9f-5def-41c7-b248-04ef487bf0c7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.942655ms
Jul 12 20:52:47.607: INFO: Pod "var-expansion-aa84df9f-5def-41c7-b248-04ef487bf0c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018441558s
STEP: Saw pod success
Jul 12 20:52:47.607: INFO: Pod "var-expansion-aa84df9f-5def-41c7-b248-04ef487bf0c7" satisfied condition "Succeeded or Failed"
Jul 12 20:52:47.610: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod var-expansion-aa84df9f-5def-41c7-b248-04ef487bf0c7 container dapi-container: <nil>
STEP: delete the pod
Jul 12 20:52:47.635: INFO: Waiting for pod var-expansion-aa84df9f-5def-41c7-b248-04ef487bf0c7 to disappear
Jul 12 20:52:47.639: INFO: Pod var-expansion-aa84df9f-5def-41c7-b248-04ef487bf0c7 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:47.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-683" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":339,"completed":116,"skipped":1994,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:47.652: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:52:52.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9012" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":339,"completed":117,"skipped":2002,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:52:52.297: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 12 20:52:52.488: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 12 20:53:52.549: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Jul 12 20:53:52.600: INFO: Created pod: pod0-sched-preemption-low-priority
Jul 12 20:53:52.654: INFO: Created pod: pod1-sched-preemption-medium-priority
Jul 12 20:53:52.690: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:54:20.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2948" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:88.679 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":339,"completed":118,"skipped":2004,"failed":0}
SSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:54:20.976: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7278, will wait for the garbage collector to delete the pods
Jul 12 20:54:23.153: INFO: Deleting Job.batch foo took: 5.37176ms
Jul 12 20:54:23.354: INFO: Terminating Job.batch foo pods took: 201.081951ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:55:01.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7278" for this suite.

• [SLOW TEST:40.696 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":339,"completed":119,"skipped":2010,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:55:01.679: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 20:55:02.535: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 20:55:05.567: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 20:55:05.574: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7145-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:55:09.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6527" for this suite.
STEP: Destroying namespace "webhook-6527-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.153 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":339,"completed":120,"skipped":2033,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:55:09.831: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-1334
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jul 12 20:55:10.283: INFO: Found 0 stateful pods, waiting for 3
Jul 12 20:55:20.295: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 20:55:20.295: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 20:55:20.295: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 20:55:20.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-1334 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 20:55:20.567: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 20:55:20.567: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 20:55:20.567: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Jul 12 20:55:30.636: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 12 20:55:40.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-1334 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 20:55:40.865: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 20:55:40.865: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 20:55:40.865: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 20:55:50.890: INFO: Waiting for StatefulSet statefulset-1334/ss2 to complete update
Jul 12 20:55:50.890: INFO: Waiting for Pod statefulset-1334/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 12 20:55:50.890: INFO: Waiting for Pod statefulset-1334/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 12 20:55:50.890: INFO: Waiting for Pod statefulset-1334/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 12 20:56:00.914: INFO: Waiting for StatefulSet statefulset-1334/ss2 to complete update
Jul 12 20:56:00.914: INFO: Waiting for Pod statefulset-1334/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 12 20:56:00.914: INFO: Waiting for Pod statefulset-1334/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 12 20:56:10.903: INFO: Waiting for StatefulSet statefulset-1334/ss2 to complete update
Jul 12 20:56:10.903: INFO: Waiting for Pod statefulset-1334/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 12 20:56:10.903: INFO: Waiting for Pod statefulset-1334/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 12 20:56:20.901: INFO: Waiting for StatefulSet statefulset-1334/ss2 to complete update
Jul 12 20:56:20.902: INFO: Waiting for Pod statefulset-1334/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Rolling back to a previous revision
Jul 12 20:56:30.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-1334 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 20:56:31.129: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 20:56:31.129: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 20:56:31.129: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 20:56:41.265: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 12 20:56:51.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-1334 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 20:56:51.516: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 20:56:51.516: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 20:56:51.516: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 20:57:01.575: INFO: Waiting for StatefulSet statefulset-1334/ss2 to complete update
Jul 12 20:57:01.575: INFO: Waiting for Pod statefulset-1334/ss2-0 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Jul 12 20:57:01.575: INFO: Waiting for Pod statefulset-1334/ss2-1 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Jul 12 20:57:01.575: INFO: Waiting for Pod statefulset-1334/ss2-2 to have revision ss2-677d6db895 update revision ss2-5bbbc9fc94
Jul 12 20:57:21.598: INFO: Waiting for StatefulSet statefulset-1334/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul 12 20:57:31.590: INFO: Deleting all statefulset in ns statefulset-1334
Jul 12 20:57:31.593: INFO: Scaling statefulset ss2 to 0
Jul 12 20:58:01.624: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 20:58:01.627: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:58:01.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1334" for this suite.

• [SLOW TEST:171.856 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":339,"completed":121,"skipped":2035,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:58:01.690: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1021
STEP: creating service affinity-nodeport-transition in namespace services-1021
STEP: creating replication controller affinity-nodeport-transition in namespace services-1021
I0712 20:58:01.767007      20 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-1021, replica count: 3
I0712 20:58:04.818300      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 20:58:07.819105      20 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 20:58:07.876: INFO: Creating new exec pod
Jul 12 20:58:10.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1021 exec execpod-affinity4pmkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jul 12 20:58:12.306: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jul 12 20:58:12.306: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 20:58:12.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1021 exec execpod-affinity4pmkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.141.119 80'
Jul 12 20:58:12.523: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.141.119 80\nConnection to 10.37.141.119 80 port [tcp/http] succeeded!\n"
Jul 12 20:58:12.523: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 20:58:12.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1021 exec execpod-affinity4pmkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.28 30614'
Jul 12 20:58:12.740: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.128.0.28 30614\nConnection to 10.128.0.28 30614 port [tcp/*] succeeded!\n"
Jul 12 20:58:12.740: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 20:58:12.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1021 exec execpod-affinity4pmkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.26 30614'
Jul 12 20:58:12.976: INFO: stderr: "+ nc -v -t -w 2 10.128.0.26 30614\n+ echo hostName\nConnection to 10.128.0.26 30614 port [tcp/*] succeeded!\n"
Jul 12 20:58:12.976: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 20:58:12.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1021 exec execpod-affinity4pmkd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.26:30614/ ; done'
Jul 12 20:58:13.462: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n"
Jul 12 20:58:13.462: INFO: stdout: "\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562"
Jul 12 20:58:13.462: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.462: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.462: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.462: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.462: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.462: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.462: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.462: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.462: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.462: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.462: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.463: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.463: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.463: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.463: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:13.463: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:43.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1021 exec execpod-affinity4pmkd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.26:30614/ ; done'
Jul 12 20:58:43.829: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n"
Jul 12 20:58:43.829: INFO: stdout: "\naffinity-nodeport-transition-zlldg\naffinity-nodeport-transition-nxb6r\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-nxb6r\naffinity-nodeport-transition-nxb6r\naffinity-nodeport-transition-nxb6r\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-zlldg\naffinity-nodeport-transition-zlldg\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-zlldg\naffinity-nodeport-transition-zlldg\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562"
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-zlldg
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-nxb6r
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-nxb6r
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-nxb6r
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-nxb6r
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-zlldg
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-zlldg
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-zlldg
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-zlldg
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:43.829: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:43.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1021 exec execpod-affinity4pmkd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.26:30614/ ; done'
Jul 12 20:58:44.283: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30614/\n"
Jul 12 20:58:44.283: INFO: stdout: "\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562\naffinity-nodeport-transition-45562"
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Received response from host: affinity-nodeport-transition-45562
Jul 12 20:58:44.283: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1021, will wait for the garbage collector to delete the pods
Jul 12 20:58:44.427: INFO: Deleting ReplicationController affinity-nodeport-transition took: 7.594541ms
Jul 12 20:58:44.727: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 300.199173ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:58:51.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1021" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:50.079 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":122,"skipped":2058,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:58:51.771: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 12 20:58:51.817: INFO: Waiting up to 5m0s for pod "pod-68d112af-59cf-44ac-acf0-2b3bd941fe3b" in namespace "emptydir-6374" to be "Succeeded or Failed"
Jul 12 20:58:51.821: INFO: Pod "pod-68d112af-59cf-44ac-acf0-2b3bd941fe3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.377364ms
Jul 12 20:58:53.831: INFO: Pod "pod-68d112af-59cf-44ac-acf0-2b3bd941fe3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014111712s
Jul 12 20:58:55.846: INFO: Pod "pod-68d112af-59cf-44ac-acf0-2b3bd941fe3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02868733s
STEP: Saw pod success
Jul 12 20:58:55.846: INFO: Pod "pod-68d112af-59cf-44ac-acf0-2b3bd941fe3b" satisfied condition "Succeeded or Failed"
Jul 12 20:58:55.852: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-68d112af-59cf-44ac-acf0-2b3bd941fe3b container test-container: <nil>
STEP: delete the pod
Jul 12 20:58:55.915: INFO: Waiting for pod pod-68d112af-59cf-44ac-acf0-2b3bd941fe3b to disappear
Jul 12 20:58:55.920: INFO: Pod pod-68d112af-59cf-44ac-acf0-2b3bd941fe3b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:58:55.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6374" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":123,"skipped":2070,"failed":0}

------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:58:55.936: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jul 12 20:58:58.185: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:59:00.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2197" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":339,"completed":124,"skipped":2070,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:59:00.291: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-5232
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 12 20:59:00.367: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 12 20:59:00.476: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:59:02.486: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 20:59:04.485: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 20:59:06.487: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 20:59:08.487: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 20:59:10.481: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 20:59:12.484: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 20:59:14.489: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 20:59:16.485: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 20:59:18.489: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 20:59:20.487: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 12 20:59:20.493: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul 12 20:59:20.500: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jul 12 20:59:22.522: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jul 12 20:59:26.618: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jul 12 20:59:26.618: INFO: Going to poll 10.40.0.24 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Jul 12 20:59:26.622: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.40.0.24:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5232 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:59:26.622: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:59:26.713: INFO: Found all 1 expected endpoints: [netserver-0]
Jul 12 20:59:26.713: INFO: Going to poll 10.40.1.142 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Jul 12 20:59:26.717: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.40.1.142:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5232 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:59:26.717: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:59:26.805: INFO: Found all 1 expected endpoints: [netserver-1]
Jul 12 20:59:26.805: INFO: Going to poll 10.40.2.27 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Jul 12 20:59:26.810: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.40.2.27:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5232 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 20:59:26.810: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 20:59:26.900: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:59:26.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5232" for this suite.

• [SLOW TEST:26.630 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":125,"skipped":2103,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:59:26.923: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating pod
Jul 12 20:59:27.046: INFO: The status of Pod pod-hostip-7894f0bf-3cf1-468a-afd0-409603f8b518 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 20:59:29.096: INFO: The status of Pod pod-hostip-7894f0bf-3cf1-468a-afd0-409603f8b518 is Running (Ready = true)
Jul 12 20:59:29.113: INFO: Pod pod-hostip-7894f0bf-3cf1-468a-afd0-409603f8b518 has hostIP: 10.128.0.27
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 20:59:29.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2190" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":339,"completed":126,"skipped":2115,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 20:59:29.139: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 12 20:59:29.272: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4110  f7680e96-375a-4c95-a4c8-b0b12285f078 19360 0 2021-07-12 20:59:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 20:59:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 20:59:29.272: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4110  f7680e96-375a-4c95-a4c8-b0b12285f078 19360 0 2021-07-12 20:59:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 20:59:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 12 20:59:39.364: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4110  f7680e96-375a-4c95-a4c8-b0b12285f078 19431 0 2021-07-12 20:59:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 20:59:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 20:59:39.365: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4110  f7680e96-375a-4c95-a4c8-b0b12285f078 19431 0 2021-07-12 20:59:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 20:59:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 12 20:59:49.385: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4110  f7680e96-375a-4c95-a4c8-b0b12285f078 19484 0 2021-07-12 20:59:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 20:59:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 20:59:49.386: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4110  f7680e96-375a-4c95-a4c8-b0b12285f078 19484 0 2021-07-12 20:59:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 20:59:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 12 20:59:59.400: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4110  f7680e96-375a-4c95-a4c8-b0b12285f078 19535 0 2021-07-12 20:59:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 20:59:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 20:59:59.401: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4110  f7680e96-375a-4c95-a4c8-b0b12285f078 19535 0 2021-07-12 20:59:29 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-07-12 20:59:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 12 21:00:09.434: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4110  73ed2af6-37d4-4ab0-8dc7-c16cea68d454 19589 0 2021-07-12 21:00:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-12 21:00:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 21:00:09.434: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4110  73ed2af6-37d4-4ab0-8dc7-c16cea68d454 19589 0 2021-07-12 21:00:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-12 21:00:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 12 21:00:19.462: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4110  73ed2af6-37d4-4ab0-8dc7-c16cea68d454 19637 0 2021-07-12 21:00:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-12 21:00:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 21:00:19.462: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4110  73ed2af6-37d4-4ab0-8dc7-c16cea68d454 19637 0 2021-07-12 21:00:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-07-12 21:00:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:00:29.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4110" for this suite.

• [SLOW TEST:60.336 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":339,"completed":127,"skipped":2118,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:00:29.479: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jul 12 21:00:29.595: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jul 12 21:00:29.634: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:00:29.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5949" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":339,"completed":128,"skipped":2128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:00:29.686: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7614.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7614.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7614.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7614.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 21:00:33.830: INFO: DNS probes using dns-test-58e0d36d-4e4e-42c8-96bf-29bb094ce019 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7614.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7614.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7614.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7614.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 21:00:37.951: INFO: DNS probes using dns-test-917e4c67-e2e5-4e2b-9c23-0124515efbed succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7614.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7614.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7614.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7614.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 21:00:42.177: INFO: DNS probes using dns-test-fa334a45-ad52-457a-8cbe-5e2834096a3c succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:00:42.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7614" for this suite.

• [SLOW TEST:12.667 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":339,"completed":129,"skipped":2154,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:00:42.359: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:00:44.500: INFO: Deleting pod "var-expansion-7a5a03b9-d8a5-46a3-890d-f3d0be39ffdd" in namespace "var-expansion-704"
Jul 12 21:00:44.512: INFO: Wait up to 5m0s for pod "var-expansion-7a5a03b9-d8a5-46a3-890d-f3d0be39ffdd" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:00:52.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-704" for this suite.

• [SLOW TEST:10.297 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":339,"completed":130,"skipped":2155,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:00:52.657: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:00:52.947: INFO: Creating pod...
Jul 12 21:00:53.087: INFO: Pod Quantity: 1 Status: Pending
Jul 12 21:00:54.099: INFO: Pod Quantity: 1 Status: Pending
Jul 12 21:00:55.105: INFO: Pod Status: Running
Jul 12 21:00:55.106: INFO: Creating service...
Jul 12 21:00:55.129: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/pods/agnhost/proxy/some/path/with/DELETE
Jul 12 21:00:55.151: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 12 21:00:55.151: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/pods/agnhost/proxy/some/path/with/GET
Jul 12 21:00:55.158: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul 12 21:00:55.159: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/pods/agnhost/proxy/some/path/with/HEAD
Jul 12 21:00:55.167: INFO: http.Client request:HEAD | StatusCode:200
Jul 12 21:00:55.168: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/pods/agnhost/proxy/some/path/with/OPTIONS
Jul 12 21:00:55.184: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 12 21:00:55.184: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/pods/agnhost/proxy/some/path/with/PATCH
Jul 12 21:00:55.196: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 12 21:00:55.196: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/pods/agnhost/proxy/some/path/with/POST
Jul 12 21:00:55.200: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 12 21:00:55.200: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/pods/agnhost/proxy/some/path/with/PUT
Jul 12 21:00:55.206: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul 12 21:00:55.206: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/services/test-service/proxy/some/path/with/DELETE
Jul 12 21:00:55.212: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 12 21:00:55.213: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/services/test-service/proxy/some/path/with/GET
Jul 12 21:00:55.222: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul 12 21:00:55.222: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/services/test-service/proxy/some/path/with/HEAD
Jul 12 21:00:55.236: INFO: http.Client request:HEAD | StatusCode:200
Jul 12 21:00:55.236: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/services/test-service/proxy/some/path/with/OPTIONS
Jul 12 21:00:55.246: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 12 21:00:55.246: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/services/test-service/proxy/some/path/with/PATCH
Jul 12 21:00:55.252: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 12 21:00:55.252: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/services/test-service/proxy/some/path/with/POST
Jul 12 21:00:55.260: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 12 21:00:55.261: INFO: Starting http.Client for https://10.37.128.1:443/api/v1/namespaces/proxy-2906/services/test-service/proxy/some/path/with/PUT
Jul 12 21:00:55.268: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:00:55.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2906" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":339,"completed":131,"skipped":2215,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:00:55.285: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:01:11.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9968" for this suite.

• [SLOW TEST:16.373 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":339,"completed":132,"skipped":2217,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:01:11.659: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:01:38.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5742" for this suite.

• [SLOW TEST:26.865 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":339,"completed":133,"skipped":2223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:01:38.524: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:01:38.692: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:01:42.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5302" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":339,"completed":134,"skipped":2252,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:01:42.132: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:01:42.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9314" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":339,"completed":135,"skipped":2260,"failed":0}
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:01:42.240: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-6fsh2 in namespace proxy-4622
I0712 21:01:42.376615      20 runners.go:190] Created replication controller with name: proxy-service-6fsh2, namespace: proxy-4622, replica count: 1
I0712 21:01:43.428945      20 runners.go:190] proxy-service-6fsh2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 21:01:44.429165      20 runners.go:190] proxy-service-6fsh2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 21:01:45.429592      20 runners.go:190] proxy-service-6fsh2 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 21:01:45.566: INFO: setup took 3.249406095s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 12 21:01:45.866: INFO: (0) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 299.262946ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 301.612631ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 301.228291ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 302.201989ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 301.588204ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 300.686003ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 302.067965ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 301.887346ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 301.815037ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 301.527519ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 302.058007ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 300.722479ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 301.166854ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 301.530674ms)
Jul 12 21:01:45.868: INFO: (0) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 301.347543ms)
Jul 12 21:01:45.869: INFO: (0) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 301.741317ms)
Jul 12 21:01:45.935: INFO: (1) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 60.766856ms)
Jul 12 21:01:45.959: INFO: (1) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 84.862271ms)
Jul 12 21:01:45.972: INFO: (1) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 98.250689ms)
Jul 12 21:01:45.972: INFO: (1) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 97.899648ms)
Jul 12 21:01:45.972: INFO: (1) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 98.663402ms)
Jul 12 21:01:45.972: INFO: (1) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 98.804153ms)
Jul 12 21:01:45.972: INFO: (1) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 98.753899ms)
Jul 12 21:01:45.972: INFO: (1) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 98.508916ms)
Jul 12 21:01:45.972: INFO: (1) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 98.948345ms)
Jul 12 21:01:45.973: INFO: (1) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 98.230529ms)
Jul 12 21:01:45.973: INFO: (1) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 98.185136ms)
Jul 12 21:01:45.973: INFO: (1) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 99.199578ms)
Jul 12 21:01:45.973: INFO: (1) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 98.336487ms)
Jul 12 21:01:45.977: INFO: (1) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 102.755352ms)
Jul 12 21:01:45.982: INFO: (1) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 108.453323ms)
Jul 12 21:01:45.982: INFO: (1) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 108.526903ms)
Jul 12 21:01:46.020: INFO: (2) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 36.397397ms)
Jul 12 21:01:46.020: INFO: (2) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 36.448288ms)
Jul 12 21:01:46.020: INFO: (2) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 37.410282ms)
Jul 12 21:01:46.020: INFO: (2) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 37.127194ms)
Jul 12 21:01:46.020: INFO: (2) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 37.207608ms)
Jul 12 21:01:46.020: INFO: (2) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 36.94436ms)
Jul 12 21:01:46.020: INFO: (2) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 36.833311ms)
Jul 12 21:01:46.020: INFO: (2) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 36.787342ms)
Jul 12 21:01:46.022: INFO: (2) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 39.057786ms)
Jul 12 21:01:46.023: INFO: (2) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 39.894836ms)
Jul 12 21:01:46.057: INFO: (2) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 73.54823ms)
Jul 12 21:01:46.058: INFO: (2) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 74.274585ms)
Jul 12 21:01:46.058: INFO: (2) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 74.986695ms)
Jul 12 21:01:46.058: INFO: (2) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 75.092113ms)
Jul 12 21:01:46.058: INFO: (2) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 74.790148ms)
Jul 12 21:01:46.059: INFO: (2) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 75.422676ms)
Jul 12 21:01:46.092: INFO: (3) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 33.235872ms)
Jul 12 21:01:46.093: INFO: (3) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 33.081518ms)
Jul 12 21:01:46.093: INFO: (3) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 33.298202ms)
Jul 12 21:01:46.096: INFO: (3) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 36.696263ms)
Jul 12 21:01:46.096: INFO: (3) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 37.092086ms)
Jul 12 21:01:46.099: INFO: (3) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 40.103073ms)
Jul 12 21:01:46.108: INFO: (3) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 47.653625ms)
Jul 12 21:01:46.108: INFO: (3) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 47.623704ms)
Jul 12 21:01:46.108: INFO: (3) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 48.576895ms)
Jul 12 21:01:46.108: INFO: (3) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 49.40886ms)
Jul 12 21:01:46.108: INFO: (3) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 47.485497ms)
Jul 12 21:01:46.109: INFO: (3) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 49.750474ms)
Jul 12 21:01:46.109: INFO: (3) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 49.356292ms)
Jul 12 21:01:46.110: INFO: (3) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 49.624822ms)
Jul 12 21:01:46.110: INFO: (3) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 50.107504ms)
Jul 12 21:01:46.110: INFO: (3) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 50.368972ms)
Jul 12 21:01:46.169: INFO: (4) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 58.278129ms)
Jul 12 21:01:46.179: INFO: (4) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 67.774249ms)
Jul 12 21:01:46.179: INFO: (4) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 68.405401ms)
Jul 12 21:01:46.179: INFO: (4) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 67.811262ms)
Jul 12 21:01:46.185: INFO: (4) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 74.374228ms)
Jul 12 21:01:46.185: INFO: (4) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 73.886256ms)
Jul 12 21:01:46.185: INFO: (4) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 74.683046ms)
Jul 12 21:01:46.187: INFO: (4) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 76.048348ms)
Jul 12 21:01:46.187: INFO: (4) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 76.543498ms)
Jul 12 21:01:46.188: INFO: (4) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 76.433531ms)
Jul 12 21:01:46.188: INFO: (4) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 76.662898ms)
Jul 12 21:01:46.189: INFO: (4) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 77.321555ms)
Jul 12 21:01:46.189: INFO: (4) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 77.876763ms)
Jul 12 21:01:46.190: INFO: (4) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 79.260657ms)
Jul 12 21:01:46.190: INFO: (4) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 78.815004ms)
Jul 12 21:01:46.190: INFO: (4) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 79.36211ms)
Jul 12 21:01:46.233: INFO: (5) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 42.270757ms)
Jul 12 21:01:46.234: INFO: (5) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 43.048462ms)
Jul 12 21:01:46.234: INFO: (5) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 42.683216ms)
Jul 12 21:01:46.249: INFO: (5) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 57.696438ms)
Jul 12 21:01:46.250: INFO: (5) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 59.162565ms)
Jul 12 21:01:46.250: INFO: (5) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 58.305993ms)
Jul 12 21:01:46.250: INFO: (5) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 58.351851ms)
Jul 12 21:01:46.250: INFO: (5) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 59.23311ms)
Jul 12 21:01:46.255: INFO: (5) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 63.495852ms)
Jul 12 21:01:46.255: INFO: (5) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 63.956324ms)
Jul 12 21:01:46.255: INFO: (5) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 64.545572ms)
Jul 12 21:01:46.256: INFO: (5) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 64.205906ms)
Jul 12 21:01:46.263: INFO: (5) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 71.313752ms)
Jul 12 21:01:46.263: INFO: (5) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 72.001545ms)
Jul 12 21:01:46.265: INFO: (5) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 73.288685ms)
Jul 12 21:01:46.265: INFO: (5) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 73.338491ms)
Jul 12 21:01:46.316: INFO: (6) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 49.961595ms)
Jul 12 21:01:46.316: INFO: (6) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 50.283613ms)
Jul 12 21:01:46.316: INFO: (6) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 50.336712ms)
Jul 12 21:01:46.320: INFO: (6) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 54.944312ms)
Jul 12 21:01:46.342: INFO: (6) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 76.297444ms)
Jul 12 21:01:46.342: INFO: (6) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 75.952255ms)
Jul 12 21:01:46.342: INFO: (6) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 75.828245ms)
Jul 12 21:01:46.342: INFO: (6) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 76.252315ms)
Jul 12 21:01:46.342: INFO: (6) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 76.355271ms)
Jul 12 21:01:46.342: INFO: (6) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 76.859143ms)
Jul 12 21:01:46.352: INFO: (6) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 86.470528ms)
Jul 12 21:01:46.352: INFO: (6) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 86.615852ms)
Jul 12 21:01:46.359: INFO: (6) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 92.892599ms)
Jul 12 21:01:46.359: INFO: (6) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 92.81141ms)
Jul 12 21:01:46.371: INFO: (6) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 105.200571ms)
Jul 12 21:01:46.373: INFO: (6) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 107.287509ms)
Jul 12 21:01:46.442: INFO: (7) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 67.359809ms)
Jul 12 21:01:46.448: INFO: (7) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 73.923824ms)
Jul 12 21:01:46.448: INFO: (7) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 74.204352ms)
Jul 12 21:01:46.448: INFO: (7) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 73.878579ms)
Jul 12 21:01:46.448: INFO: (7) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 73.86358ms)
Jul 12 21:01:46.448: INFO: (7) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 74.179426ms)
Jul 12 21:01:46.449: INFO: (7) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 75.448716ms)
Jul 12 21:01:46.459: INFO: (7) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 85.000992ms)
Jul 12 21:01:46.468: INFO: (7) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 93.59467ms)
Jul 12 21:01:46.478: INFO: (7) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 104.117423ms)
Jul 12 21:01:46.496: INFO: (7) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 122.188761ms)
Jul 12 21:01:46.498: INFO: (7) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 124.454052ms)
Jul 12 21:01:46.499: INFO: (7) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 124.099702ms)
Jul 12 21:01:46.501: INFO: (7) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 126.683623ms)
Jul 12 21:01:46.501: INFO: (7) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 126.64481ms)
Jul 12 21:01:46.509: INFO: (7) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 134.359311ms)
Jul 12 21:01:46.617: INFO: (8) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 108.168842ms)
Jul 12 21:01:46.685: INFO: (8) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 176.105827ms)
Jul 12 21:01:46.734: INFO: (8) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 224.251593ms)
Jul 12 21:01:46.734: INFO: (8) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 224.211059ms)
Jul 12 21:01:46.740: INFO: (8) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 230.53703ms)
Jul 12 21:01:46.749: INFO: (8) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 239.852044ms)
Jul 12 21:01:46.767: INFO: (8) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 257.791698ms)
Jul 12 21:01:46.767: INFO: (8) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 257.718759ms)
Jul 12 21:01:46.778: INFO: (8) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 268.370018ms)
Jul 12 21:01:46.785: INFO: (8) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 275.524117ms)
Jul 12 21:01:46.785: INFO: (8) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 275.702471ms)
Jul 12 21:01:46.789: INFO: (8) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 279.702272ms)
Jul 12 21:01:46.801: INFO: (8) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 291.285505ms)
Jul 12 21:01:46.801: INFO: (8) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 291.225818ms)
Jul 12 21:01:46.833: INFO: (8) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 322.940051ms)
Jul 12 21:01:46.835: INFO: (8) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 325.726879ms)
Jul 12 21:01:46.978: INFO: (9) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 141.583606ms)
Jul 12 21:01:46.978: INFO: (9) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 141.993233ms)
Jul 12 21:01:46.978: INFO: (9) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 141.789203ms)
Jul 12 21:01:46.978: INFO: (9) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 142.124222ms)
Jul 12 21:01:46.978: INFO: (9) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 141.38935ms)
Jul 12 21:01:46.978: INFO: (9) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 141.708735ms)
Jul 12 21:01:46.978: INFO: (9) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 141.673464ms)
Jul 12 21:01:46.978: INFO: (9) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 141.561173ms)
Jul 12 21:01:46.979: INFO: (9) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 142.882642ms)
Jul 12 21:01:46.980: INFO: (9) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 143.253762ms)
Jul 12 21:01:47.023: INFO: (9) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 185.816268ms)
Jul 12 21:01:47.027: INFO: (9) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 190.143344ms)
Jul 12 21:01:47.027: INFO: (9) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 190.288661ms)
Jul 12 21:01:47.027: INFO: (9) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 189.744167ms)
Jul 12 21:01:47.027: INFO: (9) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 190.581367ms)
Jul 12 21:01:47.028: INFO: (9) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 191.435866ms)
Jul 12 21:01:47.201: INFO: (10) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 171.867643ms)
Jul 12 21:01:47.211: INFO: (10) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 182.153212ms)
Jul 12 21:01:47.211: INFO: (10) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 181.928608ms)
Jul 12 21:01:47.211: INFO: (10) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 182.210941ms)
Jul 12 21:01:47.211: INFO: (10) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 182.515947ms)
Jul 12 21:01:47.211: INFO: (10) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 182.089548ms)
Jul 12 21:01:47.229: INFO: (10) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 200.026223ms)
Jul 12 21:01:47.229: INFO: (10) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 200.26284ms)
Jul 12 21:01:47.230: INFO: (10) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 201.470088ms)
Jul 12 21:01:47.230: INFO: (10) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 201.397434ms)
Jul 12 21:01:47.230: INFO: (10) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 201.383642ms)
Jul 12 21:01:47.231: INFO: (10) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 202.390539ms)
Jul 12 21:01:47.234: INFO: (10) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 205.796444ms)
Jul 12 21:01:47.248: INFO: (10) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 218.567718ms)
Jul 12 21:01:47.248: INFO: (10) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 219.420899ms)
Jul 12 21:01:47.249: INFO: (10) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 219.731177ms)
Jul 12 21:01:47.315: INFO: (11) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 64.807863ms)
Jul 12 21:01:47.323: INFO: (11) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 72.68122ms)
Jul 12 21:01:47.323: INFO: (11) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 73.593133ms)
Jul 12 21:01:47.323: INFO: (11) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 72.832923ms)
Jul 12 21:01:47.323: INFO: (11) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 72.363367ms)
Jul 12 21:01:47.323: INFO: (11) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 73.350076ms)
Jul 12 21:01:47.323: INFO: (11) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 73.313127ms)
Jul 12 21:01:47.323: INFO: (11) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 72.665288ms)
Jul 12 21:01:47.323: INFO: (11) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 73.214573ms)
Jul 12 21:01:47.342: INFO: (11) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 92.96503ms)
Jul 12 21:01:47.347: INFO: (11) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 97.326034ms)
Jul 12 21:01:47.349: INFO: (11) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 99.462783ms)
Jul 12 21:01:47.349: INFO: (11) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 98.775287ms)
Jul 12 21:01:47.349: INFO: (11) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 98.758398ms)
Jul 12 21:01:47.349: INFO: (11) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 99.381833ms)
Jul 12 21:01:47.349: INFO: (11) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 100.252714ms)
Jul 12 21:01:47.395: INFO: (12) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 44.39635ms)
Jul 12 21:01:47.413: INFO: (12) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 63.477343ms)
Jul 12 21:01:47.424: INFO: (12) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 74.34851ms)
Jul 12 21:01:47.435: INFO: (12) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 84.382341ms)
Jul 12 21:01:47.435: INFO: (12) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 84.611675ms)
Jul 12 21:01:47.435: INFO: (12) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 84.406391ms)
Jul 12 21:01:47.435: INFO: (12) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 84.352564ms)
Jul 12 21:01:47.435: INFO: (12) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 84.862609ms)
Jul 12 21:01:47.440: INFO: (12) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 89.902928ms)
Jul 12 21:01:47.442: INFO: (12) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 92.121647ms)
Jul 12 21:01:47.443: INFO: (12) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 92.686671ms)
Jul 12 21:01:47.448: INFO: (12) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 98.356917ms)
Jul 12 21:01:47.449: INFO: (12) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 98.568982ms)
Jul 12 21:01:47.459: INFO: (12) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 109.278322ms)
Jul 12 21:01:47.462: INFO: (12) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 111.616591ms)
Jul 12 21:01:47.464: INFO: (12) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 113.554604ms)
Jul 12 21:01:47.544: INFO: (13) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 79.755205ms)
Jul 12 21:01:47.544: INFO: (13) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 79.456705ms)
Jul 12 21:01:47.544: INFO: (13) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 79.351022ms)
Jul 12 21:01:47.545: INFO: (13) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 80.179767ms)
Jul 12 21:01:47.546: INFO: (13) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 81.013165ms)
Jul 12 21:01:47.548: INFO: (13) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 84.252055ms)
Jul 12 21:01:47.548: INFO: (13) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 84.568343ms)
Jul 12 21:01:47.555: INFO: (13) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 90.784959ms)
Jul 12 21:01:47.555: INFO: (13) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 90.789348ms)
Jul 12 21:01:47.555: INFO: (13) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 90.433418ms)
Jul 12 21:01:47.555: INFO: (13) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 90.859996ms)
Jul 12 21:01:47.559: INFO: (13) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 94.172498ms)
Jul 12 21:01:47.559: INFO: (13) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 94.735261ms)
Jul 12 21:01:47.559: INFO: (13) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 94.17544ms)
Jul 12 21:01:47.562: INFO: (13) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 96.72326ms)
Jul 12 21:01:47.564: INFO: (13) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 99.180417ms)
Jul 12 21:01:47.617: INFO: (14) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 52.796196ms)
Jul 12 21:01:47.617: INFO: (14) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 52.704611ms)
Jul 12 21:01:47.618: INFO: (14) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 53.072576ms)
Jul 12 21:01:47.618: INFO: (14) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 53.40369ms)
Jul 12 21:01:47.622: INFO: (14) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 57.703492ms)
Jul 12 21:01:47.622: INFO: (14) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 57.356231ms)
Jul 12 21:01:47.624: INFO: (14) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 60.461025ms)
Jul 12 21:01:47.624: INFO: (14) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 59.830253ms)
Jul 12 21:01:47.624: INFO: (14) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 60.079824ms)
Jul 12 21:01:47.624: INFO: (14) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 60.013965ms)
Jul 12 21:01:47.625: INFO: (14) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 61.145078ms)
Jul 12 21:01:47.630: INFO: (14) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 65.411837ms)
Jul 12 21:01:47.630: INFO: (14) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 65.627802ms)
Jul 12 21:01:47.635: INFO: (14) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 69.962894ms)
Jul 12 21:01:47.636: INFO: (14) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 72.477317ms)
Jul 12 21:01:47.643: INFO: (14) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 78.546534ms)
Jul 12 21:01:47.688: INFO: (15) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 44.254979ms)
Jul 12 21:01:47.693: INFO: (15) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 49.836226ms)
Jul 12 21:01:47.693: INFO: (15) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 49.620586ms)
Jul 12 21:01:47.693: INFO: (15) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 49.689926ms)
Jul 12 21:01:47.693: INFO: (15) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 49.631295ms)
Jul 12 21:01:47.693: INFO: (15) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 49.929206ms)
Jul 12 21:01:47.699: INFO: (15) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 55.717416ms)
Jul 12 21:01:47.699: INFO: (15) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 55.672661ms)
Jul 12 21:01:47.700: INFO: (15) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 56.761068ms)
Jul 12 21:01:47.700: INFO: (15) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 56.806638ms)
Jul 12 21:01:47.716: INFO: (15) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 72.640734ms)
Jul 12 21:01:47.716: INFO: (15) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 72.598359ms)
Jul 12 21:01:47.716: INFO: (15) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 72.599131ms)
Jul 12 21:01:47.716: INFO: (15) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 72.821402ms)
Jul 12 21:01:47.719: INFO: (15) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 75.653605ms)
Jul 12 21:01:47.719: INFO: (15) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 75.614216ms)
Jul 12 21:01:47.772: INFO: (16) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 53.189095ms)
Jul 12 21:01:47.772: INFO: (16) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 52.916411ms)
Jul 12 21:01:47.772: INFO: (16) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 52.632015ms)
Jul 12 21:01:47.785: INFO: (16) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 65.790648ms)
Jul 12 21:01:47.786: INFO: (16) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 66.22987ms)
Jul 12 21:01:47.786: INFO: (16) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 66.113542ms)
Jul 12 21:01:47.790: INFO: (16) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 70.336557ms)
Jul 12 21:01:47.793: INFO: (16) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 73.27739ms)
Jul 12 21:01:47.794: INFO: (16) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 74.737983ms)
Jul 12 21:01:47.804: INFO: (16) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 84.845336ms)
Jul 12 21:01:47.807: INFO: (16) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 87.448897ms)
Jul 12 21:01:47.814: INFO: (16) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 95.086525ms)
Jul 12 21:01:47.815: INFO: (16) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 95.711921ms)
Jul 12 21:01:47.815: INFO: (16) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 95.575082ms)
Jul 12 21:01:47.816: INFO: (16) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 96.412992ms)
Jul 12 21:01:47.816: INFO: (16) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 96.766655ms)
Jul 12 21:01:47.852: INFO: (17) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 35.323545ms)
Jul 12 21:01:47.853: INFO: (17) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 35.302609ms)
Jul 12 21:01:47.853: INFO: (17) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 35.843152ms)
Jul 12 21:01:47.853: INFO: (17) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 35.882491ms)
Jul 12 21:01:47.853: INFO: (17) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 35.453537ms)
Jul 12 21:01:47.852: INFO: (17) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 35.155323ms)
Jul 12 21:01:47.853: INFO: (17) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 35.609318ms)
Jul 12 21:01:47.870: INFO: (17) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 53.75328ms)
Jul 12 21:01:47.871: INFO: (17) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 53.882902ms)
Jul 12 21:01:47.870: INFO: (17) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 53.559417ms)
Jul 12 21:01:47.874: INFO: (17) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 56.832231ms)
Jul 12 21:01:47.875: INFO: (17) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 58.424293ms)
Jul 12 21:01:47.877: INFO: (17) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 59.515058ms)
Jul 12 21:01:47.887: INFO: (17) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 69.451864ms)
Jul 12 21:01:47.887: INFO: (17) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 70.625613ms)
Jul 12 21:01:47.888: INFO: (17) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 69.385975ms)
Jul 12 21:01:47.946: INFO: (18) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 57.680686ms)
Jul 12 21:01:47.973: INFO: (18) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 84.704774ms)
Jul 12 21:01:47.974: INFO: (18) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 85.22434ms)
Jul 12 21:01:47.974: INFO: (18) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 85.356951ms)
Jul 12 21:01:47.974: INFO: (18) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 85.651638ms)
Jul 12 21:01:47.980: INFO: (18) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 91.652738ms)
Jul 12 21:01:47.980: INFO: (18) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 91.92058ms)
Jul 12 21:01:47.980: INFO: (18) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 91.839739ms)
Jul 12 21:01:47.980: INFO: (18) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 91.691857ms)
Jul 12 21:01:47.980: INFO: (18) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 92.127758ms)
Jul 12 21:01:47.981: INFO: (18) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 93.029169ms)
Jul 12 21:01:47.997: INFO: (18) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 108.447018ms)
Jul 12 21:01:47.998: INFO: (18) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 109.250651ms)
Jul 12 21:01:48.017: INFO: (18) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 129.342319ms)
Jul 12 21:01:48.018: INFO: (18) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 129.453021ms)
Jul 12 21:01:48.021: INFO: (18) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 132.233377ms)
Jul 12 21:01:48.105: INFO: (19) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:460/proxy/: tls baz (200; 82.849751ms)
Jul 12 21:01:48.105: INFO: (19) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">... (200; 82.664754ms)
Jul 12 21:01:48.105: INFO: (19) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:462/proxy/: tls qux (200; 82.996678ms)
Jul 12 21:01:48.105: INFO: (19) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 82.913247ms)
Jul 12 21:01:48.105: INFO: (19) /api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/https:proxy-service-6fsh2-bcfqh:443/proxy/tlsrewritem... (200; 82.783664ms)
Jul 12 21:01:48.105: INFO: (19) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:162/proxy/: bar (200; 82.8277ms)
Jul 12 21:01:48.105: INFO: (19) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 83.304623ms)
Jul 12 21:01:48.111: INFO: (19) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh:1080/proxy/rewriteme">test<... (200; 89.841216ms)
Jul 12 21:01:48.112: INFO: (19) /api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/: <a href="/api/v1/namespaces/proxy-4622/pods/proxy-service-6fsh2-bcfqh/proxy/rewriteme">test</a> (200; 90.189469ms)
Jul 12 21:01:48.115: INFO: (19) /api/v1/namespaces/proxy-4622/pods/http:proxy-service-6fsh2-bcfqh:160/proxy/: foo (200; 93.540542ms)
Jul 12 21:01:48.123: INFO: (19) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname2/proxy/: bar (200; 101.007952ms)
Jul 12 21:01:48.124: INFO: (19) /api/v1/namespaces/proxy-4622/services/proxy-service-6fsh2:portname1/proxy/: foo (200; 102.113942ms)
Jul 12 21:01:48.127: INFO: (19) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname1/proxy/: foo (200; 105.500399ms)
Jul 12 21:01:48.128: INFO: (19) /api/v1/namespaces/proxy-4622/services/http:proxy-service-6fsh2:portname2/proxy/: bar (200; 106.393206ms)
Jul 12 21:01:48.128: INFO: (19) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname1/proxy/: tls baz (200; 105.812009ms)
Jul 12 21:01:48.128: INFO: (19) /api/v1/namespaces/proxy-4622/services/https:proxy-service-6fsh2:tlsportname2/proxy/: tls qux (200; 106.082067ms)
STEP: deleting ReplicationController proxy-service-6fsh2 in namespace proxy-4622, will wait for the garbage collector to delete the pods
Jul 12 21:01:48.231: INFO: Deleting ReplicationController proxy-service-6fsh2 took: 21.012292ms
Jul 12 21:01:48.432: INFO: Terminating ReplicationController proxy-service-6fsh2 pods took: 200.733706ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:02:01.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4622" for this suite.

• [SLOW TEST:19.407 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":339,"completed":136,"skipped":2264,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:02:01.648: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-c0cf4aa3-ea61-4dec-8f55-d7b235524b08
STEP: Creating a pod to test consume configMaps
Jul 12 21:02:01.711: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-37966e9e-13b0-4fa4-9b89-df033d0e399f" in namespace "projected-4732" to be "Succeeded or Failed"
Jul 12 21:02:01.718: INFO: Pod "pod-projected-configmaps-37966e9e-13b0-4fa4-9b89-df033d0e399f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.25826ms
Jul 12 21:02:03.727: INFO: Pod "pod-projected-configmaps-37966e9e-13b0-4fa4-9b89-df033d0e399f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016122677s
STEP: Saw pod success
Jul 12 21:02:03.727: INFO: Pod "pod-projected-configmaps-37966e9e-13b0-4fa4-9b89-df033d0e399f" satisfied condition "Succeeded or Failed"
Jul 12 21:02:03.732: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-projected-configmaps-37966e9e-13b0-4fa4-9b89-df033d0e399f container agnhost-container: <nil>
STEP: delete the pod
Jul 12 21:02:03.768: INFO: Waiting for pod pod-projected-configmaps-37966e9e-13b0-4fa4-9b89-df033d0e399f to disappear
Jul 12 21:02:03.772: INFO: Pod pod-projected-configmaps-37966e9e-13b0-4fa4-9b89-df033d0e399f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:02:03.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4732" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":137,"skipped":2268,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:02:03.783: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:02:03.856: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9e585d24-c548-40e4-850f-3b2964cbe187" in namespace "downward-api-4473" to be "Succeeded or Failed"
Jul 12 21:02:03.862: INFO: Pod "downwardapi-volume-9e585d24-c548-40e4-850f-3b2964cbe187": Phase="Pending", Reason="", readiness=false. Elapsed: 5.379603ms
Jul 12 21:02:05.869: INFO: Pod "downwardapi-volume-9e585d24-c548-40e4-850f-3b2964cbe187": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012186407s
STEP: Saw pod success
Jul 12 21:02:05.869: INFO: Pod "downwardapi-volume-9e585d24-c548-40e4-850f-3b2964cbe187" satisfied condition "Succeeded or Failed"
Jul 12 21:02:05.874: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-9e585d24-c548-40e4-850f-3b2964cbe187 container client-container: <nil>
STEP: delete the pod
Jul 12 21:02:05.898: INFO: Waiting for pod downwardapi-volume-9e585d24-c548-40e4-850f-3b2964cbe187 to disappear
Jul 12 21:02:05.902: INFO: Pod downwardapi-volume-9e585d24-c548-40e4-850f-3b2964cbe187 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:02:05.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4473" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":138,"skipped":2276,"failed":0}
SSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:02:05.914: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's args
Jul 12 21:02:05.967: INFO: Waiting up to 5m0s for pod "var-expansion-c5a80edc-4e28-418f-9db8-96496c7c4496" in namespace "var-expansion-8023" to be "Succeeded or Failed"
Jul 12 21:02:05.976: INFO: Pod "var-expansion-c5a80edc-4e28-418f-9db8-96496c7c4496": Phase="Pending", Reason="", readiness=false. Elapsed: 9.323947ms
Jul 12 21:02:07.993: INFO: Pod "var-expansion-c5a80edc-4e28-418f-9db8-96496c7c4496": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025916739s
STEP: Saw pod success
Jul 12 21:02:07.993: INFO: Pod "var-expansion-c5a80edc-4e28-418f-9db8-96496c7c4496" satisfied condition "Succeeded or Failed"
Jul 12 21:02:07.998: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod var-expansion-c5a80edc-4e28-418f-9db8-96496c7c4496 container dapi-container: <nil>
STEP: delete the pod
Jul 12 21:02:08.045: INFO: Waiting for pod var-expansion-c5a80edc-4e28-418f-9db8-96496c7c4496 to disappear
Jul 12 21:02:08.049: INFO: Pod var-expansion-c5a80edc-4e28-418f-9db8-96496c7c4496 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:02:08.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8023" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":339,"completed":139,"skipped":2282,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:02:08.073: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:02:08.259: INFO: The status of Pod server-envvars-ffb7a1bb-b2f7-4363-bfb2-acf36477d75c is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:02:10.267: INFO: The status of Pod server-envvars-ffb7a1bb-b2f7-4363-bfb2-acf36477d75c is Running (Ready = true)
Jul 12 21:02:10.352: INFO: Waiting up to 5m0s for pod "client-envvars-f182c5d8-1393-4901-bd94-c6227a6da847" in namespace "pods-2387" to be "Succeeded or Failed"
Jul 12 21:02:10.361: INFO: Pod "client-envvars-f182c5d8-1393-4901-bd94-c6227a6da847": Phase="Pending", Reason="", readiness=false. Elapsed: 8.781875ms
Jul 12 21:02:12.372: INFO: Pod "client-envvars-f182c5d8-1393-4901-bd94-c6227a6da847": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020194086s
STEP: Saw pod success
Jul 12 21:02:12.372: INFO: Pod "client-envvars-f182c5d8-1393-4901-bd94-c6227a6da847" satisfied condition "Succeeded or Failed"
Jul 12 21:02:12.379: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod client-envvars-f182c5d8-1393-4901-bd94-c6227a6da847 container env3cont: <nil>
STEP: delete the pod
Jul 12 21:02:12.423: INFO: Waiting for pod client-envvars-f182c5d8-1393-4901-bd94-c6227a6da847 to disappear
Jul 12 21:02:12.435: INFO: Pod client-envvars-f182c5d8-1393-4901-bd94-c6227a6da847 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:02:12.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2387" for this suite.
•{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":339,"completed":140,"skipped":2294,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:02:12.450: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jul 12 21:02:12.551: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 21:02:17.737: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:02:35.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1755" for this suite.

• [SLOW TEST:23.322 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":339,"completed":141,"skipped":2302,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:02:35.771: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-e52281bf-497e-4df2-8ac9-e5e020abd7e4
STEP: Creating a pod to test consume configMaps
Jul 12 21:02:35.831: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-841d439f-3488-47de-92d8-a23530ace250" in namespace "projected-1226" to be "Succeeded or Failed"
Jul 12 21:02:35.837: INFO: Pod "pod-projected-configmaps-841d439f-3488-47de-92d8-a23530ace250": Phase="Pending", Reason="", readiness=false. Elapsed: 6.403919ms
Jul 12 21:02:37.858: INFO: Pod "pod-projected-configmaps-841d439f-3488-47de-92d8-a23530ace250": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026913188s
STEP: Saw pod success
Jul 12 21:02:37.858: INFO: Pod "pod-projected-configmaps-841d439f-3488-47de-92d8-a23530ace250" satisfied condition "Succeeded or Failed"
Jul 12 21:02:37.865: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-projected-configmaps-841d439f-3488-47de-92d8-a23530ace250 container agnhost-container: <nil>
STEP: delete the pod
Jul 12 21:02:37.914: INFO: Waiting for pod pod-projected-configmaps-841d439f-3488-47de-92d8-a23530ace250 to disappear
Jul 12 21:02:37.921: INFO: Pod pod-projected-configmaps-841d439f-3488-47de-92d8-a23530ace250 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:02:37.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1226" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":142,"skipped":2302,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:02:37.969: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:03:06.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1562" for this suite.

• [SLOW TEST:28.152 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":339,"completed":143,"skipped":2310,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:03:06.124: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 12 21:03:06.181: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 12 21:03:11.243: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:03:12.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4143" for this suite.

• [SLOW TEST:6.282 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":339,"completed":144,"skipped":2320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:03:12.408: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-fdf727b4-3267-4e63-9df3-cd0370f3ece8
STEP: Creating a pod to test consume secrets
Jul 12 21:03:12.505: INFO: Waiting up to 5m0s for pod "pod-secrets-e9eeed5e-bcbb-4c33-b8ab-b45d24c16534" in namespace "secrets-7588" to be "Succeeded or Failed"
Jul 12 21:03:12.510: INFO: Pod "pod-secrets-e9eeed5e-bcbb-4c33-b8ab-b45d24c16534": Phase="Pending", Reason="", readiness=false. Elapsed: 4.517612ms
Jul 12 21:03:14.520: INFO: Pod "pod-secrets-e9eeed5e-bcbb-4c33-b8ab-b45d24c16534": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015075703s
STEP: Saw pod success
Jul 12 21:03:14.520: INFO: Pod "pod-secrets-e9eeed5e-bcbb-4c33-b8ab-b45d24c16534" satisfied condition "Succeeded or Failed"
Jul 12 21:03:14.523: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-secrets-e9eeed5e-bcbb-4c33-b8ab-b45d24c16534 container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 21:03:14.554: INFO: Waiting for pod pod-secrets-e9eeed5e-bcbb-4c33-b8ab-b45d24c16534 to disappear
Jul 12 21:03:14.562: INFO: Pod pod-secrets-e9eeed5e-bcbb-4c33-b8ab-b45d24c16534 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:03:14.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7588" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":145,"skipped":2344,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:03:14.577: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 21:03:15.866: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 12 21:03:17.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761720595, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761720595, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761720595, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761720595, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 21:03:20.975: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:03:21.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-49" for this suite.
STEP: Destroying namespace "webhook-49-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.753 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":339,"completed":146,"skipped":2348,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:03:21.329: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pods
Jul 12 21:03:21.435: INFO: created test-pod-1
Jul 12 21:03:21.488: INFO: created test-pod-2
Jul 12 21:03:21.506: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:03:21.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5377" for this suite.
•{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":339,"completed":147,"skipped":2350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:03:21.585: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-map-ec891be7-ff49-4abe-bf7e-e001d2f521a3
STEP: Creating a pod to test consume secrets
Jul 12 21:03:21.639: INFO: Waiting up to 5m0s for pod "pod-secrets-547288bd-7f66-473c-9756-6d624843003d" in namespace "secrets-7056" to be "Succeeded or Failed"
Jul 12 21:03:21.643: INFO: Pod "pod-secrets-547288bd-7f66-473c-9756-6d624843003d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.448319ms
Jul 12 21:03:23.653: INFO: Pod "pod-secrets-547288bd-7f66-473c-9756-6d624843003d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014327851s
Jul 12 21:03:25.661: INFO: Pod "pod-secrets-547288bd-7f66-473c-9756-6d624843003d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022041816s
STEP: Saw pod success
Jul 12 21:03:25.661: INFO: Pod "pod-secrets-547288bd-7f66-473c-9756-6d624843003d" satisfied condition "Succeeded or Failed"
Jul 12 21:03:25.665: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-secrets-547288bd-7f66-473c-9756-6d624843003d container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 21:03:25.690: INFO: Waiting for pod pod-secrets-547288bd-7f66-473c-9756-6d624843003d to disappear
Jul 12 21:03:25.694: INFO: Pod pod-secrets-547288bd-7f66-473c-9756-6d624843003d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:03:25.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7056" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":148,"skipped":2428,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:03:25.702: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 21:03:26.266: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 21:03:29.414: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
Jul 12 21:03:29.440: INFO: Waiting for webhook configuration to be ready...
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:03:29.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1149" for this suite.
STEP: Destroying namespace "webhook-1149-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":339,"completed":149,"skipped":2437,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:03:29.816: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 12 21:03:30.076: INFO: Waiting up to 5m0s for pod "pod-ba69d6cc-41f8-4917-b78e-0c0e603b7790" in namespace "emptydir-4125" to be "Succeeded or Failed"
Jul 12 21:03:30.093: INFO: Pod "pod-ba69d6cc-41f8-4917-b78e-0c0e603b7790": Phase="Pending", Reason="", readiness=false. Elapsed: 16.267912ms
Jul 12 21:03:32.100: INFO: Pod "pod-ba69d6cc-41f8-4917-b78e-0c0e603b7790": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023391676s
Jul 12 21:03:34.130: INFO: Pod "pod-ba69d6cc-41f8-4917-b78e-0c0e603b7790": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053847789s
STEP: Saw pod success
Jul 12 21:03:34.130: INFO: Pod "pod-ba69d6cc-41f8-4917-b78e-0c0e603b7790" satisfied condition "Succeeded or Failed"
Jul 12 21:03:34.138: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-ba69d6cc-41f8-4917-b78e-0c0e603b7790 container test-container: <nil>
STEP: delete the pod
Jul 12 21:03:34.175: INFO: Waiting for pod pod-ba69d6cc-41f8-4917-b78e-0c0e603b7790 to disappear
Jul 12 21:03:34.186: INFO: Pod pod-ba69d6cc-41f8-4917-b78e-0c0e603b7790 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:03:34.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4125" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":150,"skipped":2440,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:03:34.203: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0712 21:04:14.366034      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0712 21:04:14.366065      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0712 21:04:14.366071      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 12 21:04:14.366: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jul 12 21:04:14.366: INFO: Deleting pod "simpletest.rc-48w9j" in namespace "gc-3508"
Jul 12 21:04:14.380: INFO: Deleting pod "simpletest.rc-8nlmm" in namespace "gc-3508"
Jul 12 21:04:14.408: INFO: Deleting pod "simpletest.rc-9jwvm" in namespace "gc-3508"
Jul 12 21:04:14.434: INFO: Deleting pod "simpletest.rc-bsrkq" in namespace "gc-3508"
Jul 12 21:04:14.468: INFO: Deleting pod "simpletest.rc-btnxm" in namespace "gc-3508"
Jul 12 21:04:14.493: INFO: Deleting pod "simpletest.rc-pgn5n" in namespace "gc-3508"
Jul 12 21:04:14.519: INFO: Deleting pod "simpletest.rc-rmq82" in namespace "gc-3508"
Jul 12 21:04:14.568: INFO: Deleting pod "simpletest.rc-sc9zg" in namespace "gc-3508"
Jul 12 21:04:14.592: INFO: Deleting pod "simpletest.rc-wwtt2" in namespace "gc-3508"
Jul 12 21:04:14.608: INFO: Deleting pod "simpletest.rc-zhrvw" in namespace "gc-3508"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:04:14.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3508" for this suite.

• [SLOW TEST:40.442 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":339,"completed":151,"skipped":2445,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:04:14.646: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:04:14.715: INFO: Creating deployment "webserver-deployment"
Jul 12 21:04:14.720: INFO: Waiting for observed generation 1
Jul 12 21:04:16.734: INFO: Waiting for all required pods to come up
Jul 12 21:04:16.738: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 12 21:04:18.805: INFO: Waiting for deployment "webserver-deployment" to complete
Jul 12 21:04:18.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:10, UpdatedReplicas:10, ReadyReplicas:9, AvailableReplicas:9, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761720658, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761720658, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761720658, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761720654, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"webserver-deployment-847dcfb7fb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 12 21:04:20.830: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul 12 21:04:20.840: INFO: Updating deployment webserver-deployment
Jul 12 21:04:20.840: INFO: Waiting for observed generation 2
Jul 12 21:04:22.868: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 12 21:04:22.875: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 12 21:04:22.882: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 12 21:04:22.899: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 12 21:04:22.899: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 12 21:04:22.908: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 12 21:04:22.923: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul 12 21:04:22.923: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul 12 21:04:22.942: INFO: Updating deployment webserver-deployment
Jul 12 21:04:22.942: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul 12 21:04:22.990: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 12 21:04:23.015: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 12 21:04:23.133: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3434  90597b97-b6e6-4967-babf-787fab4fc13f 21770 3 2021-07-12 21:04:14 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-07-12 21:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 21:04:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00491ac98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-07-12 21:04:21 +0000 UTC,LastTransitionTime:2021-07-12 21:04:14 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-12 21:04:23 +0000 UTC,LastTransitionTime:2021-07-12 21:04:23 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jul 12 21:04:23.176: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-3434  4756368f-864d-4ccc-afe1-4237f772a5f9 21766 3 2021-07-12 21:04:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 90597b97-b6e6-4967-babf-787fab4fc13f 0xc00491b0c7 0xc00491b0c8}] []  [{kube-controller-manager Update apps/v1 2021-07-12 21:04:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90597b97-b6e6-4967-babf-787fab4fc13f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00491b148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 21:04:23.176: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul 12 21:04:23.176: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-847dcfb7fb  deployment-3434  2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 21764 3 2021-07-12 21:04:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 90597b97-b6e6-4967-babf-787fab4fc13f 0xc00491b1a7 0xc00491b1a8}] []  [{kube-controller-manager Update apps/v1 2021-07-12 21:04:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90597b97-b6e6-4967-babf-787fab4fc13f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00491b218 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jul 12 21:04:23.294: INFO: Pod "webserver-deployment-795d758f88-gvmfr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gvmfr webserver-deployment-795d758f88- deployment-3434  8a333289-5307-4448-ba70-f3c366b1a49f 21777 0 2021-07-12 21:04:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4756368f-864d-4ccc-afe1-4237f772a5f9 0xc003e80817 0xc003e80818}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4756368f-864d-4ccc-afe1-4237f772a5f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m6h2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m6h2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-1pbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.295: INFO: Pod "webserver-deployment-795d758f88-jrx2h" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jrx2h webserver-deployment-795d758f88- deployment-3434  fc61329b-834f-48aa-9aa8-c87f0be773a2 21736 0 2021-07-12 21:04:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4756368f-864d-4ccc-afe1-4237f772a5f9 0xc003e80980 0xc003e80981}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4756368f-864d-4ccc-afe1-4237f772a5f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72v2b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72v2b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-g6xf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.28,PodIP:,StartTime:2021-07-12 21:04:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.295: INFO: Pod "webserver-deployment-795d758f88-lj6w6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lj6w6 webserver-deployment-795d758f88- deployment-3434  9d282caf-2523-4baf-93bd-37773a7c634c 21781 0 2021-07-12 21:04:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4756368f-864d-4ccc-afe1-4237f772a5f9 0xc003e80ce0 0xc003e80ce1}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4756368f-864d-4ccc-afe1-4237f772a5f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvgf9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvgf9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.296: INFO: Pod "webserver-deployment-795d758f88-nncxv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nncxv webserver-deployment-795d758f88- deployment-3434  a23c7d31-d038-43c0-b463-9d81009b92de 21731 0 2021-07-12 21:04:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4756368f-864d-4ccc-afe1-4237f772a5f9 0xc003e80f77 0xc003e80f78}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4756368f-864d-4ccc-afe1-4237f772a5f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-26ltt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-26ltt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.27,PodIP:,StartTime:2021-07-12 21:04:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.296: INFO: Pod "webserver-deployment-795d758f88-qnlb5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-qnlb5 webserver-deployment-795d758f88- deployment-3434  0563b467-cb8d-4c1c-968f-40ba5fc2d3df 21753 0 2021-07-12 21:04:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4756368f-864d-4ccc-afe1-4237f772a5f9 0xc003e81320 0xc003e81321}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4756368f-864d-4ccc-afe1-4237f772a5f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qdvrl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qdvrl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-g6xf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.28,PodIP:,StartTime:2021-07-12 21:04:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.296: INFO: Pod "webserver-deployment-795d758f88-sb2b7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-sb2b7 webserver-deployment-795d758f88- deployment-3434  c29459de-e520-47b0-9a09-65c9a5b12ae6 21780 0 2021-07-12 21:04:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4756368f-864d-4ccc-afe1-4237f772a5f9 0xc003e81500 0xc003e81501}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4756368f-864d-4ccc-afe1-4237f772a5f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wmj4p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wmj4p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.297: INFO: Pod "webserver-deployment-795d758f88-xcd2h" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xcd2h webserver-deployment-795d758f88- deployment-3434  f93bc9e1-3466-45a4-a900-0ea158fd2a43 21735 0 2021-07-12 21:04:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4756368f-864d-4ccc-afe1-4237f772a5f9 0xc003e81737 0xc003e81738}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4756368f-864d-4ccc-afe1-4237f772a5f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rvdwc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rvdwc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-1pbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.26,PodIP:,StartTime:2021-07-12 21:04:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.298: INFO: Pod "webserver-deployment-795d758f88-xls7j" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xls7j webserver-deployment-795d758f88- deployment-3434  13b746b6-0939-451a-b666-7e24fccdc618 21749 0 2021-07-12 21:04:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4756368f-864d-4ccc-afe1-4237f772a5f9 0xc003e81910 0xc003e81911}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4756368f-864d-4ccc-afe1-4237f772a5f9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7k6lm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7k6lm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.27,PodIP:,StartTime:2021-07-12 21:04:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.298: INFO: Pod "webserver-deployment-847dcfb7fb-2w57f" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-2w57f webserver-deployment-847dcfb7fb- deployment-3434  6a948c0c-d25c-4ecc-a1dc-d31a55cced8d 21774 0 2021-07-12 21:04:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003e81ae0 0xc003e81ae1}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dnr8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dnr8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.299: INFO: Pod "webserver-deployment-847dcfb7fb-4cbrc" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-4cbrc webserver-deployment-847dcfb7fb- deployment-3434  cac57fe6-92d4-4ae8-b742-9fd6fa29d602 21664 0 2021-07-12 21:04:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003e81c37 0xc003e81c38}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.0.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l48kv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l48kv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-1pbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.26,PodIP:10.40.0.28,StartTime:2021-07-12 21:04:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 21:04:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://8d2453800adf19daa37aa5ea64dc599b17a864ce30ffa14b307a9f89f137df84,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.0.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.299: INFO: Pod "webserver-deployment-847dcfb7fb-642lg" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-642lg webserver-deployment-847dcfb7fb- deployment-3434  7f628c15-ac83-4a93-8735-790af295c9bf 21686 0 2021-07-12 21:04:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003e81e00 0xc003e81e01}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.0.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dkf2t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dkf2t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-1pbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.26,PodIP:10.40.0.30,StartTime:2021-07-12 21:04:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 21:04:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://c6b062662564009e4de1dff814592b6de348dd079f5b4da09cf183d75ec05eac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.0.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.299: INFO: Pod "webserver-deployment-847dcfb7fb-bxkk4" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-bxkk4 webserver-deployment-847dcfb7fb- deployment-3434  76da068a-61a5-4ce3-b560-73d6b4eec5f3 21678 0 2021-07-12 21:04:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003e81fd0 0xc003e81fd1}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.1.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4t4jv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4t4jv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.27,PodIP:10.40.1.173,StartTime:2021-07-12 21:04:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 21:04:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://9cf914d9d857ef0bfb1cfffea2f885f5dcf3d0a7666057ae3c3e0e41a554b0f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.1.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.300: INFO: Pod "webserver-deployment-847dcfb7fb-f4xpw" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-f4xpw webserver-deployment-847dcfb7fb- deployment-3434  c1d7a9d2-98b0-4925-9ed5-26d638ec1fa0 21772 0 2021-07-12 21:04:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003c74230 0xc003c74231}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbjdv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbjdv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.27,PodIP:,StartTime:2021-07-12 21:04:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.300: INFO: Pod "webserver-deployment-847dcfb7fb-fks7z" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-fks7z webserver-deployment-847dcfb7fb- deployment-3434  55856266-4226-4e18-94f2-8f8106e97de7 21701 0 2021-07-12 21:04:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003c74c77 0xc003c74c78}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.2.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-92p4m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-92p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-g6xf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.28,PodIP:10.40.2.33,StartTime:2021-07-12 21:04:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 21:04:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://b737fc7588ac17f4723cfe45f799830f8194a4dbfb5287305792edd1b31d4f1b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.2.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.301: INFO: Pod "webserver-deployment-847dcfb7fb-fzvkl" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-fzvkl webserver-deployment-847dcfb7fb- deployment-3434  1d9a2826-8cb0-4f04-ab6f-6666c0c21ad8 21683 0 2021-07-12 21:04:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003c74f90 0xc003c74f91}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.2.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pnsxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pnsxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-g6xf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.28,PodIP:10.40.2.31,StartTime:2021-07-12 21:04:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 21:04:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://bc98acfdea5b2d74e50e2938c52ee15b3714e5062c17228945357e329a59c5e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.2.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.301: INFO: Pod "webserver-deployment-847dcfb7fb-gtnx5" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-gtnx5 webserver-deployment-847dcfb7fb- deployment-3434  b947e64c-7457-46cd-8d57-1341b5de54bb 21778 0 2021-07-12 21:04:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003c75200 0xc003c75201}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmvjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmvjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-1pbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.301: INFO: Pod "webserver-deployment-847dcfb7fb-ldtrm" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-ldtrm webserver-deployment-847dcfb7fb- deployment-3434  feb7ba97-9b5a-4e6c-8fc6-f342afbd1613 21699 0 2021-07-12 21:04:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003c75510 0xc003c75511}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.2.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hswmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hswmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-g6xf,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.28,PodIP:10.40.2.32,StartTime:2021-07-12 21:04:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 21:04:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://85dc6825c65c6569a042879beaf1e17024c7ac73bf74230904d642df56b61062,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.2.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.302: INFO: Pod "webserver-deployment-847dcfb7fb-q8xtg" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-q8xtg webserver-deployment-847dcfb7fb- deployment-3434  59426e0d-95ee-4f61-bf3a-091a802ce708 21773 0 2021-07-12 21:04:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003c75870 0xc003c75871}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sl4s7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sl4s7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.304: INFO: Pod "webserver-deployment-847dcfb7fb-s744p" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-s744p webserver-deployment-847dcfb7fb- deployment-3434  2228de73-695a-4f6d-8c4c-2ea8f42b750e 21776 0 2021-07-12 21:04:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003c75b57 0xc003c75b58}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-95gfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-95gfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.307: INFO: Pod "webserver-deployment-847dcfb7fb-wkfsv" is not available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wkfsv webserver-deployment-847dcfb7fb- deployment-3434  2368ef97-c07f-42d9-bb5e-2db3f5b52844 21775 0 2021-07-12 21:04:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003c75df0 0xc003c75df1}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2jk5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2jk5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.307: INFO: Pod "webserver-deployment-847dcfb7fb-wx7kw" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-wx7kw webserver-deployment-847dcfb7fb- deployment-3434  9ae46de0-a6de-4b1a-9b87-32ae0e1ae2dc 21672 0 2021-07-12 21:04:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003d2e037 0xc003d2e038}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.1.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fzrqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fzrqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.27,PodIP:10.40.1.171,StartTime:2021-07-12 21:04:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 21:04:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://b6b2d978a2afc648c7509c04941104a2905ac7a9b52d3cf3f2f3e2939afd03a7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.1.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:04:23.308: INFO: Pod "webserver-deployment-847dcfb7fb-xhfqt" is available:
&Pod{ObjectMeta:{webserver-deployment-847dcfb7fb-xhfqt webserver-deployment-847dcfb7fb- deployment-3434  13d1f90e-0543-4dbe-bc2a-69071b687831 21666 0 2021-07-12 21:04:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet webserver-deployment-847dcfb7fb 2c69eb62-0f8e-4ac1-8169-220a2bdd0e48 0xc003d2e4b0 0xc003d2e4b1}] []  [{kube-controller-manager Update v1 2021-07-12 21:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c69eb62-0f8e-4ac1-8169-220a2bdd0e48\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:04:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.0.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tnsvx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tnsvx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-1pbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:04:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.26,PodIP:10.40.0.29,StartTime:2021-07-12 21:04:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 21:04:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://8065282e876e78646e845af8cb8a88f7ad88e41ed5d2d0fbefcbceba829f49ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.0.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:04:23.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3434" for this suite.

• [SLOW TEST:8.813 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":339,"completed":152,"skipped":2454,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:04:23.459: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul 12 21:04:24.563: INFO: Waiting up to 5m0s for pod "downward-api-efa15107-8bd9-437d-a70f-3caa28fb1406" in namespace "downward-api-3787" to be "Succeeded or Failed"
Jul 12 21:04:24.631: INFO: Pod "downward-api-efa15107-8bd9-437d-a70f-3caa28fb1406": Phase="Pending", Reason="", readiness=false. Elapsed: 67.705039ms
Jul 12 21:04:26.643: INFO: Pod "downward-api-efa15107-8bd9-437d-a70f-3caa28fb1406": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.080358399s
STEP: Saw pod success
Jul 12 21:04:26.643: INFO: Pod "downward-api-efa15107-8bd9-437d-a70f-3caa28fb1406" satisfied condition "Succeeded or Failed"
Jul 12 21:04:26.648: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downward-api-efa15107-8bd9-437d-a70f-3caa28fb1406 container dapi-container: <nil>
STEP: delete the pod
Jul 12 21:04:26.688: INFO: Waiting for pod downward-api-efa15107-8bd9-437d-a70f-3caa28fb1406 to disappear
Jul 12 21:04:26.698: INFO: Pod downward-api-efa15107-8bd9-437d-a70f-3caa28fb1406 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:04:26.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3787" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":339,"completed":153,"skipped":2457,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:04:26.708: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:04:26.773: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Creating first CR 
Jul 12 21:04:29.719: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-12T21:04:29Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-12T21:04:29Z]] name:name1 resourceVersion:21911 uid:feebe4fa-1e4c-4a6f-a9ef-c3023f556991] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jul 12 21:04:39.769: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-12T21:04:39Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-12T21:04:39Z]] name:name2 resourceVersion:21983 uid:ebf90519-7dd1-498b-a6e8-378e5a0f6b01] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jul 12 21:04:49.780: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-12T21:04:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-12T21:04:49Z]] name:name1 resourceVersion:22032 uid:feebe4fa-1e4c-4a6f-a9ef-c3023f556991] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jul 12 21:04:59.822: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-12T21:04:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-12T21:04:59Z]] name:name2 resourceVersion:22082 uid:ebf90519-7dd1-498b-a6e8-378e5a0f6b01] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jul 12 21:05:09.846: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-12T21:04:29Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-12T21:04:49Z]] name:name1 resourceVersion:22134 uid:feebe4fa-1e4c-4a6f-a9ef-c3023f556991] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jul 12 21:05:19.867: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-07-12T21:04:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-07-12T21:04:59Z]] name:name2 resourceVersion:22184 uid:ebf90519-7dd1-498b-a6e8-378e5a0f6b01] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:05:30.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-4176" for this suite.

• [SLOW TEST:63.695 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":339,"completed":154,"skipped":2463,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:05:30.403: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jul 12 21:05:30.464: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jul 12 21:05:51.235: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 21:05:56.141: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:06:16.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8632" for this suite.

• [SLOW TEST:46.479 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":339,"completed":155,"skipped":2467,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:06:16.885: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service multi-endpoint-test in namespace services-9604
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9604 to expose endpoints map[]
Jul 12 21:06:16.984: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jul 12 21:06:17.994: INFO: successfully validated that service multi-endpoint-test in namespace services-9604 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9604
Jul 12 21:06:18.019: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:06:20.025: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9604 to expose endpoints map[pod1:[100]]
Jul 12 21:06:20.039: INFO: successfully validated that service multi-endpoint-test in namespace services-9604 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9604
Jul 12 21:06:20.063: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:06:22.100: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9604 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 12 21:06:22.155: INFO: successfully validated that service multi-endpoint-test in namespace services-9604 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-9604
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9604 to expose endpoints map[pod2:[101]]
Jul 12 21:06:22.322: INFO: successfully validated that service multi-endpoint-test in namespace services-9604 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9604
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9604 to expose endpoints map[]
Jul 12 21:06:22.443: INFO: successfully validated that service multi-endpoint-test in namespace services-9604 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:06:22.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9604" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:5.718 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":339,"completed":156,"skipped":2476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:06:22.605: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul 12 21:06:22.732: INFO: Waiting up to 5m0s for pod "downward-api-cdd782bf-a3c6-4a1d-9079-c90c6b471e01" in namespace "downward-api-7446" to be "Succeeded or Failed"
Jul 12 21:06:22.754: INFO: Pod "downward-api-cdd782bf-a3c6-4a1d-9079-c90c6b471e01": Phase="Pending", Reason="", readiness=false. Elapsed: 22.071878ms
Jul 12 21:06:24.768: INFO: Pod "downward-api-cdd782bf-a3c6-4a1d-9079-c90c6b471e01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035543991s
Jul 12 21:06:26.776: INFO: Pod "downward-api-cdd782bf-a3c6-4a1d-9079-c90c6b471e01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043617176s
STEP: Saw pod success
Jul 12 21:06:26.776: INFO: Pod "downward-api-cdd782bf-a3c6-4a1d-9079-c90c6b471e01" satisfied condition "Succeeded or Failed"
Jul 12 21:06:26.780: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downward-api-cdd782bf-a3c6-4a1d-9079-c90c6b471e01 container dapi-container: <nil>
STEP: delete the pod
Jul 12 21:06:26.829: INFO: Waiting for pod downward-api-cdd782bf-a3c6-4a1d-9079-c90c6b471e01 to disappear
Jul 12 21:06:26.839: INFO: Pod downward-api-cdd782bf-a3c6-4a1d-9079-c90c6b471e01 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:06:26.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7446" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":339,"completed":157,"skipped":2519,"failed":0}
SSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:06:26.856: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:06:31.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4642" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":339,"completed":158,"skipped":2527,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:06:31.025: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0712 21:06:31.102245      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 12 21:06:31.135: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 12 21:06:31.142: INFO: starting watch
STEP: patching
STEP: updating
Jul 12 21:06:31.375: INFO: waiting for watch events with expected annotations
Jul 12 21:06:31.375: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:06:31.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5680" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":339,"completed":159,"skipped":2547,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:06:31.447: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 12 21:06:31.522: INFO: Waiting up to 5m0s for pod "pod-5dce5fb7-9c77-4d48-96d3-47ee60af29d4" in namespace "emptydir-6529" to be "Succeeded or Failed"
Jul 12 21:06:31.536: INFO: Pod "pod-5dce5fb7-9c77-4d48-96d3-47ee60af29d4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.463923ms
Jul 12 21:06:33.547: INFO: Pod "pod-5dce5fb7-9c77-4d48-96d3-47ee60af29d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02460459s
STEP: Saw pod success
Jul 12 21:06:33.547: INFO: Pod "pod-5dce5fb7-9c77-4d48-96d3-47ee60af29d4" satisfied condition "Succeeded or Failed"
Jul 12 21:06:33.550: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-5dce5fb7-9c77-4d48-96d3-47ee60af29d4 container test-container: <nil>
STEP: delete the pod
Jul 12 21:06:33.578: INFO: Waiting for pod pod-5dce5fb7-9c77-4d48-96d3-47ee60af29d4 to disappear
Jul 12 21:06:33.582: INFO: Pod pod-5dce5fb7-9c77-4d48-96d3-47ee60af29d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:06:33.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6529" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":160,"skipped":2558,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:06:33.597: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-4960
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a new StatefulSet
Jul 12 21:06:33.690: INFO: Found 0 stateful pods, waiting for 3
Jul 12 21:06:43.732: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 21:06:43.732: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 21:06:43.732: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-1
Jul 12 21:06:43.804: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 12 21:06:53.872: INFO: Updating stateful set ss2
Jul 12 21:06:53.930: INFO: Waiting for Pod statefulset-4960/ss2-2 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
STEP: Restoring Pods to the correct revision when they are deleted
Jul 12 21:07:04.040: INFO: Found 1 stateful pods, waiting for 3
Jul 12 21:07:14.055: INFO: Found 2 stateful pods, waiting for 3
Jul 12 21:07:24.070: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 21:07:24.070: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 21:07:24.070: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 12 21:07:24.102: INFO: Updating stateful set ss2
Jul 12 21:07:24.121: INFO: Waiting for Pod statefulset-4960/ss2-1 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
Jul 12 21:07:34.190: INFO: Updating stateful set ss2
Jul 12 21:07:34.280: INFO: Waiting for StatefulSet statefulset-4960/ss2 to complete update
Jul 12 21:07:34.280: INFO: Waiting for Pod statefulset-4960/ss2-0 to have revision ss2-5bbbc9fc94 update revision ss2-677d6db895
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul 12 21:07:44.360: INFO: Deleting all statefulset in ns statefulset-4960
Jul 12 21:07:44.386: INFO: Scaling statefulset ss2 to 0
Jul 12 21:08:04.607: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 21:08:04.610: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:08:04.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4960" for this suite.

• [SLOW TEST:91.060 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":339,"completed":161,"skipped":2592,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:08:04.660: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 12 21:08:05.476: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 21:08:08.523: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:08:08.530: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:08:12.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9038" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.685 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":339,"completed":162,"skipped":2595,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:08:12.345: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:08:12.528: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 12 21:08:17.549: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 12 21:08:17.549: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 12 21:08:17.622: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2102  4882bde2-1111-407d-ab7c-5f9edecb6c27 23448 1 2021-07-12 21:08:17 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-07-12 21:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053968b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jul 12 21:08:17.645: INFO: New ReplicaSet "test-cleanup-deployment-5b4d99b59b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5b4d99b59b  deployment-2102  cac86ae3-975a-47f7-973a-89440721c238 23451 1 2021-07-12 21:08:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 4882bde2-1111-407d-ab7c-5f9edecb6c27 0xc0053f05f7 0xc0053f05f8}] []  [{kube-controller-manager Update apps/v1 2021-07-12 21:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4882bde2-1111-407d-ab7c-5f9edecb6c27\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5b4d99b59b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.32 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053f0688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 12 21:08:17.646: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jul 12 21:08:17.646: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2102  5844a9a9-0c05-465c-b8ac-ebbc0f5be639 23449 1 2021-07-12 21:08:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4882bde2-1111-407d-ab7c-5f9edecb6c27 0xc0053f04e7 0xc0053f04e8}] []  [{e2e.test Update apps/v1 2021-07-12 21:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 21:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4882bde2-1111-407d-ab7c-5f9edecb6c27\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0053f0588 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 12 21:08:17.674: INFO: Pod "test-cleanup-controller-scz8z" is available:
&Pod{ObjectMeta:{test-cleanup-controller-scz8z test-cleanup-controller- deployment-2102  942774bf-ced6-4afc-ba0d-2a64d4ff4ef9 23427 0 2021-07-12 21:08:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 5844a9a9-0c05-465c-b8ac-ebbc0f5be639 0xc005396d77 0xc005396d78}] []  [{kube-controller-manager Update v1 2021-07-12 21:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5844a9a9-0c05-465c-b8ac-ebbc0f5be639\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:08:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.1.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rc9wl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rc9wl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:08:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:08:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.27,PodIP:10.40.1.194,StartTime:2021-07-12 21:08:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 21:08:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://37030f2d21481ebefa5005169dc81aef8e67fcb7aed39523248f249167b0a1ef,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.1.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:08:17.675: INFO: Pod "test-cleanup-deployment-5b4d99b59b-nrrtm" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5b4d99b59b-nrrtm test-cleanup-deployment-5b4d99b59b- deployment-2102  4593645e-7338-4944-af72-6af952a32c06 23453 0 2021-07-12 21:08:17 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5b4d99b59b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5b4d99b59b cac86ae3-975a-47f7-973a-89440721c238 0xc005396f67 0xc005396f68}] []  [{kube-controller-manager Update v1 2021-07-12 21:08:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cac86ae3-975a-47f7-973a-89440721c238\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bd7sk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bd7sk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:08:17.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2102" for this suite.

• [SLOW TEST:5.397 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":339,"completed":163,"skipped":2598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:08:17.743: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 12 21:08:17.899: INFO: Waiting up to 5m0s for pod "pod-93d67de6-5373-45a0-a7ab-4970261fcaaf" in namespace "emptydir-9754" to be "Succeeded or Failed"
Jul 12 21:08:17.910: INFO: Pod "pod-93d67de6-5373-45a0-a7ab-4970261fcaaf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.507822ms
Jul 12 21:08:19.920: INFO: Pod "pod-93d67de6-5373-45a0-a7ab-4970261fcaaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02123999s
STEP: Saw pod success
Jul 12 21:08:19.920: INFO: Pod "pod-93d67de6-5373-45a0-a7ab-4970261fcaaf" satisfied condition "Succeeded or Failed"
Jul 12 21:08:19.923: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-93d67de6-5373-45a0-a7ab-4970261fcaaf container test-container: <nil>
STEP: delete the pod
Jul 12 21:08:19.949: INFO: Waiting for pod pod-93d67de6-5373-45a0-a7ab-4970261fcaaf to disappear
Jul 12 21:08:19.953: INFO: Pod pod-93d67de6-5373-45a0-a7ab-4970261fcaaf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:08:19.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9754" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":164,"skipped":2654,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:08:19.970: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 21:08:20.942: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 21:08:24.109: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:08:24.117: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:08:28.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8582" for this suite.
STEP: Destroying namespace "webhook-8582-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:8.612 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":339,"completed":165,"skipped":2706,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:08:28.583: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with configMap that has name projected-configmap-test-upd-5cd95610-c4ff-4a58-ab8e-d8ac295e2d88
STEP: Creating the pod
Jul 12 21:08:29.186: INFO: The status of Pod pod-projected-configmaps-ad3a69e8-b16c-4fc4-a8bb-3c34cac6f9f7 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:08:31.191: INFO: The status of Pod pod-projected-configmaps-ad3a69e8-b16c-4fc4-a8bb-3c34cac6f9f7 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-5cd95610-c4ff-4a58-ab8e-d8ac295e2d88
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:08:33.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4482" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":166,"skipped":2709,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:08:33.351: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0712 21:08:33.664942      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:13:33.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9806" for this suite.

• [SLOW TEST:300.396 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":339,"completed":167,"skipped":2723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:13:33.752: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 12 21:13:33.985: INFO: Waiting up to 5m0s for pod "pod-c75aeb98-2221-44fb-99db-a3d4a0c6b178" in namespace "emptydir-7506" to be "Succeeded or Failed"
Jul 12 21:13:33.990: INFO: Pod "pod-c75aeb98-2221-44fb-99db-a3d4a0c6b178": Phase="Pending", Reason="", readiness=false. Elapsed: 4.31868ms
Jul 12 21:13:36.007: INFO: Pod "pod-c75aeb98-2221-44fb-99db-a3d4a0c6b178": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021442s
Jul 12 21:13:38.020: INFO: Pod "pod-c75aeb98-2221-44fb-99db-a3d4a0c6b178": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034863752s
STEP: Saw pod success
Jul 12 21:13:38.020: INFO: Pod "pod-c75aeb98-2221-44fb-99db-a3d4a0c6b178" satisfied condition "Succeeded or Failed"
Jul 12 21:13:38.025: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-c75aeb98-2221-44fb-99db-a3d4a0c6b178 container test-container: <nil>
STEP: delete the pod
Jul 12 21:13:38.096: INFO: Waiting for pod pod-c75aeb98-2221-44fb-99db-a3d4a0c6b178 to disappear
Jul 12 21:13:38.104: INFO: Pod pod-c75aeb98-2221-44fb-99db-a3d4a0c6b178 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:13:38.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7506" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":168,"skipped":2758,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:13:38.131: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override all
Jul 12 21:13:38.264: INFO: Waiting up to 5m0s for pod "client-containers-8c1abc6a-f5e4-4c06-a4b4-b8d7cbb242fc" in namespace "containers-684" to be "Succeeded or Failed"
Jul 12 21:13:38.281: INFO: Pod "client-containers-8c1abc6a-f5e4-4c06-a4b4-b8d7cbb242fc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.528991ms
Jul 12 21:13:40.289: INFO: Pod "client-containers-8c1abc6a-f5e4-4c06-a4b4-b8d7cbb242fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024351098s
STEP: Saw pod success
Jul 12 21:13:40.289: INFO: Pod "client-containers-8c1abc6a-f5e4-4c06-a4b4-b8d7cbb242fc" satisfied condition "Succeeded or Failed"
Jul 12 21:13:40.300: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod client-containers-8c1abc6a-f5e4-4c06-a4b4-b8d7cbb242fc container agnhost-container: <nil>
STEP: delete the pod
Jul 12 21:13:40.348: INFO: Waiting for pod client-containers-8c1abc6a-f5e4-4c06-a4b4-b8d7cbb242fc to disappear
Jul 12 21:13:40.363: INFO: Pod client-containers-8c1abc6a-f5e4-4c06-a4b4-b8d7cbb242fc no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:13:40.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-684" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":339,"completed":169,"skipped":2786,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:13:40.401: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jul 12 21:13:40.502: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:13:47.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6702" for this suite.

• [SLOW TEST:6.784 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":339,"completed":170,"skipped":2812,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:13:47.188: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1786
Jul 12 21:13:47.298: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:13:49.311: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jul 12 21:13:49.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1786 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 12 21:13:50.456: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul 12 21:13:50.456: INFO: stdout: "iptables"
Jul 12 21:13:50.456: INFO: proxyMode: iptables
Jul 12 21:13:50.475: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 12 21:13:50.478: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-1786
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1786
I0712 21:13:50.528386      20 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1786, replica count: 3
I0712 21:13:53.579668      20 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 21:13:53.658: INFO: Creating new exec pod
Jul 12 21:13:56.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1786 exec execpod-affinityvpkk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jul 12 21:13:57.970: INFO: rc: 1
Jul 12 21:13:57.970: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1786 exec execpod-affinityvpkk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80:
Command stdout:

stderr:
+ echo hostName
+ nc -v -t -w 2 affinity-nodeport-timeout 80
nc: connect to affinity-nodeport-timeout port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 12 21:13:58.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1786 exec execpod-affinityvpkk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jul 12 21:14:00.276: INFO: rc: 1
Jul 12 21:14:00.276: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1786 exec execpod-affinityvpkk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80:
Command stdout:

stderr:
+ nc -v -t -w 2 affinity-nodeport-timeout 80
+ echo hostName
nc: connect to affinity-nodeport-timeout port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 12 21:14:00.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1786 exec execpod-affinityvpkk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jul 12 21:14:01.162: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jul 12 21:14:01.162: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 21:14:01.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1786 exec execpod-affinityvpkk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.131.240 80'
Jul 12 21:14:01.348: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.131.240 80\nConnection to 10.37.131.240 80 port [tcp/http] succeeded!\n"
Jul 12 21:14:01.348: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 21:14:01.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1786 exec execpod-affinityvpkk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.26 30253'
Jul 12 21:14:01.515: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.26 30253\nConnection to 10.128.0.26 30253 port [tcp/*] succeeded!\n"
Jul 12 21:14:01.515: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 21:14:01.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1786 exec execpod-affinityvpkk2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.27 30253'
Jul 12 21:14:01.752: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.27 30253\nConnection to 10.128.0.27 30253 port [tcp/*] succeeded!\n"
Jul 12 21:14:01.752: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 21:14:01.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1786 exec execpod-affinityvpkk2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.26:30253/ ; done'
Jul 12 21:14:02.177: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n"
Jul 12 21:14:02.177: INFO: stdout: "\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h\naffinity-nodeport-timeout-5699h"
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Received response from host: affinity-nodeport-timeout-5699h
Jul 12 21:14:02.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1786 exec execpod-affinityvpkk2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.128.0.26:30253/'
Jul 12 21:14:02.364: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n"
Jul 12 21:14:02.364: INFO: stdout: "affinity-nodeport-timeout-5699h"
Jul 12 21:14:22.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1786 exec execpod-affinityvpkk2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.128.0.26:30253/'
Jul 12 21:14:22.604: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.128.0.26:30253/\n"
Jul 12 21:14:22.604: INFO: stdout: "affinity-nodeport-timeout-fh9rh"
Jul 12 21:14:22.604: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1786, will wait for the garbage collector to delete the pods
Jul 12 21:14:22.702: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 9.026122ms
Jul 12 21:14:23.003: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 300.546398ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:14:38.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1786" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:51.225 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":171,"skipped":2819,"failed":0}
SSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:14:38.413: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
Jul 12 21:14:38.503: INFO: created test-event-1
Jul 12 21:14:38.514: INFO: created test-event-2
Jul 12 21:14:38.531: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jul 12 21:14:38.536: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jul 12 21:14:38.589: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:14:38.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1250" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":339,"completed":172,"skipped":2823,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:14:38.629: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:14:38.783: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4a9cb855-7d50-4b25-8b3e-c240d68cdc10" in namespace "downward-api-5740" to be "Succeeded or Failed"
Jul 12 21:14:38.789: INFO: Pod "downwardapi-volume-4a9cb855-7d50-4b25-8b3e-c240d68cdc10": Phase="Pending", Reason="", readiness=false. Elapsed: 6.303288ms
Jul 12 21:14:40.796: INFO: Pod "downwardapi-volume-4a9cb855-7d50-4b25-8b3e-c240d68cdc10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013238424s
STEP: Saw pod success
Jul 12 21:14:40.796: INFO: Pod "downwardapi-volume-4a9cb855-7d50-4b25-8b3e-c240d68cdc10" satisfied condition "Succeeded or Failed"
Jul 12 21:14:40.802: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-4a9cb855-7d50-4b25-8b3e-c240d68cdc10 container client-container: <nil>
STEP: delete the pod
Jul 12 21:14:40.829: INFO: Waiting for pod downwardapi-volume-4a9cb855-7d50-4b25-8b3e-c240d68cdc10 to disappear
Jul 12 21:14:40.834: INFO: Pod downwardapi-volume-4a9cb855-7d50-4b25-8b3e-c240d68cdc10 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:14:40.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5740" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":173,"skipped":2865,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:14:40.849: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:14:40.924: INFO: Create a RollingUpdate DaemonSet
Jul 12 21:14:40.929: INFO: Check that daemon pods launch on every node of the cluster
Jul 12 21:14:40.940: INFO: Number of nodes with available pods: 0
Jul 12 21:14:40.940: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 21:14:41.956: INFO: Number of nodes with available pods: 0
Jul 12 21:14:41.956: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 21:14:42.968: INFO: Number of nodes with available pods: 2
Jul 12 21:14:42.968: INFO: Node gke-gke-1-21-default-pool-f67064dc-3tj7 is running more than one daemon pod
Jul 12 21:14:43.973: INFO: Number of nodes with available pods: 3
Jul 12 21:14:43.973: INFO: Number of running nodes: 3, number of available pods: 3
Jul 12 21:14:43.973: INFO: Update the DaemonSet to trigger a rollout
Jul 12 21:14:43.995: INFO: Updating DaemonSet daemon-set
Jul 12 21:14:52.027: INFO: Roll back the DaemonSet before rollout is complete
Jul 12 21:14:52.050: INFO: Updating DaemonSet daemon-set
Jul 12 21:14:52.050: INFO: Make sure DaemonSet rollback is complete
Jul 12 21:14:52.058: INFO: Wrong image for pod: daemon-set-2prh4. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-1, got: foo:non-existent.
Jul 12 21:14:52.058: INFO: Pod daemon-set-2prh4 is not available
Jul 12 21:14:55.082: INFO: Pod daemon-set-rf2g4 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2427, will wait for the garbage collector to delete the pods
Jul 12 21:14:55.162: INFO: Deleting DaemonSet.extensions daemon-set took: 11.404986ms
Jul 12 21:14:55.263: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.57467ms
Jul 12 21:15:08.204: INFO: Number of nodes with available pods: 0
Jul 12 21:15:08.204: INFO: Number of running nodes: 0, number of available pods: 0
Jul 12 21:15:08.211: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25891"},"items":null}

Jul 12 21:15:08.223: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25891"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:15:08.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2427" for this suite.

• [SLOW TEST:27.492 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":339,"completed":174,"skipped":2894,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:15:08.346: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 21:15:09.200: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 12 21:15:11.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761721309, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761721309, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761721309, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761721309, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 21:15:14.256: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:15:14.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6084" for this suite.
STEP: Destroying namespace "webhook-6084-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.264 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":339,"completed":175,"skipped":2944,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:15:14.610: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 21:15:15.230: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 12 21:15:17.250: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761721315, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761721315, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761721315, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761721315, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 21:15:20.294: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:15:20.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6261" for this suite.
STEP: Destroying namespace "webhook-6261-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.780 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":339,"completed":176,"skipped":2946,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:15:20.392: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:15:20.483: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00fab82a-4a4f-4451-bd16-a88f25e38846" in namespace "downward-api-5658" to be "Succeeded or Failed"
Jul 12 21:15:20.489: INFO: Pod "downwardapi-volume-00fab82a-4a4f-4451-bd16-a88f25e38846": Phase="Pending", Reason="", readiness=false. Elapsed: 6.349934ms
Jul 12 21:15:22.528: INFO: Pod "downwardapi-volume-00fab82a-4a4f-4451-bd16-a88f25e38846": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045730437s
Jul 12 21:15:24.542: INFO: Pod "downwardapi-volume-00fab82a-4a4f-4451-bd16-a88f25e38846": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059456862s
STEP: Saw pod success
Jul 12 21:15:24.542: INFO: Pod "downwardapi-volume-00fab82a-4a4f-4451-bd16-a88f25e38846" satisfied condition "Succeeded or Failed"
Jul 12 21:15:24.546: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-00fab82a-4a4f-4451-bd16-a88f25e38846 container client-container: <nil>
STEP: delete the pod
Jul 12 21:15:24.586: INFO: Waiting for pod downwardapi-volume-00fab82a-4a4f-4451-bd16-a88f25e38846 to disappear
Jul 12 21:15:24.597: INFO: Pod downwardapi-volume-00fab82a-4a4f-4451-bd16-a88f25e38846 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:15:24.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5658" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":339,"completed":177,"skipped":2960,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:15:24.613: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jul 12 21:15:24.713: INFO: The status of Pod labelsupdate95d3aa91-a48d-458b-8ad7-fa5e45432b7d is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:15:26.730: INFO: The status of Pod labelsupdate95d3aa91-a48d-458b-8ad7-fa5e45432b7d is Running (Ready = true)
Jul 12 21:15:27.265: INFO: Successfully updated pod "labelsupdate95d3aa91-a48d-458b-8ad7-fa5e45432b7d"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:15:29.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4525" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":178,"skipped":2992,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:15:29.304: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jul 12 21:15:29.428: INFO: observed Pod pod-test in namespace pods-4617 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jul 12 21:15:29.438: INFO: observed Pod pod-test in namespace pods-4617 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 21:15:29 +0000 UTC  }]
Jul 12 21:15:29.481: INFO: observed Pod pod-test in namespace pods-4617 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 21:15:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 21:15:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 21:15:29 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 21:15:29 +0000 UTC  }]
Jul 12 21:15:30.551: INFO: Found Pod pod-test in namespace pods-4617 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 21:15:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 21:15:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 21:15:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 21:15:29 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jul 12 21:15:30.569: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jul 12 21:15:30.672: INFO: observed event type ADDED
Jul 12 21:15:30.673: INFO: observed event type MODIFIED
Jul 12 21:15:30.673: INFO: observed event type MODIFIED
Jul 12 21:15:30.673: INFO: observed event type MODIFIED
Jul 12 21:15:30.674: INFO: observed event type MODIFIED
Jul 12 21:15:30.674: INFO: observed event type MODIFIED
Jul 12 21:15:30.674: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:15:30.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4617" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":339,"completed":179,"skipped":2994,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:15:30.693: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-75
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 12 21:15:30.731: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 12 21:15:30.874: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:15:32.904: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:15:34.924: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:15:36.883: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:15:38.887: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:15:40.880: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:15:42.924: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:15:44.911: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:15:46.896: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:15:48.893: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:15:50.878: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 12 21:15:50.898: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul 12 21:15:50.906: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jul 12 21:15:52.992: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jul 12 21:15:52.992: INFO: Going to poll 10.40.0.34 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jul 12 21:15:52.996: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.40.0.34 8081 | grep -v '^\s*$'] Namespace:pod-network-test-75 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:15:52.997: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 21:15:54.087: INFO: Found all 1 expected endpoints: [netserver-0]
Jul 12 21:15:54.087: INFO: Going to poll 10.40.1.213 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jul 12 21:15:54.095: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.40.1.213 8081 | grep -v '^\s*$'] Namespace:pod-network-test-75 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:15:54.095: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 21:15:55.179: INFO: Found all 1 expected endpoints: [netserver-1]
Jul 12 21:15:55.179: INFO: Going to poll 10.40.2.37 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jul 12 21:15:55.188: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.40.2.37 8081 | grep -v '^\s*$'] Namespace:pod-network-test-75 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:15:55.188: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 21:15:56.291: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:15:56.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-75" for this suite.

• [SLOW TEST:25.657 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":180,"skipped":3005,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:15:56.351: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of pod templates
Jul 12 21:15:56.742: INFO: created test-podtemplate-1
Jul 12 21:15:56.759: INFO: created test-podtemplate-2
Jul 12 21:15:56.773: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jul 12 21:15:56.780: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jul 12 21:15:56.795: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:15:56.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7222" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":339,"completed":181,"skipped":3035,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:15:56.815: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 12 21:15:56.919: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 12 21:16:57.093: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create pods that use 2/3 of node resources.
Jul 12 21:16:57.178: INFO: Created pod: pod0-sched-preemption-low-priority
Jul 12 21:16:57.231: INFO: Created pod: pod1-sched-preemption-medium-priority
Jul 12 21:16:57.289: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:17:05.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1669" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:68.718 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":339,"completed":182,"skipped":3044,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:17:05.533: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:17:05.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a656b998-8d7b-4d47-96fc-952916e48129" in namespace "downward-api-4087" to be "Succeeded or Failed"
Jul 12 21:17:05.633: INFO: Pod "downwardapi-volume-a656b998-8d7b-4d47-96fc-952916e48129": Phase="Pending", Reason="", readiness=false. Elapsed: 12.931544ms
Jul 12 21:17:07.651: INFO: Pod "downwardapi-volume-a656b998-8d7b-4d47-96fc-952916e48129": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030507451s
Jul 12 21:17:09.660: INFO: Pod "downwardapi-volume-a656b998-8d7b-4d47-96fc-952916e48129": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040106596s
STEP: Saw pod success
Jul 12 21:17:09.661: INFO: Pod "downwardapi-volume-a656b998-8d7b-4d47-96fc-952916e48129" satisfied condition "Succeeded or Failed"
Jul 12 21:17:09.664: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-a656b998-8d7b-4d47-96fc-952916e48129 container client-container: <nil>
STEP: delete the pod
Jul 12 21:17:09.698: INFO: Waiting for pod downwardapi-volume-a656b998-8d7b-4d47-96fc-952916e48129 to disappear
Jul 12 21:17:09.705: INFO: Pod downwardapi-volume-a656b998-8d7b-4d47-96fc-952916e48129 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:17:09.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4087" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":339,"completed":183,"skipped":3057,"failed":0}
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:17:09.723: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:17:09.795: INFO: The status of Pod busybox-host-aliasesb6f9f83d-4439-4c86-96b2-643344efc789 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:17:11.856: INFO: The status of Pod busybox-host-aliasesb6f9f83d-4439-4c86-96b2-643344efc789 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:17:11.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2385" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":184,"skipped":3059,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:17:11.925: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-e0ac060b-cf84-41c2-bc0f-0475a727e4b4
STEP: Creating secret with name s-test-opt-upd-29670dbf-b458-41ea-8c37-222ac9f69605
STEP: Creating the pod
Jul 12 21:17:12.182: INFO: The status of Pod pod-secrets-47cd6e7d-6ce1-4c5c-bc9c-372ebd5e3699 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:17:14.217: INFO: The status of Pod pod-secrets-47cd6e7d-6ce1-4c5c-bc9c-372ebd5e3699 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-e0ac060b-cf84-41c2-bc0f-0475a727e4b4
STEP: Updating secret s-test-opt-upd-29670dbf-b458-41ea-8c37-222ac9f69605
STEP: Creating secret with name s-test-opt-create-514bd07c-2943-4aac-a441-5bbde5a4b38b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:17:16.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4099" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":185,"skipped":3062,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:17:16.390: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:17:16.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1294 version'
Jul 12 21:17:16.637: INFO: stderr: ""
Jul 12 21:17:16.638: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.1\", GitCommit:\"5e58841cce77d4bc13713ad2b91fa0d961e69192\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T14:18:45Z\", GoVersion:\"go1.16.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.1-gke.2200\", GitCommit:\"ff8c5c7627f87aeb86921de4ae2767adac625217\", GitTreeState:\"clean\", BuildDate:\"2021-06-11T10:06:34Z\", GoVersion:\"go1.16.1b7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:17:16.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1294" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":339,"completed":186,"skipped":3063,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:17:16.668: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jul 12 21:17:18.808: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1547 PodName:var-expansion-16a7c514-fac1-4806-99f7-ae149066cb95 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:17:18.808: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: test for file in mounted path
Jul 12 21:17:18.900: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1547 PodName:var-expansion-16a7c514-fac1-4806-99f7-ae149066cb95 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:17:18.900: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: updating the annotation value
Jul 12 21:17:19.518: INFO: Successfully updated pod "var-expansion-16a7c514-fac1-4806-99f7-ae149066cb95"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jul 12 21:17:19.529: INFO: Deleting pod "var-expansion-16a7c514-fac1-4806-99f7-ae149066cb95" in namespace "var-expansion-1547"
Jul 12 21:17:19.542: INFO: Wait up to 5m0s for pod "var-expansion-16a7c514-fac1-4806-99f7-ae149066cb95" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:17:53.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1547" for this suite.

• [SLOW TEST:36.905 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":339,"completed":187,"skipped":3119,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:17:53.574: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-map-06a395a4-5bf4-440f-9c10-7af483d71dec
STEP: Creating a pod to test consume secrets
Jul 12 21:17:53.705: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bb060ded-8129-4784-87b4-3f1c959cb257" in namespace "projected-6317" to be "Succeeded or Failed"
Jul 12 21:17:53.717: INFO: Pod "pod-projected-secrets-bb060ded-8129-4784-87b4-3f1c959cb257": Phase="Pending", Reason="", readiness=false. Elapsed: 11.788368ms
Jul 12 21:17:55.728: INFO: Pod "pod-projected-secrets-bb060ded-8129-4784-87b4-3f1c959cb257": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023028462s
STEP: Saw pod success
Jul 12 21:17:55.728: INFO: Pod "pod-projected-secrets-bb060ded-8129-4784-87b4-3f1c959cb257" satisfied condition "Succeeded or Failed"
Jul 12 21:17:55.731: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-projected-secrets-bb060ded-8129-4784-87b4-3f1c959cb257 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 12 21:17:55.757: INFO: Waiting for pod pod-projected-secrets-bb060ded-8129-4784-87b4-3f1c959cb257 to disappear
Jul 12 21:17:55.766: INFO: Pod pod-projected-secrets-bb060ded-8129-4784-87b4-3f1c959cb257 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:17:55.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6317" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":188,"skipped":3124,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:17:55.780: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name projected-secret-test-6e7b6b4f-3087-4428-854b-56d8a9f33e39
STEP: Creating a pod to test consume secrets
Jul 12 21:17:55.902: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eb64c515-9bde-4723-97b1-a2b1e22008f0" in namespace "projected-68" to be "Succeeded or Failed"
Jul 12 21:17:55.908: INFO: Pod "pod-projected-secrets-eb64c515-9bde-4723-97b1-a2b1e22008f0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.64956ms
Jul 12 21:17:57.926: INFO: Pod "pod-projected-secrets-eb64c515-9bde-4723-97b1-a2b1e22008f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024028345s
STEP: Saw pod success
Jul 12 21:17:57.926: INFO: Pod "pod-projected-secrets-eb64c515-9bde-4723-97b1-a2b1e22008f0" satisfied condition "Succeeded or Failed"
Jul 12 21:17:57.931: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-projected-secrets-eb64c515-9bde-4723-97b1-a2b1e22008f0 container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 21:17:57.985: INFO: Waiting for pod pod-projected-secrets-eb64c515-9bde-4723-97b1-a2b1e22008f0 to disappear
Jul 12 21:17:57.990: INFO: Pod pod-projected-secrets-eb64c515-9bde-4723-97b1-a2b1e22008f0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:17:57.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-68" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":339,"completed":189,"skipped":3133,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:17:58.009: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 21:17:58.964: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 12 21:18:00.991: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761721479, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761721479, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761721479, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761721478, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 21:18:04.101: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:18:16.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3179" for this suite.
STEP: Destroying namespace "webhook-3179-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:18.581 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":339,"completed":190,"skipped":3145,"failed":0}
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:18:16.593: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jul 12 21:18:16.802: INFO: running pods: 0 < 3
Jul 12 21:18:18.810: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:18:20.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4812" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":339,"completed":191,"skipped":3145,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:18:20.830: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: set up a multi version CRD
Jul 12 21:18:20.877: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:18:49.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8701" for this suite.

• [SLOW TEST:29.154 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":339,"completed":192,"skipped":3150,"failed":0}
S
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:18:49.990: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod test-webserver-10ebf12e-7740-49b2-a2ce-df96fe28476d in namespace container-probe-8141
Jul 12 21:18:54.099: INFO: Started pod test-webserver-10ebf12e-7740-49b2-a2ce-df96fe28476d in namespace container-probe-8141
STEP: checking the pod's current state and verifying that restartCount is present
Jul 12 21:18:54.103: INFO: Initial restart count of pod test-webserver-10ebf12e-7740-49b2-a2ce-df96fe28476d is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:22:55.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8141" for this suite.

• [SLOW TEST:245.768 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":193,"skipped":3151,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:22:55.757: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:22:55.857: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:22:57.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4153" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":339,"completed":194,"skipped":3152,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:22:57.141: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:22:57.389: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c5e8680-0f4f-4262-a812-2f3f0c930e05" in namespace "downward-api-6917" to be "Succeeded or Failed"
Jul 12 21:22:57.416: INFO: Pod "downwardapi-volume-8c5e8680-0f4f-4262-a812-2f3f0c930e05": Phase="Pending", Reason="", readiness=false. Elapsed: 26.088712ms
Jul 12 21:22:59.425: INFO: Pod "downwardapi-volume-8c5e8680-0f4f-4262-a812-2f3f0c930e05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035392329s
Jul 12 21:23:01.435: INFO: Pod "downwardapi-volume-8c5e8680-0f4f-4262-a812-2f3f0c930e05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045965947s
STEP: Saw pod success
Jul 12 21:23:01.435: INFO: Pod "downwardapi-volume-8c5e8680-0f4f-4262-a812-2f3f0c930e05" satisfied condition "Succeeded or Failed"
Jul 12 21:23:01.440: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-8c5e8680-0f4f-4262-a812-2f3f0c930e05 container client-container: <nil>
STEP: delete the pod
Jul 12 21:23:01.502: INFO: Waiting for pod downwardapi-volume-8c5e8680-0f4f-4262-a812-2f3f0c930e05 to disappear
Jul 12 21:23:01.507: INFO: Pod downwardapi-volume-8c5e8680-0f4f-4262-a812-2f3f0c930e05 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:23:01.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6917" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":195,"skipped":3153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:23:01.523: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating server pod server in namespace prestop-886
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-886
STEP: Deleting pre-stop pod
Jul 12 21:23:12.831: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:23:12.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-886" for this suite.

• [SLOW TEST:11.402 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":339,"completed":196,"skipped":3196,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:23:12.926: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating all guestbook components
Jul 12 21:23:13.101: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jul 12 21:23:13.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7014 create -f -'
Jul 12 21:23:14.013: INFO: stderr: ""
Jul 12 21:23:14.013: INFO: stdout: "service/agnhost-replica created\n"
Jul 12 21:23:14.013: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jul 12 21:23:14.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7014 create -f -'
Jul 12 21:23:14.419: INFO: stderr: ""
Jul 12 21:23:14.419: INFO: stdout: "service/agnhost-primary created\n"
Jul 12 21:23:14.419: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 12 21:23:14.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7014 create -f -'
Jul 12 21:23:14.808: INFO: stderr: ""
Jul 12 21:23:14.808: INFO: stdout: "service/frontend created\n"
Jul 12 21:23:14.808: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul 12 21:23:14.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7014 create -f -'
Jul 12 21:23:15.124: INFO: stderr: ""
Jul 12 21:23:15.124: INFO: stdout: "deployment.apps/frontend created\n"
Jul 12 21:23:15.125: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 12 21:23:15.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7014 create -f -'
Jul 12 21:23:15.431: INFO: stderr: ""
Jul 12 21:23:15.431: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jul 12 21:23:15.431: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.32
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 12 21:23:15.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7014 create -f -'
Jul 12 21:23:15.896: INFO: stderr: ""
Jul 12 21:23:15.896: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jul 12 21:23:15.896: INFO: Waiting for all frontend pods to be Running.
Jul 12 21:23:20.947: INFO: Waiting for frontend to serve content.
Jul 12 21:23:22.017: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Jul 12 21:23:27.031: INFO: Trying to add a new entry to the guestbook.
Jul 12 21:23:27.043: INFO: Verifying that added entry can be retrieved.
Jul 12 21:23:27.056: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Jul 12 21:23:32.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7014 delete --grace-period=0 --force -f -'
Jul 12 21:23:32.209: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 21:23:32.209: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jul 12 21:23:32.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7014 delete --grace-period=0 --force -f -'
Jul 12 21:23:32.318: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 21:23:32.318: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 12 21:23:32.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7014 delete --grace-period=0 --force -f -'
Jul 12 21:23:32.427: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 21:23:32.427: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 12 21:23:32.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7014 delete --grace-period=0 --force -f -'
Jul 12 21:23:32.529: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 21:23:32.529: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 12 21:23:32.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7014 delete --grace-period=0 --force -f -'
Jul 12 21:23:32.661: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 21:23:32.661: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 12 21:23:32.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-7014 delete --grace-period=0 --force -f -'
Jul 12 21:23:32.804: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 21:23:32.804: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:23:32.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7014" for this suite.

• [SLOW TEST:19.904 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:336
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":339,"completed":197,"skipped":3207,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:23:32.830: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-projected-2c8m
STEP: Creating a pod to test atomic-volume-subpath
Jul 12 21:23:32.988: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2c8m" in namespace "subpath-6994" to be "Succeeded or Failed"
Jul 12 21:23:32.999: INFO: Pod "pod-subpath-test-projected-2c8m": Phase="Pending", Reason="", readiness=false. Elapsed: 11.552239ms
Jul 12 21:23:35.010: INFO: Pod "pod-subpath-test-projected-2c8m": Phase="Running", Reason="", readiness=true. Elapsed: 2.022255309s
Jul 12 21:23:37.019: INFO: Pod "pod-subpath-test-projected-2c8m": Phase="Running", Reason="", readiness=true. Elapsed: 4.031244739s
Jul 12 21:23:39.107: INFO: Pod "pod-subpath-test-projected-2c8m": Phase="Running", Reason="", readiness=true. Elapsed: 6.118970199s
Jul 12 21:23:41.141: INFO: Pod "pod-subpath-test-projected-2c8m": Phase="Running", Reason="", readiness=true. Elapsed: 8.152722662s
Jul 12 21:23:43.169: INFO: Pod "pod-subpath-test-projected-2c8m": Phase="Running", Reason="", readiness=true. Elapsed: 10.181486904s
Jul 12 21:23:45.215: INFO: Pod "pod-subpath-test-projected-2c8m": Phase="Running", Reason="", readiness=true. Elapsed: 12.226790928s
Jul 12 21:23:47.222: INFO: Pod "pod-subpath-test-projected-2c8m": Phase="Running", Reason="", readiness=true. Elapsed: 14.234102751s
Jul 12 21:23:49.238: INFO: Pod "pod-subpath-test-projected-2c8m": Phase="Running", Reason="", readiness=true. Elapsed: 16.250019929s
Jul 12 21:23:51.248: INFO: Pod "pod-subpath-test-projected-2c8m": Phase="Running", Reason="", readiness=true. Elapsed: 18.260258688s
Jul 12 21:23:53.273: INFO: Pod "pod-subpath-test-projected-2c8m": Phase="Running", Reason="", readiness=true. Elapsed: 20.285262288s
Jul 12 21:23:55.284: INFO: Pod "pod-subpath-test-projected-2c8m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.295958488s
STEP: Saw pod success
Jul 12 21:23:55.284: INFO: Pod "pod-subpath-test-projected-2c8m" satisfied condition "Succeeded or Failed"
Jul 12 21:23:55.287: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-subpath-test-projected-2c8m container test-container-subpath-projected-2c8m: <nil>
STEP: delete the pod
Jul 12 21:23:55.308: INFO: Waiting for pod pod-subpath-test-projected-2c8m to disappear
Jul 12 21:23:55.313: INFO: Pod pod-subpath-test-projected-2c8m no longer exists
STEP: Deleting pod pod-subpath-test-projected-2c8m
Jul 12 21:23:55.313: INFO: Deleting pod "pod-subpath-test-projected-2c8m" in namespace "subpath-6994"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:23:55.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6994" for this suite.

• [SLOW TEST:22.495 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":339,"completed":198,"skipped":3217,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:23:55.325: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:23:55.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8586" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":339,"completed":199,"skipped":3229,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:23:55.389: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:23:55.428: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jul 12 21:24:02.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 --namespace=crd-publish-openapi-3904 create -f -'
Jul 12 21:24:03.358: INFO: stderr: ""
Jul 12 21:24:03.358: INFO: stdout: "e2e-test-crd-publish-openapi-5555-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 12 21:24:03.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 --namespace=crd-publish-openapi-3904 delete e2e-test-crd-publish-openapi-5555-crds test-foo'
Jul 12 21:24:03.464: INFO: stderr: ""
Jul 12 21:24:03.464: INFO: stdout: "e2e-test-crd-publish-openapi-5555-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul 12 21:24:03.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 --namespace=crd-publish-openapi-3904 apply -f -'
Jul 12 21:24:03.846: INFO: stderr: ""
Jul 12 21:24:03.847: INFO: stdout: "e2e-test-crd-publish-openapi-5555-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 12 21:24:03.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 --namespace=crd-publish-openapi-3904 delete e2e-test-crd-publish-openapi-5555-crds test-foo'
Jul 12 21:24:03.959: INFO: stderr: ""
Jul 12 21:24:03.959: INFO: stdout: "e2e-test-crd-publish-openapi-5555-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jul 12 21:24:03.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 --namespace=crd-publish-openapi-3904 create -f -'
Jul 12 21:24:04.287: INFO: rc: 1
Jul 12 21:24:04.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 --namespace=crd-publish-openapi-3904 apply -f -'
Jul 12 21:24:04.595: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jul 12 21:24:04.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 --namespace=crd-publish-openapi-3904 create -f -'
Jul 12 21:24:04.915: INFO: rc: 1
Jul 12 21:24:04.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 --namespace=crd-publish-openapi-3904 apply -f -'
Jul 12 21:24:05.291: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jul 12 21:24:05.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 explain e2e-test-crd-publish-openapi-5555-crds'
Jul 12 21:24:05.712: INFO: stderr: ""
Jul 12 21:24:05.712: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5555-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jul 12 21:24:05.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 explain e2e-test-crd-publish-openapi-5555-crds.metadata'
Jul 12 21:24:06.098: INFO: stderr: ""
Jul 12 21:24:06.098: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5555-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul 12 21:24:06.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 explain e2e-test-crd-publish-openapi-5555-crds.spec'
Jul 12 21:24:06.653: INFO: stderr: ""
Jul 12 21:24:06.653: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5555-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul 12 21:24:06.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 explain e2e-test-crd-publish-openapi-5555-crds.spec.bars'
Jul 12 21:24:07.002: INFO: stderr: ""
Jul 12 21:24:07.002: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5555-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jul 12 21:24:07.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3904 explain e2e-test-crd-publish-openapi-5555-crds.spec.bars2'
Jul 12 21:24:07.343: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:24:12.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3904" for this suite.

• [SLOW TEST:17.468 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":339,"completed":200,"skipped":3231,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:24:12.857: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:24:12.970: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d6feb4e-a060-4cc6-a8e5-b076bdd98cce" in namespace "downward-api-8673" to be "Succeeded or Failed"
Jul 12 21:24:12.984: INFO: Pod "downwardapi-volume-7d6feb4e-a060-4cc6-a8e5-b076bdd98cce": Phase="Pending", Reason="", readiness=false. Elapsed: 14.077703ms
Jul 12 21:24:14.993: INFO: Pod "downwardapi-volume-7d6feb4e-a060-4cc6-a8e5-b076bdd98cce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02305789s
STEP: Saw pod success
Jul 12 21:24:14.993: INFO: Pod "downwardapi-volume-7d6feb4e-a060-4cc6-a8e5-b076bdd98cce" satisfied condition "Succeeded or Failed"
Jul 12 21:24:14.997: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-7d6feb4e-a060-4cc6-a8e5-b076bdd98cce container client-container: <nil>
STEP: delete the pod
Jul 12 21:24:15.025: INFO: Waiting for pod downwardapi-volume-7d6feb4e-a060-4cc6-a8e5-b076bdd98cce to disappear
Jul 12 21:24:15.028: INFO: Pod downwardapi-volume-7d6feb4e-a060-4cc6-a8e5-b076bdd98cce no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:24:15.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8673" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":201,"skipped":3259,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:24:15.038: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 12 21:24:15.151: INFO: Number of nodes with available pods: 0
Jul 12 21:24:15.151: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 21:24:16.167: INFO: Number of nodes with available pods: 0
Jul 12 21:24:16.167: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 21:24:17.167: INFO: Number of nodes with available pods: 3
Jul 12 21:24:17.167: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 12 21:24:17.193: INFO: Number of nodes with available pods: 2
Jul 12 21:24:17.193: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:18.205: INFO: Number of nodes with available pods: 2
Jul 12 21:24:18.206: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:19.207: INFO: Number of nodes with available pods: 2
Jul 12 21:24:19.207: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:20.207: INFO: Number of nodes with available pods: 2
Jul 12 21:24:20.207: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:21.209: INFO: Number of nodes with available pods: 2
Jul 12 21:24:21.210: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:22.216: INFO: Number of nodes with available pods: 2
Jul 12 21:24:22.217: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:23.226: INFO: Number of nodes with available pods: 2
Jul 12 21:24:23.226: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:24.206: INFO: Number of nodes with available pods: 2
Jul 12 21:24:24.206: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:25.205: INFO: Number of nodes with available pods: 2
Jul 12 21:24:25.206: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:26.206: INFO: Number of nodes with available pods: 2
Jul 12 21:24:26.206: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:27.210: INFO: Number of nodes with available pods: 2
Jul 12 21:24:27.210: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:28.214: INFO: Number of nodes with available pods: 2
Jul 12 21:24:28.214: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:29.206: INFO: Number of nodes with available pods: 2
Jul 12 21:24:29.206: INFO: Node gke-gke-1-21-default-pool-f67064dc-g6xf is running more than one daemon pod
Jul 12 21:24:30.205: INFO: Number of nodes with available pods: 3
Jul 12 21:24:30.205: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-979, will wait for the garbage collector to delete the pods
Jul 12 21:24:30.271: INFO: Deleting DaemonSet.extensions daemon-set took: 5.580645ms
Jul 12 21:24:30.474: INFO: Terminating DaemonSet.extensions daemon-set pods took: 203.10696ms
Jul 12 21:24:38.192: INFO: Number of nodes with available pods: 0
Jul 12 21:24:38.192: INFO: Number of running nodes: 0, number of available pods: 0
Jul 12 21:24:38.198: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29664"},"items":null}

Jul 12 21:24:38.204: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29664"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:24:38.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-979" for this suite.

• [SLOW TEST:23.223 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":339,"completed":202,"skipped":3264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:24:38.262: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-map-39f81d63-2585-4622-b79a-bff1b18c0e9b
STEP: Creating a pod to test consume configMaps
Jul 12 21:24:38.508: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-91875b7d-de65-4c86-a2f1-0eed33648065" in namespace "projected-3150" to be "Succeeded or Failed"
Jul 12 21:24:38.525: INFO: Pod "pod-projected-configmaps-91875b7d-de65-4c86-a2f1-0eed33648065": Phase="Pending", Reason="", readiness=false. Elapsed: 16.903758ms
Jul 12 21:24:40.546: INFO: Pod "pod-projected-configmaps-91875b7d-de65-4c86-a2f1-0eed33648065": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037000737s
STEP: Saw pod success
Jul 12 21:24:40.546: INFO: Pod "pod-projected-configmaps-91875b7d-de65-4c86-a2f1-0eed33648065" satisfied condition "Succeeded or Failed"
Jul 12 21:24:40.550: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-projected-configmaps-91875b7d-de65-4c86-a2f1-0eed33648065 container agnhost-container: <nil>
STEP: delete the pod
Jul 12 21:24:40.576: INFO: Waiting for pod pod-projected-configmaps-91875b7d-de65-4c86-a2f1-0eed33648065 to disappear
Jul 12 21:24:40.582: INFO: Pod pod-projected-configmaps-91875b7d-de65-4c86-a2f1-0eed33648065 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:24:40.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3150" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":203,"skipped":3287,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:24:40.598: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jul 12 21:24:40.676: INFO: The status of Pod annotationupdate7fddd170-91eb-41a0-b23d-414863db7410 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:24:42.694: INFO: The status of Pod annotationupdate7fddd170-91eb-41a0-b23d-414863db7410 is Running (Ready = true)
Jul 12 21:24:43.246: INFO: Successfully updated pod "annotationupdate7fddd170-91eb-41a0-b23d-414863db7410"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:24:47.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7587" for this suite.

• [SLOW TEST:6.717 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":339,"completed":204,"skipped":3300,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:24:47.316: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:24:47.529: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a195fd72-d16f-45c0-b026-bec3d6682454", Controller:(*bool)(0xc003be7c96), BlockOwnerDeletion:(*bool)(0xc003be7c97)}}
Jul 12 21:24:47.557: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f8e83bd5-8411-4edc-99d2-20fe0e1a3e07", Controller:(*bool)(0xc003be7f16), BlockOwnerDeletion:(*bool)(0xc003be7f17)}}
Jul 12 21:24:47.586: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"fc50a91c-29a3-47c4-a4f0-0f9fd56d1ec5", Controller:(*bool)(0xc0006083c6), BlockOwnerDeletion:(*bool)(0xc0006083c7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:24:52.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3182" for this suite.

• [SLOW TEST:5.322 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":339,"completed":205,"skipped":3364,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:24:52.639: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:24:52.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7a8d78b0-3454-4101-82fe-414de4286ffc" in namespace "projected-9009" to be "Succeeded or Failed"
Jul 12 21:24:52.782: INFO: Pod "downwardapi-volume-7a8d78b0-3454-4101-82fe-414de4286ffc": Phase="Pending", Reason="", readiness=false. Elapsed: 24.210681ms
Jul 12 21:24:54.791: INFO: Pod "downwardapi-volume-7a8d78b0-3454-4101-82fe-414de4286ffc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033616268s
STEP: Saw pod success
Jul 12 21:24:54.791: INFO: Pod "downwardapi-volume-7a8d78b0-3454-4101-82fe-414de4286ffc" satisfied condition "Succeeded or Failed"
Jul 12 21:24:54.801: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-7a8d78b0-3454-4101-82fe-414de4286ffc container client-container: <nil>
STEP: delete the pod
Jul 12 21:24:54.831: INFO: Waiting for pod downwardapi-volume-7a8d78b0-3454-4101-82fe-414de4286ffc to disappear
Jul 12 21:24:54.837: INFO: Pod downwardapi-volume-7a8d78b0-3454-4101-82fe-414de4286ffc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:24:54.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9009" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":339,"completed":206,"skipped":3365,"failed":0}
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:24:54.853: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jul 12 21:24:54.897: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 12 21:25:54.935: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:25:54.938: INFO: Starting informer...
STEP: Starting pod...
Jul 12 21:25:55.161: INFO: Pod is running on gke-gke-1-21-default-pool-f67064dc-3tj7. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jul 12 21:25:55.295: INFO: Pod wasn't evicted. Proceeding
Jul 12 21:25:55.295: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jul 12 21:27:10.324: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:27:10.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2075" for this suite.

• [SLOW TEST:135.493 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":339,"completed":207,"skipped":3371,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:27:10.347: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:186
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul 12 21:27:10.403: INFO: The status of Pod pod-update-activedeadlineseconds-10067d86-ef89-45bd-99d3-216591c68048 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:27:12.417: INFO: The status of Pod pod-update-activedeadlineseconds-10067d86-ef89-45bd-99d3-216591c68048 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 12 21:27:12.974: INFO: Successfully updated pod "pod-update-activedeadlineseconds-10067d86-ef89-45bd-99d3-216591c68048"
Jul 12 21:27:12.974: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-10067d86-ef89-45bd-99d3-216591c68048" in namespace "pods-1903" to be "terminated due to deadline exceeded"
Jul 12 21:27:12.981: INFO: Pod "pod-update-activedeadlineseconds-10067d86-ef89-45bd-99d3-216591c68048": Phase="Running", Reason="", readiness=true. Elapsed: 6.724905ms
Jul 12 21:27:14.988: INFO: Pod "pod-update-activedeadlineseconds-10067d86-ef89-45bd-99d3-216591c68048": Phase="Running", Reason="", readiness=true. Elapsed: 2.014330233s
Jul 12 21:27:16.997: INFO: Pod "pod-update-activedeadlineseconds-10067d86-ef89-45bd-99d3-216591c68048": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.022842806s
Jul 12 21:27:16.997: INFO: Pod "pod-update-activedeadlineseconds-10067d86-ef89-45bd-99d3-216591c68048" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:27:16.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1903" for this suite.

• [SLOW TEST:6.661 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":339,"completed":208,"skipped":3373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:27:17.012: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:27:17.052: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9831
I0712 21:27:17.066329      20 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9831, replica count: 1
I0712 21:27:18.117930      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 21:27:19.119127      20 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 21:27:19.239: INFO: Created: latency-svc-x4x5v
Jul 12 21:27:19.259: INFO: Got endpoints: latency-svc-x4x5v [39.60198ms]
Jul 12 21:27:19.314: INFO: Created: latency-svc-x96mm
Jul 12 21:27:19.326: INFO: Created: latency-svc-bcbg9
Jul 12 21:27:19.330: INFO: Got endpoints: latency-svc-x96mm [69.510094ms]
Jul 12 21:27:19.338: INFO: Got endpoints: latency-svc-bcbg9 [77.289524ms]
Jul 12 21:27:19.369: INFO: Created: latency-svc-txtfq
Jul 12 21:27:19.373: INFO: Got endpoints: latency-svc-txtfq [112.758702ms]
Jul 12 21:27:19.425: INFO: Created: latency-svc-w2ggn
Jul 12 21:27:19.435: INFO: Got endpoints: latency-svc-w2ggn [173.335179ms]
Jul 12 21:27:19.454: INFO: Created: latency-svc-rcg46
Jul 12 21:27:19.462: INFO: Got endpoints: latency-svc-rcg46 [201.171764ms]
Jul 12 21:27:19.543: INFO: Created: latency-svc-mqf95
Jul 12 21:27:19.547: INFO: Got endpoints: latency-svc-mqf95 [286.395575ms]
Jul 12 21:27:19.605: INFO: Created: latency-svc-t59r6
Jul 12 21:27:19.612: INFO: Got endpoints: latency-svc-t59r6 [349.861762ms]
Jul 12 21:27:19.644: INFO: Created: latency-svc-jgrdn
Jul 12 21:27:19.644: INFO: Created: latency-svc-6zhnr
Jul 12 21:27:19.651: INFO: Got endpoints: latency-svc-jgrdn [320.873977ms]
Jul 12 21:27:19.660: INFO: Created: latency-svc-2c4ds
Jul 12 21:27:19.675: INFO: Created: latency-svc-44vfs
Jul 12 21:27:19.681: INFO: Got endpoints: latency-svc-6zhnr [418.812045ms]
Jul 12 21:27:19.694: INFO: Created: latency-svc-4cvkq
Jul 12 21:27:19.728: INFO: Got endpoints: latency-svc-4cvkq [466.057709ms]
Jul 12 21:27:19.735: INFO: Got endpoints: latency-svc-2c4ds [473.709469ms]
Jul 12 21:27:19.762: INFO: Created: latency-svc-t85m8
Jul 12 21:27:19.765: INFO: Got endpoints: latency-svc-44vfs [504.053637ms]
Jul 12 21:27:19.816: INFO: Created: latency-svc-wlwgg
Jul 12 21:27:19.817: INFO: Got endpoints: latency-svc-t85m8 [555.417328ms]
Jul 12 21:27:19.841: INFO: Created: latency-svc-rmwwt
Jul 12 21:27:19.847: INFO: Got endpoints: latency-svc-wlwgg [384.956561ms]
Jul 12 21:27:19.870: INFO: Created: latency-svc-c45bw
Jul 12 21:27:19.882: INFO: Got endpoints: latency-svc-rmwwt [508.484921ms]
Jul 12 21:27:19.885: INFO: Created: latency-svc-pzmhq
Jul 12 21:27:19.901: INFO: Created: latency-svc-9nqfj
Jul 12 21:27:19.902: INFO: Got endpoints: latency-svc-c45bw [640.42174ms]
Jul 12 21:27:19.923: INFO: Got endpoints: latency-svc-pzmhq [661.13933ms]
Jul 12 21:27:19.939: INFO: Created: latency-svc-jmm24
Jul 12 21:27:19.941: INFO: Got endpoints: latency-svc-9nqfj [394.05508ms]
Jul 12 21:27:19.961: INFO: Created: latency-svc-ndm87
Jul 12 21:27:19.969: INFO: Got endpoints: latency-svc-jmm24 [356.966749ms]
Jul 12 21:27:19.985: INFO: Created: latency-svc-xbbwf
Jul 12 21:27:20.002: INFO: Created: latency-svc-8vg86
Jul 12 21:27:20.002: INFO: Got endpoints: latency-svc-ndm87 [663.622667ms]
Jul 12 21:27:20.011: INFO: Created: latency-svc-4zfbs
Jul 12 21:27:20.037: INFO: Created: latency-svc-578rq
Jul 12 21:27:20.051: INFO: Created: latency-svc-ldt7t
Jul 12 21:27:20.070: INFO: Created: latency-svc-jb7pz
Jul 12 21:27:20.085: INFO: Got endpoints: latency-svc-xbbwf [434.675451ms]
Jul 12 21:27:20.094: INFO: Got endpoints: latency-svc-8vg86 [659.098956ms]
Jul 12 21:27:20.107: INFO: Created: latency-svc-dz9v8
Jul 12 21:27:20.109: INFO: Got endpoints: latency-svc-578rq [380.769063ms]
Jul 12 21:27:20.115: INFO: Created: latency-svc-9c9bw
Jul 12 21:27:20.140: INFO: Created: latency-svc-qbrzz
Jul 12 21:27:20.146: INFO: Got endpoints: latency-svc-4zfbs [465.685194ms]
Jul 12 21:27:20.152: INFO: Created: latency-svc-jdtml
Jul 12 21:27:20.172: INFO: Created: latency-svc-mpbnv
Jul 12 21:27:20.172: INFO: Got endpoints: latency-svc-9c9bw [406.776357ms]
Jul 12 21:27:20.181: INFO: Got endpoints: latency-svc-qbrzz [364.173754ms]
Jul 12 21:27:20.195: INFO: Got endpoints: latency-svc-jb7pz [459.50275ms]
Jul 12 21:27:20.205: INFO: Created: latency-svc-4xmdm
Jul 12 21:27:20.220: INFO: Got endpoints: latency-svc-ldt7t [958.140063ms]
Jul 12 21:27:20.230: INFO: Created: latency-svc-zqc8k
Jul 12 21:27:20.265: INFO: Got endpoints: latency-svc-dz9v8 [417.399913ms]
Jul 12 21:27:20.266: INFO: Created: latency-svc-bxjrk
Jul 12 21:27:20.297: INFO: Created: latency-svc-5kzzk
Jul 12 21:27:20.306: INFO: Got endpoints: latency-svc-bxjrk [424.529084ms]
Jul 12 21:27:20.324: INFO: Created: latency-svc-cmqmq
Jul 12 21:27:20.335: INFO: Created: latency-svc-tn9ng
Jul 12 21:27:20.336: INFO: Created: latency-svc-kcb7l
Jul 12 21:27:20.367: INFO: Created: latency-svc-qjwnv
Jul 12 21:27:20.369: INFO: Got endpoints: latency-svc-mpbnv [428.004331ms]
Jul 12 21:27:20.377: INFO: Created: latency-svc-k766f
Jul 12 21:27:20.385: INFO: Created: latency-svc-qcph6
Jul 12 21:27:20.401: INFO: Created: latency-svc-h55w4
Jul 12 21:27:20.402: INFO: Got endpoints: latency-svc-4xmdm [292.242119ms]
Jul 12 21:27:20.436: INFO: Created: latency-svc-qrbnw
Jul 12 21:27:20.437: INFO: Got endpoints: latency-svc-jdtml [535.094724ms]
Jul 12 21:27:20.459: INFO: Got endpoints: latency-svc-zqc8k [535.485213ms]
Jul 12 21:27:20.467: INFO: Created: latency-svc-47dnj
Jul 12 21:27:20.491: INFO: Created: latency-svc-gdcv2
Jul 12 21:27:20.505: INFO: Got endpoints: latency-svc-cmqmq [536.40672ms]
Jul 12 21:27:20.523: INFO: Created: latency-svc-shs8s
Jul 12 21:27:20.541: INFO: Created: latency-svc-cxpnm
Jul 12 21:27:20.558: INFO: Got endpoints: latency-svc-5kzzk [555.927361ms]
Jul 12 21:27:20.582: INFO: Got endpoints: latency-svc-kcb7l [487.81341ms]
Jul 12 21:27:20.583: INFO: Created: latency-svc-8p5md
Jul 12 21:27:20.593: INFO: Got endpoints: latency-svc-qjwnv [445.775624ms]
Jul 12 21:27:20.593: INFO: Created: latency-svc-xvp57
Jul 12 21:27:20.617: INFO: Got endpoints: latency-svc-k766f [436.176832ms]
Jul 12 21:27:20.619: INFO: Created: latency-svc-ckmbf
Jul 12 21:27:20.633: INFO: Created: latency-svc-hhjq5
Jul 12 21:27:20.644: INFO: Got endpoints: latency-svc-h55w4 [448.812414ms]
Jul 12 21:27:20.663: INFO: Got endpoints: latency-svc-qrbnw [489.747257ms]
Jul 12 21:27:20.671: INFO: Created: latency-svc-l2c8h
Jul 12 21:27:20.682: INFO: Got endpoints: latency-svc-tn9ng [596.117573ms]
Jul 12 21:27:20.682: INFO: Created: latency-svc-72qfn
Jul 12 21:27:20.701: INFO: Got endpoints: latency-svc-shs8s [331.230855ms]
Jul 12 21:27:20.713: INFO: Created: latency-svc-ddr27
Jul 12 21:27:20.747: INFO: Created: latency-svc-fdlw5
Jul 12 21:27:20.751: INFO: Got endpoints: latency-svc-qcph6 [530.947367ms]
Jul 12 21:27:20.784: INFO: Got endpoints: latency-svc-47dnj [519.1559ms]
Jul 12 21:27:20.791: INFO: Created: latency-svc-x2qdm
Jul 12 21:27:20.811: INFO: Created: latency-svc-6zs96
Jul 12 21:27:20.838: INFO: Created: latency-svc-dqfwp
Jul 12 21:27:20.847: INFO: Got endpoints: latency-svc-gdcv2 [540.315068ms]
Jul 12 21:27:20.857: INFO: Created: latency-svc-jlwvv
Jul 12 21:27:20.869: INFO: Created: latency-svc-mblsc
Jul 12 21:27:20.881: INFO: Got endpoints: latency-svc-xvp57 [422.621867ms]
Jul 12 21:27:20.898: INFO: Created: latency-svc-gcdmn
Jul 12 21:27:20.907: INFO: Got endpoints: latency-svc-cxpnm [505.240107ms]
Jul 12 21:27:20.920: INFO: Got endpoints: latency-svc-l2c8h [337.541015ms]
Jul 12 21:27:20.924: INFO: Created: latency-svc-vj245
Jul 12 21:27:20.944: INFO: Created: latency-svc-lxz4t
Jul 12 21:27:20.958: INFO: Created: latency-svc-fnz6p
Jul 12 21:27:20.970: INFO: Got endpoints: latency-svc-8p5md [533.020422ms]
Jul 12 21:27:20.976: INFO: Got endpoints: latency-svc-ddr27 [358.846932ms]
Jul 12 21:27:21.001: INFO: Created: latency-svc-5bbs9
Jul 12 21:27:21.014: INFO: Created: latency-svc-m9cxm
Jul 12 21:27:21.024: INFO: Got endpoints: latency-svc-72qfn [431.215221ms]
Jul 12 21:27:21.036: INFO: Got endpoints: latency-svc-ckmbf [531.344736ms]
Jul 12 21:27:21.049: INFO: Got endpoints: latency-svc-6zs96 [367.133162ms]
Jul 12 21:27:21.054: INFO: Got endpoints: latency-svc-hhjq5 [495.598196ms]
Jul 12 21:27:21.074: INFO: Created: latency-svc-kpwfl
Jul 12 21:27:21.079: INFO: Got endpoints: latency-svc-dqfwp [377.911225ms]
Jul 12 21:27:21.095: INFO: Created: latency-svc-r9z6z
Jul 12 21:27:21.105: INFO: Got endpoints: latency-svc-fdlw5 [460.91668ms]
Jul 12 21:27:21.110: INFO: Created: latency-svc-8vbb4
Jul 12 21:27:21.113: INFO: Got endpoints: latency-svc-x2qdm [450.117601ms]
Jul 12 21:27:21.123: INFO: Got endpoints: latency-svc-jlwvv [371.906709ms]
Jul 12 21:27:21.128: INFO: Created: latency-svc-wh286
Jul 12 21:27:21.150: INFO: Got endpoints: latency-svc-gcdmn [303.452974ms]
Jul 12 21:27:21.153: INFO: Created: latency-svc-nc44c
Jul 12 21:27:21.160: INFO: Got endpoints: latency-svc-mblsc [375.745756ms]
Jul 12 21:27:21.172: INFO: Created: latency-svc-nz4hb
Jul 12 21:27:21.180: INFO: Created: latency-svc-8tff5
Jul 12 21:27:21.183: INFO: Got endpoints: latency-svc-lxz4t [275.882415ms]
Jul 12 21:27:21.188: INFO: Created: latency-svc-qrcq6
Jul 12 21:27:21.192: INFO: Got endpoints: latency-svc-vj245 [310.491494ms]
Jul 12 21:27:21.204: INFO: Created: latency-svc-w8tzh
Jul 12 21:27:21.204: INFO: Created: latency-svc-tnqhh
Jul 12 21:27:21.212: INFO: Got endpoints: latency-svc-5bbs9 [241.635039ms]
Jul 12 21:27:21.222: INFO: Got endpoints: latency-svc-m9cxm [245.768694ms]
Jul 12 21:27:21.224: INFO: Created: latency-svc-t2t8t
Jul 12 21:27:21.237: INFO: Got endpoints: latency-svc-fnz6p [317.306482ms]
Jul 12 21:27:21.248: INFO: Got endpoints: latency-svc-r9z6z [211.400997ms]
Jul 12 21:27:21.262: INFO: Created: latency-svc-mwfnc
Jul 12 21:27:21.279: INFO: Created: latency-svc-p76vs
Jul 12 21:27:21.287: INFO: Created: latency-svc-5mrjt
Jul 12 21:27:21.288: INFO: Got endpoints: latency-svc-kpwfl [263.571521ms]
Jul 12 21:27:21.296: INFO: Got endpoints: latency-svc-8vbb4 [246.741825ms]
Jul 12 21:27:21.298: INFO: Created: latency-svc-ngc59
Jul 12 21:27:21.305: INFO: Created: latency-svc-xfq2n
Jul 12 21:27:21.315: INFO: Got endpoints: latency-svc-wh286 [260.486471ms]
Jul 12 21:27:21.332: INFO: Created: latency-svc-cpmvq
Jul 12 21:27:21.345: INFO: Created: latency-svc-vk7p7
Jul 12 21:27:21.350: INFO: Created: latency-svc-rrmlj
Jul 12 21:27:21.373: INFO: Got endpoints: latency-svc-nc44c [293.477313ms]
Jul 12 21:27:21.391: INFO: Created: latency-svc-8clpt
Jul 12 21:27:21.434: INFO: Got endpoints: latency-svc-nz4hb [329.871317ms]
Jul 12 21:27:21.446: INFO: Created: latency-svc-xxxlg
Jul 12 21:27:21.463: INFO: Got endpoints: latency-svc-8tff5 [340.034246ms]
Jul 12 21:27:21.495: INFO: Created: latency-svc-j8psj
Jul 12 21:27:21.519: INFO: Got endpoints: latency-svc-qrcq6 [406.111845ms]
Jul 12 21:27:21.537: INFO: Created: latency-svc-q4v4f
Jul 12 21:27:21.566: INFO: Got endpoints: latency-svc-tnqhh [415.0211ms]
Jul 12 21:27:21.584: INFO: Created: latency-svc-w54cw
Jul 12 21:27:21.614: INFO: Got endpoints: latency-svc-w8tzh [454.170114ms]
Jul 12 21:27:21.628: INFO: Created: latency-svc-phzlc
Jul 12 21:27:21.664: INFO: Got endpoints: latency-svc-t2t8t [471.775161ms]
Jul 12 21:27:21.677: INFO: Created: latency-svc-zwn4x
Jul 12 21:27:21.714: INFO: Got endpoints: latency-svc-mwfnc [530.484734ms]
Jul 12 21:27:21.735: INFO: Created: latency-svc-6c48j
Jul 12 21:27:21.774: INFO: Got endpoints: latency-svc-p76vs [562.019497ms]
Jul 12 21:27:21.796: INFO: Created: latency-svc-55jnm
Jul 12 21:27:21.818: INFO: Got endpoints: latency-svc-5mrjt [595.559677ms]
Jul 12 21:27:21.864: INFO: Got endpoints: latency-svc-ngc59 [627.082813ms]
Jul 12 21:27:21.884: INFO: Created: latency-svc-tg5b8
Jul 12 21:27:21.888: INFO: Created: latency-svc-x6r4l
Jul 12 21:27:21.913: INFO: Got endpoints: latency-svc-xfq2n [664.600987ms]
Jul 12 21:27:21.966: INFO: Got endpoints: latency-svc-cpmvq [677.127122ms]
Jul 12 21:27:21.985: INFO: Created: latency-svc-4lxgt
Jul 12 21:27:22.004: INFO: Created: latency-svc-l9d5q
Jul 12 21:27:22.019: INFO: Got endpoints: latency-svc-vk7p7 [722.690297ms]
Jul 12 21:27:22.054: INFO: Created: latency-svc-l4ms6
Jul 12 21:27:22.246: INFO: Got endpoints: latency-svc-8clpt [873.357828ms]
Jul 12 21:27:22.327: INFO: Got endpoints: latency-svc-xxxlg [892.580564ms]
Jul 12 21:27:22.371: INFO: Got endpoints: latency-svc-q4v4f [851.314702ms]
Jul 12 21:27:22.393: INFO: Got endpoints: latency-svc-rrmlj [1.077493303s]
Jul 12 21:27:22.418: INFO: Created: latency-svc-xq98k
Jul 12 21:27:22.444: INFO: Got endpoints: latency-svc-j8psj [980.590538ms]
Jul 12 21:27:22.454: INFO: Created: latency-svc-wm7mn
Jul 12 21:27:22.478: INFO: Created: latency-svc-wp8pl
Jul 12 21:27:22.486: INFO: Got endpoints: latency-svc-6c48j [772.320356ms]
Jul 12 21:27:22.511: INFO: Got endpoints: latency-svc-w54cw [944.399163ms]
Jul 12 21:27:22.545: INFO: Created: latency-svc-78vxp
Jul 12 21:27:22.582: INFO: Got endpoints: latency-svc-zwn4x [917.703041ms]
Jul 12 21:27:22.604: INFO: Created: latency-svc-nqjt8
Jul 12 21:27:22.665: INFO: Created: latency-svc-4hv74
Jul 12 21:27:22.676: INFO: Got endpoints: latency-svc-phzlc [1.060815591s]
Jul 12 21:27:22.827: INFO: Got endpoints: latency-svc-x6r4l [961.612368ms]
Jul 12 21:27:22.859: INFO: Created: latency-svc-6458d
Jul 12 21:27:22.869: INFO: Got endpoints: latency-svc-55jnm [1.094411281s]
Jul 12 21:27:22.928: INFO: Got endpoints: latency-svc-tg5b8 [1.11006561s]
Jul 12 21:27:22.943: INFO: Created: latency-svc-648nn
Jul 12 21:27:22.954: INFO: Got endpoints: latency-svc-4lxgt [987.965115ms]
Jul 12 21:27:23.014: INFO: Created: latency-svc-2zh9p
Jul 12 21:27:23.042: INFO: Got endpoints: latency-svc-l9d5q [1.128743209s]
Jul 12 21:27:23.060: INFO: Got endpoints: latency-svc-xq98k [813.810472ms]
Jul 12 21:27:23.095: INFO: Created: latency-svc-nsrwb
Jul 12 21:27:23.106: INFO: Created: latency-svc-mrwck
Jul 12 21:27:23.153: INFO: Created: latency-svc-h98m8
Jul 12 21:27:23.201: INFO: Created: latency-svc-nrqrl
Jul 12 21:27:23.201: INFO: Got endpoints: latency-svc-78vxp [808.076479ms]
Jul 12 21:27:23.325: INFO: Created: latency-svc-q5tlt
Jul 12 21:27:23.349: INFO: Got endpoints: latency-svc-l4ms6 [1.330254577s]
Jul 12 21:27:23.350: INFO: Created: latency-svc-4lmc7
Jul 12 21:27:23.380: INFO: Got endpoints: latency-svc-wp8pl [1.009409442s]
Jul 12 21:27:23.388: INFO: Created: latency-svc-fbtnr
Jul 12 21:27:23.390: INFO: Got endpoints: latency-svc-wm7mn [1.063256401s]
Jul 12 21:27:23.441: INFO: Created: latency-svc-6zt82
Jul 12 21:27:23.459: INFO: Got endpoints: latency-svc-nqjt8 [1.014889244s]
Jul 12 21:27:23.475: INFO: Got endpoints: latency-svc-648nn [892.65642ms]
Jul 12 21:27:23.495: INFO: Created: latency-svc-5gntx
Jul 12 21:27:23.499: INFO: Got endpoints: latency-svc-2zh9p [822.693047ms]
Jul 12 21:27:23.515: INFO: Created: latency-svc-txbxn
Jul 12 21:27:23.551: INFO: Created: latency-svc-wj249
Jul 12 21:27:23.562: INFO: Got endpoints: latency-svc-6458d [1.051094154s]
Jul 12 21:27:23.591: INFO: Created: latency-svc-7bdfp
Jul 12 21:27:23.598: INFO: Got endpoints: latency-svc-h98m8 [669.908751ms]
Jul 12 21:27:23.609: INFO: Got endpoints: latency-svc-4hv74 [1.122926503s]
Jul 12 21:27:23.653: INFO: Got endpoints: latency-svc-nsrwb [826.88523ms]
Jul 12 21:27:23.654: INFO: Created: latency-svc-w2bkx
Jul 12 21:27:23.732: INFO: Got endpoints: latency-svc-q5tlt [689.88707ms]
Jul 12 21:27:23.742: INFO: Created: latency-svc-6zxlr
Jul 12 21:27:23.761: INFO: Got endpoints: latency-svc-mrwck [892.232457ms]
Jul 12 21:27:23.801: INFO: Created: latency-svc-md7hb
Jul 12 21:27:23.829: INFO: Got endpoints: latency-svc-6zt82 [479.865659ms]
Jul 12 21:27:23.833: INFO: Created: latency-svc-8f5t4
Jul 12 21:27:23.859: INFO: Created: latency-svc-bnx95
Jul 12 21:27:23.872: INFO: Got endpoints: latency-svc-4lmc7 [811.951623ms]
Jul 12 21:27:23.892: INFO: Got endpoints: latency-svc-nrqrl [938.118053ms]
Jul 12 21:27:23.906: INFO: Created: latency-svc-fp7c7
Jul 12 21:27:23.943: INFO: Created: latency-svc-v6f6z
Jul 12 21:27:23.959: INFO: Got endpoints: latency-svc-5gntx [578.773874ms]
Jul 12 21:27:23.990: INFO: Created: latency-svc-gmwxx
Jul 12 21:27:23.998: INFO: Got endpoints: latency-svc-txbxn [607.596038ms]
Jul 12 21:27:24.011: INFO: Created: latency-svc-v7qzs
Jul 12 21:27:24.030: INFO: Created: latency-svc-qtgnj
Jul 12 21:27:24.044: INFO: Got endpoints: latency-svc-wj249 [585.576114ms]
Jul 12 21:27:24.051: INFO: Created: latency-svc-str94
Jul 12 21:27:24.079: INFO: Got endpoints: latency-svc-w2bkx [580.156603ms]
Jul 12 21:27:24.094: INFO: Got endpoints: latency-svc-6zxlr [495.926333ms]
Jul 12 21:27:24.112: INFO: Created: latency-svc-4qqm2
Jul 12 21:27:24.149: INFO: Got endpoints: latency-svc-fbtnr [946.915895ms]
Jul 12 21:27:24.153: INFO: Got endpoints: latency-svc-bnx95 [498.895293ms]
Jul 12 21:27:24.164: INFO: Got endpoints: latency-svc-7bdfp [689.726271ms]
Jul 12 21:27:24.166: INFO: Created: latency-svc-rjdwn
Jul 12 21:27:24.182: INFO: Got endpoints: latency-svc-fp7c7 [450.615798ms]
Jul 12 21:27:24.216: INFO: Created: latency-svc-q7pf8
Jul 12 21:27:24.246: INFO: Created: latency-svc-n5hvq
Jul 12 21:27:24.279: INFO: Created: latency-svc-94wcr
Jul 12 21:27:24.279: INFO: Got endpoints: latency-svc-md7hb [716.85975ms]
Jul 12 21:27:24.322: INFO: Got endpoints: latency-svc-v7qzs [449.705679ms]
Jul 12 21:27:24.353: INFO: Created: latency-svc-wbc7p
Jul 12 21:27:24.400: INFO: Created: latency-svc-tn2ms
Jul 12 21:27:24.414: INFO: Created: latency-svc-qtt7x
Jul 12 21:27:24.455: INFO: Got endpoints: latency-svc-gmwxx [622.144166ms]
Jul 12 21:27:24.456: INFO: Created: latency-svc-ldvgz
Jul 12 21:27:24.477: INFO: Got endpoints: latency-svc-str94 [517.423827ms]
Jul 12 21:27:24.524: INFO: Got endpoints: latency-svc-v6f6z [762.647604ms]
Jul 12 21:27:24.568: INFO: Got endpoints: latency-svc-4qqm2 [569.772871ms]
Jul 12 21:27:24.568: INFO: Created: latency-svc-dz9mc
Jul 12 21:27:24.591: INFO: Got endpoints: latency-svc-8f5t4 [981.517438ms]
Jul 12 21:27:24.602: INFO: Created: latency-svc-qkjzm
Jul 12 21:27:24.621: INFO: Got endpoints: latency-svc-qtgnj [729.161123ms]
Jul 12 21:27:24.641: INFO: Got endpoints: latency-svc-q7pf8 [561.923006ms]
Jul 12 21:27:24.671: INFO: Created: latency-svc-bw5v6
Jul 12 21:27:24.680: INFO: Created: latency-svc-7bdkm
Jul 12 21:27:24.690: INFO: Created: latency-svc-7s8kz
Jul 12 21:27:24.698: INFO: Created: latency-svc-jdqht
Jul 12 21:27:24.729: INFO: Created: latency-svc-jfm5r
Jul 12 21:27:24.757: INFO: Got endpoints: latency-svc-wbc7p [608.39981ms]
Jul 12 21:27:24.769: INFO: Created: latency-svc-f4dv6
Jul 12 21:27:24.788: INFO: Got endpoints: latency-svc-94wcr [634.96203ms]
Jul 12 21:27:24.921: INFO: Created: latency-svc-hwb4q
Jul 12 21:27:24.930: INFO: Got endpoints: latency-svc-qtt7x [650.961382ms]
Jul 12 21:27:24.930: INFO: Got endpoints: latency-svc-rjdwn [886.051579ms]
Jul 12 21:27:24.936: INFO: Got endpoints: latency-svc-tn2ms [772.118342ms]
Jul 12 21:27:24.979: INFO: Created: latency-svc-t6w8w
Jul 12 21:27:24.980: INFO: Got endpoints: latency-svc-n5hvq [885.977286ms]
Jul 12 21:27:25.009: INFO: Got endpoints: latency-svc-qkjzm [553.24444ms]
Jul 12 21:27:25.020: INFO: Created: latency-svc-75jln
Jul 12 21:27:25.026: INFO: Got endpoints: latency-svc-dz9mc [843.379488ms]
Jul 12 21:27:25.063: INFO: Created: latency-svc-68dnm
Jul 12 21:27:25.081: INFO: Got endpoints: latency-svc-7bdkm [604.49385ms]
Jul 12 21:27:25.096: INFO: Got endpoints: latency-svc-bw5v6 [572.3362ms]
Jul 12 21:27:25.114: INFO: Created: latency-svc-2wnzt
Jul 12 21:27:25.147: INFO: Created: latency-svc-96mr7
Jul 12 21:27:25.154: INFO: Got endpoints: latency-svc-ldvgz [831.839351ms]
Jul 12 21:27:25.184: INFO: Created: latency-svc-ttj7m
Jul 12 21:27:25.211: INFO: Got endpoints: latency-svc-7s8kz [643.448998ms]
Jul 12 21:27:25.244: INFO: Created: latency-svc-hvkzt
Jul 12 21:27:25.273: INFO: Got endpoints: latency-svc-jdqht [682.188004ms]
Jul 12 21:27:25.281: INFO: Created: latency-svc-8h5jf
Jul 12 21:27:25.323: INFO: Created: latency-svc-7gcbb
Jul 12 21:27:25.353: INFO: Created: latency-svc-scdfr
Jul 12 21:27:25.380: INFO: Created: latency-svc-g87rq
Jul 12 21:27:25.386: INFO: Got endpoints: latency-svc-jfm5r [764.283177ms]
Jul 12 21:27:25.402: INFO: Created: latency-svc-txvv2
Jul 12 21:27:25.408: INFO: Got endpoints: latency-svc-f4dv6 [766.460809ms]
Jul 12 21:27:25.446: INFO: Created: latency-svc-r2kgj
Jul 12 21:27:25.452: INFO: Got endpoints: latency-svc-68dnm [521.494977ms]
Jul 12 21:27:25.489: INFO: Created: latency-svc-ngk99
Jul 12 21:27:25.522: INFO: Created: latency-svc-88dzx
Jul 12 21:27:25.552: INFO: Got endpoints: latency-svc-hwb4q [794.72318ms]
Jul 12 21:27:25.585: INFO: Got endpoints: latency-svc-t6w8w [796.449613ms]
Jul 12 21:27:25.597: INFO: Created: latency-svc-7lgjb
Jul 12 21:27:25.660: INFO: Created: latency-svc-svpzx
Jul 12 21:27:25.665: INFO: Got endpoints: latency-svc-75jln [728.18356ms]
Jul 12 21:27:25.717: INFO: Got endpoints: latency-svc-96mr7 [736.393255ms]
Jul 12 21:27:25.738: INFO: Got endpoints: latency-svc-ttj7m [729.733994ms]
Jul 12 21:27:25.765: INFO: Created: latency-svc-m6mvf
Jul 12 21:27:25.772: INFO: Got endpoints: latency-svc-hvkzt [746.361156ms]
Jul 12 21:27:25.803: INFO: Got endpoints: latency-svc-2wnzt [872.921623ms]
Jul 12 21:27:25.816: INFO: Created: latency-svc-hbmbj
Jul 12 21:27:25.819: INFO: Got endpoints: latency-svc-7gcbb [738.220305ms]
Jul 12 21:27:25.849: INFO: Created: latency-svc-gl4tr
Jul 12 21:27:25.857: INFO: Got endpoints: latency-svc-8h5jf [760.234409ms]
Jul 12 21:27:25.876: INFO: Created: latency-svc-2pmxv
Jul 12 21:27:25.883: INFO: Got endpoints: latency-svc-g87rq [671.773848ms]
Jul 12 21:27:25.897: INFO: Created: latency-svc-fbq7p
Jul 12 21:27:25.897: INFO: Got endpoints: latency-svc-txvv2 [623.853348ms]
Jul 12 21:27:25.904: INFO: Created: latency-svc-dvcg8
Jul 12 21:27:25.910: INFO: Got endpoints: latency-svc-scdfr [756.046621ms]
Jul 12 21:27:25.923: INFO: Created: latency-svc-wk6hx
Jul 12 21:27:25.925: INFO: Got endpoints: latency-svc-r2kgj [539.49595ms]
Jul 12 21:27:25.962: INFO: Created: latency-svc-2tn2t
Jul 12 21:27:25.971: INFO: Got endpoints: latency-svc-ngk99 [562.406794ms]
Jul 12 21:27:25.983: INFO: Created: latency-svc-knq2j
Jul 12 21:27:26.014: INFO: Created: latency-svc-7mrfk
Jul 12 21:27:26.023: INFO: Created: latency-svc-hmvbs
Jul 12 21:27:26.031: INFO: Got endpoints: latency-svc-88dzx [579.301157ms]
Jul 12 21:27:26.049: INFO: Created: latency-svc-ftjkz
Jul 12 21:27:26.067: INFO: Created: latency-svc-fxtf2
Jul 12 21:27:26.075: INFO: Got endpoints: latency-svc-7lgjb [522.582903ms]
Jul 12 21:27:26.095: INFO: Created: latency-svc-pnsjp
Jul 12 21:27:26.131: INFO: Got endpoints: latency-svc-svpzx [545.72569ms]
Jul 12 21:27:26.149: INFO: Created: latency-svc-c58kh
Jul 12 21:27:26.167: INFO: Got endpoints: latency-svc-m6mvf [502.390921ms]
Jul 12 21:27:26.193: INFO: Created: latency-svc-jn5xd
Jul 12 21:27:26.525: INFO: Got endpoints: latency-svc-hbmbj [807.915489ms]
Jul 12 21:27:26.833: INFO: Got endpoints: latency-svc-gl4tr [1.094218461s]
Jul 12 21:27:26.902: INFO: Got endpoints: latency-svc-2pmxv [1.129607661s]
Jul 12 21:27:26.967: INFO: Created: latency-svc-82hhv
Jul 12 21:27:27.083: INFO: Got endpoints: latency-svc-fbq7p [1.27911221s]
Jul 12 21:27:27.186: INFO: Created: latency-svc-5hv7z
Jul 12 21:27:27.194: INFO: Got endpoints: latency-svc-wk6hx [1.336555002s]
Jul 12 21:27:27.244: INFO: Created: latency-svc-cjsvd
Jul 12 21:27:27.341: INFO: Created: latency-svc-pj2w6
Jul 12 21:27:27.357: INFO: Got endpoints: latency-svc-hmvbs [1.446878396s]
Jul 12 21:27:27.383: INFO: Got endpoints: latency-svc-2tn2t [1.500125611s]
Jul 12 21:27:27.409: INFO: Created: latency-svc-2bm82
Jul 12 21:27:27.443: INFO: Got endpoints: latency-svc-fxtf2 [1.411437453s]
Jul 12 21:27:27.470: INFO: Created: latency-svc-hj8bs
Jul 12 21:27:27.471: INFO: Got endpoints: latency-svc-dvcg8 [1.651405783s]
Jul 12 21:27:27.495: INFO: Created: latency-svc-slj5w
Jul 12 21:27:27.525: INFO: Got endpoints: latency-svc-knq2j [1.599588717s]
Jul 12 21:27:27.557: INFO: Created: latency-svc-782t4
Jul 12 21:27:27.574: INFO: Got endpoints: latency-svc-7mrfk [1.676952454s]
Jul 12 21:27:27.591: INFO: Got endpoints: latency-svc-ftjkz [1.620233465s]
Jul 12 21:27:27.612: INFO: Got endpoints: latency-svc-82hhv [1.087260969s]
Jul 12 21:27:27.624: INFO: Created: latency-svc-rzc5d
Jul 12 21:27:27.636: INFO: Got endpoints: latency-svc-5hv7z [733.27226ms]
Jul 12 21:27:27.682: INFO: Created: latency-svc-9pglr
Jul 12 21:27:27.691: INFO: Got endpoints: latency-svc-pj2w6 [858.57777ms]
Jul 12 21:27:27.707: INFO: Created: latency-svc-nxrrr
Jul 12 21:27:27.722: INFO: Created: latency-svc-lnp2k
Jul 12 21:27:27.722: INFO: Got endpoints: latency-svc-2bm82 [528.430941ms]
Jul 12 21:27:27.795: INFO: Got endpoints: latency-svc-jn5xd [1.627809514s]
Jul 12 21:27:27.806: INFO: Created: latency-svc-tt89p
Jul 12 21:27:27.839: INFO: Created: latency-svc-xj46p
Jul 12 21:27:27.840: INFO: Got endpoints: latency-svc-cjsvd [757.74519ms]
Jul 12 21:27:27.860: INFO: Got endpoints: latency-svc-pnsjp [1.785816571s]
Jul 12 21:27:27.871: INFO: Got endpoints: latency-svc-c58kh [1.740092585s]
Jul 12 21:27:27.958: INFO: Created: latency-svc-h9m9r
Jul 12 21:27:27.999: INFO: Got endpoints: latency-svc-hj8bs [641.282217ms]
Jul 12 21:27:28.024: INFO: Created: latency-svc-qcx5k
Jul 12 21:27:28.036: INFO: Got endpoints: latency-svc-slj5w [648.580137ms]
Jul 12 21:27:28.069: INFO: Created: latency-svc-f9r7v
Jul 12 21:27:28.084: INFO: Got endpoints: latency-svc-nxrrr [558.143369ms]
Jul 12 21:27:28.085: INFO: Created: latency-svc-5mrdh
Jul 12 21:27:28.097: INFO: Got endpoints: latency-svc-782t4 [654.051007ms]
Jul 12 21:27:28.114: INFO: Got endpoints: latency-svc-rzc5d [642.435859ms]
Jul 12 21:27:28.141: INFO: Got endpoints: latency-svc-lnp2k [528.615993ms]
Jul 12 21:27:28.153: INFO: Got endpoints: latency-svc-h9m9r [461.990191ms]
Jul 12 21:27:28.180: INFO: Got endpoints: latency-svc-qcx5k [457.540287ms]
Jul 12 21:27:28.192: INFO: Got endpoints: latency-svc-tt89p [600.186842ms]
Jul 12 21:27:28.234: INFO: Got endpoints: latency-svc-f9r7v [439.163101ms]
Jul 12 21:27:28.271: INFO: Got endpoints: latency-svc-9pglr [695.31679ms]
Jul 12 21:27:28.271: INFO: Got endpoints: latency-svc-xj46p [634.97836ms]
Jul 12 21:27:28.273: INFO: Got endpoints: latency-svc-5mrdh [432.638083ms]
Jul 12 21:27:28.273: INFO: Latencies: [69.510094ms 77.289524ms 112.758702ms 173.335179ms 201.171764ms 211.400997ms 241.635039ms 245.768694ms 246.741825ms 260.486471ms 263.571521ms 275.882415ms 286.395575ms 292.242119ms 293.477313ms 303.452974ms 310.491494ms 317.306482ms 320.873977ms 329.871317ms 331.230855ms 337.541015ms 340.034246ms 349.861762ms 356.966749ms 358.846932ms 364.173754ms 367.133162ms 371.906709ms 375.745756ms 377.911225ms 380.769063ms 384.956561ms 394.05508ms 406.111845ms 406.776357ms 415.0211ms 417.399913ms 418.812045ms 422.621867ms 424.529084ms 428.004331ms 431.215221ms 432.638083ms 434.675451ms 436.176832ms 439.163101ms 445.775624ms 448.812414ms 449.705679ms 450.117601ms 450.615798ms 454.170114ms 457.540287ms 459.50275ms 460.91668ms 461.990191ms 465.685194ms 466.057709ms 471.775161ms 473.709469ms 479.865659ms 487.81341ms 489.747257ms 495.598196ms 495.926333ms 498.895293ms 502.390921ms 504.053637ms 505.240107ms 508.484921ms 517.423827ms 519.1559ms 521.494977ms 522.582903ms 528.430941ms 528.615993ms 530.484734ms 530.947367ms 531.344736ms 533.020422ms 535.094724ms 535.485213ms 536.40672ms 539.49595ms 540.315068ms 545.72569ms 553.24444ms 555.417328ms 555.927361ms 558.143369ms 561.923006ms 562.019497ms 562.406794ms 569.772871ms 572.3362ms 578.773874ms 579.301157ms 580.156603ms 585.576114ms 595.559677ms 596.117573ms 600.186842ms 604.49385ms 607.596038ms 608.39981ms 622.144166ms 623.853348ms 627.082813ms 634.96203ms 634.97836ms 640.42174ms 641.282217ms 642.435859ms 643.448998ms 648.580137ms 650.961382ms 654.051007ms 659.098956ms 661.13933ms 663.622667ms 664.600987ms 669.908751ms 671.773848ms 677.127122ms 682.188004ms 689.726271ms 689.88707ms 695.31679ms 716.85975ms 722.690297ms 728.18356ms 729.161123ms 729.733994ms 733.27226ms 736.393255ms 738.220305ms 746.361156ms 756.046621ms 757.74519ms 760.234409ms 762.647604ms 764.283177ms 766.460809ms 772.118342ms 772.320356ms 794.72318ms 796.449613ms 807.915489ms 808.076479ms 811.951623ms 813.810472ms 822.693047ms 826.88523ms 831.839351ms 843.379488ms 851.314702ms 858.57777ms 872.921623ms 873.357828ms 885.977286ms 886.051579ms 892.232457ms 892.580564ms 892.65642ms 917.703041ms 938.118053ms 944.399163ms 946.915895ms 958.140063ms 961.612368ms 980.590538ms 981.517438ms 987.965115ms 1.009409442s 1.014889244s 1.051094154s 1.060815591s 1.063256401s 1.077493303s 1.087260969s 1.094218461s 1.094411281s 1.11006561s 1.122926503s 1.128743209s 1.129607661s 1.27911221s 1.330254577s 1.336555002s 1.411437453s 1.446878396s 1.500125611s 1.599588717s 1.620233465s 1.627809514s 1.651405783s 1.676952454s 1.740092585s 1.785816571s]
Jul 12 21:27:28.273: INFO: 50 %ile: 595.559677ms
Jul 12 21:27:28.273: INFO: 90 %ile: 1.087260969s
Jul 12 21:27:28.273: INFO: 99 %ile: 1.740092585s
Jul 12 21:27:28.273: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:27:28.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9831" for this suite.

• [SLOW TEST:11.278 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":339,"completed":209,"skipped":3400,"failed":0}
SSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:27:28.291: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 12 21:27:28.640: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 12 21:27:28.648: INFO: starting watch
STEP: patching
STEP: updating
Jul 12 21:27:28.894: INFO: waiting for watch events with expected annotations
Jul 12 21:27:28.905: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:27:28.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-1280" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":339,"completed":210,"skipped":3406,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:27:28.988: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 12 21:27:29.762: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 21:27:32.786: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:27:32.793: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:27:36.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3798" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.407 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":339,"completed":211,"skipped":3439,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:27:37.395: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jul 12 21:27:38.387: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-857  1a12a949-848b-4108-ad7f-ca71525c9a74 31882 0 2021-07-12 21:27:38 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-07-12 21:27:38 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nwc6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nwc6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:27:38.459: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:27:40.526: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:27:42.537: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jul 12 21:27:42.537: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-857 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:27:42.537: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Verifying customized DNS server is configured on pod...
Jul 12 21:27:42.833: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-857 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:27:42.833: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 21:27:43.136: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:27:43.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-857" for this suite.

• [SLOW TEST:6.008 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":339,"completed":212,"skipped":3447,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:27:43.403: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-8726
Jul 12 21:27:44.643: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:27:46.692: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:27:48.672: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jul 12 21:27:48.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8726 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 12 21:27:49.042: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul 12 21:27:49.042: INFO: stdout: "iptables"
Jul 12 21:27:49.042: INFO: proxyMode: iptables
Jul 12 21:27:49.150: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 12 21:27:49.209: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-8726
STEP: creating replication controller affinity-clusterip-timeout in namespace services-8726
I0712 21:27:49.579291      20 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8726, replica count: 3
I0712 21:27:52.630529      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0712 21:27:55.632242      20 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 21:27:55.704: INFO: Creating new exec pod
Jul 12 21:27:58.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8726 exec execpod-affinitysrfb9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jul 12 21:28:00.923: INFO: rc: 1
Jul 12 21:28:00.923: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8726 exec execpod-affinitysrfb9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80:
Command stdout:

stderr:
+ echo hostName
+ nc -v -t -w 2 affinity-clusterip-timeout 80
nc: connect to affinity-clusterip-timeout port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 12 21:28:01.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8726 exec execpod-affinitysrfb9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jul 12 21:28:02.139: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout 80\n+ echo hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jul 12 21:28:02.140: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 21:28:02.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8726 exec execpod-affinitysrfb9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.132.150 80'
Jul 12 21:28:02.336: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.37.132.150 80\nConnection to 10.37.132.150 80 port [tcp/http] succeeded!\n"
Jul 12 21:28:02.336: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 21:28:02.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8726 exec execpod-affinitysrfb9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.37.132.150:80/ ; done'
Jul 12 21:28:02.651: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n"
Jul 12 21:28:02.651: INFO: stdout: "\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n\naffinity-clusterip-timeout-lpc2n"
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.651: INFO: Received response from host: affinity-clusterip-timeout-lpc2n
Jul 12 21:28:02.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8726 exec execpod-affinitysrfb9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.37.132.150:80/'
Jul 12 21:28:02.841: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n"
Jul 12 21:28:02.841: INFO: stdout: "affinity-clusterip-timeout-lpc2n"
Jul 12 21:28:22.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8726 exec execpod-affinitysrfb9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.37.132.150:80/'
Jul 12 21:28:23.136: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.37.132.150:80/\n"
Jul 12 21:28:23.137: INFO: stdout: "affinity-clusterip-timeout-qpv88"
Jul 12 21:28:23.137: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8726, will wait for the garbage collector to delete the pods
Jul 12 21:28:23.251: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 7.80557ms
Jul 12 21:28:23.452: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 200.63909ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:28:31.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8726" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:48.432 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":213,"skipped":3448,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:28:31.835: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:86
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:28:31.883: INFO: Creating simple deployment test-new-deployment
Jul 12 21:28:31.898: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:80
Jul 12 21:28:34.171: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9287  aff6aef7-1a9d-4480-ad7c-94693798ed60 33203 3 2021-07-12 21:28:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-07-12 21:28:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-07-12 21:28:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043a3798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-847dcfb7fb" has successfully progressed.,LastUpdateTime:2021-07-12 21:28:33 +0000 UTC,LastTransitionTime:2021-07-12 21:28:31 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-07-12 21:28:34 +0000 UTC,LastTransitionTime:2021-07-12 21:28:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 12 21:28:34.176: INFO: New ReplicaSet "test-new-deployment-847dcfb7fb" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-847dcfb7fb  deployment-9287  b2625e1e-466f-43ed-b90c-d935972ffbf2 33204 3 2021-07-12 21:28:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment aff6aef7-1a9d-4480-ad7c-94693798ed60 0xc0043a3ba7 0xc0043a3ba8}] []  [{kube-controller-manager Update apps/v1 2021-07-12 21:28:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aff6aef7-1a9d-4480-ad7c-94693798ed60\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 847dcfb7fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043a3c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 12 21:28:34.196: INFO: Pod "test-new-deployment-847dcfb7fb-6kn45" is not available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-6kn45 test-new-deployment-847dcfb7fb- deployment-9287  cc1da03c-473d-4e1a-9636-540c48311b85 33205 0 2021-07-12 21:28:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb b2625e1e-466f-43ed-b90c-d935972ffbf2 0xc004901c27 0xc004901c28}] []  [{kube-controller-manager Update v1 2021-07-12 21:28:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2625e1e-466f-43ed-b90c-d935972ffbf2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:28:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9c4k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9c4k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:28:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:28:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:28:34 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:28:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.27,PodIP:,StartTime:2021-07-12 21:28:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 12 21:28:34.197: INFO: Pod "test-new-deployment-847dcfb7fb-phqml" is available:
&Pod{ObjectMeta:{test-new-deployment-847dcfb7fb-phqml test-new-deployment-847dcfb7fb- deployment-9287  462f5be6-b92d-435d-8107-ae984df79e1d 33191 0 2021-07-12 21:28:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:847dcfb7fb] map[] [{apps/v1 ReplicaSet test-new-deployment-847dcfb7fb b2625e1e-466f-43ed-b90c-d935972ffbf2 0xc004901de7 0xc004901de8}] []  [{kube-controller-manager Update v1 2021-07-12 21:28:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2625e1e-466f-43ed-b90c-d935972ffbf2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 21:28:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.1.251\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z9qmn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z9qmn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:28:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:28:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:28:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 21:28:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.27,PodIP:10.40.1.251,StartTime:2021-07-12 21:28:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 21:28:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-1,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50,ContainerID:containerd://5f90e28bd553497ed4c0a224ea4b66351388acb1e9ccd5c5e98ea145bc2e89fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.1.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:28:34.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9287" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":339,"completed":214,"skipped":3449,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:28:34.256: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 12 21:28:34.355: INFO: Waiting up to 5m0s for pod "pod-03db6582-329c-4486-80bf-66f15ac94a8d" in namespace "emptydir-3132" to be "Succeeded or Failed"
Jul 12 21:28:34.363: INFO: Pod "pod-03db6582-329c-4486-80bf-66f15ac94a8d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.295822ms
Jul 12 21:28:36.374: INFO: Pod "pod-03db6582-329c-4486-80bf-66f15ac94a8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019205771s
STEP: Saw pod success
Jul 12 21:28:36.374: INFO: Pod "pod-03db6582-329c-4486-80bf-66f15ac94a8d" satisfied condition "Succeeded or Failed"
Jul 12 21:28:36.377: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-03db6582-329c-4486-80bf-66f15ac94a8d container test-container: <nil>
STEP: delete the pod
Jul 12 21:28:36.413: INFO: Waiting for pod pod-03db6582-329c-4486-80bf-66f15ac94a8d to disappear
Jul 12 21:28:36.419: INFO: Pod pod-03db6582-329c-4486-80bf-66f15ac94a8d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:28:36.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3132" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":215,"skipped":3469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:28:36.436: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 12 21:28:36.522: INFO: Waiting up to 5m0s for pod "pod-6ebff773-881a-41f7-b386-5713ff30030b" in namespace "emptydir-5284" to be "Succeeded or Failed"
Jul 12 21:28:36.528: INFO: Pod "pod-6ebff773-881a-41f7-b386-5713ff30030b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.565452ms
Jul 12 21:28:38.539: INFO: Pod "pod-6ebff773-881a-41f7-b386-5713ff30030b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017149507s
STEP: Saw pod success
Jul 12 21:28:38.539: INFO: Pod "pod-6ebff773-881a-41f7-b386-5713ff30030b" satisfied condition "Succeeded or Failed"
Jul 12 21:28:38.544: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-6ebff773-881a-41f7-b386-5713ff30030b container test-container: <nil>
STEP: delete the pod
Jul 12 21:28:38.597: INFO: Waiting for pod pod-6ebff773-881a-41f7-b386-5713ff30030b to disappear
Jul 12 21:28:38.603: INFO: Pod pod-6ebff773-881a-41f7-b386-5713ff30030b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:28:38.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5284" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":216,"skipped":3507,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:28:38.630: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:28:38.782: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 12 21:28:45.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3503 --namespace=crd-publish-openapi-3503 create -f -'
Jul 12 21:28:46.230: INFO: stderr: ""
Jul 12 21:28:46.230: INFO: stdout: "e2e-test-crd-publish-openapi-5818-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 12 21:28:46.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3503 --namespace=crd-publish-openapi-3503 delete e2e-test-crd-publish-openapi-5818-crds test-cr'
Jul 12 21:28:46.367: INFO: stderr: ""
Jul 12 21:28:46.367: INFO: stdout: "e2e-test-crd-publish-openapi-5818-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul 12 21:28:46.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3503 --namespace=crd-publish-openapi-3503 apply -f -'
Jul 12 21:28:46.722: INFO: stderr: ""
Jul 12 21:28:46.722: INFO: stdout: "e2e-test-crd-publish-openapi-5818-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 12 21:28:46.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3503 --namespace=crd-publish-openapi-3503 delete e2e-test-crd-publish-openapi-5818-crds test-cr'
Jul 12 21:28:46.832: INFO: stderr: ""
Jul 12 21:28:46.832: INFO: stdout: "e2e-test-crd-publish-openapi-5818-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 12 21:28:46.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-3503 explain e2e-test-crd-publish-openapi-5818-crds'
Jul 12 21:28:47.141: INFO: stderr: ""
Jul 12 21:28:47.141: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5818-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:28:51.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3503" for this suite.

• [SLOW TEST:13.311 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":339,"completed":217,"skipped":3523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:28:51.943: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name secret-emptykey-test-b936c27d-aebf-4206-9b0a-8246254e6fc4
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:28:51.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1691" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":339,"completed":218,"skipped":3578,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:28:52.003: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jul 12 21:28:52.168: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:28:54.182: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 12 21:28:55.223: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:28:56.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3868" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":339,"completed":219,"skipped":3595,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:28:56.272: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-bde1804f-f314-49ba-be0e-a451de2f9250
STEP: Creating a pod to test consume configMaps
Jul 12 21:28:56.409: INFO: Waiting up to 5m0s for pod "pod-configmaps-b52afa2c-916a-4bc5-919b-fd7fb98aa2f8" in namespace "configmap-9162" to be "Succeeded or Failed"
Jul 12 21:28:56.462: INFO: Pod "pod-configmaps-b52afa2c-916a-4bc5-919b-fd7fb98aa2f8": Phase="Pending", Reason="", readiness=false. Elapsed: 53.647192ms
Jul 12 21:28:58.488: INFO: Pod "pod-configmaps-b52afa2c-916a-4bc5-919b-fd7fb98aa2f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.078906103s
STEP: Saw pod success
Jul 12 21:28:58.488: INFO: Pod "pod-configmaps-b52afa2c-916a-4bc5-919b-fd7fb98aa2f8" satisfied condition "Succeeded or Failed"
Jul 12 21:28:58.498: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-configmaps-b52afa2c-916a-4bc5-919b-fd7fb98aa2f8 container agnhost-container: <nil>
STEP: delete the pod
Jul 12 21:28:58.537: INFO: Waiting for pod pod-configmaps-b52afa2c-916a-4bc5-919b-fd7fb98aa2f8 to disappear
Jul 12 21:28:58.549: INFO: Pod pod-configmaps-b52afa2c-916a-4bc5-919b-fd7fb98aa2f8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:28:58.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9162" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":339,"completed":220,"skipped":3606,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:28:58.570: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:28:58.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb133c07-2da8-406e-8198-964459f7616e" in namespace "projected-1468" to be "Succeeded or Failed"
Jul 12 21:28:58.702: INFO: Pod "downwardapi-volume-eb133c07-2da8-406e-8198-964459f7616e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.427139ms
Jul 12 21:29:00.715: INFO: Pod "downwardapi-volume-eb133c07-2da8-406e-8198-964459f7616e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024388354s
Jul 12 21:29:02.728: INFO: Pod "downwardapi-volume-eb133c07-2da8-406e-8198-964459f7616e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036562423s
STEP: Saw pod success
Jul 12 21:29:02.728: INFO: Pod "downwardapi-volume-eb133c07-2da8-406e-8198-964459f7616e" satisfied condition "Succeeded or Failed"
Jul 12 21:29:02.732: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-eb133c07-2da8-406e-8198-964459f7616e container client-container: <nil>
STEP: delete the pod
Jul 12 21:29:02.768: INFO: Waiting for pod downwardapi-volume-eb133c07-2da8-406e-8198-964459f7616e to disappear
Jul 12 21:29:02.785: INFO: Pod downwardapi-volume-eb133c07-2da8-406e-8198-964459f7616e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:29:02.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1468" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":339,"completed":221,"skipped":3617,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:29:02.806: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-3a6b69b8-95f3-4bd3-8fad-edc0ec16a4b3
STEP: Creating a pod to test consume secrets
Jul 12 21:29:02.928: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-464c4c41-4c4f-4e64-8f21-f1593b578d65" in namespace "projected-9554" to be "Succeeded or Failed"
Jul 12 21:29:02.933: INFO: Pod "pod-projected-secrets-464c4c41-4c4f-4e64-8f21-f1593b578d65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.153234ms
Jul 12 21:29:04.942: INFO: Pod "pod-projected-secrets-464c4c41-4c4f-4e64-8f21-f1593b578d65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014103853s
Jul 12 21:29:06.952: INFO: Pod "pod-projected-secrets-464c4c41-4c4f-4e64-8f21-f1593b578d65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023591511s
STEP: Saw pod success
Jul 12 21:29:06.952: INFO: Pod "pod-projected-secrets-464c4c41-4c4f-4e64-8f21-f1593b578d65" satisfied condition "Succeeded or Failed"
Jul 12 21:29:06.955: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-projected-secrets-464c4c41-4c4f-4e64-8f21-f1593b578d65 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 12 21:29:06.982: INFO: Waiting for pod pod-projected-secrets-464c4c41-4c4f-4e64-8f21-f1593b578d65 to disappear
Jul 12 21:29:06.990: INFO: Pod pod-projected-secrets-464c4c41-4c4f-4e64-8f21-f1593b578d65 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:29:06.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9554" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":222,"skipped":3640,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:29:07.028: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:29:13.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9443" for this suite.
STEP: Destroying namespace "nsdeletetest-9821" for this suite.
Jul 12 21:29:13.546: INFO: Namespace nsdeletetest-9821 was already deleted
STEP: Destroying namespace "nsdeletetest-1127" for this suite.

• [SLOW TEST:6.524 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":339,"completed":223,"skipped":3645,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:29:13.557: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-ec4fa69e-48c7-45d5-be91-50e8aac9d379
STEP: Creating a pod to test consume configMaps
Jul 12 21:29:13.713: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4c6dd070-2377-4528-9265-703d91af25c7" in namespace "projected-4508" to be "Succeeded or Failed"
Jul 12 21:29:13.725: INFO: Pod "pod-projected-configmaps-4c6dd070-2377-4528-9265-703d91af25c7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.900371ms
Jul 12 21:29:15.732: INFO: Pod "pod-projected-configmaps-4c6dd070-2377-4528-9265-703d91af25c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019010433s
STEP: Saw pod success
Jul 12 21:29:15.733: INFO: Pod "pod-projected-configmaps-4c6dd070-2377-4528-9265-703d91af25c7" satisfied condition "Succeeded or Failed"
Jul 12 21:29:15.735: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-projected-configmaps-4c6dd070-2377-4528-9265-703d91af25c7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 12 21:29:15.759: INFO: Waiting for pod pod-projected-configmaps-4c6dd070-2377-4528-9265-703d91af25c7 to disappear
Jul 12 21:29:15.763: INFO: Pod pod-projected-configmaps-4c6dd070-2377-4528-9265-703d91af25c7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:29:15.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4508" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":339,"completed":224,"skipped":3657,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:29:15.776: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:29:15.816: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 12 21:29:20.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-9961 --namespace=crd-publish-openapi-9961 create -f -'
Jul 12 21:29:21.149: INFO: stderr: ""
Jul 12 21:29:21.149: INFO: stdout: "e2e-test-crd-publish-openapi-6172-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 12 21:29:21.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-9961 --namespace=crd-publish-openapi-9961 delete e2e-test-crd-publish-openapi-6172-crds test-cr'
Jul 12 21:29:21.242: INFO: stderr: ""
Jul 12 21:29:21.242: INFO: stdout: "e2e-test-crd-publish-openapi-6172-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul 12 21:29:21.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-9961 --namespace=crd-publish-openapi-9961 apply -f -'
Jul 12 21:29:21.590: INFO: stderr: ""
Jul 12 21:29:21.590: INFO: stdout: "e2e-test-crd-publish-openapi-6172-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 12 21:29:21.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-9961 --namespace=crd-publish-openapi-9961 delete e2e-test-crd-publish-openapi-6172-crds test-cr'
Jul 12 21:29:21.684: INFO: stderr: ""
Jul 12 21:29:21.685: INFO: stdout: "e2e-test-crd-publish-openapi-6172-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 12 21:29:21.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=crd-publish-openapi-9961 explain e2e-test-crd-publish-openapi-6172-crds'
Jul 12 21:29:22.012: INFO: stderr: ""
Jul 12 21:29:22.012: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6172-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:29:27.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9961" for this suite.

• [SLOW TEST:11.621 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":339,"completed":225,"skipped":3659,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:29:27.398: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 12 21:29:27.494: INFO: Waiting up to 5m0s for pod "pod-20b0b4e0-7711-4eef-930b-dff854820b08" in namespace "emptydir-4873" to be "Succeeded or Failed"
Jul 12 21:29:27.500: INFO: Pod "pod-20b0b4e0-7711-4eef-930b-dff854820b08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.129464ms
Jul 12 21:29:29.507: INFO: Pod "pod-20b0b4e0-7711-4eef-930b-dff854820b08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01252953s
STEP: Saw pod success
Jul 12 21:29:29.507: INFO: Pod "pod-20b0b4e0-7711-4eef-930b-dff854820b08" satisfied condition "Succeeded or Failed"
Jul 12 21:29:29.511: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-20b0b4e0-7711-4eef-930b-dff854820b08 container test-container: <nil>
STEP: delete the pod
Jul 12 21:29:29.538: INFO: Waiting for pod pod-20b0b4e0-7711-4eef-930b-dff854820b08 to disappear
Jul 12 21:29:29.545: INFO: Pod pod-20b0b4e0-7711-4eef-930b-dff854820b08 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:29:29.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4873" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":226,"skipped":3664,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:29:29.571: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:29:29.635: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d475b1aa-8d58-47c8-a608-112f240b07a0" in namespace "projected-2683" to be "Succeeded or Failed"
Jul 12 21:29:29.644: INFO: Pod "downwardapi-volume-d475b1aa-8d58-47c8-a608-112f240b07a0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.691802ms
Jul 12 21:29:31.653: INFO: Pod "downwardapi-volume-d475b1aa-8d58-47c8-a608-112f240b07a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017907068s
STEP: Saw pod success
Jul 12 21:29:31.653: INFO: Pod "downwardapi-volume-d475b1aa-8d58-47c8-a608-112f240b07a0" satisfied condition "Succeeded or Failed"
Jul 12 21:29:31.656: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-d475b1aa-8d58-47c8-a608-112f240b07a0 container client-container: <nil>
STEP: delete the pod
Jul 12 21:29:31.681: INFO: Waiting for pod downwardapi-volume-d475b1aa-8d58-47c8-a608-112f240b07a0 to disappear
Jul 12 21:29:31.685: INFO: Pod downwardapi-volume-d475b1aa-8d58-47c8-a608-112f240b07a0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:29:31.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2683" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":227,"skipped":3678,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:29:31.699: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:29:31.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5297" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":339,"completed":228,"skipped":3701,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:29:31.804: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-8628
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating statefulset ss in namespace statefulset-8628
Jul 12 21:29:31.886: INFO: Found 0 stateful pods, waiting for 1
Jul 12 21:29:41.906: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul 12 21:29:42.014: INFO: Deleting all statefulset in ns statefulset-8628
Jul 12 21:29:42.022: INFO: Scaling statefulset ss to 0
Jul 12 21:30:02.106: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 21:30:02.109: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:30:02.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8628" for this suite.

• [SLOW TEST:30.348 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":339,"completed":229,"skipped":3714,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:30:02.155: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:30:02.217: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c2e71927-37ad-498e-9a7c-3ba739d8afc9" in namespace "projected-5322" to be "Succeeded or Failed"
Jul 12 21:30:02.222: INFO: Pod "downwardapi-volume-c2e71927-37ad-498e-9a7c-3ba739d8afc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.288983ms
Jul 12 21:30:04.227: INFO: Pod "downwardapi-volume-c2e71927-37ad-498e-9a7c-3ba739d8afc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009578623s
STEP: Saw pod success
Jul 12 21:30:04.227: INFO: Pod "downwardapi-volume-c2e71927-37ad-498e-9a7c-3ba739d8afc9" satisfied condition "Succeeded or Failed"
Jul 12 21:30:04.232: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-c2e71927-37ad-498e-9a7c-3ba739d8afc9 container client-container: <nil>
STEP: delete the pod
Jul 12 21:30:04.256: INFO: Waiting for pod downwardapi-volume-c2e71927-37ad-498e-9a7c-3ba739d8afc9 to disappear
Jul 12 21:30:04.264: INFO: Pod downwardapi-volume-c2e71927-37ad-498e-9a7c-3ba739d8afc9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:30:04.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5322" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":339,"completed":230,"skipped":3733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:30:04.278: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-1575
STEP: creating service affinity-nodeport in namespace services-1575
STEP: creating replication controller affinity-nodeport in namespace services-1575
I0712 21:30:04.379058      20 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-1575, replica count: 3
I0712 21:30:07.431738      20 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 21:30:07.547: INFO: Creating new exec pod
Jul 12 21:30:10.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1575 exec execpod-affinitymtztn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jul 12 21:30:11.857: INFO: rc: 1
Jul 12 21:30:11.857: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1575 exec execpod-affinitymtztn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80:
Command stdout:

stderr:
+ echo hostName
+ nc -v -t -w 2 affinity-nodeport 80
nc: connect to affinity-nodeport port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 12 21:30:12.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1575 exec execpod-affinitymtztn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jul 12 21:30:14.098: INFO: rc: 1
Jul 12 21:30:14.098: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1575 exec execpod-affinitymtztn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80:
Command stdout:

stderr:
+ + echonc -v hostName -t
 -w 2 affinity-nodeport 80
nc: connect to affinity-nodeport port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 12 21:30:14.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1575 exec execpod-affinitymtztn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jul 12 21:30:15.065: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jul 12 21:30:15.065: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 21:30:15.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1575 exec execpod-affinitymtztn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.140.187 80'
Jul 12 21:30:15.262: INFO: stderr: "+ nc -v -t -w 2 10.37.140.187 80\n+ echo hostName\nConnection to 10.37.140.187 80 port [tcp/http] succeeded!\n"
Jul 12 21:30:15.263: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 21:30:15.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1575 exec execpod-affinitymtztn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.26 31311'
Jul 12 21:30:15.441: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.26 31311\nConnection to 10.128.0.26 31311 port [tcp/*] succeeded!\n"
Jul 12 21:30:15.441: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 21:30:15.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1575 exec execpod-affinitymtztn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.28 31311'
Jul 12 21:30:15.620: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.28 31311\nConnection to 10.128.0.28 31311 port [tcp/*] succeeded!\n"
Jul 12 21:30:15.620: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 21:30:15.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-1575 exec execpod-affinitymtztn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.128.0.26:31311/ ; done'
Jul 12 21:30:15.979: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.128.0.26:31311/\n"
Jul 12 21:30:15.979: INFO: stdout: "\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd\naffinity-nodeport-sstjd"
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Received response from host: affinity-nodeport-sstjd
Jul 12 21:30:15.979: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1575, will wait for the garbage collector to delete the pods
Jul 12 21:30:16.058: INFO: Deleting ReplicationController affinity-nodeport took: 4.36457ms
Jul 12 21:30:16.159: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.154232ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:30:31.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1575" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:27.475 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":339,"completed":231,"skipped":3765,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:30:31.755: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jul 12 21:30:31.881: INFO: Waiting up to 5m0s for pod "security-context-9d840b00-858c-4af5-a3eb-b4bce5dc4358" in namespace "security-context-249" to be "Succeeded or Failed"
Jul 12 21:30:31.886: INFO: Pod "security-context-9d840b00-858c-4af5-a3eb-b4bce5dc4358": Phase="Pending", Reason="", readiness=false. Elapsed: 4.636791ms
Jul 12 21:30:33.894: INFO: Pod "security-context-9d840b00-858c-4af5-a3eb-b4bce5dc4358": Phase="Running", Reason="", readiness=true. Elapsed: 2.013470336s
Jul 12 21:30:35.904: INFO: Pod "security-context-9d840b00-858c-4af5-a3eb-b4bce5dc4358": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023498468s
STEP: Saw pod success
Jul 12 21:30:35.905: INFO: Pod "security-context-9d840b00-858c-4af5-a3eb-b4bce5dc4358" satisfied condition "Succeeded or Failed"
Jul 12 21:30:35.908: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod security-context-9d840b00-858c-4af5-a3eb-b4bce5dc4358 container test-container: <nil>
STEP: delete the pod
Jul 12 21:30:35.936: INFO: Waiting for pod security-context-9d840b00-858c-4af5-a3eb-b4bce5dc4358 to disappear
Jul 12 21:30:35.941: INFO: Pod security-context-9d840b00-858c-4af5-a3eb-b4bce5dc4358 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:30:35.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-249" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":339,"completed":232,"skipped":3772,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:30:35.953: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override arguments
Jul 12 21:30:36.012: INFO: Waiting up to 5m0s for pod "client-containers-511d04e4-ece2-4872-bd8e-eeb134596a48" in namespace "containers-9238" to be "Succeeded or Failed"
Jul 12 21:30:36.019: INFO: Pod "client-containers-511d04e4-ece2-4872-bd8e-eeb134596a48": Phase="Pending", Reason="", readiness=false. Elapsed: 6.598914ms
Jul 12 21:30:38.040: INFO: Pod "client-containers-511d04e4-ece2-4872-bd8e-eeb134596a48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027956112s
STEP: Saw pod success
Jul 12 21:30:38.040: INFO: Pod "client-containers-511d04e4-ece2-4872-bd8e-eeb134596a48" satisfied condition "Succeeded or Failed"
Jul 12 21:30:38.047: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod client-containers-511d04e4-ece2-4872-bd8e-eeb134596a48 container agnhost-container: <nil>
STEP: delete the pod
Jul 12 21:30:38.107: INFO: Waiting for pod client-containers-511d04e4-ece2-4872-bd8e-eeb134596a48 to disappear
Jul 12 21:30:38.112: INFO: Pod client-containers-511d04e4-ece2-4872-bd8e-eeb134596a48 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:30:38.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9238" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":339,"completed":233,"skipped":3783,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:30:38.142: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 21:30:39.516: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 21:30:42.582: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:30:42.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5355" for this suite.
STEP: Destroying namespace "webhook-5355-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":339,"completed":234,"skipped":3792,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:30:42.919: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-configmap-r74n
STEP: Creating a pod to test atomic-volume-subpath
Jul 12 21:30:43.053: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-r74n" in namespace "subpath-5626" to be "Succeeded or Failed"
Jul 12 21:30:43.058: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.843558ms
Jul 12 21:30:45.077: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023089729s
Jul 12 21:30:47.087: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Running", Reason="", readiness=true. Elapsed: 4.033265865s
Jul 12 21:30:49.097: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Running", Reason="", readiness=true. Elapsed: 6.043297434s
Jul 12 21:30:51.102: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Running", Reason="", readiness=true. Elapsed: 8.048270451s
Jul 12 21:30:53.115: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Running", Reason="", readiness=true. Elapsed: 10.062023586s
Jul 12 21:30:55.125: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Running", Reason="", readiness=true. Elapsed: 12.071648717s
Jul 12 21:30:57.138: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Running", Reason="", readiness=true. Elapsed: 14.084939656s
Jul 12 21:30:59.154: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Running", Reason="", readiness=true. Elapsed: 16.100246575s
Jul 12 21:31:01.163: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Running", Reason="", readiness=true. Elapsed: 18.109588192s
Jul 12 21:31:03.183: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Running", Reason="", readiness=true. Elapsed: 20.129837395s
Jul 12 21:31:05.192: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Running", Reason="", readiness=true. Elapsed: 22.139051727s
Jul 12 21:31:07.210: INFO: Pod "pod-subpath-test-configmap-r74n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.156209418s
STEP: Saw pod success
Jul 12 21:31:07.210: INFO: Pod "pod-subpath-test-configmap-r74n" satisfied condition "Succeeded or Failed"
Jul 12 21:31:07.223: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-subpath-test-configmap-r74n container test-container-subpath-configmap-r74n: <nil>
STEP: delete the pod
Jul 12 21:31:07.291: INFO: Waiting for pod pod-subpath-test-configmap-r74n to disappear
Jul 12 21:31:07.314: INFO: Pod pod-subpath-test-configmap-r74n no longer exists
STEP: Deleting pod pod-subpath-test-configmap-r74n
Jul 12 21:31:07.315: INFO: Deleting pod "pod-subpath-test-configmap-r74n" in namespace "subpath-5626"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:31:07.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5626" for this suite.

• [SLOW TEST:24.439 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":339,"completed":235,"skipped":3811,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:31:07.360: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:31:09.595: INFO: Deleting pod "var-expansion-a7624b8e-27e3-441f-916d-df52f0000078" in namespace "var-expansion-8698"
Jul 12 21:31:09.600: INFO: Wait up to 5m0s for pod "var-expansion-a7624b8e-27e3-441f-916d-df52f0000078" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:31:21.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8698" for this suite.

• [SLOW TEST:14.263 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":339,"completed":236,"skipped":3828,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:31:21.624: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul 12 21:31:21.684: INFO: Waiting up to 5m0s for pod "downward-api-2182fd63-b3ca-435c-ba0f-248928876812" in namespace "downward-api-8222" to be "Succeeded or Failed"
Jul 12 21:31:21.692: INFO: Pod "downward-api-2182fd63-b3ca-435c-ba0f-248928876812": Phase="Pending", Reason="", readiness=false. Elapsed: 7.427116ms
Jul 12 21:31:23.699: INFO: Pod "downward-api-2182fd63-b3ca-435c-ba0f-248928876812": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014898625s
STEP: Saw pod success
Jul 12 21:31:23.700: INFO: Pod "downward-api-2182fd63-b3ca-435c-ba0f-248928876812" satisfied condition "Succeeded or Failed"
Jul 12 21:31:23.703: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downward-api-2182fd63-b3ca-435c-ba0f-248928876812 container dapi-container: <nil>
STEP: delete the pod
Jul 12 21:31:23.719: INFO: Waiting for pod downward-api-2182fd63-b3ca-435c-ba0f-248928876812 to disappear
Jul 12 21:31:23.726: INFO: Pod downward-api-2182fd63-b3ca-435c-ba0f-248928876812 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:31:23.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8222" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":339,"completed":237,"skipped":3846,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:31:23.747: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 12 21:31:25.907: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:31:25.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8623" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":238,"skipped":3874,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:31:25.944: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4040
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4040
STEP: creating replication controller externalsvc in namespace services-4040
I0712 21:31:26.041607      20 runners.go:190] Created replication controller with name: externalsvc, namespace: services-4040, replica count: 2
I0712 21:31:29.103608      20 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jul 12 21:31:29.259: INFO: Creating new exec pod
Jul 12 21:31:31.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-4040 exec execpodcw5jv -- /bin/sh -x -c nslookup clusterip-service.services-4040.svc.cluster.local'
Jul 12 21:31:31.784: INFO: stderr: "+ nslookup clusterip-service.services-4040.svc.cluster.local\n"
Jul 12 21:31:31.784: INFO: stdout: "Server:\t\t10.37.128.10\nAddress:\t10.37.128.10#53\n\nclusterip-service.services-4040.svc.cluster.local\tcanonical name = externalsvc.services-4040.svc.cluster.local.\nName:\texternalsvc.services-4040.svc.cluster.local\nAddress: 10.37.140.93\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4040, will wait for the garbage collector to delete the pods
Jul 12 21:31:31.848: INFO: Deleting ReplicationController externalsvc took: 6.531152ms
Jul 12 21:31:31.949: INFO: Terminating ReplicationController externalsvc pods took: 100.516553ms
Jul 12 21:31:41.690: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:31:41.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4040" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:15.809 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":339,"completed":239,"skipped":3885,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:31:41.753: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-59c69078-6b65-4a63-a085-75dbbaec45d6 in namespace container-probe-5018
Jul 12 21:31:43.894: INFO: Started pod busybox-59c69078-6b65-4a63-a085-75dbbaec45d6 in namespace container-probe-5018
STEP: checking the pod's current state and verifying that restartCount is present
Jul 12 21:31:43.897: INFO: Initial restart count of pod busybox-59c69078-6b65-4a63-a085-75dbbaec45d6 is 0
Jul 12 21:32:34.195: INFO: Restart count of pod container-probe-5018/busybox-59c69078-6b65-4a63-a085-75dbbaec45d6 is now 1 (50.297865749s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:32:34.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5018" for this suite.

• [SLOW TEST:52.481 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":240,"skipped":3891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:32:34.236: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7292 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7292;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7292 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7292;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7292.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7292.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7292.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7292.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7292.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7292.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7292.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7292.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7292.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7292.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7292.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7292.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7292.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 113.140.37.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.37.140.113_udp@PTR;check="$$(dig +tcp +noall +answer +search 113.140.37.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.37.140.113_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7292 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7292;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7292 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7292;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7292.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7292.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7292.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7292.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7292.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7292.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7292.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7292.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7292.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7292.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7292.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7292.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7292.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 113.140.37.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.37.140.113_udp@PTR;check="$$(dig +tcp +noall +answer +search 113.140.37.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.37.140.113_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 21:32:38.554: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7292/dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2: the server could not find the requested resource (get pods dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2)
Jul 12 21:32:38.564: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7292/dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2: the server could not find the requested resource (get pods dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2)
Jul 12 21:32:38.571: INFO: Unable to read jessie_udp@dns-test-service.dns-7292 from pod dns-7292/dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2: the server could not find the requested resource (get pods dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2)
Jul 12 21:32:38.577: INFO: Unable to read jessie_tcp@dns-test-service.dns-7292 from pod dns-7292/dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2: the server could not find the requested resource (get pods dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2)
Jul 12 21:32:38.586: INFO: Unable to read jessie_udp@dns-test-service.dns-7292.svc from pod dns-7292/dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2: the server could not find the requested resource (get pods dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2)
Jul 12 21:32:38.594: INFO: Unable to read jessie_tcp@dns-test-service.dns-7292.svc from pod dns-7292/dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2: the server could not find the requested resource (get pods dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2)
Jul 12 21:32:38.604: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7292.svc from pod dns-7292/dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2: the server could not find the requested resource (get pods dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2)
Jul 12 21:32:38.610: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7292.svc from pod dns-7292/dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2: the server could not find the requested resource (get pods dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2)
Jul 12 21:32:38.669: INFO: Lookups using dns-7292/dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2 failed for: [jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7292 jessie_tcp@dns-test-service.dns-7292 jessie_udp@dns-test-service.dns-7292.svc jessie_tcp@dns-test-service.dns-7292.svc jessie_udp@_http._tcp.dns-test-service.dns-7292.svc jessie_tcp@_http._tcp.dns-test-service.dns-7292.svc]

Jul 12 21:32:43.990: INFO: DNS probes using dns-7292/dns-test-cb04823e-065b-43fd-92fd-d93cbf4069c2 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:32:44.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7292" for this suite.

• [SLOW TEST:10.290 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":339,"completed":241,"skipped":3930,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:32:44.525: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: starting the proxy server
Jul 12 21:32:44.656: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-6729 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:32:44.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6729" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":339,"completed":242,"skipped":3933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:32:44.784: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jul 12 21:32:44.905: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:32:51.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3233" for this suite.

• [SLOW TEST:7.040 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":339,"completed":243,"skipped":3970,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:32:51.827: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:32:51.876: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-9573
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:32:56.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-7016" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:32:56.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9573" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":339,"completed":244,"skipped":3982,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:32:56.069: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:32:56.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-927" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":339,"completed":245,"skipped":4005,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:32:56.262: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod with failed condition
STEP: updating the pod
Jul 12 21:34:57.071: INFO: Successfully updated pod "var-expansion-545b9a08-f719-45ed-ac49-fac2483b2a27"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jul 12 21:34:59.083: INFO: Deleting pod "var-expansion-545b9a08-f719-45ed-ac49-fac2483b2a27" in namespace "var-expansion-3923"
Jul 12 21:34:59.092: INFO: Wait up to 5m0s for pod "var-expansion-545b9a08-f719-45ed-ac49-fac2483b2a27" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:35:43.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3923" for this suite.

• [SLOW TEST:166.860 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":339,"completed":246,"skipped":4039,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:35:43.122: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating secret secrets-6450/secret-test-c13f78fd-06a7-4a49-88b2-024762f004da
STEP: Creating a pod to test consume secrets
Jul 12 21:35:43.388: INFO: Waiting up to 5m0s for pod "pod-configmaps-177a5791-602b-44f7-be52-4ad99a701f6d" in namespace "secrets-6450" to be "Succeeded or Failed"
Jul 12 21:35:43.410: INFO: Pod "pod-configmaps-177a5791-602b-44f7-be52-4ad99a701f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 21.734135ms
Jul 12 21:35:45.418: INFO: Pod "pod-configmaps-177a5791-602b-44f7-be52-4ad99a701f6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029999355s
STEP: Saw pod success
Jul 12 21:35:45.418: INFO: Pod "pod-configmaps-177a5791-602b-44f7-be52-4ad99a701f6d" satisfied condition "Succeeded or Failed"
Jul 12 21:35:45.422: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-configmaps-177a5791-602b-44f7-be52-4ad99a701f6d container env-test: <nil>
STEP: delete the pod
Jul 12 21:35:45.478: INFO: Waiting for pod pod-configmaps-177a5791-602b-44f7-be52-4ad99a701f6d to disappear
Jul 12 21:35:45.484: INFO: Pod pod-configmaps-177a5791-602b-44f7-be52-4ad99a701f6d no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:35:45.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6450" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":247,"skipped":4072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:35:45.496: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 12 21:35:45.573: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 12 21:36:45.634: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:36:45.644: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:36:45.800: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jul 12 21:36:45.808: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:36:45.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8316" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:36:45.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6800" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.515 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":339,"completed":248,"skipped":4101,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:36:46.013: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-6947/configmap-test-9a9d3ac5-165b-4480-8088-d40b8a284598
STEP: Creating a pod to test consume configMaps
Jul 12 21:36:46.219: INFO: Waiting up to 5m0s for pod "pod-configmaps-64f0d146-fe33-460f-9cc0-4042b46ca662" in namespace "configmap-6947" to be "Succeeded or Failed"
Jul 12 21:36:46.230: INFO: Pod "pod-configmaps-64f0d146-fe33-460f-9cc0-4042b46ca662": Phase="Pending", Reason="", readiness=false. Elapsed: 10.27639ms
Jul 12 21:36:48.241: INFO: Pod "pod-configmaps-64f0d146-fe33-460f-9cc0-4042b46ca662": Phase="Running", Reason="", readiness=true. Elapsed: 2.021933488s
Jul 12 21:36:50.250: INFO: Pod "pod-configmaps-64f0d146-fe33-460f-9cc0-4042b46ca662": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031089424s
STEP: Saw pod success
Jul 12 21:36:50.250: INFO: Pod "pod-configmaps-64f0d146-fe33-460f-9cc0-4042b46ca662" satisfied condition "Succeeded or Failed"
Jul 12 21:36:50.254: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-configmaps-64f0d146-fe33-460f-9cc0-4042b46ca662 container env-test: <nil>
STEP: delete the pod
Jul 12 21:36:50.285: INFO: Waiting for pod pod-configmaps-64f0d146-fe33-460f-9cc0-4042b46ca662 to disappear
Jul 12 21:36:50.288: INFO: Pod pod-configmaps-64f0d146-fe33-460f-9cc0-4042b46ca662 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:36:50.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6947" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":339,"completed":249,"skipped":4109,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:36:50.299: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:36:50.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-4502" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":339,"completed":250,"skipped":4136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:36:50.434: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7784.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7784.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7784.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7784.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7784.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7784.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 21:36:52.590: INFO: DNS probes using dns-7784/dns-test-2d42525c-cd11-4c64-aabe-76063da9cc0d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:36:52.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7784" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":339,"completed":251,"skipped":4185,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:36:52.625: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:36:52.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4229" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":339,"completed":252,"skipped":4214,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:36:52.829: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:36:52.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4624" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":339,"completed":253,"skipped":4260,"failed":0}
SSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:36:52.980: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:36:53.095: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-e741a648-bdef-4367-9c2b-fecbd1a910c8" in namespace "security-context-test-1515" to be "Succeeded or Failed"
Jul 12 21:36:53.102: INFO: Pod "busybox-readonly-false-e741a648-bdef-4367-9c2b-fecbd1a910c8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.392666ms
Jul 12 21:36:55.111: INFO: Pod "busybox-readonly-false-e741a648-bdef-4367-9c2b-fecbd1a910c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01529239s
Jul 12 21:36:55.111: INFO: Pod "busybox-readonly-false-e741a648-bdef-4367-9c2b-fecbd1a910c8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:36:55.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1515" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":339,"completed":254,"skipped":4264,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:36:55.124: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 12 21:36:55.203: INFO: Waiting up to 5m0s for pod "pod-3bed1b3e-0719-4a69-ac64-ec18ec841932" in namespace "emptydir-1067" to be "Succeeded or Failed"
Jul 12 21:36:55.208: INFO: Pod "pod-3bed1b3e-0719-4a69-ac64-ec18ec841932": Phase="Pending", Reason="", readiness=false. Elapsed: 4.488934ms
Jul 12 21:36:57.215: INFO: Pod "pod-3bed1b3e-0719-4a69-ac64-ec18ec841932": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011954735s
STEP: Saw pod success
Jul 12 21:36:57.215: INFO: Pod "pod-3bed1b3e-0719-4a69-ac64-ec18ec841932" satisfied condition "Succeeded or Failed"
Jul 12 21:36:57.221: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-3bed1b3e-0719-4a69-ac64-ec18ec841932 container test-container: <nil>
STEP: delete the pod
Jul 12 21:36:57.280: INFO: Waiting for pod pod-3bed1b3e-0719-4a69-ac64-ec18ec841932 to disappear
Jul 12 21:36:57.286: INFO: Pod pod-3bed1b3e-0719-4a69-ac64-ec18ec841932 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:36:57.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1067" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":255,"skipped":4277,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:36:57.297: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-2429
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 12 21:36:57.382: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 12 21:36:57.549: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:36:59.608: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:01.565: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:03.556: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:05.557: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:07.561: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:09.560: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:11.569: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:13.554: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:15.560: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:17.564: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 12 21:37:17.592: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul 12 21:37:17.604: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jul 12 21:37:19.654: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jul 12 21:37:19.654: INFO: Breadth first check of 10.40.0.41 on host 10.128.0.26...
Jul 12 21:37:19.658: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.1.36:9080/dial?request=hostname&protocol=http&host=10.40.0.41&port=8080&tries=1'] Namespace:pod-network-test-2429 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:37:19.658: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 21:37:19.760: INFO: Waiting for responses: map[]
Jul 12 21:37:19.760: INFO: reached 10.40.0.41 after 0/1 tries
Jul 12 21:37:19.760: INFO: Breadth first check of 10.40.1.35 on host 10.128.0.27...
Jul 12 21:37:19.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.1.36:9080/dial?request=hostname&protocol=http&host=10.40.1.35&port=8080&tries=1'] Namespace:pod-network-test-2429 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:37:19.764: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 21:37:19.848: INFO: Waiting for responses: map[]
Jul 12 21:37:19.848: INFO: reached 10.40.1.35 after 0/1 tries
Jul 12 21:37:19.849: INFO: Breadth first check of 10.40.2.42 on host 10.128.0.28...
Jul 12 21:37:19.852: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.1.36:9080/dial?request=hostname&protocol=http&host=10.40.2.42&port=8080&tries=1'] Namespace:pod-network-test-2429 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:37:19.852: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 21:37:19.939: INFO: Waiting for responses: map[]
Jul 12 21:37:19.940: INFO: reached 10.40.2.42 after 0/1 tries
Jul 12 21:37:19.940: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:37:19.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2429" for this suite.

• [SLOW TEST:22.652 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":339,"completed":256,"skipped":4289,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:37:19.952: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service endpoint-test2 in namespace services-4276
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4276 to expose endpoints map[]
Jul 12 21:37:20.035: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jul 12 21:37:21.049: INFO: successfully validated that service endpoint-test2 in namespace services-4276 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4276
Jul 12 21:37:21.076: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:37:23.099: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4276 to expose endpoints map[pod1:[80]]
Jul 12 21:37:23.126: INFO: successfully validated that service endpoint-test2 in namespace services-4276 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-4276
Jul 12 21:37:23.148: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:37:25.160: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4276 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 12 21:37:25.178: INFO: successfully validated that service endpoint-test2 in namespace services-4276 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-4276
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4276 to expose endpoints map[pod2:[80]]
Jul 12 21:37:25.222: INFO: successfully validated that service endpoint-test2 in namespace services-4276 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-4276
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4276 to expose endpoints map[]
Jul 12 21:37:25.261: INFO: successfully validated that service endpoint-test2 in namespace services-4276 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:37:25.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4276" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:5.391 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":339,"completed":257,"skipped":4343,"failed":0}
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:37:25.344: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Performing setup for networking test in namespace pod-network-test-3957
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 12 21:37:25.390: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 12 21:37:25.507: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:37:27.519: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:29.519: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:31.517: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:33.513: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:35.514: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:37.517: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:39.522: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:41.534: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:43.529: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 12 21:37:45.518: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 12 21:37:45.526: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jul 12 21:37:47.543: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul 12 21:37:47.549: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jul 12 21:37:51.588: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jul 12 21:37:51.588: INFO: Breadth first check of 10.40.0.43 on host 10.128.0.26...
Jul 12 21:37:51.591: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.1.39:9080/dial?request=hostname&protocol=udp&host=10.40.0.43&port=8081&tries=1'] Namespace:pod-network-test-3957 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:37:51.591: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 21:37:51.714: INFO: Waiting for responses: map[]
Jul 12 21:37:51.715: INFO: reached 10.40.0.43 after 0/1 tries
Jul 12 21:37:51.715: INFO: Breadth first check of 10.40.1.38 on host 10.128.0.27...
Jul 12 21:37:51.718: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.1.39:9080/dial?request=hostname&protocol=udp&host=10.40.1.38&port=8081&tries=1'] Namespace:pod-network-test-3957 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:37:51.718: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 21:37:51.827: INFO: Waiting for responses: map[]
Jul 12 21:37:51.827: INFO: reached 10.40.1.38 after 0/1 tries
Jul 12 21:37:51.827: INFO: Breadth first check of 10.40.2.43 on host 10.128.0.28...
Jul 12 21:37:51.831: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.40.1.39:9080/dial?request=hostname&protocol=udp&host=10.40.2.43&port=8081&tries=1'] Namespace:pod-network-test-3957 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:37:51.831: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
Jul 12 21:37:51.921: INFO: Waiting for responses: map[]
Jul 12 21:37:51.921: INFO: reached 10.40.2.43 after 0/1 tries
Jul 12 21:37:51.921: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:37:51.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3957" for this suite.

• [SLOW TEST:26.589 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":339,"completed":258,"skipped":4343,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:37:51.936: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:37:52.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3301" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":339,"completed":259,"skipped":4359,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:37:52.044: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-9000f2d5-fc35-4cc0-95e0-6e512ee8b604
STEP: Creating a pod to test consume configMaps
Jul 12 21:37:52.349: INFO: Waiting up to 5m0s for pod "pod-configmaps-8d8efbc3-f753-4be3-a604-292f873b14f3" in namespace "configmap-1347" to be "Succeeded or Failed"
Jul 12 21:37:52.374: INFO: Pod "pod-configmaps-8d8efbc3-f753-4be3-a604-292f873b14f3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.929727ms
Jul 12 21:37:54.383: INFO: Pod "pod-configmaps-8d8efbc3-f753-4be3-a604-292f873b14f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033212689s
STEP: Saw pod success
Jul 12 21:37:54.383: INFO: Pod "pod-configmaps-8d8efbc3-f753-4be3-a604-292f873b14f3" satisfied condition "Succeeded or Failed"
Jul 12 21:37:54.387: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-configmaps-8d8efbc3-f753-4be3-a604-292f873b14f3 container agnhost-container: <nil>
STEP: delete the pod
Jul 12 21:37:54.412: INFO: Waiting for pod pod-configmaps-8d8efbc3-f753-4be3-a604-292f873b14f3 to disappear
Jul 12 21:37:54.417: INFO: Pod pod-configmaps-8d8efbc3-f753-4be3-a604-292f873b14f3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:37:54.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1347" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":339,"completed":260,"skipped":4364,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:37:54.433: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 12 21:37:54.498: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 12 21:37:54.511: INFO: Waiting for terminating namespaces to be deleted...
Jul 12 21:37:54.516: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-21-default-pool-f67064dc-1pbx before test
Jul 12 21:37:54.551: INFO: fluentbit-gke-glzv5 from kube-system started at 2021-07-12 20:13:31 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.551: INFO: 	Container fluentbit ready: true, restart count 0
Jul 12 21:37:54.551: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jul 12 21:37:54.551: INFO: gke-metrics-agent-kcr97 from kube-system started at 2021-07-12 20:13:31 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.551: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jul 12 21:37:54.551: INFO: kube-dns-684cf9fd88-7bwb8 from kube-system started at 2021-07-12 20:13:38 +0000 UTC (4 container statuses recorded)
Jul 12 21:37:54.551: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 12 21:37:54.551: INFO: 	Container kubedns ready: true, restart count 0
Jul 12 21:37:54.551: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jul 12 21:37:54.551: INFO: 	Container sidecar ready: true, restart count 0
Jul 12 21:37:54.551: INFO: kube-proxy-gke-gke-1-21-default-pool-f67064dc-1pbx from kube-system started at 2021-07-12 20:11:28 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.551: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 12 21:37:54.551: INFO: pdcsi-node-7nnjh from kube-system started at 2021-07-12 20:13:31 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.551: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jul 12 21:37:54.551: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jul 12 21:37:54.551: INFO: stackdriver-metadata-agent-cluster-level-5dbdc4b7f5-mz7cm from kube-system started at 2021-07-12 20:39:43 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.551: INFO: 	Container metadata-agent ready: true, restart count 0
Jul 12 21:37:54.552: INFO: 	Container metadata-agent-nanny ready: true, restart count 0
Jul 12 21:37:54.552: INFO: netserver-0 from pod-network-test-3957 started at 2021-07-12 21:37:25 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.552: INFO: 	Container webserver ready: true, restart count 0
Jul 12 21:37:54.552: INFO: sonobuoy-e2e-job-a64b1c75ccf14259 from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.552: INFO: 	Container e2e ready: true, restart count 0
Jul 12 21:37:54.552: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 21:37:54.552: INFO: sonobuoy-systemd-logs-daemon-set-bf15683044874f68-xh7xd from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.552: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Jul 12 21:37:54.552: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 12 21:37:54.552: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-21-default-pool-f67064dc-3tj7 before test
Jul 12 21:37:54.568: INFO: fluentbit-gke-4spgg from kube-system started at 2021-07-12 20:13:34 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.568: INFO: 	Container fluentbit ready: true, restart count 0
Jul 12 21:37:54.568: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jul 12 21:37:54.568: INFO: gke-metrics-agent-wc245 from kube-system started at 2021-07-12 20:13:34 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.568: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jul 12 21:37:54.568: INFO: kube-proxy-gke-gke-1-21-default-pool-f67064dc-3tj7 from kube-system started at 2021-07-12 20:11:31 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.568: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 12 21:37:54.568: INFO: pdcsi-node-n2vm7 from kube-system started at 2021-07-12 20:13:34 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.568: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jul 12 21:37:54.568: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jul 12 21:37:54.568: INFO: netserver-1 from pod-network-test-3957 started at 2021-07-12 21:37:25 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.568: INFO: 	Container webserver ready: true, restart count 0
Jul 12 21:37:54.568: INFO: test-container-pod from pod-network-test-3957 started at 2021-07-12 21:37:47 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.568: INFO: 	Container webserver ready: true, restart count 0
Jul 12 21:37:54.568: INFO: sonobuoy-systemd-logs-daemon-set-bf15683044874f68-hqmt5 from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.568: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Jul 12 21:37:54.568: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 12 21:37:54.568: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-21-default-pool-f67064dc-g6xf before test
Jul 12 21:37:54.578: INFO: event-exporter-gke-67986489c8-2lvzn from kube-system started at 2021-07-12 20:13:35 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.578: INFO: 	Container event-exporter ready: true, restart count 0
Jul 12 21:37:54.579: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Jul 12 21:37:54.579: INFO: fluentbit-gke-26jmp from kube-system started at 2021-07-12 20:13:31 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.579: INFO: 	Container fluentbit ready: true, restart count 0
Jul 12 21:37:54.579: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jul 12 21:37:54.579: INFO: gke-metrics-agent-59rd5 from kube-system started at 2021-07-12 20:13:30 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.579: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jul 12 21:37:54.579: INFO: kube-dns-684cf9fd88-6t2mb from kube-system started at 2021-07-12 20:39:43 +0000 UTC (4 container statuses recorded)
Jul 12 21:37:54.579: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 12 21:37:54.580: INFO: 	Container kubedns ready: true, restart count 0
Jul 12 21:37:54.580: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jul 12 21:37:54.580: INFO: 	Container sidecar ready: true, restart count 0
Jul 12 21:37:54.580: INFO: kube-dns-autoscaler-844c9d9448-562v6 from kube-system started at 2021-07-12 20:13:35 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.580: INFO: 	Container autoscaler ready: true, restart count 0
Jul 12 21:37:54.580: INFO: kube-proxy-gke-gke-1-21-default-pool-f67064dc-g6xf from kube-system started at 2021-07-12 20:11:28 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.580: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 12 21:37:54.581: INFO: l7-default-backend-75b656946c-77f5m from kube-system started at 2021-07-12 20:13:39 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.581: INFO: 	Container default-http-backend ready: true, restart count 0
Jul 12 21:37:54.581: INFO: metrics-server-v0.4.4-57979b57f-kwlsr from kube-system started at 2021-07-12 20:14:03 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.581: INFO: 	Container metrics-server ready: true, restart count 0
Jul 12 21:37:54.581: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 12 21:37:54.581: INFO: pdcsi-node-5vxw2 from kube-system started at 2021-07-12 20:13:30 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.582: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jul 12 21:37:54.582: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jul 12 21:37:54.582: INFO: netserver-2 from pod-network-test-3957 started at 2021-07-12 21:37:25 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.582: INFO: 	Container webserver ready: true, restart count 0
Jul 12 21:37:54.582: INFO: sonobuoy from sonobuoy started at 2021-07-12 20:29:56 +0000 UTC (1 container statuses recorded)
Jul 12 21:37:54.582: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 12 21:37:54.582: INFO: sonobuoy-systemd-logs-daemon-set-bf15683044874f68-fgclf from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 21:37:54.583: INFO: 	Container sonobuoy-worker ready: false, restart count 6
Jul 12 21:37:54.583: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-4f006476-9d18-408f-9e9e-ba0734259d28 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.128.0.27 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-4f006476-9d18-408f-9e9e-ba0734259d28 off the node gke-gke-1-21-default-pool-f67064dc-3tj7
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4f006476-9d18-408f-9e9e-ba0734259d28
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:43:00.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4062" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:306.415 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":339,"completed":261,"skipped":4364,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:43:00.849: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:43:04.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3218" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":339,"completed":262,"skipped":4373,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:43:04.846: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:43:07.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-1663" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support unsafe sysctls which are actually allowed [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":263,"skipped":4389,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:43:07.038: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:43:18.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6789" for this suite.

• [SLOW TEST:11.201 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":339,"completed":264,"skipped":4389,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:43:18.243: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:43:29.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7516" for this suite.

• [SLOW TEST:11.248 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":339,"completed":265,"skipped":4454,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:43:29.492: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:43:40.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8185" for this suite.

• [SLOW TEST:11.472 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":339,"completed":266,"skipped":4459,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:43:40.969: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating projection with secret that has name projected-secret-test-b7fa1a5b-4bca-4e2f-89d8-45ad7149cd80
STEP: Creating a pod to test consume secrets
Jul 12 21:43:41.147: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-281a72aa-4241-4298-b26b-1062303a1358" in namespace "projected-8366" to be "Succeeded or Failed"
Jul 12 21:43:41.158: INFO: Pod "pod-projected-secrets-281a72aa-4241-4298-b26b-1062303a1358": Phase="Pending", Reason="", readiness=false. Elapsed: 10.709596ms
Jul 12 21:43:43.171: INFO: Pod "pod-projected-secrets-281a72aa-4241-4298-b26b-1062303a1358": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024267297s
STEP: Saw pod success
Jul 12 21:43:43.171: INFO: Pod "pod-projected-secrets-281a72aa-4241-4298-b26b-1062303a1358" satisfied condition "Succeeded or Failed"
Jul 12 21:43:43.174: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-projected-secrets-281a72aa-4241-4298-b26b-1062303a1358 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 12 21:43:43.268: INFO: Waiting for pod pod-projected-secrets-281a72aa-4241-4298-b26b-1062303a1358 to disappear
Jul 12 21:43:43.274: INFO: Pod pod-projected-secrets-281a72aa-4241-4298-b26b-1062303a1358 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:43:43.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8366" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":267,"skipped":4463,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:43:43.291: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name s-test-opt-del-5fbf1a29-e82c-4d3d-bdf7-9383ed66c194
STEP: Creating secret with name s-test-opt-upd-6db427ef-9e26-4e5b-a2d7-e69c549b6574
STEP: Creating the pod
Jul 12 21:43:43.453: INFO: The status of Pod pod-projected-secrets-287be518-6eaa-407d-b0d4-8691866e9bdb is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:43:45.472: INFO: The status of Pod pod-projected-secrets-287be518-6eaa-407d-b0d4-8691866e9bdb is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:43:47.474: INFO: The status of Pod pod-projected-secrets-287be518-6eaa-407d-b0d4-8691866e9bdb is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-5fbf1a29-e82c-4d3d-bdf7-9383ed66c194
STEP: Updating secret s-test-opt-upd-6db427ef-9e26-4e5b-a2d7-e69c549b6574
STEP: Creating secret with name s-test-opt-create-64560546-f043-4f54-9288-e1acc6cfdad3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:44:53.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2543" for this suite.

• [SLOW TEST:70.680 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":268,"skipped":4468,"failed":0}
SSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:44:53.972: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:44:54.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2486" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":339,"completed":269,"skipped":4476,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:44:54.105: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1548
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jul 12 21:44:54.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1699 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1 --labels=run=e2e-test-httpd-pod'
Jul 12 21:44:54.798: INFO: stderr: ""
Jul 12 21:44:54.798: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jul 12 21:44:59.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1699 get pod e2e-test-httpd-pod -o json'
Jul 12 21:44:59.981: INFO: stderr: ""
Jul 12 21:44:59.982: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-07-12T21:44:54Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1699\",\n        \"resourceVersion\": \"39707\",\n        \"uid\": \"15517bac-2562-493d-8f87-8cdf0a089477\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-7hdpr\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"gke-gke-1-21-default-pool-f67064dc-3tj7\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-7hdpr\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-12T21:44:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-12T21:44:56Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-12T21:44:56Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-07-12T21:44:54Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://b4d3ee2be1b4487fa2dfff7a3a89b1eeac826e85ea605dd80c1efaa025a30662\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-1\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-07-12T21:44:55Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.128.0.27\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.40.1.48\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.40.1.48\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-07-12T21:44:54Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 12 21:44:59.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1699 replace -f -'
Jul 12 21:45:00.442: INFO: stderr: ""
Jul 12 21:45:00.442: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-1
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1552
Jul 12 21:45:00.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-1699 delete pods e2e-test-httpd-pod'
Jul 12 21:45:11.655: INFO: stderr: ""
Jul 12 21:45:11.655: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:45:11.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1699" for this suite.

• [SLOW TEST:17.635 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":339,"completed":270,"skipped":4477,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:45:11.741: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service nodeport-test with type=NodePort in namespace services-8118
STEP: creating replication controller nodeport-test in namespace services-8118
I0712 21:45:11.987039      20 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-8118, replica count: 2
I0712 21:45:15.037531      20 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 21:45:15.037: INFO: Creating new exec pod
Jul 12 21:45:18.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8118 exec execpod59w74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul 12 21:45:19.378: INFO: rc: 1
Jul 12 21:45:19.378: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8118 exec execpod59w74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80:
Command stdout:

stderr:
+ echo hostName
+ nc -v -t -w 2 nodeport-test 80
nc: connect to nodeport-test port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 12 21:45:20.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8118 exec execpod59w74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul 12 21:45:21.617: INFO: rc: 1
Jul 12 21:45:21.617: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8118 exec execpod59w74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80:
Command stdout:

stderr:
+ echo hostName
+ nc -v -t -w 2 nodeport-test 80
nc: connect to nodeport-test port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 12 21:45:22.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8118 exec execpod59w74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul 12 21:45:22.582: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 12 21:45:22.582: INFO: stdout: ""
Jul 12 21:45:23.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8118 exec execpod59w74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul 12 21:45:23.626: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 12 21:45:23.626: INFO: stdout: ""
Jul 12 21:45:24.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8118 exec execpod59w74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul 12 21:45:24.588: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 12 21:45:24.588: INFO: stdout: "nodeport-test-642r9"
Jul 12 21:45:24.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8118 exec execpod59w74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.138.41 80'
Jul 12 21:45:24.780: INFO: stderr: "+ nc -v -t -w 2 10.37.138.41 80\n+ echo hostName\nConnection to 10.37.138.41 80 port [tcp/http] succeeded!\n"
Jul 12 21:45:24.780: INFO: stdout: "nodeport-test-642r9"
Jul 12 21:45:24.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8118 exec execpod59w74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.26 30825'
Jul 12 21:45:24.995: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.128.0.26 30825\nConnection to 10.128.0.26 30825 port [tcp/*] succeeded!\n"
Jul 12 21:45:24.995: INFO: stdout: "nodeport-test-vmt7z"
Jul 12 21:45:24.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-8118 exec execpod59w74 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.128.0.27 30825'
Jul 12 21:45:25.215: INFO: stderr: "+ nc -v -t -w 2 10.128.0.27 30825\n+ echo hostName\nConnection to 10.128.0.27 30825 port [tcp/*] succeeded!\n"
Jul 12 21:45:25.215: INFO: stdout: "nodeport-test-642r9"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:45:25.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8118" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:13.498 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":339,"completed":271,"skipped":4484,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:45:25.239: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:45:25.322: INFO: Got root ca configmap in namespace "svcaccounts-7288"
Jul 12 21:45:25.331: INFO: Deleted root ca configmap in namespace "svcaccounts-7288"
STEP: waiting for a new root ca configmap created
Jul 12 21:45:25.838: INFO: Recreated root ca configmap in namespace "svcaccounts-7288"
Jul 12 21:45:25.843: INFO: Updated root ca configmap in namespace "svcaccounts-7288"
STEP: waiting for the root ca configmap reconciled
Jul 12 21:45:26.391: INFO: Reconciled root ca configmap in namespace "svcaccounts-7288"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:45:26.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7288" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":339,"completed":272,"skipped":4507,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:45:26.588: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 12 21:45:26.839: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 12 21:45:26.885: INFO: Waiting for terminating namespaces to be deleted...
Jul 12 21:45:26.889: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-21-default-pool-f67064dc-1pbx before test
Jul 12 21:45:26.900: INFO: fluentbit-gke-glzv5 from kube-system started at 2021-07-12 20:13:31 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.901: INFO: 	Container fluentbit ready: true, restart count 0
Jul 12 21:45:26.901: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jul 12 21:45:26.902: INFO: gke-metrics-agent-kcr97 from kube-system started at 2021-07-12 20:13:31 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:26.902: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jul 12 21:45:26.902: INFO: kube-dns-684cf9fd88-7bwb8 from kube-system started at 2021-07-12 20:13:38 +0000 UTC (4 container statuses recorded)
Jul 12 21:45:26.903: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 12 21:45:26.903: INFO: 	Container kubedns ready: true, restart count 0
Jul 12 21:45:26.903: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jul 12 21:45:26.904: INFO: 	Container sidecar ready: true, restart count 0
Jul 12 21:45:26.904: INFO: kube-proxy-gke-gke-1-21-default-pool-f67064dc-1pbx from kube-system started at 2021-07-12 20:11:28 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:26.904: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 12 21:45:26.904: INFO: pdcsi-node-7nnjh from kube-system started at 2021-07-12 20:13:31 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.905: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jul 12 21:45:26.905: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jul 12 21:45:26.906: INFO: stackdriver-metadata-agent-cluster-level-5dbdc4b7f5-mz7cm from kube-system started at 2021-07-12 20:39:43 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.906: INFO: 	Container metadata-agent ready: true, restart count 0
Jul 12 21:45:26.906: INFO: 	Container metadata-agent-nanny ready: true, restart count 0
Jul 12 21:45:26.907: INFO: sonobuoy-e2e-job-a64b1c75ccf14259 from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.908: INFO: 	Container e2e ready: true, restart count 0
Jul 12 21:45:26.908: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 21:45:26.919: INFO: sonobuoy-systemd-logs-daemon-set-bf15683044874f68-xh7xd from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.919: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Jul 12 21:45:26.919: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 12 21:45:26.919: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-21-default-pool-f67064dc-3tj7 before test
Jul 12 21:45:26.936: INFO: fluentbit-gke-4spgg from kube-system started at 2021-07-12 20:13:34 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.936: INFO: 	Container fluentbit ready: true, restart count 0
Jul 12 21:45:26.936: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jul 12 21:45:26.936: INFO: gke-metrics-agent-wc245 from kube-system started at 2021-07-12 20:13:34 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:26.936: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jul 12 21:45:26.936: INFO: kube-proxy-gke-gke-1-21-default-pool-f67064dc-3tj7 from kube-system started at 2021-07-12 20:11:31 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:26.936: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 12 21:45:26.936: INFO: pdcsi-node-n2vm7 from kube-system started at 2021-07-12 20:13:34 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.937: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jul 12 21:45:26.937: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jul 12 21:45:26.937: INFO: execpod59w74 from services-8118 started at 2021-07-12 21:45:15 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:26.937: INFO: 	Container agnhost-container ready: true, restart count 0
Jul 12 21:45:26.937: INFO: nodeport-test-642r9 from services-8118 started at 2021-07-12 21:45:12 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:26.937: INFO: 	Container nodeport-test ready: true, restart count 0
Jul 12 21:45:26.937: INFO: nodeport-test-vmt7z from services-8118 started at 2021-07-12 21:45:12 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:26.937: INFO: 	Container nodeport-test ready: true, restart count 0
Jul 12 21:45:26.937: INFO: sonobuoy-systemd-logs-daemon-set-bf15683044874f68-hqmt5 from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.938: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Jul 12 21:45:26.938: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 12 21:45:26.938: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-21-default-pool-f67064dc-g6xf before test
Jul 12 21:45:26.960: INFO: event-exporter-gke-67986489c8-2lvzn from kube-system started at 2021-07-12 20:13:35 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.960: INFO: 	Container event-exporter ready: true, restart count 0
Jul 12 21:45:26.960: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Jul 12 21:45:26.960: INFO: fluentbit-gke-26jmp from kube-system started at 2021-07-12 20:13:31 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.961: INFO: 	Container fluentbit ready: true, restart count 0
Jul 12 21:45:26.961: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jul 12 21:45:26.961: INFO: gke-metrics-agent-59rd5 from kube-system started at 2021-07-12 20:13:30 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:26.961: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jul 12 21:45:26.961: INFO: kube-dns-684cf9fd88-6t2mb from kube-system started at 2021-07-12 20:39:43 +0000 UTC (4 container statuses recorded)
Jul 12 21:45:26.961: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 12 21:45:26.962: INFO: 	Container kubedns ready: true, restart count 0
Jul 12 21:45:26.962: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jul 12 21:45:26.962: INFO: 	Container sidecar ready: true, restart count 0
Jul 12 21:45:26.962: INFO: kube-dns-autoscaler-844c9d9448-562v6 from kube-system started at 2021-07-12 20:13:35 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:26.962: INFO: 	Container autoscaler ready: true, restart count 0
Jul 12 21:45:26.962: INFO: kube-proxy-gke-gke-1-21-default-pool-f67064dc-g6xf from kube-system started at 2021-07-12 20:11:28 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:26.962: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 12 21:45:26.962: INFO: l7-default-backend-75b656946c-77f5m from kube-system started at 2021-07-12 20:13:39 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:26.963: INFO: 	Container default-http-backend ready: true, restart count 0
Jul 12 21:45:26.963: INFO: metrics-server-v0.4.4-57979b57f-kwlsr from kube-system started at 2021-07-12 20:14:03 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.963: INFO: 	Container metrics-server ready: true, restart count 0
Jul 12 21:45:26.963: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 12 21:45:26.963: INFO: pdcsi-node-5vxw2 from kube-system started at 2021-07-12 20:13:30 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.963: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jul 12 21:45:26.963: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jul 12 21:45:26.963: INFO: sonobuoy from sonobuoy started at 2021-07-12 20:29:56 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:26.964: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 12 21:45:26.964: INFO: sonobuoy-systemd-logs-daemon-set-bf15683044874f68-fgclf from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:26.964: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Jul 12 21:45:26.964: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16912947625358ad], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:45:28.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9610" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":339,"completed":273,"skipped":4514,"failed":0}
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:35
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:45:28.055: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:64
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:45:28.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7039" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":339,"completed":274,"skipped":4520,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:45:28.204: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-upd-8cb99bf9-acca-437d-872c-083b1cc9cfe7
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:45:30.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3676" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":339,"completed":275,"skipped":4533,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:45:30.321: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:45:43.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8840" for this suite.

• [SLOW TEST:13.360 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":339,"completed":276,"skipped":4534,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:45:43.682: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating secret with name secret-test-8d1c6857-75f6-4e74-a24f-54e75bf7a587
STEP: Creating a pod to test consume secrets
Jul 12 21:45:44.248: INFO: Waiting up to 5m0s for pod "pod-secrets-da7d27cd-af9a-4d30-91f3-d80b61100f0a" in namespace "secrets-1464" to be "Succeeded or Failed"
Jul 12 21:45:44.390: INFO: Pod "pod-secrets-da7d27cd-af9a-4d30-91f3-d80b61100f0a": Phase="Pending", Reason="", readiness=false. Elapsed: 142.185821ms
Jul 12 21:45:46.400: INFO: Pod "pod-secrets-da7d27cd-af9a-4d30-91f3-d80b61100f0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.151774453s
STEP: Saw pod success
Jul 12 21:45:46.400: INFO: Pod "pod-secrets-da7d27cd-af9a-4d30-91f3-d80b61100f0a" satisfied condition "Succeeded or Failed"
Jul 12 21:45:46.402: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-secrets-da7d27cd-af9a-4d30-91f3-d80b61100f0a container secret-volume-test: <nil>
STEP: delete the pod
Jul 12 21:45:46.419: INFO: Waiting for pod pod-secrets-da7d27cd-af9a-4d30-91f3-d80b61100f0a to disappear
Jul 12 21:45:46.423: INFO: Pod pod-secrets-da7d27cd-af9a-4d30-91f3-d80b61100f0a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:45:46.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1464" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":277,"skipped":4573,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:45:46.436: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in container's command
Jul 12 21:45:46.520: INFO: Waiting up to 5m0s for pod "var-expansion-739f914e-49dd-4449-b5fe-d9ac7f390076" in namespace "var-expansion-9878" to be "Succeeded or Failed"
Jul 12 21:45:46.546: INFO: Pod "var-expansion-739f914e-49dd-4449-b5fe-d9ac7f390076": Phase="Pending", Reason="", readiness=false. Elapsed: 26.068063ms
Jul 12 21:45:48.554: INFO: Pod "var-expansion-739f914e-49dd-4449-b5fe-d9ac7f390076": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033451434s
STEP: Saw pod success
Jul 12 21:45:48.554: INFO: Pod "var-expansion-739f914e-49dd-4449-b5fe-d9ac7f390076" satisfied condition "Succeeded or Failed"
Jul 12 21:45:48.560: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod var-expansion-739f914e-49dd-4449-b5fe-d9ac7f390076 container dapi-container: <nil>
STEP: delete the pod
Jul 12 21:45:48.587: INFO: Waiting for pod var-expansion-739f914e-49dd-4449-b5fe-d9ac7f390076 to disappear
Jul 12 21:45:48.592: INFO: Pod var-expansion-739f914e-49dd-4449-b5fe-d9ac7f390076 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:45:48.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9878" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":339,"completed":278,"skipped":4574,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:45:48.619: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-542322d0-c4f8-43ad-89b4-fade390a6e91
STEP: Creating a pod to test consume configMaps
Jul 12 21:45:48.743: INFO: Waiting up to 5m0s for pod "pod-configmaps-2227ac8e-190e-4fec-8c1c-8e64ca83843d" in namespace "configmap-8297" to be "Succeeded or Failed"
Jul 12 21:45:48.772: INFO: Pod "pod-configmaps-2227ac8e-190e-4fec-8c1c-8e64ca83843d": Phase="Pending", Reason="", readiness=false. Elapsed: 28.147106ms
Jul 12 21:45:50.778: INFO: Pod "pod-configmaps-2227ac8e-190e-4fec-8c1c-8e64ca83843d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034723188s
STEP: Saw pod success
Jul 12 21:45:50.778: INFO: Pod "pod-configmaps-2227ac8e-190e-4fec-8c1c-8e64ca83843d" satisfied condition "Succeeded or Failed"
Jul 12 21:45:50.781: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-configmaps-2227ac8e-190e-4fec-8c1c-8e64ca83843d container agnhost-container: <nil>
STEP: delete the pod
Jul 12 21:45:50.807: INFO: Waiting for pod pod-configmaps-2227ac8e-190e-4fec-8c1c-8e64ca83843d to disappear
Jul 12 21:45:50.811: INFO: Pod pod-configmaps-2227ac8e-190e-4fec-8c1c-8e64ca83843d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:45:50.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8297" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":339,"completed":279,"skipped":4577,"failed":0}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:45:50.821: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 12 21:45:50.865: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 12 21:45:50.872: INFO: Waiting for terminating namespaces to be deleted...
Jul 12 21:45:50.875: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-21-default-pool-f67064dc-1pbx before test
Jul 12 21:45:50.883: INFO: fluentbit-gke-glzv5 from kube-system started at 2021-07-12 20:13:31 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.884: INFO: 	Container fluentbit ready: true, restart count 0
Jul 12 21:45:50.884: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jul 12 21:45:50.884: INFO: gke-metrics-agent-kcr97 from kube-system started at 2021-07-12 20:13:31 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:50.884: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jul 12 21:45:50.884: INFO: kube-dns-684cf9fd88-7bwb8 from kube-system started at 2021-07-12 20:13:38 +0000 UTC (4 container statuses recorded)
Jul 12 21:45:50.884: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 12 21:45:50.884: INFO: 	Container kubedns ready: true, restart count 0
Jul 12 21:45:50.884: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jul 12 21:45:50.884: INFO: 	Container sidecar ready: true, restart count 0
Jul 12 21:45:50.885: INFO: kube-proxy-gke-gke-1-21-default-pool-f67064dc-1pbx from kube-system started at 2021-07-12 20:11:28 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:50.885: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 12 21:45:50.885: INFO: pdcsi-node-7nnjh from kube-system started at 2021-07-12 20:13:31 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.885: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jul 12 21:45:50.885: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jul 12 21:45:50.885: INFO: stackdriver-metadata-agent-cluster-level-5dbdc4b7f5-mz7cm from kube-system started at 2021-07-12 20:39:43 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.885: INFO: 	Container metadata-agent ready: true, restart count 0
Jul 12 21:45:50.886: INFO: 	Container metadata-agent-nanny ready: true, restart count 0
Jul 12 21:45:50.886: INFO: sonobuoy-e2e-job-a64b1c75ccf14259 from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.886: INFO: 	Container e2e ready: true, restart count 0
Jul 12 21:45:50.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 21:45:50.886: INFO: sonobuoy-systemd-logs-daemon-set-bf15683044874f68-xh7xd from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.886: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Jul 12 21:45:50.886: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 12 21:45:50.887: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-21-default-pool-f67064dc-3tj7 before test
Jul 12 21:45:50.896: INFO: fluentbit-gke-4spgg from kube-system started at 2021-07-12 20:13:34 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.896: INFO: 	Container fluentbit ready: true, restart count 0
Jul 12 21:45:50.896: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jul 12 21:45:50.896: INFO: gke-metrics-agent-wc245 from kube-system started at 2021-07-12 20:13:34 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:50.896: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jul 12 21:45:50.896: INFO: kube-proxy-gke-gke-1-21-default-pool-f67064dc-3tj7 from kube-system started at 2021-07-12 20:11:31 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:50.896: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 12 21:45:50.896: INFO: pdcsi-node-n2vm7 from kube-system started at 2021-07-12 20:13:34 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.896: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jul 12 21:45:50.896: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jul 12 21:45:50.896: INFO: sonobuoy-systemd-logs-daemon-set-bf15683044874f68-hqmt5 from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.897: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Jul 12 21:45:50.897: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 12 21:45:50.897: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-21-default-pool-f67064dc-g6xf before test
Jul 12 21:45:50.907: INFO: event-exporter-gke-67986489c8-2lvzn from kube-system started at 2021-07-12 20:13:35 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.907: INFO: 	Container event-exporter ready: true, restart count 0
Jul 12 21:45:50.907: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Jul 12 21:45:50.907: INFO: fluentbit-gke-26jmp from kube-system started at 2021-07-12 20:13:31 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.907: INFO: 	Container fluentbit ready: true, restart count 0
Jul 12 21:45:50.907: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jul 12 21:45:50.907: INFO: gke-metrics-agent-59rd5 from kube-system started at 2021-07-12 20:13:30 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:50.907: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jul 12 21:45:50.907: INFO: kube-dns-684cf9fd88-6t2mb from kube-system started at 2021-07-12 20:39:43 +0000 UTC (4 container statuses recorded)
Jul 12 21:45:50.907: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 12 21:45:50.907: INFO: 	Container kubedns ready: true, restart count 0
Jul 12 21:45:50.907: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jul 12 21:45:50.907: INFO: 	Container sidecar ready: true, restart count 0
Jul 12 21:45:50.907: INFO: kube-dns-autoscaler-844c9d9448-562v6 from kube-system started at 2021-07-12 20:13:35 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:50.907: INFO: 	Container autoscaler ready: true, restart count 0
Jul 12 21:45:50.907: INFO: kube-proxy-gke-gke-1-21-default-pool-f67064dc-g6xf from kube-system started at 2021-07-12 20:11:28 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:50.907: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 12 21:45:50.907: INFO: l7-default-backend-75b656946c-77f5m from kube-system started at 2021-07-12 20:13:39 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:50.907: INFO: 	Container default-http-backend ready: true, restart count 0
Jul 12 21:45:50.907: INFO: metrics-server-v0.4.4-57979b57f-kwlsr from kube-system started at 2021-07-12 20:14:03 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.907: INFO: 	Container metrics-server ready: true, restart count 0
Jul 12 21:45:50.907: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 12 21:45:50.907: INFO: pdcsi-node-5vxw2 from kube-system started at 2021-07-12 20:13:30 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.907: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jul 12 21:45:50.907: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jul 12 21:45:50.907: INFO: sonobuoy from sonobuoy started at 2021-07-12 20:29:56 +0000 UTC (1 container statuses recorded)
Jul 12 21:45:50.907: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 12 21:45:50.907: INFO: sonobuoy-systemd-logs-daemon-set-bf15683044874f68-fgclf from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 21:45:50.907: INFO: 	Container sonobuoy-worker ready: false, restart count 7
Jul 12 21:45:50.907: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b31ace47-9501-45db-87b3-b95de1589be3 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b31ace47-9501-45db-87b3-b95de1589be3 off the node gke-gke-1-21-default-pool-f67064dc-3tj7
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b31ace47-9501-45db-87b3-b95de1589be3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:45:55.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7125" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":339,"completed":280,"skipped":4580,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:45:55.180: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0712 21:45:56.373222      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0712 21:45:56.373612      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0712 21:45:56.373990      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 12 21:45:56.374: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:45:56.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9394" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":339,"completed":281,"skipped":4600,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:45:56.430: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod busybox-1fd4a1ab-8b5b-4c7e-9569-487477b18292 in namespace container-probe-1765
Jul 12 21:45:58.727: INFO: Started pod busybox-1fd4a1ab-8b5b-4c7e-9569-487477b18292 in namespace container-probe-1765
STEP: checking the pod's current state and verifying that restartCount is present
Jul 12 21:45:58.736: INFO: Initial restart count of pod busybox-1fd4a1ab-8b5b-4c7e-9569-487477b18292 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:50:00.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1765" for this suite.

• [SLOW TEST:244.046 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":339,"completed":282,"skipped":4602,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:50:00.481: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:50:00.603: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4807396d-700e-458d-823f-cce2a3cf19da" in namespace "projected-8633" to be "Succeeded or Failed"
Jul 12 21:50:00.626: INFO: Pod "downwardapi-volume-4807396d-700e-458d-823f-cce2a3cf19da": Phase="Pending", Reason="", readiness=false. Elapsed: 23.439026ms
Jul 12 21:50:02.631: INFO: Pod "downwardapi-volume-4807396d-700e-458d-823f-cce2a3cf19da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028333801s
Jul 12 21:50:04.653: INFO: Pod "downwardapi-volume-4807396d-700e-458d-823f-cce2a3cf19da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050230165s
STEP: Saw pod success
Jul 12 21:50:04.653: INFO: Pod "downwardapi-volume-4807396d-700e-458d-823f-cce2a3cf19da" satisfied condition "Succeeded or Failed"
Jul 12 21:50:04.661: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-4807396d-700e-458d-823f-cce2a3cf19da container client-container: <nil>
STEP: delete the pod
Jul 12 21:50:04.725: INFO: Waiting for pod downwardapi-volume-4807396d-700e-458d-823f-cce2a3cf19da to disappear
Jul 12 21:50:04.732: INFO: Pod downwardapi-volume-4807396d-700e-458d-823f-cce2a3cf19da no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:50:04.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8633" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":339,"completed":283,"skipped":4619,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:50:04.760: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jul 12 21:50:05.951: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0712 21:50:05.951153      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0712 21:50:05.951196      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0712 21:50:05.951203      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:50:05.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1975" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":339,"completed":284,"skipped":4637,"failed":0}
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:50:05.963: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 12 21:50:08.101: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:50:08.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4490" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":339,"completed":285,"skipped":4638,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:50:08.163: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jul 12 21:50:08.282: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:50:10.291: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.128.0.26 on the node which pod1 resides and expect scheduled
Jul 12 21:50:10.309: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:50:12.324: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.128.0.26 but use UDP protocol on the node which pod2 resides
Jul 12 21:50:12.346: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:50:14.360: INFO: The status of Pod pod3 is Running (Ready = true)
Jul 12 21:50:14.378: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:50:16.388: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jul 12 21:50:16.390: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.128.0.26 http://127.0.0.1:54323/hostname] Namespace:hostport-338 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:50:16.390: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.128.0.26, port: 54323
Jul 12 21:50:16.485: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.128.0.26:54323/hostname] Namespace:hostport-338 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:50:16.485: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.128.0.26, port: 54323 UDP
Jul 12 21:50:16.564: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.128.0.26 54323] Namespace:hostport-338 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 12 21:50:16.564: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:50:21.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-338" for this suite.

• [SLOW TEST:13.486 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":339,"completed":286,"skipped":4662,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:50:21.650: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:50:23.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1588" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":339,"completed":287,"skipped":4673,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:50:23.822: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jul 12 21:50:24.050: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:50:24.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-24" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":339,"completed":288,"skipped":4684,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:50:24.087: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9640.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9640.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9640.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9640.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9640.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9640.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9640.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9640.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9640.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9640.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9640.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9640.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9640.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 91.136.37.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.37.136.91_udp@PTR;check="$$(dig +tcp +noall +answer +search 91.136.37.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.37.136.91_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9640.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9640.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9640.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9640.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9640.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9640.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9640.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9640.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9640.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9640.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9640.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9640.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9640.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 91.136.37.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.37.136.91_udp@PTR;check="$$(dig +tcp +noall +answer +search 91.136.37.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.37.136.91_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 12 21:50:28.471: INFO: DNS probes using dns-9640/dns-test-e27ae390-7d08-47be-a836-4165d4b91fae succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:50:28.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9640" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":339,"completed":289,"skipped":4694,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:50:28.558: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:50:32.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7346" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":339,"completed":290,"skipped":4713,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:50:32.679: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:50:32.831: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1ba24e0-dc84-4eb8-b31e-4f797467bdcd" in namespace "projected-927" to be "Succeeded or Failed"
Jul 12 21:50:32.908: INFO: Pod "downwardapi-volume-d1ba24e0-dc84-4eb8-b31e-4f797467bdcd": Phase="Pending", Reason="", readiness=false. Elapsed: 76.441007ms
Jul 12 21:50:34.921: INFO: Pod "downwardapi-volume-d1ba24e0-dc84-4eb8-b31e-4f797467bdcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.089930538s
STEP: Saw pod success
Jul 12 21:50:34.921: INFO: Pod "downwardapi-volume-d1ba24e0-dc84-4eb8-b31e-4f797467bdcd" satisfied condition "Succeeded or Failed"
Jul 12 21:50:34.931: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-d1ba24e0-dc84-4eb8-b31e-4f797467bdcd container client-container: <nil>
STEP: delete the pod
Jul 12 21:50:34.965: INFO: Waiting for pod downwardapi-volume-d1ba24e0-dc84-4eb8-b31e-4f797467bdcd to disappear
Jul 12 21:50:34.969: INFO: Pod downwardapi-volume-d1ba24e0-dc84-4eb8-b31e-4f797467bdcd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:50:34.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-927" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":291,"skipped":4721,"failed":0}

------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:50:34.983: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/cronjob.go:63
W0712 21:50:35.055053      20 warnings.go:70] batch/v1beta1 CronJob is deprecated in v1.21+, unavailable in v1.25+; use batch/v1 CronJob
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:56:01.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8751" for this suite.

• [SLOW TEST:326.372 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":339,"completed":292,"skipped":4721,"failed":0}
SS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:56:01.356: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:56:01.559: INFO: created pod
Jul 12 21:56:01.560: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1231" to be "Succeeded or Failed"
Jul 12 21:56:01.563: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.69162ms
Jul 12 21:56:03.572: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012603695s
Jul 12 21:56:05.582: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021790944s
STEP: Saw pod success
Jul 12 21:56:05.582: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jul 12 21:56:35.582: INFO: polling logs
Jul 12 21:56:35.604: INFO: Pod logs: 
2021/07/12 21:56:02 OK: Got token
2021/07/12 21:56:02 validating with in-cluster discovery
2021/07/12 21:56:02 OK: got issuer https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-central1-c/clusters/gke-1-21
2021/07/12 21:56:02 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-central1-c/clusters/gke-1-21", Subject:"system:serviceaccount:svcaccounts-1231:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1626127561, NotBefore:1626126961, IssuedAt:1626126961, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1231", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"00f275ab-c8aa-4fcc-b643-e0772bd692e1"}}}
2021/07/12 21:56:02 failed to validate with in-cluster discovery: Get "https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-central1-c/clusters/gke-1-21/.well-known/openid-configuration": x509: certificate signed by unknown authority
2021/07/12 21:56:02 falling back to validating with external discovery
2021/07/12 21:56:02 OK: got issuer https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-central1-c/clusters/gke-1-21
2021/07/12 21:56:02 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-central1-c/clusters/gke-1-21", Subject:"system:serviceaccount:svcaccounts-1231:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1626127561, NotBefore:1626126961, IssuedAt:1626126961, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1231", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"00f275ab-c8aa-4fcc-b643-e0772bd692e1"}}}
2021/07/12 21:56:03 OK: Constructed OIDC provider for issuer https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-central1-c/clusters/gke-1-21
2021/07/12 21:56:03 OK: Validated signature on JWT
2021/07/12 21:56:03 OK: Got valid claims from token!
2021/07/12 21:56:03 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-central1-c/clusters/gke-1-21", Subject:"system:serviceaccount:svcaccounts-1231:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1626127561, NotBefore:1626126961, IssuedAt:1626126961, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1231", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"00f275ab-c8aa-4fcc-b643-e0772bd692e1"}}}

Jul 12 21:56:35.604: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:56:35.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1231" for this suite.

• [SLOW TEST:34.302 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":339,"completed":293,"skipped":4723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:56:35.659: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:56:35.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-850" for this suite.
STEP: Destroying namespace "nspatchtest-e45d31ce-5761-4e69-9e6b-aa611c12df99-3871" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":339,"completed":294,"skipped":4761,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:56:35.816: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-a0e961b3-6a70-4924-a749-7683613f2dcd in namespace container-probe-49
Jul 12 21:56:37.908: INFO: Started pod liveness-a0e961b3-6a70-4924-a749-7683613f2dcd in namespace container-probe-49
STEP: checking the pod's current state and verifying that restartCount is present
Jul 12 21:56:37.911: INFO: Initial restart count of pod liveness-a0e961b3-6a70-4924-a749-7683613f2dcd is 0
Jul 12 21:56:58.097: INFO: Restart count of pod container-probe-49/liveness-a0e961b3-6a70-4924-a749-7683613f2dcd is now 1 (20.186102747s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:56:58.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-49" for this suite.

• [SLOW TEST:22.348 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":339,"completed":295,"skipped":4771,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:56:58.165: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 12 21:56:58.262: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5777  ec415276-bd19-48fe-805a-6bf168e65e5f 43944 0 2021-07-12 21:56:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-12 21:56:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 21:56:58.263: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5777  ec415276-bd19-48fe-805a-6bf168e65e5f 43945 0 2021-07-12 21:56:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-12 21:56:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 12 21:56:58.280: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5777  ec415276-bd19-48fe-805a-6bf168e65e5f 43946 0 2021-07-12 21:56:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-12 21:56:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 12 21:56:58.280: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5777  ec415276-bd19-48fe-805a-6bf168e65e5f 43947 0 2021-07-12 21:56:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-07-12 21:56:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:56:58.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5777" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":339,"completed":296,"skipped":4791,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:56:58.299: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test override command
Jul 12 21:56:58.367: INFO: Waiting up to 5m0s for pod "client-containers-7fe127d8-ee84-4b1f-a542-a27cd13b92bd" in namespace "containers-7204" to be "Succeeded or Failed"
Jul 12 21:56:58.372: INFO: Pod "client-containers-7fe127d8-ee84-4b1f-a542-a27cd13b92bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.850365ms
Jul 12 21:57:00.380: INFO: Pod "client-containers-7fe127d8-ee84-4b1f-a542-a27cd13b92bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013137863s
STEP: Saw pod success
Jul 12 21:57:00.380: INFO: Pod "client-containers-7fe127d8-ee84-4b1f-a542-a27cd13b92bd" satisfied condition "Succeeded or Failed"
Jul 12 21:57:00.383: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod client-containers-7fe127d8-ee84-4b1f-a542-a27cd13b92bd container agnhost-container: <nil>
STEP: delete the pod
Jul 12 21:57:00.404: INFO: Waiting for pod client-containers-7fe127d8-ee84-4b1f-a542-a27cd13b92bd to disappear
Jul 12 21:57:00.411: INFO: Pod client-containers-7fe127d8-ee84-4b1f-a542-a27cd13b92bd no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:57:00.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7204" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":339,"completed":297,"skipped":4833,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:57:00.423: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:746
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating service in namespace services-5882
STEP: creating service affinity-clusterip-transition in namespace services-5882
STEP: creating replication controller affinity-clusterip-transition in namespace services-5882
I0712 21:57:00.561998      20 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-5882, replica count: 3
I0712 21:57:03.613076      20 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 12 21:57:03.635: INFO: Creating new exec pod
Jul 12 21:57:06.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5882 exec execpod-affinitysk2hn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jul 12 21:57:08.370: INFO: rc: 1
Jul 12 21:57:08.370: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5882 exec execpod-affinitysk2hn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80:
Command stdout:

stderr:
+ nc -v -t -w 2 affinity-clusterip-transition 80
+ echo hostName
nc: connect to affinity-clusterip-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 12 21:57:09.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5882 exec execpod-affinitysk2hn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jul 12 21:57:10.610: INFO: rc: 1
Jul 12 21:57:10.610: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5882 exec execpod-affinitysk2hn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80:
Command stdout:

stderr:
+ echo hostName
+ nc -v -t -w 2 affinity-clusterip-transition 80
nc: connect to affinity-clusterip-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jul 12 21:57:11.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5882 exec execpod-affinitysk2hn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jul 12 21:57:11.582: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jul 12 21:57:11.582: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 21:57:11.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5882 exec execpod-affinitysk2hn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.37.142.1 80'
Jul 12 21:57:11.770: INFO: stderr: "+ nc -v -t -w 2 10.37.142.1 80\n+ echo hostName\nConnection to 10.37.142.1 80 port [tcp/http] succeeded!\n"
Jul 12 21:57:11.770: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 12 21:57:11.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5882 exec execpod-affinitysk2hn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.37.142.1:80/ ; done'
Jul 12 21:57:12.109: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n"
Jul 12 21:57:12.109: INFO: stdout: "\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc"
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:12.109: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5882 exec execpod-affinitysk2hn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.37.142.1:80/ ; done'
Jul 12 21:57:42.407: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n"
Jul 12 21:57:42.407: INFO: stdout: "\naffinity-clusterip-transition-z2l29\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-8c9v8\naffinity-clusterip-transition-z2l29\naffinity-clusterip-transition-z2l29\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-z2l29\naffinity-clusterip-transition-z2l29\naffinity-clusterip-transition-8c9v8\naffinity-clusterip-transition-z2l29\naffinity-clusterip-transition-z2l29\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-8c9v8\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-8c9v8\naffinity-clusterip-transition-z2l29"
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-z2l29
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-8c9v8
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-z2l29
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-z2l29
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-z2l29
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-z2l29
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-8c9v8
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-z2l29
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-z2l29
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-8c9v8
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-8c9v8
Jul 12 21:57:42.407: INFO: Received response from host: affinity-clusterip-transition-z2l29
Jul 12 21:57:42.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=services-5882 exec execpod-affinitysk2hn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.37.142.1:80/ ; done'
Jul 12 21:57:42.723: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.37.142.1:80/\n"
Jul 12 21:57:42.723: INFO: stdout: "\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc\naffinity-clusterip-transition-hsrbc"
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.723: INFO: Received response from host: affinity-clusterip-transition-hsrbc
Jul 12 21:57:42.724: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5882, will wait for the garbage collector to delete the pods
Jul 12 21:57:42.867: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.850804ms
Jul 12 21:57:43.068: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.916258ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:57:51.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5882" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:750

• [SLOW TEST:51.286 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":339,"completed":298,"skipped":4838,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:57:51.709: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1514
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-1
Jul 12 21:57:51.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8068 run e2e-test-httpd-pod --restart=Never --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-1'
Jul 12 21:57:51.873: INFO: stderr: ""
Jul 12 21:57:51.873: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1518
Jul 12 21:57:51.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-8068 delete pods e2e-test-httpd-pod'
Jul 12 21:58:01.579: INFO: stderr: ""
Jul 12 21:58:01.579: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:58:01.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8068" for this suite.

• [SLOW TEST:9.887 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":339,"completed":299,"skipped":4839,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:58:01.597: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name configmap-test-volume-map-96cbeafa-bfbc-4938-834d-9caf4541362b
STEP: Creating a pod to test consume configMaps
Jul 12 21:58:01.671: INFO: Waiting up to 5m0s for pod "pod-configmaps-f495c47f-752f-46af-962d-df729b657322" in namespace "configmap-6003" to be "Succeeded or Failed"
Jul 12 21:58:01.675: INFO: Pod "pod-configmaps-f495c47f-752f-46af-962d-df729b657322": Phase="Pending", Reason="", readiness=false. Elapsed: 4.42001ms
Jul 12 21:58:03.686: INFO: Pod "pod-configmaps-f495c47f-752f-46af-962d-df729b657322": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015104999s
STEP: Saw pod success
Jul 12 21:58:03.686: INFO: Pod "pod-configmaps-f495c47f-752f-46af-962d-df729b657322" satisfied condition "Succeeded or Failed"
Jul 12 21:58:03.694: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-configmaps-f495c47f-752f-46af-962d-df729b657322 container agnhost-container: <nil>
STEP: delete the pod
Jul 12 21:58:03.731: INFO: Waiting for pod pod-configmaps-f495c47f-752f-46af-962d-df729b657322 to disappear
Jul 12 21:58:03.740: INFO: Pod pod-configmaps-f495c47f-752f-46af-962d-df729b657322 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:58:03.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6003" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":300,"skipped":4896,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:58:03.757: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:58:03.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5771" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":339,"completed":301,"skipped":4915,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:58:03.968: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jul 12 21:58:04.057: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:58:06.068: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jul 12 21:58:06.140: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:58:08.154: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jul 12 21:58:08.175: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 12 21:58:08.191: INFO: Pod pod-with-prestop-http-hook still exists
Jul 12 21:58:10.192: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 12 21:58:10.199: INFO: Pod pod-with-prestop-http-hook still exists
Jul 12 21:58:12.192: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 12 21:58:12.203: INFO: Pod pod-with-prestop-http-hook still exists
Jul 12 21:58:14.192: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 12 21:58:14.204: INFO: Pod pod-with-prestop-http-hook still exists
Jul 12 21:58:16.193: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 12 21:58:16.199: INFO: Pod pod-with-prestop-http-hook still exists
Jul 12 21:58:18.192: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 12 21:58:18.197: INFO: Pod pod-with-prestop-http-hook still exists
Jul 12 21:58:20.192: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 12 21:58:20.202: INFO: Pod pod-with-prestop-http-hook still exists
Jul 12 21:58:22.192: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 12 21:58:22.221: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:58:22.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7149" for this suite.

• [SLOW TEST:18.306 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":339,"completed":302,"skipped":4924,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:58:22.275: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test substitution in volume subpath
Jul 12 21:58:22.511: INFO: Waiting up to 5m0s for pod "var-expansion-7596e93c-e091-45b7-a53c-d5542726565a" in namespace "var-expansion-977" to be "Succeeded or Failed"
Jul 12 21:58:22.527: INFO: Pod "var-expansion-7596e93c-e091-45b7-a53c-d5542726565a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.741486ms
Jul 12 21:58:24.535: INFO: Pod "var-expansion-7596e93c-e091-45b7-a53c-d5542726565a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023465889s
STEP: Saw pod success
Jul 12 21:58:24.535: INFO: Pod "var-expansion-7596e93c-e091-45b7-a53c-d5542726565a" satisfied condition "Succeeded or Failed"
Jul 12 21:58:24.538: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod var-expansion-7596e93c-e091-45b7-a53c-d5542726565a container dapi-container: <nil>
STEP: delete the pod
Jul 12 21:58:24.560: INFO: Waiting for pod var-expansion-7596e93c-e091-45b7-a53c-d5542726565a to disappear
Jul 12 21:58:24.564: INFO: Pod var-expansion-7596e93c-e091-45b7-a53c-d5542726565a no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:58:24.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-977" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":339,"completed":303,"skipped":4939,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:58:24.577: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting the auto-created API token
Jul 12 21:58:25.210: INFO: created pod pod-service-account-defaultsa
Jul 12 21:58:25.210: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 12 21:58:25.220: INFO: created pod pod-service-account-mountsa
Jul 12 21:58:25.220: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 12 21:58:25.255: INFO: created pod pod-service-account-nomountsa
Jul 12 21:58:25.255: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 12 21:58:25.292: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 12 21:58:25.293: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 12 21:58:25.314: INFO: created pod pod-service-account-mountsa-mountspec
Jul 12 21:58:25.314: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 12 21:58:25.348: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 12 21:58:25.348: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 12 21:58:25.387: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 12 21:58:25.387: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 12 21:58:25.406: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 12 21:58:25.406: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 12 21:58:25.435: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 12 21:58:25.436: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:58:25.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8446" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":339,"completed":304,"skipped":4942,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:58:25.470: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 21:58:26.657: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 12 21:58:28.680: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761723907, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761723907, loc:(*time.Location)(0x9dc0820)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63761723907, loc:(*time.Location)(0x9dc0820)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63761723906, loc:(*time.Location)(0x9dc0820)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78988fc6cd\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 21:58:31.716: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:58:32.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-891" for this suite.
STEP: Destroying namespace "webhook-891-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.861 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":339,"completed":305,"skipped":4949,"failed":0}
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:58:32.340: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:58:32.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7423" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":339,"completed":306,"skipped":4950,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:58:32.487: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 21:58:32.578: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d8d241a-d737-4089-8a02-fadac64074fa" in namespace "projected-2688" to be "Succeeded or Failed"
Jul 12 21:58:32.584: INFO: Pod "downwardapi-volume-6d8d241a-d737-4089-8a02-fadac64074fa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.176212ms
Jul 12 21:58:34.613: INFO: Pod "downwardapi-volume-6d8d241a-d737-4089-8a02-fadac64074fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035035841s
Jul 12 21:58:36.621: INFO: Pod "downwardapi-volume-6d8d241a-d737-4089-8a02-fadac64074fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043041419s
STEP: Saw pod success
Jul 12 21:58:36.621: INFO: Pod "downwardapi-volume-6d8d241a-d737-4089-8a02-fadac64074fa" satisfied condition "Succeeded or Failed"
Jul 12 21:58:36.624: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-6d8d241a-d737-4089-8a02-fadac64074fa container client-container: <nil>
STEP: delete the pod
Jul 12 21:58:36.639: INFO: Waiting for pod downwardapi-volume-6d8d241a-d737-4089-8a02-fadac64074fa to disappear
Jul 12 21:58:36.647: INFO: Pod downwardapi-volume-6d8d241a-d737-4089-8a02-fadac64074fa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:58:36.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2688" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":307,"skipped":4952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:58:36.681: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap configmap-1530/configmap-test-1827c30c-6bc6-4c4b-8c80-b46bd186abe3
STEP: Creating a pod to test consume configMaps
Jul 12 21:58:36.746: INFO: Waiting up to 5m0s for pod "pod-configmaps-1833c3b7-4dc7-4dfa-bb00-896dd41de595" in namespace "configmap-1530" to be "Succeeded or Failed"
Jul 12 21:58:36.751: INFO: Pod "pod-configmaps-1833c3b7-4dc7-4dfa-bb00-896dd41de595": Phase="Pending", Reason="", readiness=false. Elapsed: 4.891291ms
Jul 12 21:58:38.778: INFO: Pod "pod-configmaps-1833c3b7-4dc7-4dfa-bb00-896dd41de595": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032408836s
STEP: Saw pod success
Jul 12 21:58:38.778: INFO: Pod "pod-configmaps-1833c3b7-4dc7-4dfa-bb00-896dd41de595" satisfied condition "Succeeded or Failed"
Jul 12 21:58:38.791: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-configmaps-1833c3b7-4dc7-4dfa-bb00-896dd41de595 container env-test: <nil>
STEP: delete the pod
Jul 12 21:58:38.881: INFO: Waiting for pod pod-configmaps-1833c3b7-4dc7-4dfa-bb00-896dd41de595 to disappear
Jul 12 21:58:38.920: INFO: Pod pod-configmaps-1833c3b7-4dc7-4dfa-bb00-896dd41de595 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:58:38.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1530" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":339,"completed":308,"skipped":4983,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:58:38.968: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
Jul 12 21:58:39.078: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:58:41.088: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:58:43.102: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the pod with lifecycle hook
Jul 12 21:58:43.155: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:58:45.176: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 12 21:58:47.168: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 12 21:58:47.233: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 21:58:47.271: INFO: Pod pod-with-poststart-http-hook still exists
Jul 12 21:58:49.271: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 21:58:49.289: INFO: Pod pod-with-poststart-http-hook still exists
Jul 12 21:58:51.272: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 21:58:51.280: INFO: Pod pod-with-poststart-http-hook still exists
Jul 12 21:58:53.271: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 12 21:58:53.289: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:58:53.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-224" for this suite.

• [SLOW TEST:14.339 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":339,"completed":309,"skipped":4999,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:58:53.309: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 12 21:58:53.458: INFO: Number of nodes with available pods: 0
Jul 12 21:58:53.458: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 21:58:54.476: INFO: Number of nodes with available pods: 0
Jul 12 21:58:54.476: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 21:58:55.471: INFO: Number of nodes with available pods: 1
Jul 12 21:58:55.471: INFO: Node gke-gke-1-21-default-pool-f67064dc-1pbx is running more than one daemon pod
Jul 12 21:58:56.517: INFO: Number of nodes with available pods: 3
Jul 12 21:58:56.518: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 12 21:58:56.608: INFO: Number of nodes with available pods: 3
Jul 12 21:58:56.608: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7756, will wait for the garbage collector to delete the pods
Jul 12 21:58:57.711: INFO: Deleting DaemonSet.extensions daemon-set took: 16.099734ms
Jul 12 21:58:57.912: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.429003ms
Jul 12 21:59:11.626: INFO: Number of nodes with available pods: 0
Jul 12 21:59:11.627: INFO: Number of running nodes: 0, number of available pods: 0
Jul 12 21:59:11.630: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45072"},"items":null}

Jul 12 21:59:11.633: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45072"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:59:11.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7756" for this suite.

• [SLOW TEST:18.363 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":339,"completed":310,"skipped":5014,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:59:11.673: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 21:59:12.904: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 21:59:15.938: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:59:26.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5382" for this suite.
STEP: Destroying namespace "webhook-5382-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.268 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":339,"completed":311,"skipped":5033,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:59:26.940: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 21:59:27.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-2543 create -f -'
Jul 12 21:59:27.543: INFO: stderr: ""
Jul 12 21:59:27.543: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jul 12 21:59:27.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-2543 create -f -'
Jul 12 21:59:27.900: INFO: stderr: ""
Jul 12 21:59:27.900: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 12 21:59:28.910: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 21:59:28.910: INFO: Found 0 / 1
Jul 12 21:59:29.905: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 21:59:29.905: INFO: Found 1 / 1
Jul 12 21:59:29.905: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 12 21:59:29.909: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 12 21:59:29.909: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 12 21:59:29.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-2543 describe pod agnhost-primary-xqfnv'
Jul 12 21:59:30.032: INFO: stderr: ""
Jul 12 21:59:30.032: INFO: stdout: "Name:         agnhost-primary-xqfnv\nNamespace:    kubectl-2543\nPriority:     0\nNode:         gke-gke-1-21-default-pool-f67064dc-3tj7/10.128.0.27\nStart Time:   Mon, 12 Jul 2021 21:59:27 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.40.1.89\nIPs:\n  IP:           10.40.1.89\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://5a11dbd74e6fe59c91b2b1eeb9609bbb4dc276dc9d07251c86cc7151c853a434\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 12 Jul 2021 21:59:28 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4rmkw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-4rmkw:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-2543/agnhost-primary-xqfnv to gke-gke-1-21-default-pool-f67064dc-3tj7\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.32\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jul 12 21:59:30.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-2543 describe rc agnhost-primary'
Jul 12 21:59:30.156: INFO: stderr: ""
Jul 12 21:59:30.156: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2543\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.32\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-xqfnv\n"
Jul 12 21:59:30.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-2543 describe service agnhost-primary'
Jul 12 21:59:30.268: INFO: stderr: ""
Jul 12 21:59:30.268: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2543\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       cloud.google.com/neg: {\"ingress\":true}\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.37.135.213\nIPs:               10.37.135.213\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.40.1.89:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 12 21:59:30.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-2543 describe node gke-gke-1-21-default-pool-f67064dc-1pbx'
Jul 12 21:59:30.422: INFO: stderr: ""
Jul 12 21:59:30.422: INFO: stdout: "Name:               gke-gke-1-21-default-pool-f67064dc-1pbx\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=e2-medium\n                    beta.kubernetes.io/os=linux\n                    cloud.google.com/gke-boot-disk=pd-standard\n                    cloud.google.com/gke-container-runtime=containerd\n                    cloud.google.com/gke-nodepool=default-pool\n                    cloud.google.com/gke-os-distribution=cos\n                    cloud.google.com/machine-family=e2\n                    failure-domain.beta.kubernetes.io/region=us-central1\n                    failure-domain.beta.kubernetes.io/zone=us-central1-c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=gke-gke-1-21-default-pool-f67064dc-1pbx\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=e2-medium\n                    topology.gke.io/zone=us-central1-c\n                    topology.kubernetes.io/region=us-central1\n                    topology.kubernetes.io/zone=us-central1-c\nAnnotations:        container.googleapis.com/instance_id: 3708221732302167849\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"pd.csi.storage.gke.io\":\"projects/liggitt-gke-dev/zones/us-central1-c/instances/gke-gke-1-21-default-pool-f67064dc-1pbx\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.gke.io/last-applied-node-labels:\n                      cloud.google.com/gke-boot-disk=pd-standard,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-nodepool=default-pool,cl...\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 12 Jul 2021 20:13:30 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  gke-gke-1-21-default-pool-f67064dc-1pbx\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 12 Jul 2021 21:59:28 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  FrequentDockerRestart         False   Mon, 12 Jul 2021 21:58:46 +0000   Mon, 12 Jul 2021 20:13:33 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Mon, 12 Jul 2021 21:58:46 +0000   Mon, 12 Jul 2021 20:13:33 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  KernelDeadlock                False   Mon, 12 Jul 2021 21:58:46 +0000   Mon, 12 Jul 2021 20:13:33 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Mon, 12 Jul 2021 21:58:46 +0000   Mon, 12 Jul 2021 20:13:33 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  CorruptDockerOverlay2         False   Mon, 12 Jul 2021 21:58:46 +0000   Mon, 12 Jul 2021 20:13:33 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentUnregisterNetDevice   False   Mon, 12 Jul 2021 21:58:46 +0000   Mon, 12 Jul 2021 20:13:33 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  FrequentKubeletRestart        False   Mon, 12 Jul 2021 21:58:46 +0000   Mon, 12 Jul 2021 20:13:33 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  NetworkUnavailable            False   Mon, 12 Jul 2021 20:13:31 +0000   Mon, 12 Jul 2021 20:13:31 +0000   RouteCreated                    NodeController create implicit route\n  MemoryPressure                False   Mon, 12 Jul 2021 21:57:56 +0000   Mon, 12 Jul 2021 20:11:27 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Mon, 12 Jul 2021 21:57:56 +0000   Mon, 12 Jul 2021 20:11:27 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Mon, 12 Jul 2021 21:57:56 +0000   Mon, 12 Jul 2021 20:11:27 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Mon, 12 Jul 2021 21:57:56 +0000   Mon, 12 Jul 2021 20:13:31 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.128.0.26\n  InternalDNS:  gke-gke-1-21-default-pool-f67064dc-1pbx.c.liggitt-gke-dev.internal\n  Hostname:     gke-gke-1-21-default-pool-f67064dc-1pbx.c.liggitt-gke-dev.internal\nCapacity:\n  attachable-volumes-gce-pd:  15\n  cpu:                        2\n  ephemeral-storage:          98868448Ki\n  hugepages-1Gi:              0\n  hugepages-2Mi:              0\n  memory:                     4034068Ki\n  pods:                       110\nAllocatable:\n  attachable-volumes-gce-pd:  15\n  cpu:                        940m\n  ephemeral-storage:          47093746742\n  hugepages-1Gi:              0\n  hugepages-2Mi:              0\n  memory:                     2888212Ki\n  pods:                       110\nSystem Info:\n  Machine ID:                 de7bdee06cfcaa25959b83b9df8e664a\n  System UUID:                de7bdee0-6cfc-aa25-959b-83b9df8e664a\n  Boot ID:                    236643b1-104a-4d35-87c0-dd44da16cdf9\n  Kernel Version:             5.4.104+\n  OS Image:                   Container-Optimized OS from Google\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.3\n  Kubelet Version:            v1.21.1-gke.2200\n  Kube-Proxy Version:         v1.21.1-gke.2200\nPodCIDR:                      10.40.0.0/24\nPodCIDRs:                     10.40.0.0/24\nProviderID:                   gce://liggitt-gke-dev/us-central1-c/gke-gke-1-21-default-pool-f67064dc-1pbx\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                         ------------  ----------  ---------------  -------------  ---\n  kube-system                 fluentbit-gke-glzv5                                          100m (10%)    0 (0%)      200Mi (7%)       500Mi (17%)    105m\n  kube-system                 gke-metrics-agent-kcr97                                      3m (0%)       0 (0%)      50Mi (1%)        50Mi (1%)      105m\n  kube-system                 kube-dns-684cf9fd88-7bwb8                                    260m (27%)    0 (0%)      110Mi (3%)       210Mi (7%)     105m\n  kube-system                 kube-proxy-gke-gke-1-21-default-pool-f67064dc-1pbx           100m (10%)    0 (0%)      0 (0%)           0 (0%)         105m\n  kube-system                 pdcsi-node-7nnjh                                             0 (0%)        0 (0%)      20Mi (0%)        100Mi (3%)     105m\n  kube-system                 stackdriver-metadata-agent-cluster-level-5dbdc4b7f5-mz7cm    98m (10%)     48m (5%)    202Mi (7%)       202Mi (7%)     79m\n  sonobuoy                    sonobuoy-e2e-job-a64b1c75ccf14259                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         89m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-bf15683044874f68-xh7xd      0 (0%)        0 (0%)      0 (0%)           0 (0%)         89m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests     Limits\n  --------                   --------     ------\n  cpu                        561m (59%)   48m (5%)\n  memory                     582Mi (20%)  1062Mi (37%)\n  ephemeral-storage          0 (0%)       0 (0%)\n  hugepages-1Gi              0 (0%)       0 (0%)\n  hugepages-2Mi              0 (0%)       0 (0%)\n  attachable-volumes-gce-pd  0            0\nEvents:                      <none>\n"
Jul 12 21:59:30.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-2543 describe namespace kubectl-2543'
Jul 12 21:59:30.529: INFO: stderr: ""
Jul 12 21:59:30.529: INFO: stdout: "Name:         kubectl-2543\nLabels:       e2e-framework=kubectl\n              e2e-run=9c130501-7225-4214-917d-490219467f1b\n              kubernetes.io/metadata.name=kubectl-2543\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 21:59:30.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2543" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":339,"completed":312,"skipped":5035,"failed":0}

------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 21:59:30.539: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-3555
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3555
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3555
Jul 12 21:59:30.858: INFO: Found 0 stateful pods, waiting for 1
Jul 12 21:59:40.864: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 12 21:59:40.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-3555 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 21:59:41.067: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 21:59:41.068: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 21:59:41.068: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 21:59:41.073: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 12 21:59:51.082: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 21:59:51.082: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 21:59:51.132: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999636s
Jul 12 21:59:52.139: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.988213061s
Jul 12 21:59:53.164: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.981280661s
Jul 12 21:59:54.172: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.955738113s
Jul 12 21:59:55.180: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.948167847s
Jul 12 21:59:56.191: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.941017938s
Jul 12 21:59:57.197: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.929932675s
Jul 12 21:59:58.205: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.923488733s
Jul 12 21:59:59.215: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.915429834s
Jul 12 22:00:00.222: INFO: Verifying statefulset ss doesn't scale past 1 for another 905.723576ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3555
Jul 12 22:00:01.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-3555 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:00:01.412: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 22:00:01.412: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 22:00:01.412: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 22:00:01.416: INFO: Found 1 stateful pods, waiting for 3
Jul 12 22:00:11.448: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 22:00:11.448: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 22:00:11.448: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 12 22:00:11.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-3555 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 22:00:11.670: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 22:00:11.670: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 22:00:11.670: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 22:00:11.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-3555 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 22:00:11.855: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 22:00:11.855: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 22:00:11.855: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 22:00:11.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-3555 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 22:00:12.042: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 22:00:12.043: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 22:00:12.043: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 22:00:12.043: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 22:00:12.047: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul 12 22:00:22.060: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 22:00:22.060: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 22:00:22.060: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 22:00:22.137: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999663s
Jul 12 22:00:23.146: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.983100584s
Jul 12 22:00:24.154: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974258305s
Jul 12 22:00:25.161: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.9659177s
Jul 12 22:00:26.170: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.958210504s
Jul 12 22:00:27.180: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.948977723s
Jul 12 22:00:28.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.939970323s
Jul 12 22:00:29.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.847457559s
Jul 12 22:00:30.286: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.841443894s
Jul 12 22:00:31.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 833.804755ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3555
Jul 12 22:00:32.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-3555 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:00:32.517: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 22:00:32.517: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 22:00:32.517: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 22:00:32.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-3555 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:00:32.717: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 22:00:32.717: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 22:00:32.717: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 22:00:32.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-3555 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:00:32.910: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 22:00:32.910: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 22:00:32.910: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 22:00:32.910: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul 12 22:01:02.938: INFO: Deleting all statefulset in ns statefulset-3555
Jul 12 22:01:02.945: INFO: Scaling statefulset ss to 0
Jul 12 22:01:02.969: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 22:01:02.976: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:01:03.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3555" for this suite.

• [SLOW TEST:92.526 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":339,"completed":313,"skipped":5035,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:01:03.069: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 12 22:01:03.539: INFO: Pod name wrapped-volume-race-50d4cafe-113a-4d18-8a5a-36a0f004e7f5: Found 0 pods out of 5
Jul 12 22:01:08.731: INFO: Pod name wrapped-volume-race-50d4cafe-113a-4d18-8a5a-36a0f004e7f5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-50d4cafe-113a-4d18-8a5a-36a0f004e7f5 in namespace emptydir-wrapper-9420, will wait for the garbage collector to delete the pods
Jul 12 22:01:20.876: INFO: Deleting ReplicationController wrapped-volume-race-50d4cafe-113a-4d18-8a5a-36a0f004e7f5 took: 11.544521ms
Jul 12 22:01:20.977: INFO: Terminating ReplicationController wrapped-volume-race-50d4cafe-113a-4d18-8a5a-36a0f004e7f5 pods took: 100.516638ms
STEP: Creating RC which spawns configmap-volume pods
Jul 12 22:01:28.354: INFO: Pod name wrapped-volume-race-6bf81512-3bfb-403e-9d6e-8b611cfaaee6: Found 0 pods out of 5
Jul 12 22:01:33.375: INFO: Pod name wrapped-volume-race-6bf81512-3bfb-403e-9d6e-8b611cfaaee6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6bf81512-3bfb-403e-9d6e-8b611cfaaee6 in namespace emptydir-wrapper-9420, will wait for the garbage collector to delete the pods
Jul 12 22:01:55.468: INFO: Deleting ReplicationController wrapped-volume-race-6bf81512-3bfb-403e-9d6e-8b611cfaaee6 took: 6.436637ms
Jul 12 22:01:55.769: INFO: Terminating ReplicationController wrapped-volume-race-6bf81512-3bfb-403e-9d6e-8b611cfaaee6 pods took: 300.505756ms
STEP: Creating RC which spawns configmap-volume pods
Jul 12 22:02:01.724: INFO: Pod name wrapped-volume-race-ed2fb8d5-5d00-4070-98d8-458913ec13f5: Found 0 pods out of 5
Jul 12 22:02:06.742: INFO: Pod name wrapped-volume-race-ed2fb8d5-5d00-4070-98d8-458913ec13f5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ed2fb8d5-5d00-4070-98d8-458913ec13f5 in namespace emptydir-wrapper-9420, will wait for the garbage collector to delete the pods
Jul 12 22:02:16.957: INFO: Deleting ReplicationController wrapped-volume-race-ed2fb8d5-5d00-4070-98d8-458913ec13f5 took: 12.947476ms
Jul 12 22:02:17.360: INFO: Terminating ReplicationController wrapped-volume-race-ed2fb8d5-5d00-4070-98d8-458913ec13f5 pods took: 402.928141ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:02:28.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9420" for this suite.

• [SLOW TEST:85.644 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":339,"completed":314,"skipped":5050,"failed":0}
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:02:28.716: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
Jul 12 22:02:28.784: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:02:32.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8179" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":339,"completed":315,"skipped":5057,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:02:32.133: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 22:02:32.222: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2ae7c84-19d7-48ba-b1a8-c688a4c64bbc" in namespace "downward-api-7438" to be "Succeeded or Failed"
Jul 12 22:02:32.226: INFO: Pod "downwardapi-volume-b2ae7c84-19d7-48ba-b1a8-c688a4c64bbc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.599055ms
Jul 12 22:02:34.241: INFO: Pod "downwardapi-volume-b2ae7c84-19d7-48ba-b1a8-c688a4c64bbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018634583s
STEP: Saw pod success
Jul 12 22:02:34.241: INFO: Pod "downwardapi-volume-b2ae7c84-19d7-48ba-b1a8-c688a4c64bbc" satisfied condition "Succeeded or Failed"
Jul 12 22:02:34.248: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-b2ae7c84-19d7-48ba-b1a8-c688a4c64bbc container client-container: <nil>
STEP: delete the pod
Jul 12 22:02:34.333: INFO: Waiting for pod downwardapi-volume-b2ae7c84-19d7-48ba-b1a8-c688a4c64bbc to disappear
Jul 12 22:02:34.343: INFO: Pod downwardapi-volume-b2ae7c84-19d7-48ba-b1a8-c688a4c64bbc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:02:34.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7438" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":316,"skipped":5077,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:02:34.371: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 12 22:02:36.577: INFO: &Pod{ObjectMeta:{send-events-b1bedcee-66a1-467f-bc7f-8fc5aa6297c6  events-4812  d3205191-3dbc-4ec1-8c4f-b420c0295dc6 46602 0 2021-07-12 22:02:34 +0000 UTC <nil> <nil> map[name:foo time:544694417] map[] [] []  [{e2e.test Update v1 2021-07-12 22:02:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-07-12 22:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.40.1.100\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-klz8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-klz8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-21-default-pool-f67064dc-3tj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 22:02:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 22:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 22:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-07-12 22:02:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.128.0.27,PodIP:10.40.1.100,StartTime:2021-07-12 22:02:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-07-12 22:02:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.32,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1,ContainerID:containerd://ca0e5a5085377999979daf07eccdb0fa0e184d7983058ef8be4978f6e360c8c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.40.1.100,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jul 12 22:02:38.599: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 12 22:02:40.615: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:02:40.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4812" for this suite.

• [SLOW TEST:6.288 seconds]
[sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":339,"completed":317,"skipped":5106,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:02:40.660: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating configMap with name projected-configmap-test-volume-e7e9eb08-c106-49b6-9dd5-4403a21d20a1
STEP: Creating a pod to test consume configMaps
Jul 12 22:02:40.831: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8ec954ca-7c74-4a8f-9520-38fde823fc5e" in namespace "projected-3502" to be "Succeeded or Failed"
Jul 12 22:02:40.842: INFO: Pod "pod-projected-configmaps-8ec954ca-7c74-4a8f-9520-38fde823fc5e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.336337ms
Jul 12 22:02:42.856: INFO: Pod "pod-projected-configmaps-8ec954ca-7c74-4a8f-9520-38fde823fc5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023985993s
Jul 12 22:02:44.887: INFO: Pod "pod-projected-configmaps-8ec954ca-7c74-4a8f-9520-38fde823fc5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055485953s
STEP: Saw pod success
Jul 12 22:02:44.887: INFO: Pod "pod-projected-configmaps-8ec954ca-7c74-4a8f-9520-38fde823fc5e" satisfied condition "Succeeded or Failed"
Jul 12 22:02:44.894: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-projected-configmaps-8ec954ca-7c74-4a8f-9520-38fde823fc5e container agnhost-container: <nil>
STEP: delete the pod
Jul 12 22:02:44.938: INFO: Waiting for pod pod-projected-configmaps-8ec954ca-7c74-4a8f-9520-38fde823fc5e to disappear
Jul 12 22:02:44.945: INFO: Pod pod-projected-configmaps-8ec954ca-7c74-4a8f-9520-38fde823fc5e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:02:44.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3502" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":339,"completed":318,"skipped":5125,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:02:44.973: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jul 12 22:02:49.624: INFO: Successfully updated pod "adopt-release-h8z98"
STEP: Checking that the Job readopts the Pod
Jul 12 22:02:49.624: INFO: Waiting up to 15m0s for pod "adopt-release-h8z98" in namespace "job-4833" to be "adopted"
Jul 12 22:02:49.630: INFO: Pod "adopt-release-h8z98": Phase="Running", Reason="", readiness=true. Elapsed: 6.053009ms
Jul 12 22:02:51.640: INFO: Pod "adopt-release-h8z98": Phase="Running", Reason="", readiness=true. Elapsed: 2.015606654s
Jul 12 22:02:51.640: INFO: Pod "adopt-release-h8z98" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jul 12 22:02:52.181: INFO: Successfully updated pod "adopt-release-h8z98"
STEP: Checking that the Job releases the Pod
Jul 12 22:02:52.181: INFO: Waiting up to 15m0s for pod "adopt-release-h8z98" in namespace "job-4833" to be "released"
Jul 12 22:02:52.220: INFO: Pod "adopt-release-h8z98": Phase="Running", Reason="", readiness=true. Elapsed: 38.696776ms
Jul 12 22:02:52.220: INFO: Pod "adopt-release-h8z98" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:02:52.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4833" for this suite.

• [SLOW TEST:7.297 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":339,"completed":319,"skipped":5127,"failed":0}
SS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:02:52.270: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 12 22:02:52.447: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 12 22:02:52.458: INFO: starting watch
STEP: patching
STEP: updating
Jul 12 22:02:52.507: INFO: waiting for watch events with expected annotations
Jul 12 22:02:52.507: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:02:52.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9138" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":339,"completed":320,"skipped":5129,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:02:52.576: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 22:02:52.691: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 12 22:02:57.713: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jul 12 22:02:57.737: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jul 12 22:02:57.807: INFO: observed ReplicaSet test-rs in namespace replicaset-7198 with ReadyReplicas 1, AvailableReplicas 1
Jul 12 22:02:57.874: INFO: observed ReplicaSet test-rs in namespace replicaset-7198 with ReadyReplicas 1, AvailableReplicas 1
Jul 12 22:02:57.954: INFO: observed ReplicaSet test-rs in namespace replicaset-7198 with ReadyReplicas 1, AvailableReplicas 1
Jul 12 22:02:57.977: INFO: observed ReplicaSet test-rs in namespace replicaset-7198 with ReadyReplicas 1, AvailableReplicas 1
Jul 12 22:02:59.688: INFO: observed ReplicaSet test-rs in namespace replicaset-7198 with ReadyReplicas 2, AvailableReplicas 2
Jul 12 22:02:59.837: INFO: observed Replicaset test-rs in namespace replicaset-7198 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:02:59.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7198" for this suite.

• [SLOW TEST:7.279 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":339,"completed":321,"skipped":5148,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:02:59.866: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 22:03:00.008: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:03:00.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6490" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":339,"completed":322,"skipped":5157,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:03:00.785: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1308
STEP: creating the pod
Jul 12 22:03:00.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-4842 create -f -'
Jul 12 22:03:01.230: INFO: stderr: ""
Jul 12 22:03:01.230: INFO: stdout: "pod/pause created\n"
Jul 12 22:03:01.230: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 12 22:03:01.231: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4842" to be "running and ready"
Jul 12 22:03:01.241: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.917257ms
Jul 12 22:03:03.251: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.020351s
Jul 12 22:03:03.251: INFO: Pod "pause" satisfied condition "running and ready"
Jul 12 22:03:03.251: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 12 22:03:03.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-4842 label pods pause testing-label=testing-label-value'
Jul 12 22:03:03.355: INFO: stderr: ""
Jul 12 22:03:03.355: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 12 22:03:03.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-4842 get pod pause -L testing-label'
Jul 12 22:03:03.537: INFO: stderr: ""
Jul 12 22:03:03.537: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 12 22:03:03.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-4842 label pods pause testing-label-'
Jul 12 22:03:03.642: INFO: stderr: ""
Jul 12 22:03:03.642: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 12 22:03:03.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-4842 get pod pause -L testing-label'
Jul 12 22:03:03.756: INFO: stderr: ""
Jul 12 22:03:03.756: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: using delete to clean up resources
Jul 12 22:03:03.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-4842 delete --grace-period=0 --force -f -'
Jul 12 22:03:03.899: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 22:03:03.899: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 12 22:03:03.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-4842 get rc,svc -l name=pause --no-headers'
Jul 12 22:03:04.018: INFO: stderr: "No resources found in kubectl-4842 namespace.\n"
Jul 12 22:03:04.018: INFO: stdout: ""
Jul 12 22:03:04.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-4842 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 12 22:03:04.128: INFO: stderr: ""
Jul 12 22:03:04.128: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:03:04.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4842" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":339,"completed":323,"skipped":5170,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:03:04.142: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward API volume plugin
Jul 12 22:03:04.238: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f082411c-102f-4985-85e2-3a13f9d25e29" in namespace "downward-api-9160" to be "Succeeded or Failed"
Jul 12 22:03:04.244: INFO: Pod "downwardapi-volume-f082411c-102f-4985-85e2-3a13f9d25e29": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903135ms
Jul 12 22:03:06.249: INFO: Pod "downwardapi-volume-f082411c-102f-4985-85e2-3a13f9d25e29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010512845s
STEP: Saw pod success
Jul 12 22:03:06.249: INFO: Pod "downwardapi-volume-f082411c-102f-4985-85e2-3a13f9d25e29" satisfied condition "Succeeded or Failed"
Jul 12 22:03:06.252: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downwardapi-volume-f082411c-102f-4985-85e2-3a13f9d25e29 container client-container: <nil>
STEP: delete the pod
Jul 12 22:03:06.271: INFO: Waiting for pod downwardapi-volume-f082411c-102f-4985-85e2-3a13f9d25e29 to disappear
Jul 12 22:03:06.276: INFO: Pod downwardapi-volume-f082411c-102f-4985-85e2-3a13f9d25e29 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:03:06.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9160" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":339,"completed":324,"skipped":5172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:03:06.286: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jul 12 22:03:06.335: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jul 12 22:03:06.536: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 12 22:03:06.536: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jul 12 22:03:06.554: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 12 22:03:06.555: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jul 12 22:03:06.566: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jul 12 22:03:06.566: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jul 12 22:03:13.795: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:03:13.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5536" for this suite.

• [SLOW TEST:7.550 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":339,"completed":325,"skipped":5198,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:03:13.839: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating a pod to test downward api env vars
Jul 12 22:03:13.919: INFO: Waiting up to 5m0s for pod "downward-api-3190ea54-d230-4fb6-80ab-c5e44dc914f7" in namespace "downward-api-7329" to be "Succeeded or Failed"
Jul 12 22:03:13.924: INFO: Pod "downward-api-3190ea54-d230-4fb6-80ab-c5e44dc914f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.43425ms
Jul 12 22:03:15.935: INFO: Pod "downward-api-3190ea54-d230-4fb6-80ab-c5e44dc914f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016061511s
STEP: Saw pod success
Jul 12 22:03:15.939: INFO: Pod "downward-api-3190ea54-d230-4fb6-80ab-c5e44dc914f7" satisfied condition "Succeeded or Failed"
Jul 12 22:03:15.942: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod downward-api-3190ea54-d230-4fb6-80ab-c5e44dc914f7 container dapi-container: <nil>
STEP: delete the pod
Jul 12 22:03:15.977: INFO: Waiting for pod downward-api-3190ea54-d230-4fb6-80ab-c5e44dc914f7 to disappear
Jul 12 22:03:15.982: INFO: Pod downward-api-3190ea54-d230-4fb6-80ab-c5e44dc914f7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:03:15.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7329" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":339,"completed":326,"skipped":5207,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:03:16.001: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 22:03:17.092: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 22:03:20.148: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
Jul 12 22:03:20.212: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7485-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:03:25.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6787" for this suite.
STEP: Destroying namespace "webhook-6787-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.754 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":339,"completed":327,"skipped":5209,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:03:25.755: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jul 12 22:03:25.966: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 12 22:03:30.985: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:03:31.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2716" for this suite.

• [SLOW TEST:5.630 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":339,"completed":328,"skipped":5213,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:03:31.388: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod pod-subpath-test-secret-mcfr
STEP: Creating a pod to test atomic-volume-subpath
Jul 12 22:03:31.786: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-mcfr" in namespace "subpath-9786" to be "Succeeded or Failed"
Jul 12 22:03:31.809: INFO: Pod "pod-subpath-test-secret-mcfr": Phase="Pending", Reason="", readiness=false. Elapsed: 22.103794ms
Jul 12 22:03:33.840: INFO: Pod "pod-subpath-test-secret-mcfr": Phase="Running", Reason="", readiness=true. Elapsed: 2.053823509s
Jul 12 22:03:35.850: INFO: Pod "pod-subpath-test-secret-mcfr": Phase="Running", Reason="", readiness=true. Elapsed: 4.063304786s
Jul 12 22:03:37.863: INFO: Pod "pod-subpath-test-secret-mcfr": Phase="Running", Reason="", readiness=true. Elapsed: 6.076740713s
Jul 12 22:03:39.875: INFO: Pod "pod-subpath-test-secret-mcfr": Phase="Running", Reason="", readiness=true. Elapsed: 8.088658339s
Jul 12 22:03:41.896: INFO: Pod "pod-subpath-test-secret-mcfr": Phase="Running", Reason="", readiness=true. Elapsed: 10.109218074s
Jul 12 22:03:43.925: INFO: Pod "pod-subpath-test-secret-mcfr": Phase="Running", Reason="", readiness=true. Elapsed: 12.138387943s
Jul 12 22:03:45.935: INFO: Pod "pod-subpath-test-secret-mcfr": Phase="Running", Reason="", readiness=true. Elapsed: 14.148698479s
Jul 12 22:03:47.960: INFO: Pod "pod-subpath-test-secret-mcfr": Phase="Running", Reason="", readiness=true. Elapsed: 16.173122796s
Jul 12 22:03:49.967: INFO: Pod "pod-subpath-test-secret-mcfr": Phase="Running", Reason="", readiness=true. Elapsed: 18.180927157s
Jul 12 22:03:51.974: INFO: Pod "pod-subpath-test-secret-mcfr": Phase="Running", Reason="", readiness=true. Elapsed: 20.187417046s
Jul 12 22:03:53.983: INFO: Pod "pod-subpath-test-secret-mcfr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.196855978s
STEP: Saw pod success
Jul 12 22:03:53.983: INFO: Pod "pod-subpath-test-secret-mcfr" satisfied condition "Succeeded or Failed"
Jul 12 22:03:53.986: INFO: Trying to get logs from node gke-gke-1-21-default-pool-f67064dc-3tj7 pod pod-subpath-test-secret-mcfr container test-container-subpath-secret-mcfr: <nil>
STEP: delete the pod
Jul 12 22:03:54.008: INFO: Waiting for pod pod-subpath-test-secret-mcfr to disappear
Jul 12 22:03:54.011: INFO: Pod pod-subpath-test-secret-mcfr no longer exists
STEP: Deleting pod pod-subpath-test-secret-mcfr
Jul 12 22:03:54.011: INFO: Deleting pod "pod-subpath-test-secret-mcfr" in namespace "subpath-9786"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:03:54.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9786" for this suite.

• [SLOW TEST:22.639 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":339,"completed":329,"skipped":5223,"failed":0}
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:03:54.027: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating the pod
Jul 12 22:03:54.164: INFO: The status of Pod labelsupdate80112341-c337-48b2-83ca-f213802b2db9 is Pending, waiting for it to be Running (with Ready = true)
Jul 12 22:03:56.171: INFO: The status of Pod labelsupdate80112341-c337-48b2-83ca-f213802b2db9 is Running (Ready = true)
Jul 12 22:03:56.703: INFO: Successfully updated pod "labelsupdate80112341-c337-48b2-83ca-f213802b2db9"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:04:00.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5914" for this suite.

• [SLOW TEST:6.757 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":339,"completed":330,"skipped":5223,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:04:00.789: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-4324
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4324
STEP: Creating statefulset with conflicting port in namespace statefulset-4324
STEP: Waiting until pod test-pod will start running in namespace statefulset-4324
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4324
Jul 12 22:04:04.907: INFO: Observed stateful pod in namespace: statefulset-4324, name: ss-0, uid: 9244184c-a760-4431-bbda-aa8e1206665f, status phase: Pending. Waiting for statefulset controller to delete.
Jul 12 22:04:05.282: INFO: Observed stateful pod in namespace: statefulset-4324, name: ss-0, uid: 9244184c-a760-4431-bbda-aa8e1206665f, status phase: Failed. Waiting for statefulset controller to delete.
Jul 12 22:04:05.294: INFO: Observed stateful pod in namespace: statefulset-4324, name: ss-0, uid: 9244184c-a760-4431-bbda-aa8e1206665f, status phase: Failed. Waiting for statefulset controller to delete.
Jul 12 22:04:05.299: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4324
STEP: Removing pod with conflicting port in namespace statefulset-4324
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4324 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul 12 22:04:09.374: INFO: Deleting all statefulset in ns statefulset-4324
Jul 12 22:04:09.379: INFO: Scaling statefulset ss to 0
Jul 12 22:04:19.447: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 22:04:19.467: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:04:19.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4324" for this suite.

• [SLOW TEST:18.778 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":339,"completed":331,"skipped":5224,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:04:19.579: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:90
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:105
STEP: Creating service test in namespace statefulset-8224
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating stateful set ss in namespace statefulset-8224
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8224
Jul 12 22:04:19.688: INFO: Found 0 stateful pods, waiting for 1
Jul 12 22:04:29.700: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 12 22:04:29.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 22:04:30.029: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 22:04:30.029: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 22:04:30.029: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 22:04:30.033: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 12 22:04:40.056: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 22:04:40.056: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 22:04:40.071: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 12 22:04:40.071: INFO: ss-0  gke-gke-1-21-default-pool-f67064dc-3tj7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  }]
Jul 12 22:04:40.071: INFO: 
Jul 12 22:04:40.071: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 12 22:04:41.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995695983s
Jul 12 22:04:42.080: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99088647s
Jul 12 22:04:43.112: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98628926s
Jul 12 22:04:44.120: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.954327255s
Jul 12 22:04:45.129: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.946300846s
Jul 12 22:04:46.135: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.937960057s
Jul 12 22:04:47.142: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.931699707s
Jul 12 22:04:48.153: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.92444577s
Jul 12 22:04:49.167: INFO: Verifying statefulset ss doesn't scale past 3 for another 913.202849ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8224
Jul 12 22:04:50.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:04:50.354: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 12 22:04:50.354: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 22:04:50.354: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 22:04:50.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:04:50.532: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 12 22:04:50.532: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 22:04:50.532: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 22:04:50.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:04:50.736: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 12 22:04:50.736: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 12 22:04:50.736: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 12 22:04:50.743: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jul 12 22:05:00.771: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 22:05:00.771: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 12 22:05:00.771: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 12 22:05:00.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 22:05:00.953: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 22:05:00.953: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 22:05:00.953: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 22:05:00.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 22:05:01.225: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 22:05:01.226: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 22:05:01.226: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 22:05:01.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 12 22:05:01.446: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 12 22:05:01.446: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 12 22:05:01.446: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 12 22:05:01.446: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 22:05:01.450: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul 12 22:05:11.474: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 22:05:11.474: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 22:05:11.474: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 12 22:05:11.507: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 12 22:05:11.507: INFO: ss-0  gke-gke-1-21-default-pool-f67064dc-3tj7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  }]
Jul 12 22:05:11.507: INFO: ss-1  gke-gke-1-21-default-pool-f67064dc-3tj7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:11.507: INFO: ss-2  gke-gke-1-21-default-pool-f67064dc-3tj7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:11.507: INFO: 
Jul 12 22:05:11.507: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 22:05:12.516: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 12 22:05:12.530: INFO: ss-0  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  }]
Jul 12 22:05:12.530: INFO: ss-1  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:12.531: INFO: ss-2  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:12.531: INFO: 
Jul 12 22:05:12.532: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 22:05:13.540: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 12 22:05:13.540: INFO: ss-0  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  }]
Jul 12 22:05:13.540: INFO: ss-1  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:13.540: INFO: ss-2  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:13.540: INFO: 
Jul 12 22:05:13.540: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 22:05:14.550: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 12 22:05:14.550: INFO: ss-0  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  }]
Jul 12 22:05:14.550: INFO: ss-1  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:14.550: INFO: ss-2  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:14.550: INFO: 
Jul 12 22:05:14.550: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 22:05:15.557: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 12 22:05:15.557: INFO: ss-0  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  }]
Jul 12 22:05:15.557: INFO: ss-1  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:15.558: INFO: ss-2  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:15.558: INFO: 
Jul 12 22:05:15.558: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 22:05:16.564: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 12 22:05:16.564: INFO: ss-0  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  }]
Jul 12 22:05:16.564: INFO: ss-1  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:16.564: INFO: ss-2  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:16.565: INFO: 
Jul 12 22:05:16.565: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 22:05:17.576: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 12 22:05:17.576: INFO: ss-0  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  }]
Jul 12 22:05:17.576: INFO: ss-1  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:17.577: INFO: ss-2  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:17.577: INFO: 
Jul 12 22:05:17.577: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 22:05:18.584: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 12 22:05:18.585: INFO: ss-0  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  }]
Jul 12 22:05:18.585: INFO: ss-1  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:18.585: INFO: ss-2  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:18.585: INFO: 
Jul 12 22:05:18.586: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 22:05:19.593: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 12 22:05:19.593: INFO: ss-0  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  }]
Jul 12 22:05:19.593: INFO: ss-1  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:19.593: INFO: ss-2  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:19.593: INFO: 
Jul 12 22:05:19.593: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 12 22:05:20.600: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jul 12 22:05:20.600: INFO: ss-0  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:19 +0000 UTC  }]
Jul 12 22:05:20.600: INFO: ss-1  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:20.600: INFO: ss-2  gke-gke-1-21-default-pool-f67064dc-3tj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:05:02 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-07-12 22:04:40 +0000 UTC  }]
Jul 12 22:05:20.600: INFO: 
Jul 12 22:05:20.600: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8224
Jul 12 22:05:21.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:05:21.726: INFO: rc: 1
Jul 12 22:05:21.726: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:05:31.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:05:31.827: INFO: rc: 1
Jul 12 22:05:31.827: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:05:41.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:05:41.930: INFO: rc: 1
Jul 12 22:05:41.930: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:05:51.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:05:52.030: INFO: rc: 1
Jul 12 22:05:52.030: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:06:02.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:06:02.150: INFO: rc: 1
Jul 12 22:06:02.150: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:06:12.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:06:12.242: INFO: rc: 1
Jul 12 22:06:12.242: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:06:22.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:06:22.364: INFO: rc: 1
Jul 12 22:06:22.365: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:06:32.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:06:32.479: INFO: rc: 1
Jul 12 22:06:32.479: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:06:42.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:06:42.585: INFO: rc: 1
Jul 12 22:06:42.585: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:06:52.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:06:52.683: INFO: rc: 1
Jul 12 22:06:52.683: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:07:02.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:07:02.794: INFO: rc: 1
Jul 12 22:07:02.794: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:07:12.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:07:13.526: INFO: rc: 1
Jul 12 22:07:13.526: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:07:23.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:07:23.617: INFO: rc: 1
Jul 12 22:07:23.617: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:07:33.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:07:33.732: INFO: rc: 1
Jul 12 22:07:33.732: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:07:43.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:07:43.862: INFO: rc: 1
Jul 12 22:07:43.862: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:07:53.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:07:53.964: INFO: rc: 1
Jul 12 22:07:53.964: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:08:03.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:08:04.069: INFO: rc: 1
Jul 12 22:08:04.069: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:08:14.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:08:14.164: INFO: rc: 1
Jul 12 22:08:14.164: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:08:24.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:08:24.263: INFO: rc: 1
Jul 12 22:08:24.263: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:08:34.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:08:34.374: INFO: rc: 1
Jul 12 22:08:34.374: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:08:44.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:08:44.540: INFO: rc: 1
Jul 12 22:08:44.540: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:08:54.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:08:54.718: INFO: rc: 1
Jul 12 22:08:54.718: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:09:04.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:09:04.825: INFO: rc: 1
Jul 12 22:09:04.825: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:09:14.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:09:14.927: INFO: rc: 1
Jul 12 22:09:14.927: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:09:24.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:09:25.041: INFO: rc: 1
Jul 12 22:09:25.041: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:09:35.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:09:35.155: INFO: rc: 1
Jul 12 22:09:35.155: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:09:45.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:09:45.261: INFO: rc: 1
Jul 12 22:09:45.261: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:09:55.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:09:55.369: INFO: rc: 1
Jul 12 22:09:55.369: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:10:05.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:10:05.476: INFO: rc: 1
Jul 12 22:10:05.476: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:10:15.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:10:15.577: INFO: rc: 1
Jul 12 22:10:15.577: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 12 22:10:25.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=statefulset-8224 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 12 22:10:25.681: INFO: rc: 1
Jul 12 22:10:25.681: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Jul 12 22:10:25.681: INFO: Scaling statefulset ss to 0
Jul 12 22:10:25.691: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:116
Jul 12 22:10:25.694: INFO: Deleting all statefulset in ns statefulset-8224
Jul 12 22:10:25.697: INFO: Scaling statefulset ss to 0
Jul 12 22:10:25.709: INFO: Waiting for statefulset status.replicas updated to 0
Jul 12 22:10:25.718: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:10:25.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8224" for this suite.

• [SLOW TEST:366.195 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:95
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":339,"completed":332,"skipped":5245,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:10:25.774: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: validating api versions
Jul 12 22:10:25.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-5838 api-versions'
Jul 12 22:10:25.961: INFO: stderr: ""
Jul 12 22:10:25.961: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncloud.google.com/v1\ncloud.google.com/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nnetworking.gke.io/v1\nnetworking.gke.io/v1beta1\nnetworking.gke.io/v1beta2\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\nnodemanagement.gke.io/v1alpha1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:10:25.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5838" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":339,"completed":333,"skipped":5256,"failed":0}
S
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:10:25.974: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Creating pod liveness-5ed3c976-5ca7-41ac-9978-7c11c981ee1d in namespace container-probe-6053
Jul 12 22:10:28.050: INFO: Started pod liveness-5ed3c976-5ca7-41ac-9978-7c11c981ee1d in namespace container-probe-6053
STEP: checking the pod's current state and verifying that restartCount is present
Jul 12 22:10:28.055: INFO: Initial restart count of pod liveness-5ed3c976-5ca7-41ac-9978-7c11c981ee1d is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:14:29.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6053" for this suite.

• [SLOW TEST:243.668 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":339,"completed":334,"skipped":5257,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:14:29.646: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0712 22:14:39.858265      20 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0712 22:14:39.858422      20 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0712 22:14:39.858455      20 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jul 12 22:14:39.858: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:14:39.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1114" for this suite.

• [SLOW TEST:10.250 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":339,"completed":335,"skipped":5263,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:14:39.897: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 22:14:40.659: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 22:14:43.724: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:14:44.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3366" for this suite.
STEP: Destroying namespace "webhook-3366-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":339,"completed":336,"skipped":5274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:14:44.479: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 12 22:14:44.699: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 12 22:14:44.716: INFO: Waiting for terminating namespaces to be deleted...
Jul 12 22:14:44.721: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-21-default-pool-f67064dc-1pbx before test
Jul 12 22:14:44.738: INFO: fluentbit-gke-glzv5 from kube-system started at 2021-07-12 20:13:31 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.738: INFO: 	Container fluentbit ready: true, restart count 0
Jul 12 22:14:44.738: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jul 12 22:14:44.738: INFO: gke-metrics-agent-kcr97 from kube-system started at 2021-07-12 20:13:31 +0000 UTC (1 container statuses recorded)
Jul 12 22:14:44.738: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jul 12 22:14:44.738: INFO: kube-dns-684cf9fd88-7bwb8 from kube-system started at 2021-07-12 20:13:38 +0000 UTC (4 container statuses recorded)
Jul 12 22:14:44.738: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 12 22:14:44.738: INFO: 	Container kubedns ready: true, restart count 0
Jul 12 22:14:44.738: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jul 12 22:14:44.738: INFO: 	Container sidecar ready: true, restart count 0
Jul 12 22:14:44.738: INFO: kube-proxy-gke-gke-1-21-default-pool-f67064dc-1pbx from kube-system started at 2021-07-12 20:11:28 +0000 UTC (1 container statuses recorded)
Jul 12 22:14:44.739: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 12 22:14:44.739: INFO: pdcsi-node-7nnjh from kube-system started at 2021-07-12 20:13:31 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.739: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jul 12 22:14:44.739: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jul 12 22:14:44.739: INFO: stackdriver-metadata-agent-cluster-level-5dbdc4b7f5-mz7cm from kube-system started at 2021-07-12 20:39:43 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.739: INFO: 	Container metadata-agent ready: true, restart count 0
Jul 12 22:14:44.739: INFO: 	Container metadata-agent-nanny ready: true, restart count 0
Jul 12 22:14:44.739: INFO: sonobuoy-e2e-job-a64b1c75ccf14259 from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.739: INFO: 	Container e2e ready: true, restart count 0
Jul 12 22:14:44.739: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 12 22:14:44.739: INFO: sonobuoy-systemd-logs-daemon-set-bf15683044874f68-xh7xd from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.739: INFO: 	Container sonobuoy-worker ready: false, restart count 13
Jul 12 22:14:44.739: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 12 22:14:44.739: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-21-default-pool-f67064dc-3tj7 before test
Jul 12 22:14:44.765: INFO: fluentbit-gke-4spgg from kube-system started at 2021-07-12 20:13:34 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.765: INFO: 	Container fluentbit ready: true, restart count 0
Jul 12 22:14:44.766: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jul 12 22:14:44.766: INFO: gke-metrics-agent-wc245 from kube-system started at 2021-07-12 20:13:34 +0000 UTC (1 container statuses recorded)
Jul 12 22:14:44.766: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jul 12 22:14:44.766: INFO: kube-proxy-gke-gke-1-21-default-pool-f67064dc-3tj7 from kube-system started at 2021-07-12 20:11:31 +0000 UTC (1 container statuses recorded)
Jul 12 22:14:44.766: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 12 22:14:44.766: INFO: pdcsi-node-n2vm7 from kube-system started at 2021-07-12 20:13:34 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.766: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jul 12 22:14:44.766: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jul 12 22:14:44.767: INFO: sonobuoy-systemd-logs-daemon-set-bf15683044874f68-hqmt5 from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.767: INFO: 	Container sonobuoy-worker ready: false, restart count 13
Jul 12 22:14:44.767: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 12 22:14:44.767: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-21-default-pool-f67064dc-g6xf before test
Jul 12 22:14:44.798: INFO: event-exporter-gke-67986489c8-2lvzn from kube-system started at 2021-07-12 20:13:35 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.798: INFO: 	Container event-exporter ready: true, restart count 0
Jul 12 22:14:44.798: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Jul 12 22:14:44.798: INFO: fluentbit-gke-26jmp from kube-system started at 2021-07-12 20:13:31 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.798: INFO: 	Container fluentbit ready: true, restart count 0
Jul 12 22:14:44.798: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jul 12 22:14:44.798: INFO: gke-metrics-agent-59rd5 from kube-system started at 2021-07-12 20:13:30 +0000 UTC (1 container statuses recorded)
Jul 12 22:14:44.798: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jul 12 22:14:44.798: INFO: kube-dns-684cf9fd88-6t2mb from kube-system started at 2021-07-12 20:39:43 +0000 UTC (4 container statuses recorded)
Jul 12 22:14:44.798: INFO: 	Container dnsmasq ready: true, restart count 0
Jul 12 22:14:44.798: INFO: 	Container kubedns ready: true, restart count 0
Jul 12 22:14:44.798: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jul 12 22:14:44.798: INFO: 	Container sidecar ready: true, restart count 0
Jul 12 22:14:44.798: INFO: kube-dns-autoscaler-844c9d9448-562v6 from kube-system started at 2021-07-12 20:13:35 +0000 UTC (1 container statuses recorded)
Jul 12 22:14:44.799: INFO: 	Container autoscaler ready: true, restart count 0
Jul 12 22:14:44.799: INFO: kube-proxy-gke-gke-1-21-default-pool-f67064dc-g6xf from kube-system started at 2021-07-12 20:11:28 +0000 UTC (1 container statuses recorded)
Jul 12 22:14:44.799: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 12 22:14:44.799: INFO: l7-default-backend-75b656946c-77f5m from kube-system started at 2021-07-12 20:13:39 +0000 UTC (1 container statuses recorded)
Jul 12 22:14:44.799: INFO: 	Container default-http-backend ready: true, restart count 0
Jul 12 22:14:44.799: INFO: metrics-server-v0.4.4-57979b57f-kwlsr from kube-system started at 2021-07-12 20:14:03 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.800: INFO: 	Container metrics-server ready: true, restart count 0
Jul 12 22:14:44.800: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jul 12 22:14:44.800: INFO: pdcsi-node-5vxw2 from kube-system started at 2021-07-12 20:13:30 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.800: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jul 12 22:14:44.800: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jul 12 22:14:44.800: INFO: sonobuoy from sonobuoy started at 2021-07-12 20:29:56 +0000 UTC (1 container statuses recorded)
Jul 12 22:14:44.800: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 12 22:14:44.800: INFO: sonobuoy-systemd-logs-daemon-set-bf15683044874f68-fgclf from sonobuoy started at 2021-07-12 20:29:59 +0000 UTC (2 container statuses recorded)
Jul 12 22:14:44.800: INFO: 	Container sonobuoy-worker ready: false, restart count 13
Jul 12 22:14:44.800: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: verifying the node has the label node gke-gke-1-21-default-pool-f67064dc-1pbx
STEP: verifying the node has the label node gke-gke-1-21-default-pool-f67064dc-3tj7
STEP: verifying the node has the label node gke-gke-1-21-default-pool-f67064dc-g6xf
Jul 12 22:14:45.044: INFO: Pod event-exporter-gke-67986489c8-2lvzn requesting resource cpu=0m on Node gke-gke-1-21-default-pool-f67064dc-g6xf
Jul 12 22:14:45.044: INFO: Pod fluentbit-gke-26jmp requesting resource cpu=100m on Node gke-gke-1-21-default-pool-f67064dc-g6xf
Jul 12 22:14:45.044: INFO: Pod fluentbit-gke-4spgg requesting resource cpu=100m on Node gke-gke-1-21-default-pool-f67064dc-3tj7
Jul 12 22:14:45.044: INFO: Pod fluentbit-gke-glzv5 requesting resource cpu=100m on Node gke-gke-1-21-default-pool-f67064dc-1pbx
Jul 12 22:14:45.044: INFO: Pod gke-metrics-agent-59rd5 requesting resource cpu=3m on Node gke-gke-1-21-default-pool-f67064dc-g6xf
Jul 12 22:14:45.044: INFO: Pod gke-metrics-agent-kcr97 requesting resource cpu=3m on Node gke-gke-1-21-default-pool-f67064dc-1pbx
Jul 12 22:14:45.044: INFO: Pod gke-metrics-agent-wc245 requesting resource cpu=3m on Node gke-gke-1-21-default-pool-f67064dc-3tj7
Jul 12 22:14:45.044: INFO: Pod kube-dns-684cf9fd88-6t2mb requesting resource cpu=260m on Node gke-gke-1-21-default-pool-f67064dc-g6xf
Jul 12 22:14:45.044: INFO: Pod kube-dns-684cf9fd88-7bwb8 requesting resource cpu=260m on Node gke-gke-1-21-default-pool-f67064dc-1pbx
Jul 12 22:14:45.044: INFO: Pod kube-dns-autoscaler-844c9d9448-562v6 requesting resource cpu=20m on Node gke-gke-1-21-default-pool-f67064dc-g6xf
Jul 12 22:14:45.044: INFO: Pod kube-proxy-gke-gke-1-21-default-pool-f67064dc-1pbx requesting resource cpu=100m on Node gke-gke-1-21-default-pool-f67064dc-1pbx
Jul 12 22:14:45.044: INFO: Pod kube-proxy-gke-gke-1-21-default-pool-f67064dc-3tj7 requesting resource cpu=100m on Node gke-gke-1-21-default-pool-f67064dc-3tj7
Jul 12 22:14:45.044: INFO: Pod kube-proxy-gke-gke-1-21-default-pool-f67064dc-g6xf requesting resource cpu=100m on Node gke-gke-1-21-default-pool-f67064dc-g6xf
Jul 12 22:14:45.044: INFO: Pod l7-default-backend-75b656946c-77f5m requesting resource cpu=10m on Node gke-gke-1-21-default-pool-f67064dc-g6xf
Jul 12 22:14:45.044: INFO: Pod metrics-server-v0.4.4-57979b57f-kwlsr requesting resource cpu=48m on Node gke-gke-1-21-default-pool-f67064dc-g6xf
Jul 12 22:14:45.044: INFO: Pod pdcsi-node-5vxw2 requesting resource cpu=0m on Node gke-gke-1-21-default-pool-f67064dc-g6xf
Jul 12 22:14:45.044: INFO: Pod pdcsi-node-7nnjh requesting resource cpu=0m on Node gke-gke-1-21-default-pool-f67064dc-1pbx
Jul 12 22:14:45.044: INFO: Pod pdcsi-node-n2vm7 requesting resource cpu=0m on Node gke-gke-1-21-default-pool-f67064dc-3tj7
Jul 12 22:14:45.044: INFO: Pod stackdriver-metadata-agent-cluster-level-5dbdc4b7f5-mz7cm requesting resource cpu=98m on Node gke-gke-1-21-default-pool-f67064dc-1pbx
Jul 12 22:14:45.044: INFO: Pod sonobuoy requesting resource cpu=0m on Node gke-gke-1-21-default-pool-f67064dc-g6xf
Jul 12 22:14:45.044: INFO: Pod sonobuoy-e2e-job-a64b1c75ccf14259 requesting resource cpu=0m on Node gke-gke-1-21-default-pool-f67064dc-1pbx
Jul 12 22:14:45.044: INFO: Pod sonobuoy-systemd-logs-daemon-set-bf15683044874f68-fgclf requesting resource cpu=0m on Node gke-gke-1-21-default-pool-f67064dc-g6xf
Jul 12 22:14:45.044: INFO: Pod sonobuoy-systemd-logs-daemon-set-bf15683044874f68-hqmt5 requesting resource cpu=0m on Node gke-gke-1-21-default-pool-f67064dc-3tj7
Jul 12 22:14:45.044: INFO: Pod sonobuoy-systemd-logs-daemon-set-bf15683044874f68-xh7xd requesting resource cpu=0m on Node gke-gke-1-21-default-pool-f67064dc-1pbx
STEP: Starting Pods to consume most of the cluster CPU.
Jul 12 22:14:45.044: INFO: Creating a pod which consumes cpu=265m on Node gke-gke-1-21-default-pool-f67064dc-1pbx
Jul 12 22:14:45.061: INFO: Creating a pod which consumes cpu=515m on Node gke-gke-1-21-default-pool-f67064dc-3tj7
Jul 12 22:14:45.089: INFO: Creating a pod which consumes cpu=279m on Node gke-gke-1-21-default-pool-f67064dc-g6xf
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-68380f8c-5c89-4624-b5b5-0ee5a6aebf6f.16912ae0b8b1edc7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4808/filler-pod-68380f8c-5c89-4624-b5b5-0ee5a6aebf6f to gke-gke-1-21-default-pool-f67064dc-3tj7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-68380f8c-5c89-4624-b5b5-0ee5a6aebf6f.16912ae0fec928a5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-68380f8c-5c89-4624-b5b5-0ee5a6aebf6f.16912ae105215c90], Reason = [Created], Message = [Created container filler-pod-68380f8c-5c89-4624-b5b5-0ee5a6aebf6f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-68380f8c-5c89-4624-b5b5-0ee5a6aebf6f.16912ae10b2ef1c3], Reason = [Started], Message = [Started container filler-pod-68380f8c-5c89-4624-b5b5-0ee5a6aebf6f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c6616f83-c350-4ec6-a765-78cfb6a8ef1b.16912ae0b6fe27bf], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4808/filler-pod-c6616f83-c350-4ec6-a765-78cfb6a8ef1b to gke-gke-1-21-default-pool-f67064dc-1pbx]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c6616f83-c350-4ec6-a765-78cfb6a8ef1b.16912ae11ffa7a3a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c6616f83-c350-4ec6-a765-78cfb6a8ef1b.16912ae1259a7487], Reason = [Created], Message = [Created container filler-pod-c6616f83-c350-4ec6-a765-78cfb6a8ef1b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c6616f83-c350-4ec6-a765-78cfb6a8ef1b.16912ae12b12ad4c], Reason = [Started], Message = [Started container filler-pod-c6616f83-c350-4ec6-a765-78cfb6a8ef1b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f7256f0f-d9c4-4f67-9ff1-e22339f9a4c4.16912ae0ba33b2a6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4808/filler-pod-f7256f0f-d9c4-4f67-9ff1-e22339f9a4c4 to gke-gke-1-21-default-pool-f67064dc-g6xf]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f7256f0f-d9c4-4f67-9ff1-e22339f9a4c4.16912ae11fe20265], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.4.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f7256f0f-d9c4-4f67-9ff1-e22339f9a4c4.16912ae12315e4df], Reason = [Created], Message = [Created container filler-pod-f7256f0f-d9c4-4f67-9ff1-e22339f9a4c4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f7256f0f-d9c4-4f67-9ff1-e22339f9a4c4.16912ae127e401ed], Reason = [Started], Message = [Started container filler-pod-f7256f0f-d9c4-4f67-9ff1-e22339f9a4c4]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16912ae1a97ea9a2], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node gke-gke-1-21-default-pool-f67064dc-1pbx
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node gke-gke-1-21-default-pool-f67064dc-3tj7
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node gke-gke-1-21-default-pool-f67064dc-g6xf
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:14:50.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4808" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.779 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":339,"completed":337,"skipped":5336,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:14:50.260: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 12 22:14:50.849: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 12 22:14:53.913: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jul 12 22:14:53.965: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:14:54.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3697" for this suite.
STEP: Destroying namespace "webhook-3697-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":339,"completed":338,"skipped":5398,"failed":0}

------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 12 22:14:54.103: INFO: >>> kubeConfig: /tmp/kubeconfig-966859563
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
STEP: creating a replication controller
Jul 12 22:14:54.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 create -f -'
Jul 12 22:14:54.813: INFO: stderr: ""
Jul 12 22:14:54.813: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 12 22:14:54.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 12 22:14:54.936: INFO: stderr: ""
Jul 12 22:14:54.936: INFO: stdout: "update-demo-nautilus-2xddp update-demo-nautilus-nzt8k "
Jul 12 22:14:54.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-2xddp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 12 22:14:55.025: INFO: stderr: ""
Jul 12 22:14:55.025: INFO: stdout: ""
Jul 12 22:14:55.025: INFO: update-demo-nautilus-2xddp is created but not running
Jul 12 22:15:00.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 12 22:15:00.147: INFO: stderr: ""
Jul 12 22:15:00.147: INFO: stdout: "update-demo-nautilus-2xddp update-demo-nautilus-nzt8k "
Jul 12 22:15:00.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-2xddp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 12 22:15:00.245: INFO: stderr: ""
Jul 12 22:15:00.245: INFO: stdout: "true"
Jul 12 22:15:00.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-2xddp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 12 22:15:00.332: INFO: stderr: ""
Jul 12 22:15:00.332: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 12 22:15:00.332: INFO: validating pod update-demo-nautilus-2xddp
Jul 12 22:15:00.420: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 22:15:00.420: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 22:15:00.420: INFO: update-demo-nautilus-2xddp is verified up and running
Jul 12 22:15:00.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-nzt8k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 12 22:15:00.517: INFO: stderr: ""
Jul 12 22:15:00.517: INFO: stdout: "true"
Jul 12 22:15:00.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-nzt8k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 12 22:15:00.614: INFO: stderr: ""
Jul 12 22:15:00.614: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 12 22:15:00.614: INFO: validating pod update-demo-nautilus-nzt8k
Jul 12 22:15:00.623: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 22:15:00.623: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 22:15:00.623: INFO: update-demo-nautilus-nzt8k is verified up and running
STEP: scaling down the replication controller
Jul 12 22:15:00.627: INFO: scanned /root for discovery docs: <nil>
Jul 12 22:15:00.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jul 12 22:15:01.934: INFO: stderr: ""
Jul 12 22:15:01.934: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 12 22:15:01.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 12 22:15:02.053: INFO: stderr: ""
Jul 12 22:15:02.053: INFO: stdout: "update-demo-nautilus-2xddp update-demo-nautilus-nzt8k "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 12 22:15:07.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 12 22:15:07.157: INFO: stderr: ""
Jul 12 22:15:07.157: INFO: stdout: "update-demo-nautilus-2xddp "
Jul 12 22:15:07.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-2xddp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 12 22:15:07.256: INFO: stderr: ""
Jul 12 22:15:07.256: INFO: stdout: "true"
Jul 12 22:15:07.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-2xddp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 12 22:15:07.347: INFO: stderr: ""
Jul 12 22:15:07.347: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 12 22:15:07.347: INFO: validating pod update-demo-nautilus-2xddp
Jul 12 22:15:07.352: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 22:15:07.352: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 22:15:07.352: INFO: update-demo-nautilus-2xddp is verified up and running
STEP: scaling up the replication controller
Jul 12 22:15:07.356: INFO: scanned /root for discovery docs: <nil>
Jul 12 22:15:07.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jul 12 22:15:08.505: INFO: stderr: ""
Jul 12 22:15:08.505: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 12 22:15:08.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 12 22:15:08.609: INFO: stderr: ""
Jul 12 22:15:08.609: INFO: stdout: "update-demo-nautilus-2xddp update-demo-nautilus-swm8m "
Jul 12 22:15:08.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-2xddp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 12 22:15:08.718: INFO: stderr: ""
Jul 12 22:15:08.718: INFO: stdout: "true"
Jul 12 22:15:08.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-2xddp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 12 22:15:08.810: INFO: stderr: ""
Jul 12 22:15:08.810: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 12 22:15:08.810: INFO: validating pod update-demo-nautilus-2xddp
Jul 12 22:15:08.813: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 22:15:08.813: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 22:15:08.813: INFO: update-demo-nautilus-2xddp is verified up and running
Jul 12 22:15:08.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-swm8m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 12 22:15:08.903: INFO: stderr: ""
Jul 12 22:15:08.903: INFO: stdout: ""
Jul 12 22:15:08.903: INFO: update-demo-nautilus-swm8m is created but not running
Jul 12 22:15:13.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 12 22:15:14.000: INFO: stderr: ""
Jul 12 22:15:14.000: INFO: stdout: "update-demo-nautilus-2xddp update-demo-nautilus-swm8m "
Jul 12 22:15:14.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-2xddp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 12 22:15:14.104: INFO: stderr: ""
Jul 12 22:15:14.104: INFO: stdout: "true"
Jul 12 22:15:14.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-2xddp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 12 22:15:14.194: INFO: stderr: ""
Jul 12 22:15:14.194: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 12 22:15:14.194: INFO: validating pod update-demo-nautilus-2xddp
Jul 12 22:15:14.203: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 22:15:14.203: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 22:15:14.203: INFO: update-demo-nautilus-2xddp is verified up and running
Jul 12 22:15:14.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-swm8m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 12 22:15:14.299: INFO: stderr: ""
Jul 12 22:15:14.299: INFO: stdout: "true"
Jul 12 22:15:14.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods update-demo-nautilus-swm8m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 12 22:15:14.393: INFO: stderr: ""
Jul 12 22:15:14.393: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.4"
Jul 12 22:15:14.393: INFO: validating pod update-demo-nautilus-swm8m
Jul 12 22:15:14.402: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 12 22:15:14.402: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 12 22:15:14.402: INFO: update-demo-nautilus-swm8m is verified up and running
STEP: using delete to clean up resources
Jul 12 22:15:14.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 delete --grace-period=0 --force -f -'
Jul 12 22:15:14.517: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 12 22:15:14.517: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 12 22:15:14.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get rc,svc -l name=update-demo --no-headers'
Jul 12 22:15:14.623: INFO: stderr: "No resources found in kubectl-9570 namespace.\n"
Jul 12 22:15:14.623: INFO: stdout: ""
Jul 12 22:15:14.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-966859563 --namespace=kubectl-9570 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 12 22:15:14.734: INFO: stderr: ""
Jul 12 22:15:14.734: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 12 22:15:14.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9570" for this suite.

• [SLOW TEST:20.658 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:291
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:630
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":339,"completed":339,"skipped":5398,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSJul 12 22:15:14.761: INFO: Running AfterSuite actions on all nodes
Jul 12 22:15:14.761: INFO: Running AfterSuite actions on node 1
Jul 12 22:15:14.761: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":339,"completed":339,"skipped":5432,"failed":0}

Ran 339 of 5771 Specs in 6293.452 seconds
SUCCESS! -- 339 Passed | 0 Failed | 0 Pending | 5432 Skipped
PASS

Ginkgo ran 1 suite in 1h44m56.006613029s
Test Suite Passed
